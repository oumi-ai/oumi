model:
  model_name: "HuggingFaceTB/SmolLM2-1.7B-Instruct"
  trust_remote_code: true
  torch_dtype_str: "bfloat16"
  device_map: "auto"

data:
  train:
    datasets:
      # - dataset_name: "HuggingFaceH4/ultrachat_200k"
      #   split: "train_sft"
      #   trust_remote_code: True
      - dataset_name: "allenai/WildChat-1M"
        split: "train"
        trust_remote_code: True
    #     shuffle: True
    #     seed: 42
    # seed: 42

training:
  output_dir: "output/smollm2-17b-wildchat"

  # For a single GPU, the following gives us a batch size of 16
  # If training with multiple GPUs, feel free to reduce gradient_accumulation_steps
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 2  # Reduce this to 1 for 8xA100-80GB GPUs
  
  # ***NOTE***
  # We set it to 10 steps to first verify that it works
  # Comment out the line below to have it train for 1 full epoch (all the data) instead.
  # Note: 1 full epoch will take about 13 minutes on 8xA100-80GB.
  # max_steps: 10

  num_train_epochs: 1
  learning_rate: 2e-5
  warmup_ratio: 0.1
  logging_steps: 10
  save_steps: 0
  max_grad_norm: 10
  max_steps: 100
  
  trainer_type: "TRL_SFT"
  optimizer: "adamw_torch_fused"
  enable_gradient_checkpointing: True
  gradient_checkpointing_kwargs:
    use_reentrant: False
  ddp_find_unused_parameters: False
  dataloader_num_workers: "auto"
  dataloader_prefetch_factor: 32
  empty_device_cache_steps: 1

  enable_wandb: True