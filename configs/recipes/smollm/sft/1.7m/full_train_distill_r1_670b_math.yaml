model:
  model_name: "HuggingFaceTB/SmolLM2-1.7B-Instruct"
  trust_remote_code: true
  torch_dtype_str: "bfloat16"
  device_map: "auto"

data:
  train:
    datasets:
      - dataset_name: "text_sft_jsonl"
        dataset_path: "data/R1/math_10k_R1_outputs.jsonl"
        split: "train"
        shuffle: True
        seed: 42
    seed: 42

training:
  output_dir: "output/smollm2-17b-distill-r1-670b-math"

  # For a single GPU, the following gives us a batch size of 16
  # If training with multiple GPUs, feel free to reduce gradient_accumulation_steps
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 2  # Reduce this to 1 for 8xA100-80GB GPUs
  
  # ***NOTE***
  # We set it to 10 steps to first verify that it works
  # Comment out the line below to have it train for 1 full epoch (all the data) instead.
  # Note: 1 full epoch will take about 13 minutes on 8xA100-80GB.
  # max_steps: 10

  num_train_epochs: 1
  learning_rate: 2e-5
  warmup_ratio: 0.1
  logging_steps: 10
  save_steps: 0
  max_grad_norm: 10

  
  trainer_type: "TRL_SFT"
  optimizer: "adamw_torch_fused"
  enable_gradient_checkpointing: True
  gradient_checkpointing_kwargs:
    use_reentrant: False
  ddp_find_unused_parameters: False
  dataloader_num_workers: "auto"
  dataloader_prefetch_factor: 32
  empty_device_cache_steps: 1

  enable_wandb: True