# ⚠️  COMPLEX: vLLM Inference config for Qwen3-Coder-480B-A35B-Instruct GGUF (LIKELY MULTI-PART)
#
# ⚠️  WARNING: This model REQUIRES MULTI-PART GGUF files due to its extreme size (480B parameters)
# Large models are always sharded into multiple parts that need special handling
#
# Usage (ADVANCED - Multi-part download required):
#   # 1. Download all parts of a quantization level to a directory:
#   mkdir -p models/qwen3_coder_480b_q4km
#   huggingface-cli download unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF \
#     --include "Q4_K_M/*" --local-dir models/qwen3_coder_480b_q4km/
#
#   # 2. vLLM should automatically detect and load all parts
#   # 3. Update model_name below to point to the directory or first file
#
#   # 4. Run inference:
#   oumi infer -i -c configs/recipes/qwen3/inference/coder_480b_a35b_gguf_infer.yaml
#
# Model Info:
#   - Repository: unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF
#   - Size: ~480B parameters (quantized)
#   - Files: Multi-part sharded GGUF (directories: Q2_K, Q3_K_M, Q4_K_M, etc.)
#   - Requirements: Extremely high VRAM (recommend 20+ A100-80GB or equivalent), Linux/Windows with GPU (macOS has Triton issues)
#   - Special: Code generation and programming assistance model
#   - Complexity: EXTREME - Multi-part file handling required
#
# See Also:
#   - Documentation: https://oumi.ai/docs/en/latest/user_guides/infer/infer.html
#   - Config class: oumi.core.configs.InferenceConfig
#   - Config source: https://github.com/oumi-ai/oumi/blob/main/src/oumi/core/configs/inference_config.py
#   - Other inference configs: configs/**/inference/

model:
  # REQUIRES MULTI-PART HANDLING - Update path based on your download approach
  # Option 1: Directory path (if vLLM supports): "models/qwen3_coder_480b_q4km/Q4_K_M/"
  # Option 2: First file: "models/qwen3_coder_480b_q4km/Q4_K_M/Qwen3-Coder-480B-A35B-Instruct-Q4_K_M-00001-of-XXXX.gguf"
  model_name: "models/qwen3_coder_480b_q4km/Q4_K_M/"
  # Original model tokenizer for proper text processing
  tokenizer_name: "Qwen/Qwen3-Coder-480B-A35B-Instruct"
  model_max_length: 8192
  torch_dtype_str: "float16"  # GGUF models require float16
  trust_remote_code: True
  attn_implementation: "sdpa"

generation:
  max_new_tokens: 4096  # Larger for code generation
  temperature: 0.3  # Lower temperature for more precise code
  top_p: 0.9

engine: VLLM

# Optional: vLLM specific parameters
# vllm_args:
#   tensor_parallel_size: 16  # Adjust based on available GPUs
#   gpu_memory_utilization: 0.90
#   max_num_seqs: 4  # Very low for extremely large model

# Available quantization directories (all are multi-part):
# "models/qwen3_coder_480b_q2k/Q2_K/"     # Lower quality, less VRAM (still massive)
# "models/qwen3_coder_480b_q3km/Q3_K_M/"  # Good quality
# "models/qwen3_coder_480b_q4km/Q4_K_M/"  # Balanced (recommended)
# "models/qwen3_coder_480b_q5km/Q5_K_M/"  # Better quality
# "models/qwen3_coder_480b_q6k/Q6_K/"     # High quality
# "models/qwen3_coder_480b_q8/Q8_0/"      # Very high quality
#
# NOTE: This config is UNTESTED due to extreme multi-part complexity and resource requirements
# Recommend using regular HuggingFace model instead for Qwen3-Coder-480B
