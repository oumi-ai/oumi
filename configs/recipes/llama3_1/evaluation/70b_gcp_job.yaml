# Config to eval Llama 3.1 70B Instruct on GCP.
# Example command:
# oumi launch up -c configs/recipes/llama3_1/evaluation/70b_gcp_job.yaml --cluster llama70b-eval
name: llama70b-eval

resources:
  cloud: gcp
  accelerators: "A100:4"
  use_spot: true
  disk_size: 400 # Disk size in GBs

# Upload working directory to remote ~/sky_workdir.
working_dir: .

# Mount local files.
file_mounts:
  ~/.netrc: ~/.netrc  # WandB credentials
  # Mount HF token, which is needed to download locked-down models from HF Hub.
  # This is created on the local machine by running `huggingface-cli login`.
  ~/.cache/huggingface/token: ~/.cache/huggingface/token

envs:
  # NOTE: For SFT, update this to point to your model checkpoint.
  MODEL_CHECKPOINT_DIR: meta-llama/Meta-Llama-3.1-70B-Instruct
  # NOTE: For LoRA, update this to point to your LoRA adapter.
  LORA_ADAPTER_DIR: ""
  WANDB_PROJECT: oumi-eval
  OUMI_RUN_NAME: llama70b.eval

setup: |
  set -e
  pip install uv && uv pip install '.[gpu]' hf_transfer
  # Install model from HF Hub. This tool increases download speed compared to
  # downloading the model during eval.
  HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download meta-llama/Meta-Llama-3.1-70B-Instruct --exclude original/*

run: |
  set -e  # Exit if any command failed.
  source ./configs/examples/misc/sky_init.sh

  if test ${OUMI_NUM_NODES} -ne 1; then
    echo "LM Harness supports max 1 node. Actual: ${OUMI_NUM_NODES} nodes."
    exit 1
  fi

  echo "Starting evaluation for ${MODEL_CHECKPOINT_DIR} ..."
  if test -n "$LORA_ADAPTER_DIR"; then
    echo "Using LoRA adapter: ${LORA_ADAPTER_DIR}"
  fi

  set -x
  python -m oumi.evaluate \
    -c configs/recipes/llama3_1/evaluation/70b_eval.yaml \
    "run_name='${OUMI_RUN_NAME}.${SKYPILOT_TASK_ID}'" \
    "model.model_name=${MODEL_CHECKPOINT_DIR}" \
    "model.adapter_model=${LORA_ADAPTER_DIR}"

  echo "Node ${SKYPILOT_NODE_RANK} is all done!"
