model:
  model_name: "meta-llama/Meta-Llama-3.1-8B-Instruct"
  model_max_length: 32_768
  torch_dtype_str: "bfloat16" # Train in BF16 to save memory
  attn_implementation: "sdpa" # pytorch native flash-attention, helps reduce vram
  load_pretrained_weights: True
  trust_remote_code: True
  enable_liger_kernel: True # Helps reduce required vram

training:
  trainer_type: TRL_SFT
  learning_rate: 2.0e-05
  output_dir: "output/llama8b.longctx.sft"

  compile: True # Enabling compilation helps reduce required vram
  per_device_train_batch_size: 1 # Keeping low to maximize context length
  gradient_accumulation_steps: 1 # Update as needed for your desired effective batch size
  optimizer: "adamw_torch_fused" # Use this for up to 32K context

  logging_steps: 5 # For debugging, change to something more reasonable
  max_steps: 100 # For debugging, change to something more reasonable
  save_steps: 0 # For debugging, change to something more reasonable
  save_final_model: False # For debugging, you probably want this as True
  include_performance_metrics: True

data:
  train:
    datasets:
      - dataset_name: "HuggingFaceFW/fineweb-edu"
        subset: "sample-10BT"
        split: "train"
    target_col: "text"
    experimental_use_async_dataset: True
    stream: True
    pack: True
