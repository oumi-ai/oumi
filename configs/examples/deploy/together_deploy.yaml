# Example deployment configuration for Together.ai
# Usage: oumi deploy up --config configs/examples/deploy/together_deploy.yaml

# Model source - can be:
# - HuggingFace repo: "meta-llama/Llama-2-7b-hf"
# - S3 URL: "s3://my-bucket/models/my-model/"
# - GCS URL: "gs://my-bucket/models/my-model/"
model_source: s3://my-bucket/models/my-finetuned-model/

# Provider: "together" or "fireworks"
provider: together

# Display name for the model in the provider's dashboard
model_name: my-finetuned-model-v1

# Model type: "full" or "adapter"
# Use "adapter" for LoRA adapters
model_type: full

# Base model (required only if model_type is "adapter")
# base_model: meta-llama/Llama-2-7b-hf

# Hardware configuration
hardware:
  # Available accelerators for Together.ai:
  # - nvidia_a100_80gb
  # - nvidia_h100_80gb
  # Run `oumi deploy list-hardware --provider together` to see all options
  accelerator: nvidia_a100_80gb

  # Number of GPUs
  count: 1

# Autoscaling configuration
autoscaling:
  # Minimum number of replicas
  min_replicas: 1

  # Maximum number of replicas
  max_replicas: 2

# Optional: test prompts to run after deployment
test_prompts:
  - "Explain quantum computing in simple terms."
  - "What is the capital of France?"
