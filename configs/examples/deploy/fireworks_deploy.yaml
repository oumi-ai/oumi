# Example deployment configuration for Fireworks.ai
# Usage: oumi deploy up --config configs/examples/deploy/fireworks_deploy.yaml

# Model source - can be:
# - HuggingFace repo: "meta-llama/Llama-2-7b-hf"
# - S3 URL: "s3://my-bucket/models/my-model/"
# - GCS URL: "gs://my-bucket/models/my-model/"
model_source: s3://my-bucket/models/my-finetuned-model/

# Provider: "together" or "fireworks"
provider: fireworks

# Display name for the model in the provider's dashboard
model_name: my-finetuned-model-v1

# Model type: "full" or "adapter"
# Use "adapter" for LoRA adapters
model_type: full

# Base model (required only if model_type is "adapter")
# base_model: meta-llama/Llama-2-7b-hf

# Hardware configuration
hardware:
  # Available accelerators for Fireworks.ai:
  # - nvidia_a100_80gb
  # - nvidia_h100_80gb
  # - nvidia_h200_141gb
  # - amd_mi300x
  # Run `oumi deploy list-hardware --provider fireworks` to see all options
  accelerator: nvidia_h100_80gb

  # Number of GPUs
  count: 2

# Autoscaling configuration
autoscaling:
  # Minimum number of replicas
  min_replicas: 1

  # Maximum number of replicas
  max_replicas: 4

# Optional: test prompts to run after deployment
test_prompts:
  - "Hello, how are you?"
  - "Write a Python function to calculate fibonacci numbers."
