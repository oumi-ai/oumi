name: oumi-bulk-inference

num_nodes: 1
resources:
  cloud: gcp
  accelerators: "A100:2"
  use_spot: true
  disk_size: 2000 # Disk size in GBs

working_dir: .

file_mounts:
  ~/.cache/huggingface/token: ~/.cache/huggingface/token

# NOTE: Update with your GCS buckets
# For more details, see https://oumi.ai/docs/latest/user_guides/launch/launch.html.
storage_mounts:
  /input_gcs_bucket:
    source: gs://<your-input-bucket> # GCS bucket that contains our input data
    store: gcs
  /output_gcs_bucket:
    source: gs://<your-output-bucket> # GCS bucket to store output
    store: gcs

envs:
  TOKENIZERS_PARALLELISM: false
  INFERENCE_CONFIG: configs/examples/bulk_inference/mistral_small_infer.yaml # Inference config file
  HF_DATASETS_TRUST_REMOTE_CODE: 1

setup: |
  set -e
  pip install uv && uv pip install '.[gpu]'

run: |
  set -e  # Exit if any command failed.
  source ./configs/examples/misc/sky_init.sh
  python ./scripts/inference/gcp_inference.py $SKYPILOT_NUM_GPUS_PER_NODE
