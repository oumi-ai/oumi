# Eval config for Llama 3.2 11B Vision Instruct.

# oumi eval --config /home/user/oumi/configs/examples/vision/llama3_2_vision/eval.yaml

# lm_eval --model vllm-vlm --model_args pretrained=llava-hf/llava-1.5-7b-hf,max_images=1,interleave=True --tasks mmmu_val --apply_chat_template
# lm_eval --model vllm-vlm --model_args pretrained=Qwen/Qwen2-VL-2B-Instruct,max_images=1,interleave=True --tasks mmmu_val --apply_chat_template

model:
  model_name: "meta-llama/Llama-3.2-11B-Vision-Instruct"
  model_max_length: 256
  torch_dtype_str: "bfloat16"
  attn_implementation: "sdpa"
  load_pretrained_weights: True
  trust_remote_code: True

generation:
  batch_size: 2

lm_harness_params:
  tasks:
    - "mmmu_val_math"
  num_fewshot: 5
  num_samples: 100

enable_wandb: True
