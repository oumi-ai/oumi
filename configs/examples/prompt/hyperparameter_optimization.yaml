# Prompt optimization with hyperparameter tuning
# This example optimizes prompts, few-shot examples, AND generation hyperparameters

model:
  model_name: "HuggingFaceTB/SmolLM2-135M"
  trust_remote_code: true
  torch_dtype_str: "bfloat16"

generation:
  max_new_tokens: 128
  temperature: 0.7
  top_p: 0.9
  use_sampling: true

optimization:
  optimizer: "mipro"
  num_trials: 100
  max_bootstrapped_demos: 4
  max_labeled_demos: 16
  optimize_instructions: true
  optimize_demos: true
  optimize_hyperparameters: true
  hyperparameter_ranges:
    temperature:
      - 0.1
      - 1.0
    top_p:
      - 0.5
      - 1.0
    max_new_tokens:
      - 50
      - 256
  save_intermediate_results: true
  verbose: true
  seed: 42

# Dataset paths (example datasets provided)
train_dataset_path: "configs/examples/prompt/datasets/qa_train.jsonl"
val_dataset_path: "configs/examples/prompt/datasets/qa_val.jsonl"

# Initial prompt
initial_prompt: "You are an expert assistant."

# Output directory
output_dir: "./prompt_optimization_results/hyperparameter_tuning"

# Evaluation metric
metric: "f1"

# Inference engine
engine: NATIVE

# Limit samples for faster iteration
max_training_samples: 100
max_validation_samples: 50
