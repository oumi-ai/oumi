# llm_compressor quantization config for TinyLlama.
#
# Usage:
#   oumi quantize -c configs/examples/quantization/llm_compressor_quantization_config.yaml
#
# See Also:
#   - Documentation: https://oumi.ai/docs/en/latest/user_guides/quantization.html
#   - Config class: oumi.core.configs.QuantizationConfig
#   - Config source: https://github.com/oumi-ai/oumi/blob/main/src/oumi/core/configs/quantization_config.py
#   - Other quantization configs: configs/examples/quantization/*quantization_config.yaml

# Model configuration
model:
  model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0" # HuggingFace model ID or local path
  tokenizer_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0" # Tokenizer to use
  model_kwargs:
    torch_dtype: "float16" # Load model in half precision for efficiency
    trust_remote_code: true
  adapter_model: null # No LoRA adapter

# Quantization settings
method: "llmc_W4A16_ASYM" # 4-bit asymmetric AWQ quantization
output_path: "tinyllama-1.1b-w4a16" # Production output path

# llm_compressor configuration
llmc_group_size: 128 # Standard grouping size
llmc_targets:
  - "Linear"
llmc_ignore:
  - "lm_head"
llmc_smoothing_strength: 0.8 # SmoothQuant strength (used for W8A8_INT)

# Calibration settings
calibration_dataset: "open_platypus" # Dataset for calibration
calibration_samples: 512 # Number of calibration samples
max_seq_length: 2048 # Maximum sequence length for calibration

# Performance tuning
batch_size: 8 # Smaller batch for memory efficiency
verbose: true # Detailed progress logging
