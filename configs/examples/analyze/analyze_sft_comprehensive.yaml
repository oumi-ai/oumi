# Comprehensive SFT Dataset Analysis Configuration
# Usage: oumi analyze --config configs/examples/analyze/analyze_sft_comprehensive.yaml
#
# This configuration provides thorough analysis for SFT (instruction tuning) datasets,
# including training quality metrics, cost optimization, and data hygiene checks.
#
# Output files:
#   - message_analysis.csv: Per-message metrics including training quality scores
#   - conversation_analysis.csv: Per-conversation aggregated metrics
#   - analysis_summary.json: Statistical summary with health score
#   - analysis_report.html: Interactive HTML report (if generate_report: true)

# Dataset configuration
dataset_name: yahma/alpaca-cleaned
split: train
sample_count: 1000  # Analyze 1000 samples for comprehensive evaluation

# Or use a local file
# dataset_path: data/my_sft_dataset.jsonl

# Output configuration
output_path: ./analysis_output
generate_report: true  # Generate interactive HTML report

# Analyzers - comprehensive set for SFT quality evaluation
analyzers:
  # Length analysis with token counting for cost estimation
  - id: length
    params:
      char_count: false
      word_count: true
      sentence_count: false
      token_count: true  # Uses tiktoken o200k_base (GPT-5) by default

  # Vocabulary diversity metrics
  - id: diversity
    params:
      unique_words_ratio: true
      type_token_ratio: true
      vocabulary_richness: true
      hapax_legomena_ratio: false
      case_sensitive: false

  # Format and structure detection
  - id: format
    params:
      detect_markdown: true
      detect_json: true
      detect_code_blocks: true
      detect_urls: true
      detect_emails: false
      compute_complexity: true

  # Data quality and safety checks
  - id: quality
    params:
      detect_pii: true
      detect_emails: true
      detect_phones: true
      detect_ssn: true
      detect_credit_cards: true
      detect_ip_addresses: false
      detect_api_keys: true
      detect_language: false  # Enable for multilingual datasets
      detect_encoding_issues: true
      detect_special_tokens: true
      detect_repetition: true
      repetition_ngram_size: 3
      repetition_threshold: 0.3
      compute_quality_score: true

  # Training quality analysis for SFT effectiveness
  - id: training_quality
    params:
      compute_instruction_clarity: true
      compute_response_completeness: true
      compute_turn_quality: true
      min_instruction_words: 3
      max_instruction_words: 500
      min_response_words: 5

  # Cost optimization analysis
  - id: cost
    params:
      target_context_windows: [4096, 8192, 16384, 32768]
      compute_packing_efficiency: true
      packing_overhead_tokens: 10

# Optional: Enable embedding-based semantic duplicate detection
# Requires: pip install 'oumi[analyze]'
# Uncomment to enable:
#
#  - id: embedding
#    params:
#      model_name: all-MiniLM-L6-v2
#      detect_duplicates: true
#      duplicate_threshold: 0.95
#      detect_fuzzy_duplicates: true
#      fuzzy_threshold: 0.8
#      fuzzy_ngram_size: 3
#      cluster_samples: false

# Optional: Enable LLM judge for deep quality analysis
# Requires: API key for remote models
# Uncomment to enable:
#
#  - id: llm_judge
#    params:
#      prompt_preset: instruction_quality  # or response_quality, safety, etc.
#      inference_config:
#        model_name: gpt-4o-mini
#        engine: remote
#        temperature: 0.1
#        max_tokens: 256
