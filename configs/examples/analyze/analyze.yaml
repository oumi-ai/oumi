# Dataset Analysis Configuration
# Usage: oumi analyze --config configs/examples/analyze/analyze.yaml
#
# Output files:
#   - message_analysis.csv: Per-message metrics
#   - conversation_analysis.csv: Per-conversation aggregated metrics
#   - analysis_summary.json: Statistical summary (mean, std, min, max, median)
# Local file in Oumi or Alpaca format
dataset_path: data/dataset_examples/oumi_format.jsonl
# Or, use a HuggingFace dataset
# dataset_name: argilla/databricks-dolly-15k-curated-en
# split: train

# sample_count: 1000  # Limit samples (null = all)

# Tokenizer for token_count metric (use same tokenizer as your target model)
# tokenizer_name: openai-community/gpt2
# tokenizer_kwargs: {}

# For multimodal (vision-language) datasets, specify a processor
# Presence of processor_name automatically enables multimodal mode
# processor_name: llava-hf/llava-1.5-7b-hf
# processor_kwargs: {}

# trust_remote_code: false

output_path: ./analysis_output

analyzers:
  - id: length
    params:
      # Tokenizer name - automatically detects tiktoken vs HuggingFace
      tokenizer_name: cl100k_base  # tiktoken encoding (GPT-4)
      # For HuggingFace tokenizers, use model ID:
      # tokenizer_name: meta-llama/Llama-3.1-8B-Instruct
      # trust_remote_code: false
