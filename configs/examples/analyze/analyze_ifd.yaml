# IFD (Instruction-Following Difficulty) Analysis Configuration
# Measures how much instructions help the model predict responses
#
# Usage: oumi analyze --config configs/examples/analyze/analyze_ifd.yaml
#
# IFD Score Interpretation:
#   - IFD > 1: Instruction helps the model (valuable sample)
#   - IFD ≈ 1: Instruction provides minimal guidance
#   - IFD < 1: Instruction may be confusing or misaligned (potentially problematic)
#
# References:
#   - Cherry LLM: https://github.com/tianyi-lab/Cherry_LLM
#   - Superfiltering: https://arxiv.org/html/2402.00530v1
#
# Key Research Findings:
#   - 5-10% of data selected via IFD matches full-data performance
#   - Weak models (Qwen3-0.6b) can effectively filter data for stronger models
#
# Requirements:
#   pip install 'oumi[analyze]' transformers torch
#
# Output files:
#   - message_analysis.csv: Per-sample IFD metrics
#   - conversation_analysis.csv: Per-conversation aggregated metrics
#   - analysis_summary.json: Statistical summary
#   - analysis_report.html: Interactive HTML report

# Dataset configuration
# Alpaca is a common SFT dataset with instruction/input/output format
dataset_name: tatsu-lab/alpaca
split: train
sample_count: 100  # Start small for testing; remove for full analysis

# Or use a local file:
# dataset_path: data/dataset_examples/sft_data.jsonl

output_path: ./analysis_output/ifd
generate_report: true
report_title: "IFD (Instruction-Following Difficulty) Analysis"

# Recommendations and report settings
generate_recommendations: true
outlier_threshold: 2.0  # Standard deviations for IFD outlier detection

analyzers:
  # ==========================================================================
  # IFD ANALYZER - Instruction-Following Difficulty
  # ==========================================================================
  # Computes IFD = PPL(response | no instruction) / PPL(response | with instruction)
  # Higher IFD means the instruction provides more guidance to the model
  - id: ifd
    params:
      # Model for perplexity calculation
      # Smaller models work well and are efficient for filtering
      model_name: Qwen/Qwen3-0.6B  # Fast, good quality (recommended)

      # Alternative models (uncomment to use):
      # model_name: gpt2  # Smallest, fastest
      # model_name: Qwen/Qwen2.5-0.5B  # Alternative small model
      # model_name: meta-llama/Llama-3.2-1B  # Good alternative

      # Column mapping (auto-detected if not specified)
      # For Alpaca format, these map to the standard columns
      instruction_column: instruction  # Or: prompt, input, question
      response_column: output  # Or: response, answer, completion

      # Hardware settings
      device: null  # Auto-detect (cuda > mps > cpu)
      torch_dtype: null  # Auto (float16 on GPU, float32 on CPU)

      # Processing settings
      batch_size: 4  # Increase for faster processing if GPU memory allows
      max_length: 2048  # Maximum sequence length

      # Model loading options
      trust_remote_code: true  # Required for some models like Qwen
      low_memory: false  # Set true for memory-constrained environments
      cache_dir: null  # Custom model cache directory

  # ==========================================================================
  # LENGTH ANALYZER - Token counts for context
  # ==========================================================================
  # Useful to correlate IFD with response length
  - id: length
    params:
      char_count: false
      word_count: true
      sentence_count: false
      token_count: true

# ==========================================================================
# OUTPUT COLUMNS REFERENCE
# ==========================================================================
#
# IFD metrics (per-sample):
#   - ifd_score: The IFD ratio (higher = more valuable)
#   - ifd_ppl_with_instruction: Perplexity with instruction context
#   - ifd_ppl_without_instruction: Perplexity without instruction
#   - ifd_response_loss: Cross-entropy loss on response tokens
#
# Length metrics (per-sample):
#   - text_content_length_word_count: Word count
#   - text_content_length_token_count: Token count

# ==========================================================================
# DATA SELECTION STRATEGY
# ==========================================================================
#
# Based on Cherry LLM and Superfiltering research:
#
# 1. High-Quality Selection (Top 5-10% by IFD):
#    - Select samples with highest IFD scores
#    - These are the most "instructive" samples
#    - Can match full-data performance with much less data
#
# 2. Filtering Strategy:
#    - Remove samples with IFD < 1.0 (instruction doesn't help)
#    - Consider removing extreme outliers (IFD > 100)
#    - Balance with response length (very short responses may have noisy IFD)
#
# 3. Cross-Model Filtering:
#    - IFD computed with weak models (0.5B) correlates with strong models
#    - You can filter data for GPT-4 class models using Qwen-0.5B IFD
#
# Example pandas filtering:
#   df_filtered = df[df['ifd_score'] > df['ifd_score'].quantile(0.9)]

# ==========================================================================
# INTERPRETING RESULTS
# ==========================================================================
#
# Good dataset characteristics:
#   - Mean IFD > 2.0: Instructions are generally helpful
#   - Low variance: Consistent quality across samples
#   - Few samples with IFD < 1.0: Minimal confusing instructions
#
# Warning signs:
#   - Mean IFD ≈ 1.0: Instructions may not be adding value
#   - Many samples with IFD < 1.0: Consider reviewing these samples
#   - Very high IFD (> 100): May indicate trivial responses or issues
#
# Correlation to check:
#   - IFD vs response length: Very short responses may have noisy IFD
#   - IFD vs instruction length: Longer instructions often have higher IFD
