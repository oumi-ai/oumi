# DEITA-style dataset analysis configuration
#
# This configuration implements the data quality analysis approach from the DEITA paper:
# "What Makes Good Data for Alignment?" (Liu et al., 2023)
# https://arxiv.org/abs/2312.15685
#
# The DEITA approach evaluates data quality along three dimensions:
# 1. Complexity - How sophisticated/challenging are the instructions?
# 2. Quality - How helpful/accurate are the responses?
# 3. Diversity - How unique/varied are the samples?
#
# Usage:
#   oumi analyze -c configs/examples/analyze/analyze_deita.yaml
#
# Requirements:
#   - sentence-transformers (for repr_diversity): pip install 'oumi[analyze_advanced]'
#   - Anthropic API key (for evol_complexity and evol_quality): export ANTHROPIC_API_KEY=...
#     OR use OpenAI/local models by changing api_provider/model_type

# Dataset configuration
dataset_name: "tatsu-lab/alpaca"
split: "train"
sample_count: 100 # Start small for testing, increase for full analysis

# Output configuration
output_path: "./analysis_output/deita"
generate_report: true
report_title: "DEITA Data Quality Analysis"

# Analyzers
analyzers:
  # Embedding-based diversity scoring (DEITA Repr Filter)
  # Measures how unique each sample is relative to the dataset
  - id: "repr_diversity"
    params:
      model_name: "sentence-transformers/all-MiniLM-L6-v2"
      k_neighbors: 5

      # Use role-specific thresholds for accurate diversity measurement
      # System prompts are typically identical and should be excluded
      role_specific_thresholds:
        system: null # Exclude system prompts from diversity analysis
        user: 0.25 # Strict diversity for instructions (keep top 90%)
        assistant: 0.30 # Standard diversity for responses

      embed_field: "all" # "all", "user", or "assistant"
      batch_size: 32
      show_progress_bar: true

  # Instruction complexity scoring via evolved variant ranking
  # Generates more complex versions of instructions and ranks the original
  - id: "evol_complexity"
    params:
      # Model configuration - choose one:
      # Option 1: Use Anthropic API (default)
      # model_type: "api"
      # api_provider: "anthropic"
      # api_model: "claude-4-5-haiku" # Cost-effective

      # Option 2: Use OpenAI API
      model_type: "api"
      api_provider: "openai"
      api_model: "gpt-5-mini"

      inference_config:
        remote_params:
          num_workers: 10 # Parallel API requests
          politeness_policy: 6.0 # 10 RPM per worker = 100 RPM total
          use_adaptive_concurrency: false # Predictable dev performance
          max_retries: 2 # Fast failure
          connection_timeout: 120.0 # 2 minutes max per request

      # Option 3: Use local model
      # model_type: "local"
      # local_model: "meta-llama/Llama-3-8B-Instruct"
      # inference_config:
      #   engine: "vllm"

      # Evolution configuration
      num_evolutions: 3 # Number of more complex variants to generate (1-6)
      evolution_operators:
        - "add_constraints"
        - "require_reasoning"
        - "increase_depth"

      # Analysis scope
      analyze_role: "user" # Analyze user instructions

      # Performance
      # temperature: 0.7
      max_retries: 2
      cache_responses: true
      show_progress: true

  # Response quality scoring via evolved variant ranking
  # Generates improved versions of responses and ranks the original
  - id: "evol_quality"
    params:
      model_type: "api"
      api_provider: "openai"
      api_model: "gpt-5-mini"

      # Quality aspects to improve
      num_evolutions: 3
      quality_aspects:
        - "helpfulness"
        - "depth"
        - "accuracy"
        - "structure"

      # Context configuration
      use_conversation_context: true # Try to find instruction for context

      # Analysis scope
      analyze_role: "assistant" # Analyze assistant responses

      # Performance
      # temperature: 0.7
      max_retries: 2
      cache_responses: true
      show_progress: true

      inference_config:
        remote_params:
          num_workers: 10 # Parallel API requests
          politeness_policy: 6.0 # 10 RPM per worker = 100 RPM total
          use_adaptive_concurrency: false # Predictable dev performance
          max_retries: 2 # Fast failure
          connection_timeout: 120.0 # 2 minutes max per request

# Recommendations configuration
generate_recommendations: true
outlier_threshold: 3.0
