# Analysis configuration for DMG invoice validation dataset
# This demonstrates analyzing contractor work order invoice validation tasks

dataset_name: "tatsu-lab/alpaca"
split: "train"
output_path: ./analysis_output/dmg

analyzers:
  # Length analyzer - counts chars, words, tokens
  - id: length
    params:
      token_count: true

  # Token stats analyzer - aggregates tokens by role (system, input, output)
  # Requires length analyzer to run first
  - id: token_stats
    params: {}

  # Cost analyzer - estimates API costs for different context windows
  - id: cost
    params:
      target_context_windows: [4096, 8192, 16384]
      compute_packing_efficiency: true
      packing_overhead_tokens: 10

  - id: fasttext
    params:
      detect_language: true
      detect_script: true # latin, cyrillic, cjk, etc.
      detect_multilingual: true # Detect mixed-language content
      min_confidence: 0.0 # Report all languages, even low confidence
      low_confidence_threshold: 0.5 # Flag samples below this threshold
      use_fast_langdetect: true # Use fast-langdetect (recommended, faster)

  - id: embedding
    params:
      model_name: all-MiniLM-L6-v2 # Fast, 384 dims
      detect_duplicates: true
      duplicate_threshold: 0.95 # Cosine similarity (0.9-0.99)
      detect_fuzzy_duplicates: true # MinHash LSH (fast)
      fuzzy_threshold: 0.8
      fuzzy_ngram_size: 3
      cluster_samples: false
      batch_size: 32
      store_embeddings: false

  # Question diversity analyzer - detect if user job contexts are too similar
  - id: question_diversity
    params:
      cluster_questions: true
      clustering_method: dbscan
      eps: 0.15 # ~99% cosine similarity
      min_samples: 2
      model_name: all-MiniLM-L6-v2
      batch_size: 32
      compute_entropy: true
      compute_concentration: true
      flag_concentrated_clusters: true
      concentration_threshold: 0.5

  # Representation diversity analyzer - DEITA-style embedding diversity
  - id: repr_diversity
    params:
      model_name: sentence-transformers/all-MiniLM-L6-v2
      k_neighbors: 5
      diversity_threshold: 0.3
      # Use role-specific thresholds to avoid system prompt inflation
      role_specific_thresholds:
        system: null # Exclude system prompts (typically identical)
        user: 0.25 # Strict diversity for user messages
        assistant: 0.30 # Standard diversity for responses
      embed_field: all # "all", "user", or "assistant"
      batch_size: 32

  # LLM Judge: Validation Quality
  # Custom conversation-level evaluation for invoice validation helpfulness
  - id: llm_judge
    instance_id: validation_quality
    params:
      prompt: |
        Evaluate how accurate and helpful this invoice line item validation is (0-10).

        Context: This is a contractor work order invoice validation task. The assistant validates invoice line items by checking:
        1) Format validity (is it properly formatted as a material/part/equipment/fee?)
        2) Job appropriateness (is the item appropriate for the job context?)

        CONVERSATION FORMAT:
        The conversation below contains three parts:
        1. SYSTEM: Detailed validation instructions and rules
        2. USER: Job context (service line, service type, job scope) and invoice details (description, quantity, rate, amount)
        3. ASSISTANT: Validation result in structured format with format_check and appropriateness_check

        ⚠️ IMPORTANT: The assistant's validation is at the VERY END of the conversation (after "ASSISTANT:").
        Look for the structured response with [FORMAT_CHECK] and [APPROPRIATENESS_CHECK] sections.

        Conversation to evaluate:
        {text}

        Evaluation criteria for validation quality:
        - Does the format validation correctly identify valid vs invalid line items?
        - Is the invalid class (if any) correctly assigned (NUMERALS_ONLY, FILLER_NON_DESCRIPTIVE, MULTIPLE_ITEMS_ONE_LINE, LABOR_CHARGE, TRIP_TRAVEL_CHARGE)?
        - Does the appropriateness check correctly assess if the item matches the job context?
        - Are the explanations clear and well-reasoned?
        - Does the assistant follow the strict output format correctly?

        Scoring guide:
        - 10: Perfect validation, correct format/appropriateness assessment, clear reasoning
        - 7-9: Good validation, mostly correct with minor issues
        - 4-6: Mediocre validation, some errors in classification or reasoning
        - 1-3: Poor validation, significant errors or missing required sections
        - 0: Invalid or completely wrong validation

        Examples of good vs bad:
        Good (score 9-10):
          USER: "PURCHASE_ITEM_LINE_DESCRIPTION: 3/4 inch PEX coupling" (plumbing job)
          ASSISTANT: format_is_valid: true, is_appropriate: true  ← Correct validation

        Bad (score 1-3):
          USER: "PURCHASE_ITEM_LINE_DESCRIPTION: labor" (plumbing job)
          ASSISTANT: format_is_valid: true  ← Should be false, invalid_class: LABOR_CHARGE

        Respond with JSON:
        - "score": 0-10 (10 = maximally accurate validation)
        - "label": "very_accurate", "accurate", "somewhat_accurate", "inaccurate"
        - "reasoning": brief explanation of why this validation is/isn't accurate

        JSON response:
      analyze_message_level: false
      analyze_conversation_level: true
      inference_config:
        model_name: gpt-4o-mini
        engine: openai
        temperature: 0.1
        max_tokens: 256
        remote_params:
          num_workers: 200 # Process requests in parallel
          politeness_policy: 1.0 # Wait 60s between requests per worker
          use_adaptive_concurrency: true # Auto-adjust based on error rate
      batch_size: 1000
      max_text_length: 16000 # DMG system prompts are very long
      parse_json_response: true

  # LLM Judge: Instruction Quality
  # Evaluates clarity and quality of system instructions (message-level only)
  - id: llm_judge
    instance_id: instruction_quality
    params:
      prompt_preset: instruction_quality
      filter_role: system
      analyze_message_level: true
      analyze_conversation_level: false
      inference_config:
        model_name: gpt-4o-mini
        engine: openai
        temperature: 0.1
        max_tokens: 256
        remote_params:
          num_workers: 200 # Process requests in parallel
          politeness_policy: 1.0 # Wait 60s between requests per worker
          use_adaptive_concurrency: true # Auto-adjust based on error rate
      batch_size: 1000
      parse_json_response: true

  # LLM Judge: Response Quality
  # Custom prompt for invoice validation response format evaluation
  - id: llm_judge
    instance_id: response_quality
    params:
      prompt: |
        Evaluate this assistant's invoice validation response (0-10).

        Context: The assistant must validate invoice line items by providing structured output with format_check and appropriateness_check sections.

        Assistant response to evaluate:
        {text}

        Evaluation criteria:
        - Does it follow the strict output format with [FORMAT_CHECK] and [APPROPRIATENESS_CHECK] sections?
        - Are all required fields present (format_explanation, format_is_valid, format_invalid_class, appropriateness_explanation, is_appropriate)?
        - Is the format correct (no extra text before/after, proper field order)?
        - Are the explanations clear and concise (1-3 sentences)?

        Good examples:
        [FORMAT_CHECK]
        format_explanation: The description identifies a specific plumbing part.
        format_is_valid: true
        format_invalid_class: none
        [APPROPRIATENESS_CHECK]
        appropriateness_explanation: The item is appropriate for the plumbing job context.
        is_appropriate: true
        [END]

        Bad examples:
        - Missing sections or fields
        - Extra text before [FORMAT_CHECK] or after [END]
        - Incorrect field names or values
        - Empty or vague explanations

        Respond with JSON:
        - "score": 0-10 (10 = perfect format compliance)
          * 10: Perfect format, all fields present and correct
          * 7-9: Good format with minor issues
          * 4-6: Missing some fields or format errors
          * 1-3: Significant format violations
          * 0: Completely wrong or missing format
        - "label": "excellent", "good", "needs_improvement", "poor"
        - "reasoning": brief explanation

        JSON response:
      filter_role: assistant
      analyze_message_level: true
      analyze_conversation_level: false
      inference_config:
        model_name: gpt-4o-mini
        engine: openai
        temperature: 0.1
        max_tokens: 256
        remote_params:
          num_workers: 200 # Process requests in parallel
          politeness_policy: 1.0 # Wait 60s between requests per worker
          use_adaptive_concurrency: true # Auto-adjust based on error rate
      batch_size: 1000
      max_text_length: 8000
      parse_json_response: true

# Optional: Generate recommendations based on analysis
generate_recommendations: true
outlier_threshold: 3.0

# Optional: Generate HTML report with visualizations
generate_report: yes
