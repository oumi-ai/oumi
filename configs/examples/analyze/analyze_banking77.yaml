# Analysis configuration for Banking77 dataset with tests
# Demonstrates the new declarative test system for quality checks

# Usage: oumi analyze --config configs/examples/analyze/analyze_banking77.yaml

dataset_path: "/Users/ryanarman/code/scratch/ryan_hillclimbing_experiments/banking77/notebooks/data/banking77_train.jsonl"
# dataset_name is not needed when using dataset_path
sample_count: 1000
output_path: ./analysis_output/banking77

analyzers:
  # Length analyzer - counts tokens
  - id: length
    params:
      token_count: true

  # Diversity analyzer
  - id: diversity
    params:
      unique_words_ratio: true

  # Quality analyzer - detect issues
  - id: quality
    params:
      detect_pii: true
      detect_encoding_issues: true

  # FastText language detection
  - id: fasttext
    params:
      detect_language: true
      detect_script: true
      detect_multilingual: true
      min_confidence: 0.0
      low_confidence_threshold: 0.5
      use_fast_langdetect: true

  # Embedding analyzer for duplicate detection (requires sentence-transformers)
  # Uncomment if you have sentence-transformers installed
  # - id: embedding
  #   params:
  #     model_name: all-MiniLM-L6-v2
  #     detect_duplicates: true
  #     duplicate_threshold: 0.95
  #     detect_fuzzy_duplicates: true
  #     fuzzy_threshold: 0.8
  #     fuzzy_ngram_size: 3
  #     cluster_samples: false
  #     batch_size: 32
  #     store_embeddings: false

# =============================================================================
# TESTS - Quality checks for the banking77 dataset
# =============================================================================
tests:
  # ---------------------------------------------------------------------------
  # LENGTH TESTS
  # ---------------------------------------------------------------------------

  # Check for empty or very short messages
  - id: no_empty_messages
    type: threshold
    metric: "text_content__length__token_count"
    operator: "<="
    value: 0
    max_percentage: 1.0
    severity: high
    title: "Empty or near-empty messages"
    description: "Messages with 1 or fewer tokens indicate data quality issues"

  # Check for very short queries (banking queries should have some substance)
  - id: substantive_queries
    type: query
    expression: "role == 'user' and text_content__length__token_count < 3"
    max_percentage: 5.0
    severity: medium
    title: "Very short user queries"
    description: "User queries with fewer than 3 tokens may lack context for proper classification"

  # ---------------------------------------------------------------------------
  # QUALITY TESTS
  # ---------------------------------------------------------------------------

  # Check for PII (important for banking data)
  - id: no_pii
    type: percentage
    metric: "text_content__quality__has_pii"
    condition: "== True"
    max_percentage: 1.0
    severity: high
    title: "PII detected in dataset"
    description: "Banking data should not contain personal information (names, account numbers, etc.)"

  # Check for encoding issues
  - id: no_encoding_issues
    type: percentage
    metric: "text_content__quality__has_encoding_issues"
    condition: "== True"
    max_percentage: 2.0
    severity: medium
    title: "Encoding issues detected"
    description: "Text encoding problems may affect classification quality"

  # ---------------------------------------------------------------------------
  # DIVERSITY TESTS
  # ---------------------------------------------------------------------------

  # Check vocabulary diversity (avoid repetitive patterns)
  - id: vocabulary_diversity
    type: threshold
    metric: "text_content__diversity__unique_words_ratio"
    operator: "<"
    value: 0.3
    max_percentage: 20.0
    severity: low
    title: "Low vocabulary diversity"
    description: "Messages with very repetitive vocabulary may indicate templated or low-quality content"

  # ---------------------------------------------------------------------------
  # LANGUAGE TESTS (requires fasttext analyzer)
  # ---------------------------------------------------------------------------

  # Check for non-English content (if dataset should be English-only)
  - id: english_only
    type: query
    expression: "text_content__fasttext__detected_language != 'en'"
    max_percentage: 5.0
    severity: medium
    title: "Non-English content detected"
    description: "Non-English content may indicate data issues"

  # Check for low language detection confidence
  - id: language_confidence
    type: percentage
    metric: "text_content__fasttext__low_confidence"
    condition: "== True"
    max_percentage: 10.0
    severity: low
    title: "Low language detection confidence"
    description: "Messages with uncertain language detection may have quality issues"

  # ---------------------------------------------------------------------------
  # DUPLICATE TESTS (requires embedding analyzer)
  # Uncomment these tests if you have the embedding analyzer enabled
  # ---------------------------------------------------------------------------

  # # Check for semantic duplicates
  # - id: no_semantic_duplicates
  #   type: percentage
  #   metric: "text_content__embedding__has_semantic_duplicate"
  #   condition: "== True"
  #   max_percentage: 10.0
  #   severity: medium
  #   title: "Semantic duplicates detected"
  #   description: "High semantic similarity between samples may reduce training diversity"

  # # Check for fuzzy (near-exact) duplicates
  # - id: no_fuzzy_duplicates
  #   type: percentage
  #   metric: "text_content__embedding__has_fuzzy_duplicate"
  #   condition: "== True"
  #   max_percentage: 5.0
  #   severity: high
  #   title: "Fuzzy duplicates detected"
  #   description: "Near-identical text samples should be deduplicated"

  # ---------------------------------------------------------------------------
  # DISTRIBUTION TESTS
  # ---------------------------------------------------------------------------

  # Check for role balance
  - id: balanced_roles
    type: distribution
    metric: "role"
    check: "max_fraction"
    threshold: 0.7
    severity: medium
    title: "Role distribution check"
    description: "No single role should dominate >70% of messages"

  # ---------------------------------------------------------------------------
  # REGEX TESTS
  # ---------------------------------------------------------------------------

  # Check for placeholder text
  - id: no_placeholders
    type: contains-any
    text_field: "text_content"
    values:
      [
        "[NAME]",
        "[COMPANY]",
        "[DATE]",
        "[INSERT",
        "[YOUR",
        "XXX",
        "PLACEHOLDER",
      ]
    max_percentage: 1.0
    severity: medium
    title: "Placeholder text detected"
    description: "Placeholder tokens should be replaced with real content"
    case_sensitive: false

  # Check for special tokens that shouldn't appear
  - id: no_special_tokens
    type: regex
    text_field: "text_content"
    pattern: "<\\|(?:endoftext|im_start|im_end|pad)\\|>"
    max_percentage: 0
    severity: high
    title: "Special token leakage"
    description: "Training tokens should not appear in content"

  # ---------------------------------------------------------------------------
  # CONVERSATION-LEVEL TESTS
  # ---------------------------------------------------------------------------

  # Check conversation length
  - id: conversation_length
    type: threshold
    metric: "conversation_text_content__length__token_count"
    operator: ">"
    value: 1800
    max_percentage: 1.0
    severity: medium
    title: "Very long conversations"
    description: "Conversations with more than 5 tokens may lack sufficient context"
    scope: conversation

# Generate HTML report with test results
generate_report: true
report_title: "Banking77 Dataset Quality Analysis"
