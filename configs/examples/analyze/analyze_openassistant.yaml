# Analysis configuration for OpenAssistant (OASST1) dataset
# OpenAssistant is a large-scale human-generated conversational dataset
# This config focuses on analyzing high-quality multi-turn conversations:
# - Conversation structure and turn patterns
# - Token distribution by role
# - Quality and diversity metrics
# - Safety and content patterns
# - Response completeness and training quality
#
# SPECIFIC FOCUS AREAS:
# 1. Multi-turn conversation patterns - OASST has rich multi-turn conversations
# 2. Human-generated quality - High-quality human annotations
# 3. Role balance - Understanding user/assistant interaction patterns
# 4. Response quality - Evaluating helpfulness and completeness
# 5. Diversity - Ensuring diverse conversation topics and styles

# Usage: oumi analyze --config configs/examples/analyze/analyze_openassistant.yaml

# dataset_name: "OpenAssistant/oasst1" # Not used when dataset_path is set
# split: "train" # Not used when dataset_path is set
dataset_path: "/Users/ryanarman/data/openassistant/openassistant_oasst1.jsonl"
sample_count: 10
output_path: /Users/ryanarman/code/oumi/analysis_output/openassistant

analyzers:
  # ---------------------------------------------------------------------------
  # BASIC METRICS - Length, diversity, format
  # ---------------------------------------------------------------------------
  # Length analyzer - counts chars, words, tokens
  - id: length
    params:
      token_count: true
      tiktoken_encoding: o200k_base # GPT-4o/GPT-5 encoding

  # Token stats analyzer - aggregates tokens by role (system, input, output)
  # Requires length analyzer to run first
  - id: token_stats
    params: {}

  # Diversity analyzer - vocabulary richness
  - id: diversity
    params:
      unique_words_ratio: true
      case_sensitive: false

  # Format analyzer - structured content detection
  - id: format
    params:
      detect_markdown: true
      detect_json: true
      detect_code_blocks: true
      detect_urls: true
      detect_emails: true
      compute_complexity: true

  # ---------------------------------------------------------------------------
  # QUALITY CHECKS - PII, encoding, repetition
  # ---------------------------------------------------------------------------
  # Quality analyzer - detects privacy and quality issues
  - id: quality
    params:
      detect_pii: true
      detect_emails: true
      detect_phones: true
      detect_ssn: true
      detect_credit_cards: true
      detect_api_keys: true
      detect_encoding_issues: true
      detect_repetition: true
      repetition_ngram_size: 3
      repetition_threshold: 0.3

  # Content pattern analyzer - AI-specific quality issues
  - id: content_pattern
    params:
      detect_placeholders: true # [Name], [Company], etc.
      detect_hallucinated_experiences: true # Fabricated stories
      detect_nooutput: true # <nooutput>, N/A, etc.
      detect_refusals: true # "I cannot provide..."
      check_output_only: false # Check all messages

  # ---------------------------------------------------------------------------
  # LANGUAGE DETECTION - Multilingual content analysis
  # ---------------------------------------------------------------------------
  # FastText analyzer - fast, accurate language detection (176+ languages)
  # OASST1 is multilingual, so language detection is important
  # Requires: pip install 'oumi[analyze]' or pip install fast-langdetect
  - id: fasttext
    params:
      detect_language: true
      detect_script: true # latin, cyrillic, cjk, etc.
      detect_multilingual: true # Detect mixed-language content
      min_confidence: 0.0 # Report all languages, even low confidence
      low_confidence_threshold: 0.5 # Flag samples below this threshold
      use_fast_langdetect: true # Use fast-langdetect (recommended, faster)

  # ---------------------------------------------------------------------------
  # DUPLICATE DETECTION - Semantic and fuzzy duplicates
  # ---------------------------------------------------------------------------
  # Embedding analyzer - semantic duplicate detection
  # Requires: pip install 'oumi[analyze_advanced]'
  - id: embedding
    params:
      model_name: all-MiniLM-L6-v2 # Fast, 384 dims
      detect_duplicates: true
      duplicate_threshold: 0.95 # Cosine similarity (0.9-0.99)
      detect_fuzzy_duplicates: true # MinHash LSH (fast)
      fuzzy_threshold: 0.8
      fuzzy_ngram_size: 3
      cluster_samples: false
      batch_size: 32
      store_embeddings: false

  # Question diversity analyzer - detect if user questions are too similar
  - id: question_diversity
    params:
      cluster_questions: true
      clustering_method: dbscan
      eps: 0.15 # ~99% cosine similarity
      min_samples: 2
      model_name: all-MiniLM-L6-v2
      batch_size: 32
      compute_entropy: true
      compute_concentration: true
      flag_concentrated_clusters: true
      concentration_threshold: 0.5

  # Representation diversity analyzer - DEITA-style embedding diversity
  - id: repr_diversity
    params:
      model_name: sentence-transformers/all-MiniLM-L6-v2
      k_neighbors: 5
      diversity_threshold: 0.3
      # Use role-specific thresholds to avoid system prompt inflation
      role_specific_thresholds:
        system: null # Exclude system prompts (typically identical)
        user: 0.25 # Strict diversity for user messages
        assistant: 0.30 # Standard diversity for responses
      embed_field: all # "all", "user", or "assistant"
      batch_size: 32

  # ---------------------------------------------------------------------------
  # CONVERSATION ANALYSIS - Structure and completeness
  # ---------------------------------------------------------------------------
  # Conversation structure analyzer - turn pattern analysis
  # OASST is known for rich multi-turn conversations
  - id: conversation_structure
    params:
      single_turn_threshold: 2
      compute_length_stats: true

  # Response completeness analyzer - truncation detection
  - id: response_completeness
    params:
      analyze_assistant_only: true
      strict_mode: false
      include_truncation_type: true

  # Training quality analyzer - SFT effectiveness metrics
  - id: training_quality
    params:
      compute_response_completeness: true
      min_response_words: 5

  # ---------------------------------------------------------------------------
  # TASK AND SAFETY CLASSIFICATION
  # ---------------------------------------------------------------------------
  # Task category analyzer - instruction type classification
  - id: task_category
    params:
      min_confidence: 0.3
      default_category: other
      analyze_user_only: true

  # Safety analyzer - safety and risk classification
  - id: safety
    params:
      strict_mode: false
      include_categories: true

  # Difficulty analyzer - instruction complexity estimation
  - id: difficulty
    params:
      analyze_user_only: true
      include_component_scores: true

  # ---------------------------------------------------------------------------
  # QUALITY SCORING (Magpie framework)
  # ---------------------------------------------------------------------------
  # Input quality analyzer - instruction quality rating
  - id: input_quality
    params:
      analyze_user_only: true
      include_component_flags: true

  # Instruct reward analyzer - response quality scoring
  - id: instruct_reward
    params:
      min_response_words: 10
      max_response_words: 2000
      analyze_assistant_only: true
      include_component_scores: true

  # ---------------------------------------------------------------------------
  # LLM JUDGES - High-quality evaluation (optional, requires API keys)
  # ---------------------------------------------------------------------------
  # LLM Judge: Helpfulness (conversation-level)
  # Evaluates overall conversation helpfulness
  - id: llm_judge
    instance_id: helpfulness
    params:
      prompt: |
        Evaluate how helpful this conversation is as training data for a chat model (0-10).

        Conversation to evaluate:
        {text}

        Evaluation criteria for helpfulness:
        - Is the assistant's response helpful and relevant to the user's query?
        - Does the conversation demonstrate good instruction-following?
        - Is the response accurate and well-structured?
        - Would this conversation help train a better chat model?

        Scoring guide:
        - 10: Excellent conversation, highly helpful, well-structured
        - 7-9: Good conversation, helpful and relevant
        - 4-6: Mediocre, somewhat helpful but has issues
        - 1-3: Poor conversation, not helpful or low quality
        - 0: Very poor or harmful conversation

        Respond with JSON:
        - "score": 0-10 (10 = maximally helpful)
        - "label": "very_helpful", "helpful", "somewhat_helpful", "not_helpful"
        - "reasoning": brief explanation of why this conversation is/isn't helpful

        JSON response:
      analyze_message_level: false
      analyze_conversation_level: true
      inference_config:
        model_name: gpt-4o-mini
        engine: openai
        temperature: 0.1
        max_tokens: 256
        remote_params:
          num_workers: 200
          politeness_policy: 1.0
          use_adaptive_concurrency: true
      batch_size: 1000
      max_text_length: 8000
      parse_json_response: true

# =============================================================================
# TESTS - Quality checks for OpenAssistant dataset
# =============================================================================
tests:
  # ---------------------------------------------------------------------------
  # LENGTH TESTS
  # ---------------------------------------------------------------------------

  # Check for empty or very short messages
  - id: no_empty_messages
    type: threshold
    metric: "text_content__length__token_count"
    operator: "<="
    value: 0
    max_percentage: 1.0
    severity: high
    title: "Empty messages"
    description: "Messages with no tokens indicate data quality issues"

  # Check for very short assistant responses
  - id: substantive_responses
    type: query
    expression: "role == 'assistant' and text_content__length__token_count < 5"
    max_percentage: 5.0
    severity: medium
    title: "Very short assistant responses"
    description: "Assistant responses with <5 tokens may not be helpful for training"

  # Check for very long messages that might exceed context
  - id: context_window_safe
    type: threshold
    metric: "text_content__length__token_count"
    operator: ">"
    value: 4096
    max_percentage: 5.0
    severity: medium
    title: "Messages exceeding 4k tokens"
    description: "Very long messages may exceed context windows"
    scope: message

  # ---------------------------------------------------------------------------
  # QUALITY TESTS
  # ---------------------------------------------------------------------------

  # Check for PII (important for training data)
  - id: no_pii
    type: percentage
    metric: "text_content__quality__has_pii"
    condition: "== True"
    max_percentage: 2.0
    severity: high
    title: "PII detected in dataset"
    description: "Personal information should be removed before training"

  # Check for encoding issues
  - id: no_encoding_issues
    type: percentage
    metric: "text_content__quality__has_encoding_issues"
    condition: "== True"
    max_percentage: 2.0
    severity: medium
    title: "Encoding issues detected"
    description: "Text encoding problems may affect model training"

  # Check for repetitive content
  - id: no_repetitive_content
    type: percentage
    metric: "text_content__quality__has_high_repetition"
    condition: "== True"
    max_percentage: 10.0
    severity: medium
    title: "Repetitive content detected"
    description: "Highly repetitive text may indicate low-quality samples"

  # ---------------------------------------------------------------------------
  # CONTENT PATTERN TESTS
  # ---------------------------------------------------------------------------

  # Check for placeholder text
  - id: no_placeholders
    type: contains-any
    text_field: "text_content"
    values:
      - "[NAME]"
      - "[COMPANY]"
      - "[DATE]"
      - "[INSERT"
      - "[YOUR"
      - "PLACEHOLDER"
    max_percentage: 1.0
    severity: medium
    title: "Placeholder text detected"
    description: "Placeholder tokens should be replaced with real content"
    case_sensitive: false

  # Check for special tokens that shouldn't appear in content
  - id: no_special_tokens
    type: regex
    text_field: "text_content"
    pattern: "<\\|(?:endoftext|im_start|im_end|pad)\\|>"
    max_percentage: 0
    severity: high
    title: "Special token leakage"
    description: "Training tokens should not appear in content"

  # Check for refusal patterns in responses
  - id: check_refusals
    type: regex
    text_field: "text_content"
    pattern: "(?i)(I cannot|I'm unable to|I am not able to|I won't|I refuse to)"
    max_percentage: 10.0
    severity: low
    title: "Refusal patterns detected"
    description: "High refusal rate may indicate overly restrictive training data"

  # ---------------------------------------------------------------------------
  # DIVERSITY TESTS
  # ---------------------------------------------------------------------------

  # Check vocabulary diversity
  - id: vocabulary_diversity
    type: threshold
    metric: "text_content__diversity__unique_words_ratio"
    operator: "<"
    value: 0.2
    max_percentage: 15.0
    severity: low
    title: "Low vocabulary diversity"
    description: "Messages with very low word diversity may be repetitive or low-quality"

  # ---------------------------------------------------------------------------
  # DISTRIBUTION TESTS
  # ---------------------------------------------------------------------------

  # Check for role balance
  - id: balanced_roles
    type: distribution
    metric: "role"
    check: "max_fraction"
    threshold: 0.7
    severity: medium
    title: "Role distribution check"
    description: "No single role should dominate >70% of messages"

  # ---------------------------------------------------------------------------
  # LANGUAGE TESTS (requires fasttext analyzer)
  # ---------------------------------------------------------------------------

  # Check for low language detection confidence
  - id: language_confidence
    type: percentage
    metric: "text_content__fasttext__low_confidence"
    condition: "== True"
    max_percentage: 15.0
    severity: low
    title: "Low language detection confidence"
    description: "Messages with uncertain language may have quality issues"

  # ---------------------------------------------------------------------------
  # HELPFULNESS TESTS (requires instruct_reward analyzer)
  # ---------------------------------------------------------------------------

  # Check for low conversation helpfulness scores (from LLM judge)
  - id: low_helpfulness
    type: threshold
    metric: "conversation_text_content__helpfulness__score"
    operator: "<="
    value: 4
    max_percentage: 5.0
    severity: high
    title: "Low helpfulness conversations"
    description: "Conversations with LLM-judged helpfulness score <= 2 (out of 10) may not be good training data"
    scope: conversation

  # ---------------------------------------------------------------------------
  # CONVERSATION-LEVEL TESTS
  # ---------------------------------------------------------------------------

  # Check for strange/unusual characters that may indicate encoding issues or corrupted data
  # This catches: control chars, escaped sequences, HTML entities, null bytes
  - id: strange_characters
    type: regex
    text_field: "conversation_text_content"
    pattern: '[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]|\\x[0-9a-fA-F]{2}|\\u[0-9a-fA-F]{4}|&(?:nbsp|amp|lt|gt|quot|apos|#\d+|#x[0-9a-fA-F]+);'
    max_percentage: 1.0
    severity: medium
    title: "Strange characters detected"
    description: "Conversations with control chars, escaped sequences, or HTML entities may cause tokenization/encoding issues during SFT"
    scope: conversation

  # Check for replacement characters indicating encoding failures
  - id: replacement_characters
    type: regex
    text_field: "conversation_text_content"
    pattern: "\uFFFD"
    max_percentage: 1.0
    severity: medium
    title: "Replacement characters detected"
    description: "The replacement character (U+FFFD) indicates prior encoding failures in the data"
    scope: conversation

  # Check for phone numbers (PII) in conversations
  - id: phone_numbers
    type: regex
    text_field: "text_content"
    pattern: '(?:\+?1[-.\s]?)?\(?[2-9]\d{2}\)?[-.\s]?\d{3}[-.\s]?\d{4}|\+\d{1,3}[-.\s]?\d{1,4}[-.\s]?\d{1,4}[-.\s]?\d{1,9}'
    max_percentage: 0.0
    severity: high
    title: "Phone numbers detected (PII)"
    description: "Conversations containing phone numbers may have PII that should be removed before training"
    scope: message

  # Python test for language consistency - assistant should respond in user's language
  - id: language_consistency
    type: python
    function: |
      def check(message_df, conversation_df, summary):
          lang_col = 'text_content__fasttext__language'
          if lang_col not in message_df.columns or 'role' not in message_df.columns:
              return {'passed': True, 'affected_samples': 0, 'details': {'skip': 'Missing language or role columns'}}
          
          # Group by conversation and check language consistency
          inconsistent_convs = []
          for conv_id, group in message_df.groupby('conversation_id'):
              user_msgs = group[group['role'] == 'user']
              assistant_msgs = group[group['role'] == 'assistant']
              
              if len(user_msgs) == 0 or len(assistant_msgs) == 0:
                  continue
              
              # Get the primary user language (most common)
              user_langs = user_msgs[lang_col].dropna()
              if len(user_langs) == 0:
                  continue
              user_lang = user_langs.mode().iloc[0] if len(user_langs.mode()) > 0 else None
              
              # Check if assistant responds in a different language
              assistant_langs = assistant_msgs[lang_col].dropna()
              if len(assistant_langs) == 0:
                  continue
              
              # Flag if assistant uses different language than user
              mismatched = assistant_langs[assistant_langs != user_lang]
              if len(mismatched) > 0:
                  inconsistent_convs.append(int(conv_id) if isinstance(conv_id, (int, float)) else conv_id)
          
          max_allowed = int(len(conversation_df) * 0.05)  # Allow 5%
          return {
              'passed': len(inconsistent_convs) <= max_allowed,
              'affected_samples': len(inconsistent_convs),
              'sample_indices': inconsistent_convs[:50],
              'threshold': max_allowed,
              'details': {'description': 'Conversations where assistant language differs from user language'}
          }
    severity: medium
    title: "Language inconsistency detected"
    description: "Checks if assistant responds in a different language than the user - important for multilingual SFT"
    max_percentage: 0.0
    scope: conversation

  # Python test for alternating turns - user/assistant should alternate properly
  - id: alternating_turns
    type: python
    function: |
      def check(message_df, conversation_df, summary):
          if 'role' not in message_df.columns or 'conversation_id' not in message_df.columns:
              return {'passed': True, 'affected_samples': 0, 'details': {'skip': 'Missing role or conversation_id'}}
          
          non_alternating_convs = []
          
          for conv_id, group in message_df.groupby('conversation_id'):
              # Sort by message order within conversation
              if 'message_index' in group.columns:
                  group = group.sort_values('message_index')
              
              roles = group['role'].tolist()
              
              # Check for consecutive same roles (excluding system messages)
              prev_role = None
              has_issue = False
              for role in roles:
                  if role == 'system':
                      continue
                  if prev_role is not None and role == prev_role:
                      has_issue = True
                      break
                  prev_role = role
              
              if has_issue:
                  non_alternating_convs.append(int(conv_id) if isinstance(conv_id, (int, float)) else conv_id)
          
          return {
              'passed': len(non_alternating_convs) == 0,
              'affected_samples': len(non_alternating_convs),
              'sample_indices': non_alternating_convs[:50],
              'details': {'description': 'Conversations with consecutive user or assistant messages'}
          }
    severity: medium
    title: "Non-alternating turns detected"
    description: "Conversations should have alternating user-assistant turns for proper SFT training"
    max_percentage: 0.0
    scope: conversation

  # Python test for empty turns - catches empty, whitespace-only, or effectively empty messages
  - id: empty_turns
    type: python
    function: |
      def check(message_df, conversation_df, summary):
          if 'text_content' not in message_df.columns:
              return {'passed': True, 'affected_samples': 0, 'details': {'skip': 'Missing text_content'}}
          
          def is_empty_content(content):
              if content is None:
                  return True
              if not isinstance(content, str):
                  content = str(content)
              stripped = content.strip()
              if len(stripped) == 0:
                  return True
              # Check if any alphanumeric character exists (handles Latin + CJK)
              has_content = any(c.isalnum() for c in stripped)
              return not has_content
          
          # Use pandas apply for efficiency
          is_empty = message_df['text_content'].apply(is_empty_content)
          empty_indices = message_df[is_empty].index.tolist()
          
          return {
              'passed': len(empty_indices) == 0,
              'affected_samples': len(empty_indices),
              'sample_indices': empty_indices[:50],
              'details': {'description': 'Messages that are empty, whitespace-only, or contain only punctuation'}
          }
    severity: high
    title: "Empty turns detected"
    description: "Messages that are empty, whitespace-only, or have no actual content can break SFT training"
    max_percentage: 0.0
    scope: message

  # Check for very short conversations
  - id: conversation_length_min
    type: threshold
    metric: "conversation_text_content__length__token_count"
    operator: "<"
    value: 10
    max_percentage: 5.0
    severity: medium
    title: "Very short conversations"
    description: "Conversations with <10 tokens may lack sufficient content"
    scope: conversation

  # Check for very long conversations
  - id: conversation_length_max
    type: threshold
    metric: "conversation_text_content__length__token_count"
    operator: ">"
    value: 8192
    max_percentage: 5.0
    severity: low
    title: "Very long conversations"
    description: "Conversations exceeding 8k tokens may be too long for some models"
    scope: conversation

# Generate HTML report with test results
generate_report: true
report_title: "OpenAssistant Dataset Quality Analysis"
