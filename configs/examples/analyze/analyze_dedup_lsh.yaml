# Comprehensive Alpaca Dataset Analysis Configuration
# Usage: oumi analyze --config configs/examples/analyze/analyze_dedup_lsh.yaml
#
# This configuration runs comprehensive analysis on the alpaca dataset including:
# - Duplicate detection (semantic + fuzzy MinHash)
# - Content pattern detection (placeholders, hallucinations, refusals)
# - Quality and training effectiveness metrics
#
# Requirements:
#   pip install 'oumi[analyze]'  # Includes sentence-transformers, datasketch
#
# Output files:
#   - message_analysis.csv: Per-message metrics with duplicate flags
#   - conversation_analysis.csv: Per-conversation aggregated metrics
#   - analysis_summary.json: Statistical summary with duplicate counts
#   - analysis_report.html: Interactive HTML report
#
# Expected runtime: ~10-30 minutes for full dataset (depends on hardware)
# GPU recommended for faster embedding computation

# Dataset configuration - FULL alpaca dataset
dataset_name: tatsu-lab/alpaca
split: train
# No sample_count = process entire dataset (~52k samples)
sample_count: 5000

# Output configuration
output_path: ./analysis_output/dedup
generate_report: true
report_title: "Alpaca Dataset Duplicate Analysis"

# Recommendation settings
generate_recommendations: true

analyzers:
  - id: embedding
    params:
      # ----- Fuzzy Duplicate Detection (MinHash LSH) -----
      # Catches: Near-exact copies, minor edits, copy-paste variants
      # Method: Jaccard similarity via MinHash with LSH indexing
      # Speed: O(n) - very fast even for large datasets
      detect_fuzzy_duplicates: true
      fuzzy_threshold: 0.8 # Jaccard similarity threshold
      fuzzy_ngram_size: 3 # Character n-gram size
      fuzzy_num_perm: 128 # MinHash permutations (accuracy vs speed)
      # Higher num_perm = more accurate but slower

      # Clustering (optional - useful for grouping similar content)
      cluster_samples: true # Set true to cluster by topic
      # clustering_method: dbscan    # or "kmeans"
      # eps: 0.5                     # DBSCAN epsilon
      # min_samples: 2               # DBSCAN min samples

      # Don't store embeddings (saves memory)
      store_embeddings: true
