# Arctic Loss Distributed Test Configuration for SFT_ULYSSES Trainer with LIMO Dataset
# This configuration tests the full Arctic loss computation with true Ulysses SP (distributed)
# using the LIMO mathematical reasoning dataset for longer, more complex sequences.
#
# Key features:
# - Enables Ulysses SP with size=4 for true distributed SP testing
# - Tests full Arctic loss computation path (tiled logits + memory optimization)
# - Uses LIMO dataset with long mathematical sequences (up to 32768 tokens)
# - Validates distributed SP-aware data processing and loss computation
# - Tests SP-aware label masking to prevent "all labels masked" issue
# - Note: Uses ArcticTraining's ZeRO + SP compatibility approach with seq_parallel_communication_data_type
#
# Usage:
#   torchrun --nproc-per-node=4 --standalone -m oumi train -c configs/examples/deepspeed/sft_ulysses_arctic_loss_distributed_test.yaml

model:
  model_name: "meta-llama/Llama-3.1-8B-Instruct"
  model_max_length: 32768  # Long sequences from LIMO dataset (reduced from 32768 for memory efficiency)
  torch_dtype_str: "float32"  # Use float32 for mixed precision compatibility
  load_pretrained_weights: True
  trust_remote_code: True
  attn_implementation: "sdpa"  # SDPA for Ulysses SP compatibility

data:
  train:
    datasets:
      - dataset_name: "PromptResponseDataset"
        split: "train"
        dataset_kwargs:
          hf_dataset_path: "GAIR/LIMO"
          prompt_column: "question"
          response_column: "solution"
        shuffle: True
        seed: 42
    collator_name: "text_with_padding"

training:
  # Use our custom SFT trainer WITH distributed Ulysses SP
  trainer_type: "SFT_ULYSSES"

  # ENABLE distributed Ulysses SP for true Arctic loss testing
  enable_ulysses_sequence_parallel: True
  ulysses_sequence_parallel_size: 4

  # Enable all Arctic memory optimizations
  tiled_mlp_compute: True  # Enable tiled MLP computation
  use_liger_kernel: True  # Use Liger kernel like ArcticTraining
  activation_checkpoint_cpu_offload: False  # Keep on GPU for testing

  # Training configuration optimized for SP with long sequences
  max_steps: 50  # Very short run to test Arctic loss with SP on long sequences
  per_device_train_batch_size: 1  # Small batch for memory efficiency with long sequences
  gradient_accumulation_steps: 4  # Test gradient accumulation with early DeepSpeed initialization

  # Learning configuration
  learning_rate: 5e-5
  warmup_steps: 1
  weight_decay: 0.01

  # Logging and saving
  logging_steps: 1
  save_steps: 5
  save_final_model: True
  output_dir: "output/sft_ulysses_arctic_loss_distributed_test_large"

  # Disable evaluation for simpler testing
  eval_strategy: "no"

  # Memory and performance for SP
  enable_gradient_checkpointing: True  # Test with gradient checkpointing
  mixed_precision_dtype: "BF16"  # Use mixed precision with SP

  # SFT kwargs for SP
  trainer_kwargs:
    pad_to_multiple_of: 4  # Pad sequences to multiples of SP size

  # Seed for reproducibility
  seed: 42
  data_seed: 42

  # Logging
  log_level: "info"  # Verbose to see Arctic loss messages
  enable_tensorboard: False
  enable_wandb: True
  enable_mlflow: False

# Enable DeepSpeed for distributed Ulysses SP support with ZeRO (using ArcticTraining approach)
deepspeed:
  enable_deepspeed: True
  zero_stage: "ZERO_3"  # ZERO_3 with CPU offloading
  precision: "BF16"

  # CRITICAL: ArcticTraining compatibility setting for ZeRO + SP
  seq_parallel_communication_data_type: "bf16"  # Essential for ZeRO + SP compatibility

  # Distributed Ulysses Sequence Parallelism
  # ulysses_sequence_parallel_size is already set in training section

  # ZERO_3 CPU offloading with memory pinning disabled
  zero_force_ds_cpu_optimizer: False  # Keep as requested

  # CPU optimizer offloading for ZERO_3 + SP compatibility
  offload_optimizer:
    device: "CPU"
    pin_memory: False
    buffer_count: 4

  # CPU parameter offloading for memory efficiency with ZERO_3
  # offload_param:
  #   device: "CPU"
  #   pin_memory: False
  #   buffer_count: 4

  # Communication settings for distributed SP (ArcticTraining settings)
  # Disable allgather partitions which may use pinned memory to avoid CUDA errors
  allgather_partitions: False
  allgather_bucket_size: 50000000  # 5e7
  overlap_comm: True
  contiguous_gradients: True
  reduce_scatter: False  # Disable reduce scatter optimizations
  
  # Additional settings to disable memory pinning and avoid CUDA errors
  # Use smaller values to minimize GPU memory usage and avoid pinning
  stage3_max_live_parameters: 100000000  # Smaller to force more CPU offloading
  stage3_max_reuse_distance: 100000000   # Smaller to force more CPU offloading  
  stage3_prefetch_bucket_size: 10000000  # Smaller prefetch buckets
  stage3_param_persistence_threshold: 10000  # Lower threshold to offload more
  stage3_gather_16bit_weights_on_model_save: False
  
  # Note: Memory pinning is controlled via offload_optimizer and offload_param settings above

  # Optimizer settings
  zero_allow_untested_optimizer: True

  # Training settings
  train_batch_size: "auto"
  train_micro_batch_size_per_gpu: "auto"
  # gradient_accumulation_steps is already set in training section
  gradient_clipping: "auto"
  steps_per_print: 20
  wall_clock_breakdown: False
