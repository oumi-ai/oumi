# Sample command:
# accelerate launch \
#   --use_deepspeed \
#   --config_file configs/accelerate/llama.deepspeed.yaml \
#   -m oumi.train \
#   -c configs/lema/llama2b.pt.yaml \
#   "training.output_dir=/tmp/train/" \
#   "training.enable_wandb=false" \
#   "training.enable_tensorboard=false" \
#   "training.include_performance_metrics=true" \
#   "training.max_steps=100" \
#   "training.logging_steps=10"

compute_environment: LOCAL_MACHINE
debug: false
deepspeed_config:
  zero3_init_flag: true
  # If value is set "auto", will be automatically inferred from model, HF configs, etc.
  # `zero_hpz_partition_size=4` enables hierarchical partioning, with 4 GPUs per node.
  # For details about the DeepSpeed config file, see:
  # https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed#deepspeed-config-file
  # https://www.deepspeed.ai/docs/config-json/
  deepspeed_config_file: "configs/accelerate/deepspeed_config.json"
distributed_type: DEEPSPEED
downcast_bf16: no
enable_cpu_affinity: false
machine_rank: 0
main_training_function: main
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
