# Sample command:
# accelerate launch \
#   --use_fsdp \
#   --config_file configs/accelerate/phi3.fsdp.dpo.yaml \
#   -m oumi.train \
#   -c configs/oumi/phi3.dpo.nvidia.24g.yaml \
#   "training.output_dir=train/" \
#   "training.enable_wandb=true" \
#   "training.enable_tensorboard=true" \
#   "training.include_performance_metrics=false" \
#   "training.per_device_train_batch_size=4" \
#   "training.max_steps=100" \
#   "training.logging_steps=10"

compute_environment: LOCAL_MACHINE
debug: false
distributed_type: FSDP
downcast_bf16: 'no'
enable_cpu_affinity: false
fsdp_config:
  fsdp_activation_checkpointing: false
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch: BACKWARD_PRE
  fsdp_cpu_ram_efficient_loading: true
  fsdp_forward_prefetch: false # Enabling this param leads to post_hook error.
  fsdp_limit_all_gathers: true
  fsdp_offload_params: false
  fsdp_sharding_strategy: FULL_SHARD
  fsdp_state_dict_type: SHARDED_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_transformer_layer_cls_to_wrap: Phi3DecoderLayer  # The basic block is named Phi3DecoderLayer (repeated 32 times)
  fsdp_use_orig_params: true
machine_rank: 0
main_training_function: main
mixed_precision: 'bf16' # 'no'
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
