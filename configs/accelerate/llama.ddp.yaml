# Sample command:
# accelerate launch \
#   --num_machines ${LEMA_NUM_NODES} \
#   --machine_rank ${SKYPILOT_NODE_RANK} \
#   --num_processes $((${LEMA_NUM_NODES} * ${SKYPILOT_NUM_GPUS_PER_NODE})) \
#   --main_process_ip ${LEMA_MASTER_ADDR} \
#   --main_process_port 8007 \
#   --multi-gpu \
#   --config_file configs/accelerate/llama.ddp.yaml \
#   -m lema.train \
#   -c configs/lema/llama2b.pt.yaml \
#   "training.output_dir=/tmp/train/" \
#   "training.enable_wandb=false" \
#   "training.enable_tensorboard=false" \
#   "training.include_performance_metrics=true" \
#   "training.max_steps=100" \
#   "training.logging_steps=10"

compute_environment: LOCAL_MACHINE
debug: false
distributed_type: MULTI_GPU
downcast_bf16: 'no'
enable_cpu_affinity: false
machine_rank: 0
main_training_function: main
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
