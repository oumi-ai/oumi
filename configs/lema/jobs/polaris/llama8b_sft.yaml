# Config for Llama 3.1 8B Instruct SFT on 1 Polaris node.
# Example command:
# lema-launch -p configs/lema/jobs/polaris/llama8b_sft.yaml -c debug-scaling.$ALCF_USER user=$ALCF_USER
name: llama8b-sft
# NOTE: Replace with your username.
user: your_username

num_nodes: 1  # Set it to N for multi-node training.
resources:
  cloud: polaris

# Upload working directory to /home/$USER/lema_launcher/hello-world.
working_dir: .

# `setup` will always be executed before `run`. It's strongly suggested to set any PBS
# directives in the `setup` section. Additional commands can also be run here after the
# PBS directives.
setup: |
  #PBS -l place=scatter
  #PBS -l walltime=02:00:00
  #PBS -l filesystems=home:eagle
  #PBS -A community_ai
  #PBS -o /eagle/community_ai/jobs/logs/
  #PBS -e /eagle/community_ai/jobs/logs/

run: |
  set -e

  # Change to the directory where the job was submitted.
  echo "Changing directory to ${PBS_O_WORKDIR} ..."
  cd ${PBS_O_WORKDIR}

  # Run several checks and export "LEMA_*" env vars.
  source ./scripts/polaris/polaris_init.sh

  # Set up default modules.
  module use /soft/modulefiles

  # Set up conda.
  module load conda

  # Activate the LeMa Conda environment.
  conda activate /home/$USER/miniconda3/envs/lema

  echo "Starting training with ${LEMA_NUM_NODES} node(s)..."

  NRANKS=1  # Num MPI ranks to spawn per node (1 `torchrun` per node)
  NDEPTH=64 # Num hardware threads per rank (Polaris has 64 logical CPU cores per node)


  set -x  # Print "mpiexec" command with expanded variables
  mpiexec --verbose \
      --np $LEMA_NUM_NODES -ppn ${NRANKS} -d ${NDEPTH} --cpu-bind depth \
      ./scripts/polaris/jobs/llama_tune.sh -m sft -s 8b

  echo -e "Finished training on ${LEMA_NUM_NODES} node(s):\n$(cat $PBS_NODEFILE)"
  echo "Polaris job is all done!"
