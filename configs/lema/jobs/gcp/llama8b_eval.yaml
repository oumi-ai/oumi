# Config to eval Llama 3.1 8B Instruct on GCP.
# Example command:
# lema-launch -p configs/lema/jobs/gcp/llama8b_eval.yaml -c llama8b-eval
name: llama8b-eval

num_nodes: 1
resources:
  cloud: gcp
  accelerators: "A100:4"
  use_spot: true

# Upload working directory to remote ~/sky_workdir.
working_dir: .

# Mount local files.
file_mounts:
  ~/.netrc: ~/.netrc  # WandB credentials
  # Mount HF token, which is needed to download locked-down models from HF Hub.
  # This is created on the local machine by running `huggingface-cli login`.
  ~/.cache/huggingface/token: ~/.cache/huggingface/token

storage_mounts:
  # See https://github.com/openlema/lema/wiki/Clouds-Setup#mounting-gcs-buckets
  # for documentation on using GCS buckets.
  /output_dir_gcs:
    source: gs://lema-dev-us-central1
    store: gcs

envs:
  WANDB_PROJECT: lema-eval
  # HF datasets require trusting remote code to be enabled.
  HF_DATASETS_TRUST_REMOTE_CODE: 1
  LEMA_EVALUATION_FRAMEWORK: lm_harness # Valid values: "lm_harness", "lema"

setup: |
  set -e
  pip install '.[train]'
  pip install flash-attn --no-build-isolation
  # Install model from HF Hub. This tool increases download speed compared to
  # downloading the model during eval.
  pip install hf_transfer
  HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct

run: |
  set -e  # Exit if any command failed.
  source ./configs/skypilot/sky_init.sh

  # NOTE: For SFT, update this to point to your model checkpoint.
  MODEL_CHECKPOINT_DIR="meta-llama/Meta-Llama-3.1-8B-Instruct"
  # NOTE: For LoRA, update this to point to your LoRA adapter.
  LORA_ADAPTER_DIR=""

  echo "Starting evaluation for ${EVAL_CHECKPOINT_DIR} ..."

  set -x # Enable command tracing.
  TOTAL_NUM_GPUS=$((${LEMA_NUM_NODES} * ${SKYPILOT_NUM_GPUS_PER_NODE}))

  if [ "$LEMA_EVALUATION_FRAMEWORK" == "lm_harness" ]; then
    accelerate launch \
      --num_processes=${TOTAL_NUM_GPUS} \
      --num_machines=${LEMA_NUM_NODES} \
      --machine_rank=${SKYPILOT_NODE_RANK} \
      --main_process_ip ${LEMA_MASTER_ADDR} \
      --main_process_port 8007 \
      -m lema.evaluate  \
      -c configs/lema/llama8b.eval.yaml \
      "model.adapter_model=${EVAL_CHECKPOINT_DIR}"
  elif [ "$LEMA_EVALUATION_FRAMEWORK" == "lema" ]; then
    echo "The custom eval framework is deprecated. Use LM_HARNESS instead."
    if test ${LEMA_NUM_NODES} -ne 1; then
      echo "Legacy evaluation can only run on 1 node. Actual: ${LEMA_NUM_NODES} nodes."
      exit 1
    fi
    python -m lema.evaluate \
      -c configs/lema/llama8b.eval.legacy.yaml \
      "model.adapter_model=${EVAL_CHECKPOINT_DIR}"
  else
    echo "Unknown evaluation framework: ${LEMA_EVALUATION_FRAMEWORK}"
    exit 1
  fi

  echo "Node ${SKYPILOT_NODE_RANK} is all done!"
