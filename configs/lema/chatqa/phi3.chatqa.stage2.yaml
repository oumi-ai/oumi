model:
  model_name: "openai-community/gpt2"
  torch_dtype_str: "float16"
  trust_remote_code: True

data:
  text_col: "prompt"
  mixture_strategy: FIRST_EXHAUSTED
  datasets:
    - dataset_name: "nvidia/ChatQA-Training-Data"
      preprocessing_function_name: "chatqa"
      subset: "drop"
      mixture_proportion: 0.069
    - dataset_name: "nvidia/ChatQA-Training-Data"
      preprocessing_function_name: "chatqa"
      subset: "narrativeqa"
      mixture_proportion: 0.095
    - dataset_name: "nvidia/ChatQA-Training-Data"
      preprocessing_function_name: "chatqa"
      subset: "quoref"
      mixture_proportion: 0.026
    - dataset_name: "nvidia/ChatQA-Training-Data"
      preprocessing_function_name: "chatqa"
      subset: "ropes"
      mixture_proportion: 0.026
    - dataset_name: "nvidia/ChatQA-Training-Data"
      preprocessing_function_name: "chatqa"
      subset: "squad1.1"
      mixture_proportion: 0.095
    - dataset_name: "nvidia/ChatQA-Training-Data"
      preprocessing_function_name: "chatqa"
      subset: "squad2.0"
      mixture_proportion: 0.095
    - dataset_name: "nvidia/ChatQA-Training-Data"
      preprocessing_function_name: "chatqa"
      subset: "newsqa"
      mixture_proportion: 0.095
    - dataset_name: "nvidia/ChatQA-Training-Data"
      preprocessing_function_name: "chatqa"
      subset: "tatqa-arithmetic"
      mixture_proportion: 0.15
    - dataset_name: "nvidia/ChatQA-Training-Data"
      preprocessing_function_name: "chatqa"
      subset: "tatqa-others"
      mixture_proportion: 0.08
    - dataset_name: "nvidia/ChatQA-Training-Data"
      preprocessing_function_name: "chatqa"
      subset: "synthetic_convqa"
      mixture_proportion: 0.3
    - dataset_name: "nvidia/ChatQA-Training-Data"
      preprocessing_function_name: "chatqa"
      subset: "synthetic_convqa"
      mixture_proportion: 0.2

training:
  optimizer: "adamw_torch"
  use_peft: false
  output_dir: "output/phi3.sft"
  trainer_type: "TRL_SFT"
  per_device_train_batch_size: 2  # Each batch seems to be approx. 1.8GB
  gradient_accumulation_steps: 8

  # Use for debugging purposes
  # max_steps: 10

peft:
  q_lora: False
  lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"
