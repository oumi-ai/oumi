model:
  #model_name: "meta-llama/Meta-Llama-3-8B"
  #torch_dtype_str: "bfloat16"
  #attn_implementation: "flash_attention_2"
  model_name: "gpt2"
  torch_dtype_str: "float16"
  trust_remote_code: True
  model_max_length: 8192

data:
  train:
    target_col: "text"
    mixture_strategy: first_exhausted
    datasets:
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "drop"
        mixture_proportion: 0.069
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "narrativeqa"
        mixture_proportion: 0.095
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "quoref"
        mixture_proportion: 0.025 # Original: 0.26
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "ropes"
        mixture_proportion: 0.026
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "squad1.1"
        mixture_proportion: 0.09 # Original: 0.95
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "squad2.0"
        mixture_proportion: 0.095
      # - dataset_name: "nvidia/ChatQA-Training-Data"
      #   subset: "newsqa"
      #   mixture_proportion: 0.095
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "tatqa-arithmetic"
        mixture_proportion: 0.15
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "tatqa-others"
        mixture_proportion: 0.08
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "synthetic_convqa"
        mixture_proportion: 0.27  # Original: 0.3
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "sft"
        mixture_proportion: 0.1  # Original: 0.2

training:
  trainer_type: "TRL_SFT"

  output_dir: "output/chatqa.stage1"
  save_steps: 200

  #optimizer: "adamw_torch_fused"
  optimizer: "adamw"
  learning_rate: 0.000005
  lr_scheduler_type: "cosine_with_min_lr"
  lr_scheduler_kwargs:
    min_lr_rate: 0.00000001

  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
