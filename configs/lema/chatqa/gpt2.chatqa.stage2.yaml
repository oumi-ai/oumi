model:
  model_name: "openai-community/gpt2"
  torch_dtype_str: "float16"
  trust_remote_code: True

data:
  train:
    target_col: "prompt"
    mixture_strategy: first_exhausted
    datasets:
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "drop"
        mixture_proportion: 0.069
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "narrativeqa"
        mixture_proportion: 0.095
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "quoref"
        mixture_proportion: 0.026
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "ropes"
        mixture_proportion: 0.026
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "squad1.1"
        mixture_proportion: 0.095
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "squad2.0"
        mixture_proportion: 0.095
      # - dataset_name: "nvidia/ChatQA-Training-Data"
      #   subset: "newsqa"
      #   mixture_proportion: 0.095
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "tatqa-arithmetic"
        mixture_proportion: 0.15
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "tatqa-others"
        mixture_proportion: 0.08
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "synthetic_convqa"
        mixture_proportion: 0.27  # Original: 0.3
      - dataset_name: "nvidia/ChatQA-Training-Data"
        subset: "sft"
        mixture_proportion: 0.103  # Original: 0.2

training:
  optimizer: "adamw_torch"
  use_peft: false
  output_dir: "output/chatqa.stage2"
  trainer_type: "TRL_SFT"
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
