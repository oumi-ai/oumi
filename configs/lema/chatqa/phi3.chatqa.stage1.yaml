model:
  model_name: "openai-community/gpt2"
  torch_dtype_str: "float16"
  trust_remote_code: True

data:
  datasets:
    - dataset_name: "nvidia/ChatQA-Training-Data"
      preprocessing_function_name: "chatqa"
      subset: "sft"
      preprocessing_function_kwargs:
        batched: False
  text_col: "text"

training:
  optimizer: "adamw_torch"
  use_peft: false
  output_dir: "output/phi3.sft"
  trainer_type: "TRL_SFT"
  per_device_train_batch_size: 2  # Each batch seems to be approx. 1.8GB
  gradient_accumulation_steps: 8

  # Use for debugging purposes
  # max_steps: 10

peft:
  q_lora: False
  lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"
