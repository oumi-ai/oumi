model:
  model_name: "/workspace/molmo_weights"
  # model_name: "llava-hf/llava-1.5-7b-hf"
  torch_dtype_str: "float32" # Molmo expect float32
  model_max_length: 2048
  trust_remote_code: True

data:
  train:
    collator_name: "vision_language_sft"
    # collator_kwargs:
    #   main_image_feature: "images"
    use_torchdata: True
    datasets:
      - dataset_name: "merve/vqav2-small" # placeholder dataset
        split: "validation"
        shuffle: True
        seed: 42
        transform_num_workers: "auto"
        dataset_kwargs:
          processor_name: "/workspace/molmo_weights"
          # processor_name: "llava-hf/llava-1.5-7b-hf"
          return_conversation: True

training:
  output_dir: "output/vlm_finetuned"
  trainer_type: "TRL_SFT"
  enable_gradient_checkpointing: False # Molmo does not support gradient checkpointing
  per_device_train_batch_size: 2
  max_steps: 20
  optimizer: "adamw_torch_fused"
  logging_steps: 5
  save_steps: 0
  include_performance_metrics: True
  log_model_summary: True
  dataloader_main_process_only: False
  save_final_model: False

fsdp:
  enable_fsdp: True
  sharding_strategy: "HYBRID_SHARD"
  mixed_precision: "bf16"
  forward_prefetch: True
  auto_wrap_policy: "SIZE_BASED_WRAP"
  min_num_params: 100000