# Test tuning config for SmolLM2-135M on Banking77 classification task.
# Used to validate the enterprise SFT pipeline before scaling to larger models.
#
# Usage:
#   oumi tune -c configs/enterprise/tuning/test_smollm2_banking77.yaml

model:
  model_name: "HuggingFaceTB/SmolLM2-135M-Instruct"
  model_max_length: 2048
  torch_dtype_str: "bfloat16"
  attn_implementation: "sdpa"
  load_pretrained_weights: true
  trust_remote_code: true

data:
  train:
    datasets:
      - dataset_name: text_sft
        dataset_path: data/enterprise/banking77/train.jsonl
        sample_count: 500  # Use subset for testing
    collator_name: text_with_padding

  validation:
    datasets:
      - dataset_name: text_sft
        dataset_path: data/enterprise/banking77/test.jsonl
        sample_count: 100  # Use subset for testing
    collator_name: text_with_padding

tuning:
  n_trials: 2  # Minimal trials for testing

  tunable_training_params:
    learning_rate:
      type: categorical
      choices: [2e-5, 5e-5]

  fixed_training_params:
    trainer_type: TRL_SFT
    num_train_epochs: 1
    per_device_train_batch_size: 4
    gradient_accumulation_steps: 2
    max_steps: 50  # Quick test
    optimizer: adamw_torch_fused
    lr_scheduler_type: cosine
    warmup_ratio: 0.1
    save_steps: 25
    logging_steps: 5
    enable_wandb: false
    output_dir: output/enterprise/test_smollm2_banking77

  evaluation_metrics: ["eval_loss"]
  evaluation_direction: ["minimize"]
  tuner_type: OPTUNA
  tuner_sampler: "RandomSampler"
  output_dir: output/enterprise/tuning/test_smollm2_banking77
