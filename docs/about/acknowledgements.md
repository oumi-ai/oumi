# Acknowledgments

Oumi makes use of several libraries and tools from the open-source community üöÄ.

We would like to acknowledge and deeply thank the contributors of these projects:

| Project | Usage in Oumi |
|-------------------|---------------|
| [Accelerate](https://github.com/huggingface/accelerate) | Distributed training and mixed precision computations |
| [Datasets](https://github.com/huggingface/datasets) | Dataset loading and processing |
| [LLama.cpp](https://github.com/ggerganov/llama.cpp) | Efficient inference capabilities for quantized models |
| [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) | Comprehensive suite for evaluating language models |
| [NumPy](https://github.com/numpy/numpy) | Supports numerical operations and data manipulation |
| [Pandas](https://github.com/pandas-dev/pandas) | Data manipulation and analysis |
| [Peft](https://github.com/huggingface/peft) | Parameter-efficient fine-tuning techniques |
| [Pydantic](https://github.com/pydantic/pydantic) | Ensures type checking and validation for configuration objects |
| [PyTorch](https://github.com/pytorch/pytorch) | Primary deep learning framework for model training and inference |
| [SkyPilot](https://github.com/skypilot-org/skypilot) | Cloud-agnostic deployment and management of training jobs |
| [Tqdm](https://github.com/tqdm/tqdm) | Adds progress bars to enhance user experience during long-running operations |
| [Transformers](https://github.com/huggingface/transformers) | Core model architectures and utilities for working with transformer-based models |
| [TRL](https://github.com/huggingface/trl) | SFT and DPO training implementations |
| [Typer](https://github.com/tiangolo/typer) | Used for building command-line interfaces for Oumi commands |
| [vllm](https://github.com/vllm-project/vllm) | Fast inference for large language models |

We are grateful to the developers and maintainers of these projects for their valuable contributions to the open-source community üôè
