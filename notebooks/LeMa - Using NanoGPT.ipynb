{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "The goal of this notebook is to show how to use an arbitrary model with the LeMa training loop.\n",
    "\n",
    "In this case, we will adapt [nanogpt](https://github.com/karpathy/nanoGPT), and train it using both the Lema and HuggingFace training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes that you have already installed the `lema` package. If you haven't, you can install it by running `!pip install lema`.\n",
    "\n",
    "We start then by cloning the nanoGPT repository, and adding nanoGPT to our python path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_folder = \"/tmp/lema/nanoGPT\"\n",
    "\n",
    "# Clone the nanoGPT repo\n",
    "if not os.path.isdir(module_folder):\n",
    "    !mkdir -p $module_folder\n",
    "    !git clone https://github.com/karpathy/nanoGPT $module_folder\n",
    "else:\n",
    "    print(\"nanoGPT already cloned!\")\n",
    "\n",
    "sys.path.append(module_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we install the required dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if importlib.util.find_spec(\"tiktoken\") is not None:\n",
    "    print(\"tiktoken is already installed!\")\n",
    "else:\n",
    "    !pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting nanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from model import GPT, GPTConfig  # import from ~/nanoGPT/model.py\n",
    "\n",
    "from oumi.core import registry\n",
    "\n",
    "\n",
    "@registry.register(\"lema-nanoGPT\", registry_type=registry.RegistryType.MODEL)\n",
    "class LemaNanoGPT(GPT):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Initializes an instance of the class.\"\"\"\n",
    "        gpt_config = GPTConfig()\n",
    "        gpt_config.bias = False\n",
    "\n",
    "        super().__init__(gpt_config)\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        \"\"\"Performs the forward pass of the model.\"\"\"\n",
    "        # Update the return format to be compatible with our Trainer.\n",
    "        logits, loss = super().forward(idx=input_ids, targets=labels)\n",
    "        outputs = {\"logits\": logits}\n",
    "        if loss:\n",
    "            outputs[\"loss\"] = loss\n",
    "        return outputs\n",
    "\n",
    "    def criterion(self):\n",
    "        \"\"\"Returns the criterion used for calculating the loss.\"\"\"\n",
    "        return F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Ok now we are ready to train our model! we can start from the default gpt2 config, and edit as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oumi\n",
    "from oumi.core.configs import TrainerType, TrainingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from the default GPT-2 config\n",
    "config_path = \"../configs/lema/gpt2.pt.mac.yaml\"\n",
    "config = TrainingConfig.from_yaml(config_path)\n",
    "\n",
    "# Update to use our newly registered nanoGPT model\n",
    "config.model.model_name = \"lema-nanoGPT\"  # needs to match the registered model name\n",
    "\n",
    "# We do not have a custom tokenizer, but we can use the GPT-2 tokenizer from HuggingFace\n",
    "config.model.tokenizer_name = \"gpt2\"\n",
    "\n",
    "config.training.trainer_type = TrainerType.LEMA\n",
    "config.training.max_steps = 10\n",
    "config.training.logging_steps = 1\n",
    "config.training.gradient_accumulation_steps = 1\n",
    "config.training.enable_wandb = False\n",
    "config.training.enable_tensorboard = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oumi.train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
