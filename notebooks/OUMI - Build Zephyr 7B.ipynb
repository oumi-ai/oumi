{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from huggingface_hub import login\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from oumi.builders import (\n",
    "    build_dataset_mixture,\n",
    "    build_model,\n",
    "    build_peft_model,\n",
    "    build_tokenizer,\n",
    "    build_trainer,\n",
    ")\n",
    "from oumi.core.types import DatasetSplit, TrainingConfig\n",
    "from oumi.utils.saver import save_model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = os.environ.get(\"HF_TOKEN\")\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filename = \"../configs/oumi/zephyr.7b/sft/config_qlora.yaml\"\n",
    "base_config = OmegaConf.structured(TrainingConfig)\n",
    "file_config = TrainingConfig.from_yaml(config_filename)\n",
    "config = OmegaConf.merge(base_config, file_config)\n",
    "config: TrainingConfig = OmegaConf.to_object(config)\n",
    "print(config.training)\n",
    "print(config.peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.training.max_steps = 2  # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = build_tokenizer(config.model)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tokenizer.model_max_length\", tokenizer.model_max_length)\n",
    "print(\"tokenizer pad_token/eos_token\", tokenizer.pad_token, tokenizer.eos_token)\n",
    "print(\"tokenizer.padding_side\", tokenizer.padding_side)\n",
    "print(\"tokenizer.chat_template\", tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data & preprocessing\n",
    "dataset = build_dataset_mixture(config, tokenizer, DatasetSplit.TRAIN)\n",
    "\n",
    "\n",
    "if not config.data.train.stream:\n",
    "    import numpy as np  # hack to subsample\n",
    "\n",
    "    print(len(dataset))\n",
    "    np.random.seed(1234)\n",
    "    ridx = np.random.choice(len(dataset), 1024, replace=False)\n",
    "    dataset = dataset.select(ridx)\n",
    "    print(len(dataset))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are we supporting PEFT?\n",
    "use_peft = config.training.use_peft and config.peft\n",
    "print(\"use_peft\", use_peft)\n",
    "\n",
    "# Build model.\n",
    "model = build_model(\n",
    "    model_params=config.model, peft_params=config.peft if use_peft else None\n",
    ")\n",
    "\n",
    "if use_peft:\n",
    "    model = build_peft_model(\n",
    "        model, config.training.enable_gradient_checkpointing, config.peft\n",
    "    )\n",
    "\n",
    "# Enable gradients for input embeddings\n",
    "if config.training.enable_gradient_checkpointing:\n",
    "    model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_cls = build_trainer(config.training.trainer_type)\n",
    "\n",
    "# Train model\n",
    "create_trainer_fn = build_trainer(config.training.trainer_type)\n",
    "\n",
    "trainer = create_trainer_fn(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=config.training.to_hf(),\n",
    "    train_dataset=dataset,\n",
    "    **config.training.trainer_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final checkpoint & training state\n",
    "trainer.save_state()\n",
    "\n",
    "save_model(\n",
    "    config=config,\n",
    "    trainer=trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See again:\n",
    "# UserWarning: You passed a tokenizer with `padding_side` not equal to `right`\n",
    "# to the SFTTrainer. This might lead to some unexpected behaviour due to overflow\n",
    "# issues when training a model in half-precision. You might\n",
    "# consider adding `tokenizer.padding_side = 'right'` to your code.\n",
    "\n",
    "# TODO - update our code base if we use optimum (build_model)\n",
    "# Using `disable_exllama` is deprecated and will be removed in version 4.37.\n",
    "# Use `use_exllama` instead and specify the version with `exllama_config`.\n",
    "# The value of `use_exllama` will be overwritten by `disable_exllama` passed\n",
    "# in `GPTQConfig` or stored in your config file. # noqa\n",
    "# WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.\n",
    "# # TODO update in main repo # noqa\n",
    "\n",
    "\n",
    "# TODO Consider adding special tokens like '<|assistant|>', '<|system|>'\n",
    "# via tokenizer.additional_special_tokens -- need to check Mistral\n",
    "\n",
    "# from alignment team:\n",
    "# tokenizer.encode(\"<|system|>\")  # We already wrap <bos> and <eos>\n",
    "# # in the chat template\n",
    "\n",
    "# Future TODO.\n",
    "# # For ChatML we need to add special tokens and resize the embedding layer\n",
    "# if \"<|im_start|>\" in tokenizer.chat_template and \"gemma-tokenizer-chatml\" not in tokenizer.name_or_path: # noqa\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs) # noqa\n",
    "#     model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "#     model_kwargs = None\n",
    "\n",
    "\n",
    "# if tokenizer.model_max_length > 100_000: # shall this condition be checked for diff.\n",
    "#  Zephyr models? Now is not.\n",
    "\n",
    "# tokenizer.all_special_tokens\n",
    "# print(tokenizer.encode(\"|system|\"))\n",
    "# dataset[0][\"text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
