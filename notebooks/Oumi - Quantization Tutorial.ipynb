{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"align-center\">\n",
    "<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n",
    "\n",
    "[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n",
    "[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n",
    "[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - Quantization Tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>\n",
    "\n",
    "üëã Welcome to Open Universal Machine Intelligence (Oumi)!\n",
    "\n",
    "üöÄ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](https://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n",
    "\n",
    "ü§ù Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n",
    "\n",
    "‚≠ê If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi).\n",
    "\n",
    "# Model Quantization Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use AWQ (Activation-aware Weight Quantization) to compress large language models while maintaining performance.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "‚ùó**NOTICE:** Model quantization requires a GPU. If running on Google Colab, you must use a GPU runtime (Colab Menu: `Runtime` -> `Change runtime type` -> Select `T4 GPU` or better).\n",
    "\n",
    "‚ö†Ô∏è **DEVELOPMENT STATUS**: The quantization feature is currently under active development. Some features may change in future releases.\n",
    "\n",
    "First, let's install Oumi with GPU support and the required quantization libraries:\n",
    "\n",
    "```bash\n",
    "pip install oumi[gpu]\n",
    "pip install autoawq\n",
    "pip install triton==3.0.0  # Required for AWQ inference compatibility\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic AWQ Quantization\n",
    "\n",
    "Let's start by quantizing TinyLlama to 4-bit using AWQ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AWQ quantization...\n",
      "[2025-08-04 17:00:12,262][oumi][rank0][pid:173685][MainThread][INFO]][awq_quantizer.py:86] Starting AWQ quantization pipeline...\n",
      "[2025-08-04 17:00:12,263][oumi][rank0][pid:173685][MainThread][INFO]][awq_quantizer.py:118] Loading model for AWQ quantization: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "[2025-08-04 17:00:12,264][oumi][rank0][pid:173685][MainThread][INFO]][awq_quantizer.py:121] üì• Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ad5bb0d51348f097a1670ae3c04061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-04 17:00:12,606][oumi][rank0][pid:173685][MainThread][INFO]][awq_quantizer.py:138] üîß Configuring AWQ quantization parameters...\n",
      "[2025-08-04 17:00:12,607][oumi][rank0][pid:173685][MainThread][INFO]][awq_quantizer.py:155] ‚öôÔ∏è  AWQ config: {'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}\n",
      "[2025-08-04 17:00:12,608][oumi][rank0][pid:173685][MainThread][INFO]][awq_quantizer.py:156] üìä Using 32 calibration samples\n",
      "[2025-08-04 17:00:12,609][oumi][rank0][pid:173685][MainThread][INFO]][awq_quantizer.py:157] üßÆ Starting AWQ calibration and quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8322 > 2048). Running this sequence through the model will result in indexing errors\n",
      "AWQ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [01:10<00:00,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-04 17:01:23,973][oumi][rank0][pid:173685][MainThread][INFO]][awq_quantizer.py:92] PyTorch format requested. Saving AWQ model...\n",
      "[2025-08-04 17:01:24,658][oumi][rank0][pid:173685][MainThread][INFO]][awq_quantizer.py:99] ‚úÖ AWQ quantization successful! Saved as PyTorch format.\n",
      "[2025-08-04 17:01:24,659][oumi][rank0][pid:173685][MainThread][INFO]][awq_quantizer.py:100] üìä Quantized size: 734.0 MB\n",
      "[2025-08-04 17:01:24,660][oumi][rank0][pid:173685][MainThread][INFO]][awq_quantizer.py:101] üí° Use this model with: AutoAWQForCausalLM.from_quantized('tinyllama_awq_4bit')\n",
      "\n",
      "‚úÖ Quantization complete!\n",
      "Original size (fp16): 2.20GB\n",
      "Quantized size (4-bit): 0.72GB\n",
      "Compression ratio: 3.1x\n",
      "Size reduction: 67.4%\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.configs import ModelParams, QuantizationConfig  # type: ignore\n",
    "from oumi.quantize import quantize  # type: ignore\n",
    "\n",
    "# Configure quantization\n",
    "config = QuantizationConfig(\n",
    "    model=ModelParams(model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"),\n",
    "    method=\"awq_q4_0\",  # 4-bit AWQ quantization\n",
    "    output_format=\"pytorch\",\n",
    "    output_path=\"tinyllama_awq_4bit_tutorial\",\n",
    "    calibration_samples=32,  # Number of calibration samples\n",
    "    # 32 for fast testing, 1024 for better accuracy\n",
    ")\n",
    "\n",
    "# Run quantization\n",
    "print(\"Starting AWQ quantization...\")\n",
    "result = quantize(config)\n",
    "\n",
    "# Calculate sizes and compression\n",
    "original_size_gb = 2.2  # TinyLlama 1.1B in fp16\n",
    "quantized_size_gb = result.quantized_size_bytes / (1024**3)  # type: ignore\n",
    "compression_ratio = original_size_gb / quantized_size_gb\n",
    "\n",
    "print(\"\\n‚úÖ Quantization complete!\")\n",
    "print(f\"Original size (fp16): {original_size_gb:.2f}GB\")\n",
    "print(f\"Quantized size (4-bit): {quantized_size_gb:.2f}GB\")\n",
    "print(f\"Compression ratio: {compression_ratio:.1f}x\")\n",
    "size_reduction_pct = (original_size_gb - quantized_size_gb) / original_size_gb * 100\n",
    "print(f\"Size reduction: {size_reduction_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the Quantized Model\n",
    "\n",
    "Now let's load and use the quantized model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AWQ model from: tinyllama_awq_4bit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:03<00:00,  5.96it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ac21b4eed34f6a889a71fff7776b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded! GPU memory: 0.03GB\n"
     ]
    }
   ],
   "source": [
    "import torch  # type: ignore\n",
    "from awq import AutoAWQForCausalLM  # type: ignore\n",
    "from transformers import AutoTokenizer  # type: ignore\n",
    "\n",
    "# Load the quantized model\n",
    "model_path = \"tinyllama_awq_4bit\"\n",
    "\n",
    "print(f\"Loading AWQ model from: {model_path}\")\n",
    "model = AutoAWQForCausalLM.from_quantized(\n",
    "    model_path,\n",
    "    fuse_layers=False,  # Disable layer fusion to avoid compatibility issues\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Model loaded! GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain the benefits of model quantization in simple terms:\n",
      "\n",
      "Generating response...\n",
      "Response:\n",
      "Explain the benefits of model quantization in simple terms: A model quantized to a lower bitwidth can achieve higher efficiency and lower power consumption compared to a model with the same parameters trained on the original data. This is because the model is compressed into a smaller space that can be processed in a faster, lower-power processor. Additionally, model quantization can be used to reduce the number of weights or parameters in a model, which can also improve its efficiency. This is because fewer weights need to be updated during inference, reducing the number of computations required to achieve the same accuracy. Overall, model quantization can help to improve the efficiency, power consumption, and accuracy of deep learning models.\n"
     ]
    }
   ],
   "source": [
    "# Test inference\n",
    "prompt = \"Explain the benefits of model quantization in simple terms:\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Generating response...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "# Decode and print response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Configuration\n",
    "\n",
    "AWQ offers several configuration options for fine-tuning the quantization process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "- Output format: safetensors\n",
      "- Calibration samples: 1024\n",
      "- Group size: 128\n",
      "- AWQ version: GEMM\n",
      "- Zero point: True\n"
     ]
    }
   ],
   "source": [
    "# Advanced AWQ configuration with more calibration samples\n",
    "advanced_config = QuantizationConfig(\n",
    "    model=ModelParams(model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"),\n",
    "    method=\"awq_q4_0\",\n",
    "    output_path=\"tinyllama_awq_advanced.safetensors\",\n",
    "    output_format=\"safetensors\",  # Use SafeTensors format\n",
    "    # AWQ-specific parameters\n",
    "    calibration_samples=1024,  # More samples for better calibration\n",
    "    awq_group_size=128,  # Weight grouping size\n",
    "    awq_version=\"GEMM\",  # AWQ kernel version (GEMM is faster)\n",
    "    awq_zero_point=True,  # Use zero-point quantization\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"- Output format: {advanced_config.output_format}\")\n",
    "print(f\"- Calibration samples: {advanced_config.calibration_samples}\")\n",
    "print(f\"- Group size: {advanced_config.awq_group_size}\")\n",
    "print(f\"- AWQ version: {advanced_config.awq_version}\")\n",
    "print(f\"- Zero point: {advanced_config.awq_zero_point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "\n",
    "1. ‚úÖ Quantize models using AWQ to 4-bit precision\n",
    "2. ‚úÖ Load and use AWQ quantized models for inference\n",
    "3. ‚úÖ Configure AWQ parameters for better quality\n",
    "\n",
    "\n",
    "### Key Benefits of AWQ:\n",
    "- **Memory Efficiency**: ~75% reduction in model size\n",
    "- **Speed**: Faster inference due to reduced memory bandwidth\n",
    "- **Quality**: Minimal performance degradation\n",
    "- **Compatibility**: Works with most transformer models\n",
    "\n",
    "Happy quantizing! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi-latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
