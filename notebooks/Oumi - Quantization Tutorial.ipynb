{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<div class=\"align-center\">\n<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n\n[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - Quantization Tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n</div>\n\nüëã Welcome to Open Universal Machine Intelligence (Oumi)!\n\nüöÄ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](https://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n\nü§ù Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n\n‚≠ê If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi).\n\n# Model Quantization Tutorial\n\nThis tutorial demonstrates how to use llm_compressor to compress large language models using AWQ (Activation-aware Weight Quantization) and other quantization methods while maintaining performance.\n\n## Prerequisites\n\n‚ùó**NOTICE:** Model quantization requires a GPU. If running on Google Colab, you must use a GPU runtime (Colab Menu: `Runtime` -> `Change runtime type` -> Select `T4 GPU` or better).\n\n‚ö†Ô∏è **DEVELOPMENT STATUS**: The quantization feature is currently under active development. Some features may change in future releases.\n\nFirst, let's install Oumi with GPU support and the required quantization libraries:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%pip install oumi[gpu,quantization]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Basic Quantization with llm_compressor\n\nLet's start by quantizing TinyLlama to 4-bit using llm_compressor's W4A16 (4-bit weights, 16-bit activations) scheme:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from oumi.core.configs import ModelParams, QuantizationConfig  # type: ignore\nfrom oumi.quantize import quantize  # type: ignore\n\n# Configure quantization\nconfig = QuantizationConfig(\n    model=ModelParams(model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"),\n    method=\"llmc_W4A16_ASYM\",  # 4-bit asymmetric AWQ quantization\n    output_path=\"tinyllama_w4a16_tutorial\",\n    calibration_samples=32,  # Number of calibration samples\n    calibration_dataset=\"open_platypus\",  # Dataset for calibration\n    # 32 for fast testing, 512 for better accuracy\n)\n\n# Run quantization\nprint(\"Starting llm_compressor quantization...\")\nresult = quantize(config)\n\n# Calculate sizes and compression\noriginal_size_gb = 2.2  # TinyLlama 1.1B in fp16\nquantized_size_gb = result.quantized_size_bytes / (1024**3)  # type: ignore\ncompression_ratio = original_size_gb / quantized_size_gb\n\nprint(\"\\n‚úÖ Quantization complete!\")\nprint(f\"Original size (fp16): {original_size_gb:.2f}GB\")\nprint(f\"Quantized size (4-bit): {quantized_size_gb:.2f}GB\")\nprint(f\"Compression ratio: {compression_ratio:.1f}x\")\nsize_reduction_pct = (original_size_gb - quantized_size_gb) / original_size_gb * 100\nprint(f\"Size reduction: {size_reduction_pct:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Using the Quantized Model\n\nNow let's load and use the quantized model for inference. Models quantized with llm_compressor can be loaded directly with transformers:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch  # type: ignore\nfrom transformers import AutoModelForCausalLM, AutoTokenizer  # type: ignore\n\n# Load the quantized model\nmodel_path = \"tinyllama_w4a16_tutorial\"\n\nprint(f\"Loading quantized model from: {model_path}\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"‚úÖ Model loaded! GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f}GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain the benefits of model quantization in simple terms:\n",
      "\n",
      "Generating response...\n",
      "Response:\n",
      "Explain the benefits of model quantization in simple terms:\n",
      "\n",
      "Model quantization is the process of compressing a neural network model into a smaller number of parameters without sacrificing performance. Here are some benefits of model quantization:\n",
      "\n",
      "1. Improved Model Size: Model quantization reduces the model's size, which can be beneficial for storage and transmission.\n",
      "\n",
      "2. Faster Training: Model quantization can lead to faster training, especially for smaller models.\n",
      "\n",
      "3. Reduced Inference Time: With less parameters, inference time can be reduced, leading to faster and more accurate inference.\n",
      "\n",
      "4. Improved Convergence: Model quantization can improve convergence, as the model's parameters are more accurately represented and optimized.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test inference\n",
    "prompt = \"Explain the benefits of model quantization in simple terms:\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Generating response...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "# Decode and print response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Advanced Configuration\n\nllm_compressor offers several configuration options for fine-tuning the quantization process:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Advanced llm_compressor configuration with more calibration samples\nadvanced_config = QuantizationConfig(\n    model=ModelParams(model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"),\n    method=\"llmc_W4A16_ASYM\",  # Can also use llmc_W8A8_INT, llmc_W8A8_FP8, etc.\n    output_path=\"tinyllama_advanced\",\n    output_format=\"safetensors\",  # Use SafeTensors format\n    # llm_compressor-specific parameters\n    calibration_samples=512,  # More samples for better calibration\n    calibration_dataset=\"open_platypus\",  # Dataset for calibration\n    max_seq_length=2048,  # Maximum sequence length for calibration\n    llmc_group_size=128,  # Weight grouping size\n    llmc_targets=[\"Linear\"],  # Target layer types\n    llmc_ignore=[\"lm_head\"],  # Layers to exclude from quantization\n)\n\nprint(\"Configuration:\")\nprint(f\"- Method: {advanced_config.method}\")\nprint(f\"- Output format: {advanced_config.output_format}\")\nprint(f\"- Calibration samples: {advanced_config.calibration_samples}\")\nprint(f\"- Calibration dataset: {advanced_config.calibration_dataset}\")\nprint(f\"- Group size: {advanced_config.llmc_group_size}\")\nprint(f\"- Target layers: {advanced_config.llmc_targets}\")\nprint(f\"- Ignored layers: {advanced_config.llmc_ignore}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nIn this tutorial, you learned how to:\n\n1. ‚úÖ Quantize models using llm_compressor to 4-bit precision (W4A16)\n2. ‚úÖ Load and use quantized models for inference with transformers\n3. ‚úÖ Configure llm_compressor parameters for better quality\n\n### Available Quantization Methods:\n- **llmc_W4A16 / llmc_W4A16_ASYM**: 4-bit weights, 16-bit activations (AWQ)\n- **llmc_W8A16**: 8-bit weights, 16-bit activations (GPTQ)\n- **llmc_W8A8_INT**: 8-bit weights and activations (INT8 with SmoothQuant)\n- **llmc_W8A8_FP8**: 8-bit weights and activations (FP8)\n- **llmc_FP8_BLOCK**: FP8 block quantization\n\n### Key Benefits:\n- **Memory Efficiency**: ~75% reduction in model size with W4A16\n- **Speed**: Faster inference due to reduced memory bandwidth\n- **Quality**: Minimal performance degradation with calibration\n- **Compatibility**: Works with most transformer models via vLLM\n\nHappy quantizing! üöÄ"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß≠ What's Next?\n",
    "\n",
    "Congrats on finishing this notebook! Feel free to check out our other [notebooks](https://github.com/oumi-ai/oumi/tree/main/notebooks) in the [Oumi GitHub](https://github.com/oumi-ai/oumi), and give us a star! You can also join the Oumi community over on [Discord](https://discord.gg/oumi).\n",
    "\n",
    "üì∞ Want to keep up with news from Oumi? Subscribe to our [Substack](https://blog.oumi.ai/) and [Youtube](https://www.youtube.com/@Oumi_AI)!\n",
    "\n",
    "‚ö° Interested in building custom AI in hours, not months? Apply to get [early access](https://oumi-ai.typeform.com/early-access) to the Oumi Platform, or [chat with us](https://calendly.com/d/ctcx-nps-47m/chat-with-us-get-early-access-to-the-oumi-platform) to learn more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi-latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}