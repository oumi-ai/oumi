{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation with Alpaca Eval 2.0\n",
    "\n",
    "This notebook discusses how you can run E2E evaluations for your trained model, using Oumi inference for generating the responses, and [Alpaca Eval 2.0](https://github.com/tatsu-lab/alpaca_eval) for automatically calculating the win-rates vs. GPT4 Turbo (or other reference models of your choice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Configuration\n",
    "\n",
    "First, start by installing the [Alpaca Eval package](https://pypi.org/project/alpaca-eval/). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet alpaca_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing your model's responses vs. the reference responses to calculate the win rates, an annotator (judge) is needed. By default, the annotator is set to GPT4 Turbo (annotator config: [weighted_alpaca_eval_gpt4_turbo](https://github.com/tatsu-lab/alpaca_eval?tab=readme-ov-file#alpacaeval-20)). To access the latest GPT-4 models, including GPT4 Turbo, an Open API key is required. Details on creating an OpenAI account and generating a key can be found at [OpenAI's quickstart webpage](https://platform.openai.com/docs/quickstart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # Set your OpenAI API key here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>⚠️ Cost considerations</b>: The cost of running a standard Alpaca evaluation 2.0 (with [weighted_alpaca_eval_gpt4_turbo](https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/evaluators_configs/README.md) config) and annotating 805 examples with GPT4 Turbo is <b>$3.5</b>. However, the sample code of this notebook only annotates 3 (of 805 examples) and costs less than <b>0.5¢</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 3  # Replace with 805 for full dataset evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your model and the max number of tokens it supports (to be used during generation). You can point to any model in HuggingFace, provide a path to a local folder that contains your model, or any other model format that Oumi inference supports. Also, please provide a (human friendly) display name for your model, to be used when displayed in leaderboards. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bartowski/Llama-3.2-1B-Instruct-GGUF\"\n",
    "MODEL_DISPLAY_NAME = \"MyLlamaTestModel\"\n",
    "MODEL_MAX_TOKENS = 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Retrieve Alpaca dataset\n",
    "\n",
    "Alpaca Eval 2.0 requires model responses for the [tatsu-lab/alpaca_eval](https://huggingface.co/datasets/tatsu-lab/alpaca_eval) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-10 12:15:21 _custom_ops.py:19] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "INFO 12-10 12:15:21 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-10 12:15:22,342][oumi][rank0][pid:3121][MainThread][INFO]][base_map_dataset.py:68] Creating map dataset (type: AlpacaEvalDataset)...\n",
      "[2024-12-10 12:15:23,349][oumi][rank0][pid:3121][MainThread][INFO]][base_map_dataset.py:437] Dataset Info:\n",
      "\tSplit: eval\n",
      "\tVersion: 1.0.0\n",
      "\tDataset size: 554496\n",
      "\tDownload size: 620778\n",
      "\tSize: 1175274 bytes\n",
      "\tRows: 805\n",
      "\tColumns: ['instruction', 'output', 'generator', 'dataset']\n",
      "[2024-12-10 12:15:23,466][oumi][rank0][pid:3121][MainThread][INFO]][base_map_dataset.py:375] Loaded DataFrame with shape: (805, 4). Columns:\n",
      "instruction    object\n",
      "output         object\n",
      "generator      object\n",
      "dataset        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from oumi.datasets.evaluation import AlpacaEvalDataset\n",
    "\n",
    "alpaca_dataset = AlpacaEvalDataset(dataset_name=\"tatsu-lab/alpaca_eval\").conversations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this notebook contains sample code, we will only run inference for the first `NUM_EXAMPLES` (of 805) from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [USER: What are the names of some famous actors that started their careers on Broadway?]\n",
      "1 [USER: How did US states get their names?]\n",
      "2 [USER: Hi, my sister and her girlfriends want me to play kickball with them. Can you explain how the game is played, so they don't take advantage of me?]\n"
     ]
    }
   ],
   "source": [
    "alpaca_dataset = alpaca_dataset[:NUM_EXAMPLES]  # For testing purposes, reduce examples.\n",
    "\n",
    "for index, conversation in enumerate(alpaca_dataset):\n",
    "    print(index, conversation.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run inference\n",
    "\n",
    "First, define all the relevant parameters and configs required for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.core.configs import GenerationParams, InferenceConfig, ModelParams\n",
    "\n",
    "generation_params = GenerationParams(max_new_tokens=MODEL_MAX_TOKENS)\n",
    "model_params = ModelParams(model_name=MODEL_NAME, model_max_length=MODEL_MAX_TOKENS)\n",
    "inference_config = InferenceConfig(model=model_params, generation=generation_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, choose an inference engine that your model is compatible with. For more information on this, see Oumi's [inference documentation](https://oumi.ai/docs/latest/user_guides/infer/infer.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-10 12:15:30,775][oumi][rank0][pid:3121][MainThread][INFO]][llama_cpp_inference_engine.py:117] Loading model from Huggingface Hub: bartowski/Llama-3.2-1B-Instruct-GGUF.\n"
     ]
    }
   ],
   "source": [
    "from oumi.inference import LlamaCppInferenceEngine\n",
    "\n",
    "inference_engine = LlamaCppInferenceEngine(model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run inference to get responses from your model for the prompts contained in the `alpaca_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3c833b9e0b427585255129344fa192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "responses = inference_engine.infer(alpaca_dataset, inference_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, convert the responses from Oumi format (list of `Conversation`s) to Alpaca format (list of `dict`s, where the data is contained under the keys `instruction` and `output`). Create a DataFrame from the data and add a new column \"`generator`\", which captures the human-readable name of the model the responses were produced with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from oumi.datasets.evaluation import utils\n",
    "\n",
    "responses_json = utils.conversations_to_alpaca_format(responses)\n",
    "responses_df = pd.DataFrame(responses_json)\n",
    "responses_df[\"generator\"] = MODEL_DISPLAY_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your DataFrame should look as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "      <th>generator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the names of some famous actors that ...</td>\n",
       "      <td>There are many famous actors who started their...</td>\n",
       "      <td>MyLlamaTestModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How did US states get their names?</td>\n",
       "      <td>The origin of US state names is a fascinating ...</td>\n",
       "      <td>MyLlamaTestModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi, my sister and her girlfriends want me to p...</td>\n",
       "      <td>Kickball is a fun team sport that's easy to le...</td>\n",
       "      <td>MyLlamaTestModel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  What are the names of some famous actors that ...   \n",
       "1                 How did US states get their names?   \n",
       "2  Hi, my sister and her girlfriends want me to p...   \n",
       "\n",
       "                                              output         generator  \n",
       "0  There are many famous actors who started their...  MyLlamaTestModel  \n",
       "1  The origin of US state names is a fascinating ...  MyLlamaTestModel  \n",
       "2  Kickball is a fun team sport that's easy to le...  MyLlamaTestModel  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Alpaca Eval 2.0\n",
    "\n",
    "You can kick off evaluations as shown below. \n",
    "\n",
    "The default annotator for Alpaca Eval 2.0 is <b>GPT-4 Turbo</b>. While Alpaca Eval 1.0 was using a binary preference, Alpaca Eval 2.0 uses the logprobs to compute a continuous preference, resulting in a <b>weighted</b> win-rate. The default annotator config of Alpaca Eval 2.0 is thus `weighted_alpaca_eval_gpt4_turbo`. There is an option to use other annotators (judges) as well; see the [Annotators configs](https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/evaluators_configs/README.md) page for details and relevant costs. However, the Alpaca 2.0 leaderboard is established with GPT4 Turbo as the reference annotator. Using other annotators is less informative if you are interested in generating comparative results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10:12:15:56,683 INFO     [main.py:136] Evaluating the MyLlamaTestModel outputs.\n",
      "2024-12-10:12:15:56,683 INFO     [base.py:104] Creating the annotator from `weighted_alpaca_eval_gpt4_turbo`.\n",
      "2024-12-10:12:15:56,687 INFO     [base.py:521] Saving annotations to `/opt/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json`.\n",
      "2024-12-10:12:15:56,687 INFO     [base.py:513] Loading all annotations from /opt/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "2024-12-10:12:15:56,744 WARNING  [pairwise_evaluator.py:231] The length of outputs before and after merge are not the same. We have len(outputs_1)==805, len(outputs_2)==3, and len(df_annotated)==3. This means that there are missing examples or duplicates. We are taking a SQL inner join.\n",
      "Annotation chunk:   0%|          | 0/1 [00:00<?, ?it/s]2024-12-10:12:15:56,757 INFO     [base.py:303] Annotating 0 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "2024-12-10:12:15:56,789 INFO     [base.py:500] Saving all annotations to /opt/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "2024-12-10:12:15:56,790 INFO     [base.py:513] Loading all annotations from /opt/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "Annotation chunk: 100%|██████████| 1/1 [00:00<00:00,  5.50it/s]\n",
      "2024-12-10:12:15:57,070 INFO     [main.py:192] Saving all results to results/MyLlamaTestModel/weighted_alpaca_eval_gpt4_turbo\n",
      "2024-12-10:12:15:57,072 INFO     [main.py:204] Saving result to the precomputed leaderboard at /opt/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/leaderboards/data_AlpacaEval_2/weighted_alpaca_eval_gpt4_turbo_leaderboard.csv\n"
     ]
    }
   ],
   "source": [
    "from alpaca_eval import evaluate\n",
    "\n",
    "ANNOTATORS_CONFIG = \"weighted_alpaca_eval_gpt4_turbo\"\n",
    "\n",
    "df_leaderboard, annotations = evaluate(\n",
    "    model_outputs=responses_df,\n",
    "    annotators_config=ANNOTATORS_CONFIG,\n",
    "    is_return_instead_of_print=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inspect the metrics\n",
    "\n",
    "Once the evaluation process completes, you can inspect the metrics produced, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for `MyLlamaTestModel`\n",
      " - win_rate=0.0331325600000006\n",
      " - standard_error=0.02389778485055507\n",
      " - n_wins=0\n",
      " - n_wins_base=3\n",
      " - n_draws=0\n",
      " - n_total=3\n",
      " - discrete_win_rate=0.0\n",
      " - mode=community\n",
      " - avg_length=2208\n",
      " - length_controlled_winrate=0.16796629807158028\n",
      " - lc_standard_error=0.044130529121658255\n"
     ]
    }
   ],
   "source": [
    "metrics = df_leaderboard.loc[MODEL_DISPLAY_NAME]\n",
    "\n",
    "print(f\"Metrics for `{MODEL_DISPLAY_NAME}`\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\" - {metric}={value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Retain your configuration for reproducibility\n",
    "\n",
    "In order to be able to repro your evaluation run in the future, do not forget to save the configuration of your evaluation, together with your evaluation metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from importlib.metadata import version\n",
    "\n",
    "evaluation_config_dict = {\n",
    "    \"packages\": {\n",
    "        \"alpaca_eval\": version(\"alpaca_eval\"),\n",
    "        \"oumi\": version(\"oumi\"),\n",
    "    },\n",
    "    \"configs\": {\n",
    "        \"inference_config\": str(inference_config),\n",
    "        \"annotators_config\": ANNOTATORS_CONFIG,\n",
    "    },\n",
    "    \"eval_metrics\": metrics.to_dict(),\n",
    "}\n",
    "\n",
    "evaluation_config_json = json.dumps(evaluation_config_dict, indent=2)\n",
    "with open(\"./output/evaluation_config.json\", \"w\") as output_file:\n",
    "    output_file.write(evaluation_config_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
