{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook discusses how you can run E2E evaluations for your trained model, using Oumi inference for generating the responses, and [Alpaca Eval 2.0](https://github.com/tatsu-lab/alpaca_eval) for automatically calculating the win-rates vs. GPT4 Turbo (or other reference models of your choice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, start by pointing to your model and the max tokens it supports (to be used during generation). You can point to a model in HuggingFace, provide a path to a local folder that contains your model, or any other model format that Oumi inference supports. Also, provide a \"human friendly\" name for your model, to be used when displayed in leaderboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"bartowski/Llama-3.2-1B-Instruct-GGUF\"\n",
    "MODEL_HUMAN_FRIENDLY_NAME = \"MyLlamaModel\"\n",
    "MODEL_MAX_TOKENS = 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Retrieve Alpaca dataset\n",
    "\n",
    "Alpaca Eval 2.0 requires that we run inference and produce model responses for the [tatsu-lab/alpaca_eval](https://huggingface.co/datasets/tatsu-lab/alpaca_eval) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-06 17:47:53 _custom_ops.py:19] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "INFO 12-06 17:47:53 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-06 17:47:54,350][oumi][rank0][pid:41039][MainThread][INFO]][base_map_dataset.py:68] Creating map dataset (type: AlpacaEvalDataset)...\n",
      "[2024-12-06 17:47:55,415][oumi][rank0][pid:41039][MainThread][INFO]][base_map_dataset.py:437] Dataset Info:\n",
      "\tSplit: eval\n",
      "\tVersion: 1.0.0\n",
      "\tDataset size: 554496\n",
      "\tDownload size: 620778\n",
      "\tSize: 1175274 bytes\n",
      "\tRows: 805\n",
      "\tColumns: ['instruction', 'output', 'generator', 'dataset']\n",
      "[2024-12-06 17:47:55,540][oumi][rank0][pid:41039][MainThread][INFO]][base_map_dataset.py:375] Loaded DataFrame with shape: (805, 4). Columns:\n",
      "instruction    object\n",
      "output         object\n",
      "generator      object\n",
      "dataset        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from oumi.datasets.evaluation import AlpacaEvalDataset\n",
    "\n",
    "alpaca_dataset = AlpacaEvalDataset(dataset_name=\"tatsu-lab/alpaca_eval\").conversations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is sample code, we will only run inference on the first 3 examples of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [USER: What are the names of some famous actors that started their careers on Broadway?]\n",
      "1 [USER: How did US states get their names?]\n",
      "2 [USER: Hi, my sister and her girlfriends want me to play kickball with them. Can you explain how the game is played, so they don't take advantage of me?]\n"
     ]
    }
   ],
   "source": [
    "alpaca_dataset = alpaca_dataset[:3]\n",
    "\n",
    "for index, conversation in enumerate(alpaca_dataset):\n",
    "    print(index, conversation.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run inference\n",
    "\n",
    "First, define all the relevant parameters and configs required for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.core.configs import GenerationParams, InferenceConfig, ModelParams\n",
    "\n",
    "generation_params = GenerationParams(max_new_tokens=MODEL_MAX_TOKENS)\n",
    "model_params = ModelParams(model_name=MODEL, model_max_length=MODEL_MAX_TOKENS)\n",
    "inference_config = InferenceConfig(model=model_params, generation=generation_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, choose an inference engine that your model is compatible with. For more information on this, see Oumi's [inference documentation](https://oumi.ai/docs/latest/user_guides/infer/infer.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-06 17:48:30,999][oumi][rank0][pid:41039][MainThread][INFO]][llama_cpp_inference_engine.py:117] Loading model from Huggingface Hub: bartowski/Llama-3.2-1B-Instruct-GGUF.\n"
     ]
    }
   ],
   "source": [
    "from oumi.inference import LlamaCppInferenceEngine\n",
    "\n",
    "inference_engine = LlamaCppInferenceEngine(model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run inference to get responses from your model for the prompts contained in the `alpaca_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf9b305066a4caf8e10eb7462f43b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "responses = inference_engine.infer(alpaca_dataset, inference_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, convert the responses from Oumi format (list of `Conversation`s) to Alpaca format (list of `dict`s, where the data is contained under the keys `instruction` and `output`). Create a DataFrame from the data and add a new column \"`generator`\", which captures the human-readable name of the model the responses were produced with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from oumi.datasets.evaluation import utils\n",
    "\n",
    "responses_json = utils.conversations_to_alpaca_format(responses)\n",
    "responses_df = pd.DataFrame(responses_json)\n",
    "responses_df[\"generator\"] = MODEL_HUMAN_FRIENDLY_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your DataFrame should look as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "      <th>generator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the names of some famous actors that ...</td>\n",
       "      <td>There are many famous actors who started their...</td>\n",
       "      <td>MyLlamaModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How did US states get their names?</td>\n",
       "      <td>The origin of US state names is a fascinating ...</td>\n",
       "      <td>MyLlamaModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi, my sister and her girlfriends want me to p...</td>\n",
       "      <td>Kickball is a fun team sport that's easy to le...</td>\n",
       "      <td>MyLlamaModel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  What are the names of some famous actors that ...   \n",
       "1                 How did US states get their names?   \n",
       "2  Hi, my sister and her girlfriends want me to p...   \n",
       "\n",
       "                                              output     generator  \n",
       "0  There are many famous actors who started their...  MyLlamaModel  \n",
       "1  The origin of US state names is a fascinating ...  MyLlamaModel  \n",
       "2  Kickball is a fun team sport that's easy to le...  MyLlamaModel  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Alpaca Eval 2.0\n",
    "\n",
    "To compare your responses vs. Alpaca Eval 2.0's reference responses (generated with GPT4 Turbo) and calculate the win rates, you need a judge. By default, the judge is GPT4 Turbo, but there is an option to use other judges too. Note that the Alpaca leaderboard is established based on GPT4 Turbo as a judge, so using other judges is less informative, if you are interested in inspecting comparative results. To access the latest GPT-4 models, an Open API key is required. Details on creating an OpenAI account and generating a key can be found at [Open AI's quickstart webpage](https://platform.openai.com/docs/quickstart). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Open AI key is: your_api_key_here\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_api_key_here\"\n",
    "print(f\"Your Open AI key is: {os.environ.get('OPENAI_API_KEY')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, if you have not already installed the Alpaca Eval package, you need to do so at this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet alpaca_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once Alpaca Eval is installed, you can run the evaluation as follows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** NOTES to self (to delete) ***\n",
    "\n",
    "1) Defaults\n",
    "    - reference_outputs: gpt4_turbo outputs retrieved from \"tatsu-lab/alpaca_eval\"\n",
    "    - annotators_config: weighted_alpaca_eval_gpt4_turbo\n",
    "    - is_avoid_reannotations: False\n",
    "    - is_overwrite_leaderboard: False\n",
    "    - output_path: None / auto\n",
    "2) TODOs\n",
    "    - Should we consider other judges that are for free? Does Alpaca allow this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06:17:51:26,884 INFO     [main.py:136] Evaluating the MyLlamaModel outputs.\n",
      "2024-12-06:17:51:26,884 INFO     [base.py:104] Creating the annotator from `weighted_alpaca_eval_gpt4_turbo`.\n",
      "2024-12-06:17:51:26,887 INFO     [base.py:521] Saving annotations to `/opt/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json`.\n",
      "2024-12-06:17:51:26,887 INFO     [base.py:513] Loading all annotations from /opt/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "2024-12-06:17:51:26,939 WARNING  [pairwise_evaluator.py:231] The length of outputs before and after merge are not the same. We have len(outputs_1)==805, len(outputs_2)==3, and len(df_annotated)==3. This means that there are missing examples or duplicates. We are taking a SQL inner join.\n",
      "Annotation chunk:   0%|          | 0/1 [00:00<?, ?it/s]2024-12-06:17:51:26,951 INFO     [base.py:303] Annotating 0 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "2024-12-06:17:51:26,988 INFO     [base.py:500] Saving all annotations to /opt/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "2024-12-06:17:51:26,989 INFO     [base.py:513] Loading all annotations from /opt/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "Annotation chunk: 100%|██████████| 1/1 [00:00<00:00,  5.69it/s]\n",
      "2024-12-06:17:51:27,271 INFO     [main.py:192] Saving all results to results/MyLlamaModel/weighted_alpaca_eval_gpt4_turbo\n",
      "2024-12-06:17:51:27,273 INFO     [main.py:204] Saving result to the precomputed leaderboard at /opt/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/leaderboards/data_AlpacaEval_2/weighted_alpaca_eval_gpt4_turbo_leaderboard.csv\n"
     ]
    }
   ],
   "source": [
    "from alpaca_eval import evaluate\n",
    "\n",
    "df_leaderboard, annotations = evaluate(\n",
    "    model_outputs=responses_df,\n",
    "    is_return_instead_of_print=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inspect the metrics\n",
    "\n",
    "Once the evaluation process completes, you can inspect the metrics produced, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for `MyLlamaModel`\n",
      " - win_rate=0.0331325600000006\n",
      " - standard_error=0.02389778485055507\n",
      " - n_wins=0\n",
      " - n_wins_base=3\n",
      " - n_draws=0\n",
      " - n_total=3\n",
      " - discrete_win_rate=0.0\n",
      " - mode=community\n",
      " - avg_length=2208\n",
      " - length_controlled_winrate=0.16796629807158028\n",
      " - lc_standard_error=0.044130529121658255\n"
     ]
    }
   ],
   "source": [
    "metrics = df_leaderboard.loc[MODEL_HUMAN_FRIENDLY_NAME]\n",
    "\n",
    "print(f\"Metrics for `{MODEL_HUMAN_FRIENDLY_NAME}`\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\" - {metric}={value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
