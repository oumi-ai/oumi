{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation with Alpaca Eval 2.0\n",
    "\n",
    "This notebook discusses how you can run E2E evaluations for your trained model, using Oumi inference for generating the responses, and [Alpaca Eval 2.0](https://github.com/tatsu-lab/alpaca_eval) for automatically calculating the win-rates vs. GPT4 Turbo (or other reference models of your choice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Configuration\n",
    "\n",
    "First, start by installing the [Alpaca Eval package](https://pypi.org/project/alpaca-eval/). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U -q alpaca_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing your model's responses vs. the reference responses to calculate the win rates, an annotator (judge) is needed. By default, the annotator is set to GPT4 Turbo (annotator config: [weighted_alpaca_eval_gpt4_turbo](https://github.com/tatsu-lab/alpaca_eval?tab=readme-ov-file#alpacaeval-20)). To access the latest GPT-4 models, including GPT4 Turbo, an Open API key is required. Details on creating an OpenAI account and generating a key can be found at [OpenAI's quickstart webpage](https://platform.openai.com/docs/quickstart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # Set your OpenAI API key here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>⚠️ Cost considerations</b>: The cost of running a standard Alpaca evaluation 2.0 (with [weighted_alpaca_eval_gpt4_turbo](https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/evaluators_configs/README.md) config) and annotating 805 examples with GPT4 Turbo is <b>$3.5</b>. However, the sample code of this notebook only annotates 3 (of 805 examples) and costs less than <b>0.5¢</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 3  # Replace with 805 for full dataset evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your model and the max number of tokens it supports (to be used during generation). You can point to any model in HuggingFace, provide a path to a local folder that contains your model, or any other model format that Oumi inference supports. Also, please provide a (human friendly) display name for your model, to be used when displayed in leaderboards. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bartowski/Llama-3.2-1B-Instruct-GGUF\"\n",
    "MODEL_DISPLAY_NAME = \"MyLlamaTestModel\"\n",
    "MODEL_MAX_TOKENS = 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Retrieve Alpaca dataset\n",
    "\n",
    "Alpaca Eval 2.0 requires model responses for the [tatsu-lab/alpaca_eval](https://huggingface.co/datasets/tatsu-lab/alpaca_eval) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.datasets.evaluation import AlpacaEvalDataset\n",
    "\n",
    "alpaca_dataset = AlpacaEvalDataset(dataset_name=\"tatsu-lab/alpaca_eval\").conversations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this notebook contains sample code, we will only run inference for the first `NUM_EXAMPLES` (of 805) from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_dataset = alpaca_dataset[:NUM_EXAMPLES]  # For testing purposes, reduce examples.\n",
    "\n",
    "for index, conversation in enumerate(alpaca_dataset):\n",
    "    print(index, conversation.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run inference\n",
    "\n",
    "First, define all the relevant parameters and configs required for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.core.configs import GenerationParams, InferenceConfig, ModelParams\n",
    "\n",
    "generation_params = GenerationParams(max_new_tokens=MODEL_MAX_TOKENS)\n",
    "model_params = ModelParams(model_name=MODEL_NAME, model_max_length=MODEL_MAX_TOKENS)\n",
    "inference_config = InferenceConfig(model=model_params, generation=generation_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, choose an inference engine that your model is compatible with. For more information on this, see Oumi's [inference documentation](https://oumi.ai/docs/latest/user_guides/infer/infer.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.inference import LlamaCppInferenceEngine\n",
    "\n",
    "inference_engine = LlamaCppInferenceEngine(model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run inference to get responses from your model for the prompts contained in the `alpaca_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = inference_engine.infer(alpaca_dataset, inference_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, convert the responses from Oumi format (list of `Conversation`s) to Alpaca format (list of `dict`s, where the data is contained under the keys `instruction` and `output`). Create a DataFrame from the data and add a new column \"`generator`\", which captures the human-readable name of the model the responses were produced with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from oumi.datasets.evaluation import utils\n",
    "\n",
    "responses_json = utils.conversations_to_alpaca_format(responses)\n",
    "responses_df = pd.DataFrame(responses_json)\n",
    "responses_df[\"generator\"] = MODEL_DISPLAY_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your DataFrame should look as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Alpaca Eval 2.0\n",
    "\n",
    "You can kick off evaluations as shown below. \n",
    "\n",
    "The default annotator for Alpaca Eval 2.0 is <b>GPT-4 Turbo</b>. While Alpaca Eval 1.0 was using a binary preference, Alpaca Eval 2.0 uses the logprobs to compute a continuous preference, resulting in a <b>weighted</b> win-rate. The default annotator config of Alpaca Eval 2.0 is thus `weighted_alpaca_eval_gpt4_turbo`. There is an option to use other annotators (judges) as well; see the [Annotators configs](https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/evaluators_configs/README.md) page for details and relevant costs. However, the Alpaca 2.0 leaderboard is established with GPT4 Turbo as the reference annotator. Using other annotators is less informative if you are interested in generating comparative results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca_eval import evaluate\n",
    "\n",
    "ANNOTATORS_CONFIG = \"weighted_alpaca_eval_gpt4_turbo\"\n",
    "\n",
    "df_leaderboard, annotations = evaluate(\n",
    "    model_outputs=responses_df,\n",
    "    annotators_config=ANNOTATORS_CONFIG,\n",
    "    is_return_instead_of_print=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inspect the metrics\n",
    "\n",
    "Once the evaluation process completes, you can inspect the metrics produced, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = df_leaderboard.loc[MODEL_DISPLAY_NAME]\n",
    "\n",
    "print(f\"Metrics for `{MODEL_DISPLAY_NAME}`\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\" - {metric}={value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Retain your configuration for reproducibility\n",
    "\n",
    "In order to be able to repro your evaluation run in the future, do not forget to save the configuration of your evaluation, together with your evaluation metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from importlib.metadata import version\n",
    "\n",
    "evaluation_config_dict = {\n",
    "    \"packages\": {\n",
    "        \"alpaca_eval\": version(\"alpaca_eval\"),\n",
    "        \"oumi\": version(\"oumi\"),\n",
    "    },\n",
    "    \"configs\": {\n",
    "        \"inference_config\": str(inference_config),\n",
    "        \"annotators_config\": ANNOTATORS_CONFIG,\n",
    "    },\n",
    "    \"eval_metrics\": metrics.to_dict(),\n",
    "}\n",
    "\n",
    "evaluation_config_json = json.dumps(evaluation_config_dict, indent=2)\n",
    "with open(\"./output/evaluation_config.json\", \"w\") as output_file:\n",
    "    output_file.write(evaluation_config_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
