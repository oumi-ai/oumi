{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"align-center\">\n",
    "<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n",
    "\n",
    "[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n",
    "[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n",
    "[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - Finetuning Tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>\n",
    "\n",
    "ðŸ‘‹ Welcome to Open Universal Machine Intelligence (Oumi)!\n",
    "\n",
    "ðŸš€ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](https://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n",
    "\n",
    "ðŸ¤ Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n",
    "\n",
    "â­ If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Overview\n",
    "\n",
    "In this tutorial, we'll LoRA tune a large language model to produce \"thoughts\" before producing its output.\n",
    "\n",
    "We'll use the Oumi framework to streamline the process and achieve high-quality results.\n",
    "\n",
    "We'll cover the following topics:\n",
    "1. Prerequisites\n",
    "2. Data Preparation & Sanity Checks\n",
    "3. Training Config Preparation\n",
    "4. Launching Training\n",
    "5. Monitoring Progress\n",
    "6. Evaluation\n",
    "7. Analyzing Results\n",
    "8. Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "â—**NOTICE:** We recommend running this notebook on a GPU. If running on Google Colab, you can use the free T4 GPU runtime (Colab Menu: `Runtime` -> `Change runtime type`). On Colab, we recommend replacing `HuggingFaceTB/SmolLM2-1.7B-Instruct` with a smaller model like `HuggingFaceTB/SmolLM2-135M-Instruct`, since the T4 only has 16GB VRAM; you can use `Edit -> Find and replace` in the menu bar to do so.\n",
    "\n",
    "First, let's install Oumi. You can find more detailed instructions [here](https://oumi.ai/docs/en/latest/get_started/installation.html). Here, we include Oumi's GPU dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install oumi[gpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our working directory\n",
    "For our experiments, we'll use the following folder to save the model, training artifacts, and our working configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"finetuning_tutorial\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "\n",
    "You may need to set the following environment variables:\n",
    "- [Optional] HF_TOKEN: Your [HuggingFace](https://huggingface.co/docs/hub/en/security-tokens) token, in case you want to access a private model like Llama.\n",
    "- [Optional] WANDB_API_KEY: Your [wandb](https://wandb.ai) token, in case you want to log your experiments to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "\n",
    "## Data Preparation\n",
    "Let's start by checking out our datasets, and seeing what the data looks like. The OpenO1-SFT dataset includes a variety of tasks, including code generation and explanation, with most examples having a \"thought\" produced prior to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"HF_HOME\"] = Path.expanduser(\"~/.cache/huggingface\")\n",
    "# os.environ[\"OUMI_SLURM_CONNECTIONS\"] = \"shanghong@192.222.48.208\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-27 00:16:15,429][oumi][rank0][pid:1073141][MainThread][INFO]][models.py:506] Using the model's built-in chat template for model 'HuggingFaceTB/SmolLM2-135M-Instruct'.\n",
      "[2025-06-27 00:16:15,430][oumi][rank0][pid:1073141][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: PromptResponseDataset)... dataset_name: 'O1-OPEN/OpenO1-SFT'\n",
      "[2025-06-27 00:16:15,941][oumi][rank0][pid:1073141][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: train\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 372897013\n",
      "\tDownload size: 383545217\n",
      "\tSize: 756442230 bytes\n",
      "\tRows: 77685\n",
      "\tColumns: ['instruction', 'output']\n",
      "[2025-06-27 00:16:16,549][oumi][rank0][pid:1073141][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (77685, 2). Columns:\n",
      "instruction    object\n",
      "output         object\n",
      "dtype: object\n",
      "Example 1:\n",
      "user: Consider a regular octagon. How many different triangles can be formed if the octagon is placed insi...\n",
      "assistant: <Thought>\n",
      "Alright, I need to figure out how many different triangles can be formed in a regular octa...\n",
      "\n",
      "\n",
      "Example 2:\n",
      "user: Create a Python class that encodes a given number using the Full KocioÅ‚ek Encryption algorithm. The ...\n",
      "assistant: <Thought>\n",
      "Alright, I need to create a Python class that implements the Full KocioÅ‚ek Encryption algo...\n",
      "\n",
      "\n",
      "Example 3:\n",
      "user: Write a Python function named `is_valid_binary` that takes in a string as a parameter. The function ...\n",
      "assistant: <Thought>\n",
      "Alright, I need to write a Python function named `is_valid_binary` that takes a string as ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from oumi.builders import build_tokenizer\n",
    "from oumi.core.configs import ModelParams\n",
    "from oumi.datasets import PromptResponseDataset\n",
    "\n",
    "# Initialize the dataset\n",
    "tokenizer = build_tokenizer(\n",
    "    ModelParams(model_name=\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
    ")\n",
    "dataset = PromptResponseDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    hf_dataset_path=\"O1-OPEN/OpenO1-SFT\",\n",
    "    prompt_column=\"instruction\",\n",
    "    response_column=\"output\",\n",
    ")\n",
    "\n",
    "# Print a few examples\n",
    "for i in range(3):\n",
    "    conversation = dataset.conversation(i)\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    for message in conversation.messages:\n",
    "        print(f\"{message.role}: {message.content[:100]}...\")  # Truncate for brevity\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "For code generation, we want a model with strong general language understanding and coding capabilities. \n",
    "\n",
    "We also want a model that is small enough to train and run on a single GPU.\n",
    "\n",
    "Some good options include:\n",
    "- [\"microsoft/Phi-3-mini-128k-instruct\"](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)\n",
    "- [\"google/gemma-2b\"](https://huggingface.co/google/gemma-2b)\n",
    "- [\"Qwen/Qwen2-1.5B-Instruct\"](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)\n",
    "- [\"meta-llama/Llama-3.2-3B-Instruct\"](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)\n",
    "- [\"HuggingFaceTB/SmolLM2-1.7B-Instruct\"](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct)\n",
    "\n",
    "\n",
    "For this tutorial, we'll use \"HuggingFaceTB/SmolLM2-1.7B-Instruct\" as our base model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Responses\n",
    "\n",
    "Let's see how our model performs on an example prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting finetuning_tutorial/infer.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/infer.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "  trust_remote_code: true\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 128\n",
    "  batch_size: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-27 00:16:16 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-06-27 00:16:17,395][oumi][rank0][pid:1073141][MainThread][WARNING]][infer.py:33] No inference engine specified. Using the default 'native' engine.\n",
      "[2025-06-27 00:16:17,396][oumi][rank0][pid:1073141][MainThread][INFO]][models.py:228] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
      "[2025-06-27 00:16:17,425][oumi][rank0][pid:1073141][MainThread][INFO]][models.py:300] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
      "[2025-06-27 00:16:18,054][oumi][rank0][pid:1073141][MainThread][INFO]][models.py:506] Using the model's built-in chat template for model 'HuggingFaceTB/SmolLM2-135M-Instruct'.\n",
      "[2025-06-27 00:16:18,061][oumi][rank0][pid:1073141][MainThread][INFO]][native_text_inference_engine.py:151] Setting EOS token id to `2`\n",
      "conversation_id='4db64724-2cb8-5c32-afe3-d81708f55a8f' messages=[USER: Write a Python function to implement the quicksort algorithm. Please include comments explaining each step., ASSISTANT: ```python\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    pivot = arr[len(arr) // 2]\n",
      "    left = [x for x in arr if x < pivot]\n",
      "    middle = [x for x in arr if x == pivot]\n",
      "    right = [x for x in arr if x > pivot]\n",
      "    return quicksort(left) + middle + quicksort(right)\n",
      "\n",
      "# Example usage:\n",
      "arr = [64, 34, 25, 12, 22, 11,] metadata={}\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.configs import InferenceConfig\n",
    "from oumi.infer import infer\n",
    "\n",
    "config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"infer.yaml\"))\n",
    "\n",
    "input_text = (\n",
    "    \"Write a Python function to implement the quicksort algorithm. \"\n",
    "    \"Please include comments explaining each step.\"\n",
    ")\n",
    "\n",
    "results = infer(config=config, inputs=[input_text])\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our training experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a YAML file for our training config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting finetuning_tutorial/train.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/train.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "  trust_remote_code: true\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  tokenizer_pad_token: \"<|endoftext|>\"\n",
    "  device_map: \"auto\"\n",
    "\n",
    "data:\n",
    "  train:\n",
    "    datasets:\n",
    "      - dataset_name: \"PromptResponseDataset\"\n",
    "        split: \"train\"\n",
    "        sample_count: 8000\n",
    "        dataset_kwargs: {\n",
    "          \"hf_dataset_path\": \"O1-OPEN/OpenO1-SFT\",\n",
    "          \"prompt_column\": \"instruction\",\n",
    "          \"response_column\": \"output\",\n",
    "          \"assistant_only\": true,\n",
    "          \"instruction_template\": \"<|im_start|>user\\n\",\n",
    "          \"response_template\": \"<|im_start|>assistant\\n\",\n",
    "        }\n",
    "        shuffle: True\n",
    "        seed: 42\n",
    "    # collator_name: \"text_with_padding\"\n",
    "    seed: 42\n",
    "\n",
    "training:\n",
    "  output_dir: \"finetuning_tutorial/output\"\n",
    "\n",
    "  # For a single GPU, the following gives us a batch size of 16\n",
    "  # If training with multiple GPUs, feel free to reduce gradient_accumulation_steps\n",
    "  per_device_train_batch_size: 2\n",
    "  gradient_accumulation_steps: 8\n",
    "  \n",
    "  # ***NOTE***\n",
    "  # We set it to 10 steps to first verify that it works\n",
    "  # Swap to 1500 steps to get more meaningful results.\n",
    "  # Note: 1500 steps will take 2-3 hours on a single A100-40GB GPU.\n",
    "  max_steps: 10\n",
    "  # max_steps: 1500\n",
    "\n",
    "  learning_rate: 1e-3\n",
    "  warmup_ratio: 0.1\n",
    "  logging_steps: 10\n",
    "  save_steps: 0\n",
    "  max_grad_norm: 1\n",
    "  weight_decay: 0.01\n",
    "\n",
    "  \n",
    "  trainer_type: \"TRL_SFT\"\n",
    "  optimizer: \"adamw_torch_fused\"\n",
    "  enable_gradient_checkpointing: True\n",
    "  gradient_checkpointing_kwargs:\n",
    "    use_reentrant: False\n",
    "  ddp_find_unused_parameters: False\n",
    "  dataloader_num_workers: \"auto\"\n",
    "  dataloader_prefetch_factor: 32\n",
    "  empty_device_cache_steps: 1\n",
    "  use_peft: true\n",
    "\n",
    "peft:\n",
    "  lora_r: 16\n",
    "  lora_alpha: 32\n",
    "  lora_dropout: 0.00\n",
    "  lora_target_modules:\n",
    "    - \"q_proj\"\n",
    "    - \"k_proj\"\n",
    "    - \"v_proj\"\n",
    "    - \"o_proj\"\n",
    "    - \"gate_proj\"\n",
    "    - \"up_proj\"\n",
    "    - \"down_proj\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "This will start the fine-tuning process using the Oumi framework. Because we set `max_steps: 5`, this should be very quick. The full fine-tuning process may take a few hours, depending on your GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SINGLE GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m   ____  _    _ __  __ _____\u001b[0m\n",
      "\u001b[32m  / __ \\| |  | |  \\/  |_   _|\u001b[0m\n",
      "\u001b[32m | |  | | |  | | \\  / | | |\u001b[0m\n",
      "\u001b[32m | |  | | |  | | |\\/| | | |\u001b[0m\n",
      "\u001b[32m | |__| | |__| | |  | |_| |_\u001b[0m\n",
      "\u001b[32m  \\____/ \\____/|_|  |_|_____|\u001b[0m\n",
      "\n",
      "\u001b[2K\u001b[32mâ §\u001b[0m \u001b[32mLoading configuration...\u001b[0m0m\n",
      "\u001b[1A\u001b[2K[2025-06-27 00:16:27,895][oumi][rank0][pid:1073415][MainThread][INFO]][distributed.py:578] Setting random seed to 42 on rank 0.\n",
      "[2025-06-27 00:16:32,581][oumi][rank0][pid:1073415][MainThread][INFO]][torch_utils.py:81] Torch version: 2.6.0+cu124. NumPy version: 1.26.4\n",
      "[2025-06-27 00:16:32,581][oumi][rank0][pid:1073415][MainThread][INFO]][torch_utils.py:89] CUDA version: 12.4 \n",
      "[2025-06-27 00:16:32,581][oumi][rank0][pid:1073415][MainThread][INFO]][torch_utils.py:92] CuDNN version: 90.8.0\n",
      "[2025-06-27 00:16:32,582][oumi][rank0][pid:1073415][MainThread][INFO]][torch_utils.py:125] CPU cores: 208 CUDA devices: 1\n",
      "device(0)='NVIDIA H100 80GB HBM3' Capability: (9, 0) Memory: [Total: 79.19GiB Free: 77.65GiB Allocated: 0.0GiB Cached: 0.0GiB]\n",
      "[2025-06-27 00:16:32,589][oumi][rank0][pid:1073415][MainThread][INFO]][train.py:153] Oumi version: 0.2.1.dev3+g91dd651c\n",
      "[2025-06-27 00:16:32,594][oumi][rank0][pid:1073415][MainThread][INFO]][train.py:155] Git revision hash: 91dd651cca462cdcd8641e5c7501deaa54e98cf5\n",
      "[2025-06-27 00:16:32,606][oumi][rank0][pid:1073415][MainThread][INFO]][train.py:156] Git tag: None\n",
      "[2025-06-27 00:16:32,608][oumi][rank0][pid:1073415][MainThread][INFO]][train.py:164] Resolved 'training.dataloader_num_workers=auto' to 'training.dataloader_num_workers=2'\n",
      "[2025-06-27 00:16:32,609][oumi][rank0][pid:1073415][MainThread][INFO]][train.py:281] TrainingConfig:\n",
      "TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='PromptResponseDataset',\n",
      "                                                                                dataset_path=None,\n",
      "                                                                                subset=None,\n",
      "                                                                                split='train',\n",
      "                                                                                dataset_kwargs={'assistant_only': True,\n",
      "                                                                                                'hf_dataset_path': 'O1-OPEN/OpenO1-SFT',\n",
      "                                                                                                'instruction_template': '<|im_start|>user\\n',\n",
      "                                                                                                'prompt_column': 'instruction',\n",
      "                                                                                                'response_column': 'output',\n",
      "                                                                                                'response_template': '<|im_start|>assistant\\n'},\n",
      "                                                                                sample_count=8000,\n",
      "                                                                                mixture_proportion=None,\n",
      "                                                                                shuffle=True,\n",
      "                                                                                seed=42,\n",
      "                                                                                shuffle_buffer_size=1000,\n",
      "                                                                                trust_remote_code=False,\n",
      "                                                                                transform_num_workers=None)],\n",
      "                                                        collator_name=None,\n",
      "                                                        collator_kwargs={},\n",
      "                                                        pack=False,\n",
      "                                                        stream=False,\n",
      "                                                        target_col=None,\n",
      "                                                        mixture_strategy='first_exhausted',\n",
      "                                                        seed=42,\n",
      "                                                        use_async_dataset=False,\n",
      "                                                        use_torchdata=None),\n",
      "                               test=DatasetSplitParams(datasets=[],\n",
      "                                                       collator_name=None,\n",
      "                                                       collator_kwargs={},\n",
      "                                                       pack=False,\n",
      "                                                       stream=False,\n",
      "                                                       target_col=None,\n",
      "                                                       mixture_strategy='first_exhausted',\n",
      "                                                       seed=None,\n",
      "                                                       use_async_dataset=False,\n",
      "                                                       use_torchdata=None),\n",
      "                               validation=DatasetSplitParams(datasets=[],\n",
      "                                                             collator_name=None,\n",
      "                                                             collator_kwargs={},\n",
      "                                                             pack=False,\n",
      "                                                             stream=False,\n",
      "                                                             target_col=None,\n",
      "                                                             mixture_strategy='first_exhausted',\n",
      "                                                             seed=None,\n",
      "                                                             use_async_dataset=False,\n",
      "                                                             use_torchdata=None)),\n",
      "               model=ModelParams(model_name='HuggingFaceTB/SmolLM2-135M-Instruct',\n",
      "                                 adapter_model=None,\n",
      "                                 tokenizer_name=None,\n",
      "                                 tokenizer_pad_token='<|endoftext|>',\n",
      "                                 tokenizer_kwargs={},\n",
      "                                 processor_kwargs={},\n",
      "                                 model_max_length=None,\n",
      "                                 load_pretrained_weights=True,\n",
      "                                 trust_remote_code=True,\n",
      "                                 torch_dtype_str='bfloat16',\n",
      "                                 compile=False,\n",
      "                                 chat_template=None,\n",
      "                                 attn_implementation=None,\n",
      "                                 device_map='auto',\n",
      "                                 model_kwargs={},\n",
      "                                 enable_liger_kernel=False,\n",
      "                                 shard_for_eval=False,\n",
      "                                 freeze_layers=[],\n",
      "                                 model_revision=None),\n",
      "               training=TrainingParams(use_peft=True,\n",
      "                                       trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>,\n",
      "                                       enable_gradient_checkpointing=True,\n",
      "                                       gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "                                       output_dir='finetuning_tutorial/output',\n",
      "                                       per_device_train_batch_size=2,\n",
      "                                       per_device_eval_batch_size=8,\n",
      "                                       gradient_accumulation_steps=8,\n",
      "                                       max_steps=10,\n",
      "                                       num_train_epochs=3,\n",
      "                                       save_epoch=False,\n",
      "                                       save_steps=0,\n",
      "                                       save_final_model=True,\n",
      "                                       seed=42,\n",
      "                                       data_seed=42,\n",
      "                                       use_deterministic=False,\n",
      "                                       full_determinism=False,\n",
      "                                       run_name=None,\n",
      "                                       metrics_function=None,\n",
      "                                       reward_functions=None,\n",
      "                                       grpo=GrpoParams(model_init_kwargs={},\n",
      "                                                       max_prompt_length=None,\n",
      "                                                       max_completion_length=None,\n",
      "                                                       num_generations=None,\n",
      "                                                       temperature=0.9,\n",
      "                                                       remove_unused_columns=False,\n",
      "                                                       repetition_penalty=1.0,\n",
      "                                                       use_vllm=False,\n",
      "                                                       vllm_mode=None,\n",
      "                                                       vllm_gpu_memory_utilization=0.9,\n",
      "                                                       vllm_dtype=None,\n",
      "                                                       vllm_max_model_len=None,\n",
      "                                                       epsilon=0.2,\n",
      "                                                       log_completions=False),\n",
      "                                       log_level='info',\n",
      "                                       dep_log_level='warning',\n",
      "                                       enable_wandb=False,\n",
      "                                       enable_mlflow=False,\n",
      "                                       enable_tensorboard=True,\n",
      "                                       logging_strategy='steps',\n",
      "                                       logging_dir=None,\n",
      "                                       logging_steps=10,\n",
      "                                       logging_first_step=False,\n",
      "                                       eval_strategy='no',\n",
      "                                       eval_steps=500,\n",
      "                                       learning_rate=0.001,\n",
      "                                       lr_scheduler_type='linear',\n",
      "                                       lr_scheduler_kwargs={},\n",
      "                                       warmup_ratio=0.1,\n",
      "                                       warmup_steps=None,\n",
      "                                       optimizer='adamw_torch_fused',\n",
      "                                       weight_decay=0.01,\n",
      "                                       adam_beta1=0.9,\n",
      "                                       adam_beta2=0.999,\n",
      "                                       adam_epsilon=1e-08,\n",
      "                                       sgd_momentum=0.0,\n",
      "                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,\n",
      "                                       compile=False,\n",
      "                                       include_performance_metrics=False,\n",
      "                                       include_alternative_mfu_metrics=False,\n",
      "                                       log_model_summary=False,\n",
      "                                       resume_from_checkpoint=None,\n",
      "                                       try_resume_from_last_checkpoint=False,\n",
      "                                       dataloader_num_workers=2,\n",
      "                                       dataloader_persistent_workers=False,\n",
      "                                       dataloader_prefetch_factor=32,\n",
      "                                       dataloader_main_process_only=None,\n",
      "                                       ddp_find_unused_parameters=False,\n",
      "                                       max_grad_norm=1.0,\n",
      "                                       trainer_kwargs={},\n",
      "                                       verl_config_overrides={},\n",
      "                                       profiler=ProfilerParams(save_dir=None,\n",
      "                                                               enable_cpu_profiling=False,\n",
      "                                                               enable_cuda_profiling=False,\n",
      "                                                               record_shapes=False,\n",
      "                                                               profile_memory=False,\n",
      "                                                               with_stack=False,\n",
      "                                                               with_flops=False,\n",
      "                                                               with_modules=False,\n",
      "                                                               row_limit=50,\n",
      "                                                               schedule=ProfilerScheduleParams(enable_schedule=False,\n",
      "                                                                                               wait=0,\n",
      "                                                                                               warmup=1,\n",
      "                                                                                               active=3,\n",
      "                                                                                               repeat=1,\n",
      "                                                                                               skip_first=1)),\n",
      "                                       telemetry=TelemetryParams(telemetry_dir='telemetry',\n",
      "                                                                 collect_telemetry_for_all_ranks=False,\n",
      "                                                                 track_gpu_temperature=False),\n",
      "                                       empty_device_cache_steps=1,\n",
      "                                       nccl_default_timeout_minutes=None,\n",
      "                                       label_ignore_index=None),\n",
      "               peft=PeftParams(lora_r=16,\n",
      "                               lora_alpha=32,\n",
      "                               lora_dropout=0.0,\n",
      "                               lora_target_modules=['q_proj',\n",
      "                                                    'k_proj',\n",
      "                                                    'v_proj',\n",
      "                                                    'o_proj',\n",
      "                                                    'gate_proj',\n",
      "                                                    'up_proj',\n",
      "                                                    'down_proj'],\n",
      "                               lora_modules_to_save=None,\n",
      "                               lora_bias='none',\n",
      "                               lora_init_weights=<LoraWeightInitialization.DEFAULT: 'default'>,\n",
      "                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
      "                               q_lora=False,\n",
      "                               q_lora_bits=4,\n",
      "                               bnb_4bit_quant_type='fp4',\n",
      "                               llm_int8_skip_modules=None,\n",
      "                               use_bnb_nested_quant=False,\n",
      "                               bnb_4bit_quant_storage='uint8',\n",
      "                               bnb_4bit_compute_dtype='float32',\n",
      "                               peft_save_mode=<PeftSaveMode.ADAPTER_ONLY: 'adapter_only'>),\n",
      "               fsdp=FSDPParams(enable_fsdp=False,\n",
      "                               sharding_strategy=<ShardingStrategy.FULL_SHARD: 'FULL_SHARD'>,\n",
      "                               cpu_offload=False,\n",
      "                               mixed_precision=None,\n",
      "                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,\n",
      "                               forward_prefetch=False,\n",
      "                               use_orig_params=None,\n",
      "                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,\n",
      "                               auto_wrap_policy=<AutoWrapPolicy.NO_WRAP: 'NO_WRAP'>,\n",
      "                               min_num_params=100000,\n",
      "                               transformer_layer_cls=None,\n",
      "                               sync_module_states=True))\n",
      "[2025-06-27 00:16:32,767][oumi][rank0][pid:1073415][MainThread][INFO]][models.py:506] Using the model's built-in chat template for model 'HuggingFaceTB/SmolLM2-135M-Instruct'.\n",
      "[2025-06-27 00:16:32,767][oumi][rank0][pid:1073415][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: PromptResponseDataset)... dataset_name: 'O1-OPEN/OpenO1-SFT'\n",
      "[2025-06-27 00:16:32,769][oumi][rank0][pid:1073415][MainThread][WARNING]][base_sft_dataset.py:275] Response template '<|im_start|>assistant\n",
      "' contains leading or trailing whitespaces. These will be ignored.\n",
      "[2025-06-27 00:16:32,771][oumi][rank0][pid:1073415][MainThread][WARNING]][base_sft_dataset.py:291] Instruction template '<|im_start|>user\n",
      "' contains leading or trailing whitespaces. These will be ignored.\n",
      "[2025-06-27 00:16:33,122][oumi][rank0][pid:1073415][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: train\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 372897013\n",
      "\tDownload size: 383545217\n",
      "\tSize: 756442230 bytes\n",
      "\tRows: 77685\n",
      "\tColumns: ['instruction', 'output']\n",
      "[2025-06-27 00:16:33,711][oumi][rank0][pid:1073415][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (77685, 2). Columns:\n",
      "instruction    object\n",
      "output         object\n",
      "dtype: object\n",
      "[2025-06-27 00:16:33,804][oumi][rank0][pid:1073415][MainThread][INFO]][base_map_dataset.py:312] PromptResponseDataset: features=dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "[2025-06-27 00:16:34,407][oumi][rank0][pid:1073415][MainThread][INFO]][base_map_dataset.py:376] Finished transforming dataset (PromptResponseDataset)! Speed: 128918.64 examples/sec. Examples: 77685. Duration: 0.6 sec. Transform workers: 1.\n",
      "[2025-06-27 00:16:34,440][oumi][rank0][pid:1073415][MainThread][INFO]][models.py:228] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
      "[2025-06-27 00:16:34,475][oumi][rank0][pid:1073415][MainThread][INFO]][models.py:300] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
      "[2025-06-27 00:16:34,774][oumi][rank0][pid:1073415][MainThread][INFO]][train.py:421] Building PEFT model...\n",
      "[2025-06-27 00:16:35,035][oumi][rank0][pid:1073415][MainThread][INFO]][torch_utils.py:289] \n",
      "Model Parameters Summary:\n",
      "ðŸ”¢ Total     parameters: 139,399,488\n",
      "ðŸ”— Embedding parameters: 28,311,552\n",
      "ðŸŽ¯ Trainable parameters: 4,884,480\n",
      "ðŸ”’ Frozen    parameters: 134,515,008 (96.50%)\n",
      "\n",
      "[2025-06-27 00:16:35,037][oumi][rank0][pid:1073415][MainThread][INFO]][train.py:444] Skipping dataset preparation for TRL_SFT trainer since the dataset is already processed.\n",
      "[2025-06-27 00:16:35,280][oumi][rank0][pid:1073415][MainThread][INFO]][torch_profiler_utils.py:164] PROF: Torch Profiler disabled!\n",
      "[2025-06-27 00:16:35,301][oumi][rank0][pid:1073415][MainThread][INFO]][training.py:62] SFTConfig(output_dir='finetuning_tutorial/output',\n",
      "          overwrite_output_dir=False,\n",
      "          do_train=False,\n",
      "          do_eval=False,\n",
      "          do_predict=False,\n",
      "          eval_strategy=<IntervalStrategy.NO: 'no'>,\n",
      "          prediction_loss_only=False,\n",
      "          per_device_train_batch_size=2,\n",
      "          per_device_eval_batch_size=8,\n",
      "          per_gpu_train_batch_size=None,\n",
      "          per_gpu_eval_batch_size=None,\n",
      "          gradient_accumulation_steps=8,\n",
      "          eval_accumulation_steps=None,\n",
      "          eval_delay=0,\n",
      "          torch_empty_cache_steps=1,\n",
      "          learning_rate=0.001,\n",
      "          weight_decay=0.01,\n",
      "          adam_beta1=0.9,\n",
      "          adam_beta2=0.999,\n",
      "          adam_epsilon=1e-08,\n",
      "          max_grad_norm=1.0,\n",
      "          num_train_epochs=3,\n",
      "          max_steps=10,\n",
      "          lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>,\n",
      "          lr_scheduler_kwargs={},\n",
      "          warmup_ratio=0.1,\n",
      "          warmup_steps=0,\n",
      "          log_level='warning',\n",
      "          log_level_replica='warning',\n",
      "          log_on_each_node=True,\n",
      "          logging_dir='finetuning_tutorial/output/runs/Jun27_00-16-35_oumi-compute001',\n",
      "          logging_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
      "          logging_first_step=False,\n",
      "          logging_steps=10,\n",
      "          logging_nan_inf_filter=True,\n",
      "          save_strategy=<SaveStrategy.NO: 'no'>,\n",
      "          save_steps=0,\n",
      "          save_total_limit=None,\n",
      "          save_safetensors=True,\n",
      "          save_on_each_node=False,\n",
      "          save_only_model=False,\n",
      "          restore_callback_states_from_checkpoint=False,\n",
      "          no_cuda=False,\n",
      "          use_cpu=False,\n",
      "          use_mps_device=False,\n",
      "          seed=42,\n",
      "          data_seed=42,\n",
      "          jit_mode_eval=False,\n",
      "          use_ipex=False,\n",
      "          bf16=False,\n",
      "          fp16=False,\n",
      "          fp16_opt_level='O1',\n",
      "          half_precision_backend='auto',\n",
      "          bf16_full_eval=False,\n",
      "          fp16_full_eval=False,\n",
      "          tf32=None,\n",
      "          local_rank=0,\n",
      "          ddp_backend=None,\n",
      "          tpu_num_cores=None,\n",
      "          tpu_metrics_debug=False,\n",
      "          debug=[],\n",
      "          dataloader_drop_last=False,\n",
      "          eval_steps=500,\n",
      "          dataloader_num_workers=2,\n",
      "          dataloader_prefetch_factor=32,\n",
      "          past_index=-1,\n",
      "          run_name='finetuning_tutorial/output',\n",
      "          disable_tqdm=False,\n",
      "          remove_unused_columns=True,\n",
      "          label_names=None,\n",
      "          load_best_model_at_end=False,\n",
      "          metric_for_best_model=None,\n",
      "          greater_is_better=None,\n",
      "          ignore_data_skip=False,\n",
      "          fsdp=[],\n",
      "          fsdp_min_num_params=0,\n",
      "          fsdp_config={'min_num_params': 0,\n",
      "                       'xla': False,\n",
      "                       'xla_fsdp_grad_ckpt': False,\n",
      "                       'xla_fsdp_v2': False},\n",
      "          tp_size=0,\n",
      "          fsdp_transformer_layer_cls_to_wrap=None,\n",
      "          accelerator_config=AcceleratorConfig(split_batches=False,\n",
      "                                               dispatch_batches=None,\n",
      "                                               even_batches=True,\n",
      "                                               use_seedable_sampler=True,\n",
      "                                               non_blocking=False,\n",
      "                                               gradient_accumulation_kwargs=None,\n",
      "                                               use_configured_state=False),\n",
      "          deepspeed=None,\n",
      "          label_smoothing_factor=0.0,\n",
      "          optim=<OptimizerNames.ADAMW_TORCH_FUSED: 'adamw_torch_fused'>,\n",
      "          optim_args=None,\n",
      "          adafactor=False,\n",
      "          group_by_length=False,\n",
      "          length_column_name='length',\n",
      "          report_to=['tensorboard'],\n",
      "          ddp_find_unused_parameters=False,\n",
      "          ddp_bucket_cap_mb=None,\n",
      "          ddp_broadcast_buffers=None,\n",
      "          dataloader_pin_memory=True,\n",
      "          dataloader_persistent_workers=False,\n",
      "          skip_memory_metrics=True,\n",
      "          use_legacy_prediction_loop=False,\n",
      "          push_to_hub=False,\n",
      "          resume_from_checkpoint=None,\n",
      "          hub_model_id=None,\n",
      "          hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>,\n",
      "          hub_token=None,\n",
      "          hub_private_repo=None,\n",
      "          hub_always_push=False,\n",
      "          gradient_checkpointing=True,\n",
      "          gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "          include_inputs_for_metrics=False,\n",
      "          include_for_metrics=[],\n",
      "          eval_do_concat_batches=True,\n",
      "          fp16_backend='auto',\n",
      "          push_to_hub_model_id=None,\n",
      "          push_to_hub_organization=None,\n",
      "          push_to_hub_token=None,\n",
      "          mp_parameters='',\n",
      "          auto_find_batch_size=False,\n",
      "          full_determinism=False,\n",
      "          torchdynamo=None,\n",
      "          ray_scope='last',\n",
      "          ddp_timeout=1800,\n",
      "          torch_compile=False,\n",
      "          torch_compile_backend=None,\n",
      "          torch_compile_mode=None,\n",
      "          include_tokens_per_second=False,\n",
      "          include_num_input_tokens_seen=False,\n",
      "          neftune_noise_alpha=None,\n",
      "          optim_target_modules=None,\n",
      "          batch_eval_metrics=False,\n",
      "          eval_on_start=False,\n",
      "          use_liger_kernel=False,\n",
      "          eval_use_gather_object=False,\n",
      "          average_tokens_across_devices=False,\n",
      "          model_init_kwargs=None,\n",
      "          dataset_text_field='text',\n",
      "          dataset_kwargs={'skip_prepare_dataset': True},\n",
      "          dataset_num_proc=None,\n",
      "          eos_token=None,\n",
      "          pad_token=None,\n",
      "          max_length=1024,\n",
      "          packing=False,\n",
      "          padding_free=False,\n",
      "          pad_to_multiple_of=None,\n",
      "          eval_packing=None,\n",
      "          completion_only_loss=None,\n",
      "          activation_offloading=False,\n",
      "          max_seq_length=None)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since \n",
      "`PeftModel` hides base models input arguments, if label_names is not given, \n",
      "label_names can't be set automatically within `Trainer`. Note that empty \n",
      "label_names list will be used instead.\n",
      "[2025-06-27 00:16:35,327][oumi][rank0][pid:1073415][MainThread][INFO]][device_utils.py:297] GPU Metrics Before Training: GPU runtime info: NVidiaGpuRuntimeInfo(device_index=0, device_count=1, used_memory_mb=2519.0, temperature=31, fan_speed=None, fan_speeds=None, power_usage_watts=119.188, power_limit_watts=700.0, gpu_utilization=0, memory_utilization=0, performance_state=0, clock_speed_graphics=1980, clock_speed_sm=1980, clock_speed_memory=2619).\n",
      "[2025-06-27 00:16:35,327][oumi][rank0][pid:1073415][MainThread][INFO]][train.py:511] Training init time: 2.752s\n",
      "[2025-06-27 00:16:35,328][oumi][rank0][pid:1073415][MainThread][INFO]][train.py:512] Starting training... (TrainerType.TRL_SFT, transformers: 4.51.3)\n",
      "{'loss': 1.3551, 'grad_norm': 0.08210515975952148, 'learning_rate': 0.0001111111111111111, 'num_tokens': 203364.0, 'mean_token_accuracy': 0.6810682222247124, 'epoch': 0.02}\n",
      "{'train_runtime': 18.3309, 'train_samples_per_second': 8.728, 'train_steps_per_second': 0.546, 'train_loss': 1.355083179473877, 'epoch': 0.02}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:18<00:00,  1.83s/it]\n",
      "[2025-06-27 00:16:53,922][oumi][rank0][pid:1073415][MainThread][INFO]][train.py:519] Training is Complete.\n",
      "[2025-06-27 00:16:53,923][oumi][rank0][pid:1073415][MainThread][INFO]][device_utils.py:297] GPU Metrics After Training: GPU runtime info: NVidiaGpuRuntimeInfo(device_index=0, device_count=1, used_memory_mb=5369.0, temperature=37, fan_speed=None, fan_speeds=None, power_usage_watts=228.012, power_limit_watts=700.0, gpu_utilization=64, memory_utilization=17, performance_state=0, clock_speed_graphics=1980, clock_speed_sm=1980, clock_speed_memory=2619).\n",
      "[2025-06-27 00:16:53,925][oumi][rank0][pid:1073415][MainThread][INFO]][torch_utils.py:136] Peak GPU memory usage: 5.54 GB\n",
      "[2025-06-27 00:16:53,926][oumi][rank0][pid:1073415][MainThread][INFO]][train.py:526] Saving final state...\n",
      "[2025-06-27 00:16:53,933][oumi][rank0][pid:1073415][MainThread][INFO]][train.py:531] Saving final model...\n",
      "[2025-06-27 00:16:54,139][oumi][rank0][pid:1073415][MainThread][INFO]][hf_trainer.py:124] Model has been saved at finetuning_tutorial/output\n",
      "[2025-06-27 00:16:54,140][oumi][rank0][pid:1073415][MainThread][INFO]][train.py:216] \n",
      "\n",
      "Â» We're always looking for feedback. What's one thing we can improve? https://oumi.ai/feedback\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!oumi train -c \"$tutorial_dir/train.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTI-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-27 00:16:58,240][oumi][rank0][pid:1073724][MainThread][INFO]][distributed_run.py:308] Running the command: ['oumi', 'train', '-c', 'finetuning_tutorial/train.yaml']\n",
      "\n",
      "\u001b[32m   ____  _    _ __  __ _____\u001b[0m\n",
      "\u001b[32m  / __ \\| |  | |  \\/  |_   _|\u001b[0m\n",
      "\u001b[32m | |  | | |  | | \\  / | | |\u001b[0m\n",
      "\u001b[32m | |  | | |  | | |\\/| | | |\u001b[0m\n",
      "\u001b[32m | |__| | |__| | |  | |_| |_\u001b[0m\n",
      "\u001b[32m  \\____/ \\____/|_|  |_|_____|\u001b[0m\n",
      "\n",
      "\u001b[2K\u001b[32mâ §\u001b[0m \u001b[32mLoading configuration...\u001b[0m0m\n",
      "\u001b[1A\u001b[2K[2025-06-27 00:17:05,479][oumi][rank0][pid:1073734][MainThread][INFO]][distributed.py:578] Setting random seed to 42 on rank 0.\n",
      "INFO:datasets:PyTorch version 2.6.0 available.\n",
      "[2025-06-27 00:17:10,164][oumi][rank0][pid:1073734][MainThread][INFO]][torch_utils.py:81] Torch version: 2.6.0+cu124. NumPy version: 1.26.4\n",
      "[2025-06-27 00:17:10,164][oumi][rank0][pid:1073734][MainThread][INFO]][torch_utils.py:89] CUDA version: 12.4 \n",
      "[2025-06-27 00:17:10,167][oumi][rank0][pid:1073734][MainThread][INFO]][torch_utils.py:92] CuDNN version: 90.8.0\n",
      "[2025-06-27 00:17:10,167][oumi][rank0][pid:1073734][MainThread][INFO]][torch_utils.py:125] CPU cores: 208 CUDA devices: 1\n",
      "device(0)='NVIDIA H100 80GB HBM3' Capability: (9, 0) Memory: [Total: 79.19GiB Free: 77.65GiB Allocated: 0.0GiB Cached: 0.0GiB]\n",
      "[2025-06-27 00:17:10,175][oumi][rank0][pid:1073734][MainThread][INFO]][train.py:153] Oumi version: 0.2.1.dev3+g91dd651c\n",
      "[2025-06-27 00:17:10,181][oumi][rank0][pid:1073734][MainThread][INFO]][train.py:155] Git revision hash: 91dd651cca462cdcd8641e5c7501deaa54e98cf5\n",
      "[2025-06-27 00:17:10,192][oumi][rank0][pid:1073734][MainThread][INFO]][train.py:156] Git tag: None\n",
      "[2025-06-27 00:17:10,194][oumi][rank0][pid:1073734][MainThread][INFO]][train.py:164] Resolved 'training.dataloader_num_workers=auto' to 'training.dataloader_num_workers=2'\n",
      "[2025-06-27 00:17:10,195][oumi][rank0][pid:1073734][MainThread][INFO]][train.py:281] TrainingConfig:\n",
      "TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='PromptResponseDataset',\n",
      "                                                                                dataset_path=None,\n",
      "                                                                                subset=None,\n",
      "                                                                                split='train',\n",
      "                                                                                dataset_kwargs={'assistant_only': True,\n",
      "                                                                                                'hf_dataset_path': 'O1-OPEN/OpenO1-SFT',\n",
      "                                                                                                'instruction_template': '<|im_start|>user\\n',\n",
      "                                                                                                'prompt_column': 'instruction',\n",
      "                                                                                                'response_column': 'output',\n",
      "                                                                                                'response_template': '<|im_start|>assistant\\n'},\n",
      "                                                                                sample_count=8000,\n",
      "                                                                                mixture_proportion=None,\n",
      "                                                                                shuffle=True,\n",
      "                                                                                seed=42,\n",
      "                                                                                shuffle_buffer_size=1000,\n",
      "                                                                                trust_remote_code=False,\n",
      "                                                                                transform_num_workers=None)],\n",
      "                                                        collator_name=None,\n",
      "                                                        collator_kwargs={},\n",
      "                                                        pack=False,\n",
      "                                                        stream=False,\n",
      "                                                        target_col=None,\n",
      "                                                        mixture_strategy='first_exhausted',\n",
      "                                                        seed=42,\n",
      "                                                        use_async_dataset=False,\n",
      "                                                        use_torchdata=None),\n",
      "                               test=DatasetSplitParams(datasets=[],\n",
      "                                                       collator_name=None,\n",
      "                                                       collator_kwargs={},\n",
      "                                                       pack=False,\n",
      "                                                       stream=False,\n",
      "                                                       target_col=None,\n",
      "                                                       mixture_strategy='first_exhausted',\n",
      "                                                       seed=None,\n",
      "                                                       use_async_dataset=False,\n",
      "                                                       use_torchdata=None),\n",
      "                               validation=DatasetSplitParams(datasets=[],\n",
      "                                                             collator_name=None,\n",
      "                                                             collator_kwargs={},\n",
      "                                                             pack=False,\n",
      "                                                             stream=False,\n",
      "                                                             target_col=None,\n",
      "                                                             mixture_strategy='first_exhausted',\n",
      "                                                             seed=None,\n",
      "                                                             use_async_dataset=False,\n",
      "                                                             use_torchdata=None)),\n",
      "               model=ModelParams(model_name='HuggingFaceTB/SmolLM2-135M-Instruct',\n",
      "                                 adapter_model=None,\n",
      "                                 tokenizer_name=None,\n",
      "                                 tokenizer_pad_token='<|endoftext|>',\n",
      "                                 tokenizer_kwargs={},\n",
      "                                 processor_kwargs={},\n",
      "                                 model_max_length=None,\n",
      "                                 load_pretrained_weights=True,\n",
      "                                 trust_remote_code=True,\n",
      "                                 torch_dtype_str='bfloat16',\n",
      "                                 compile=False,\n",
      "                                 chat_template=None,\n",
      "                                 attn_implementation=None,\n",
      "                                 device_map='auto',\n",
      "                                 model_kwargs={},\n",
      "                                 enable_liger_kernel=False,\n",
      "                                 shard_for_eval=False,\n",
      "                                 freeze_layers=[],\n",
      "                                 model_revision=None),\n",
      "               training=TrainingParams(use_peft=True,\n",
      "                                       trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>,\n",
      "                                       enable_gradient_checkpointing=True,\n",
      "                                       gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "                                       output_dir='finetuning_tutorial/output',\n",
      "                                       per_device_train_batch_size=2,\n",
      "                                       per_device_eval_batch_size=8,\n",
      "                                       gradient_accumulation_steps=8,\n",
      "                                       max_steps=10,\n",
      "                                       num_train_epochs=3,\n",
      "                                       save_epoch=False,\n",
      "                                       save_steps=0,\n",
      "                                       save_final_model=True,\n",
      "                                       seed=42,\n",
      "                                       data_seed=42,\n",
      "                                       use_deterministic=False,\n",
      "                                       full_determinism=False,\n",
      "                                       run_name=None,\n",
      "                                       metrics_function=None,\n",
      "                                       reward_functions=None,\n",
      "                                       grpo=GrpoParams(model_init_kwargs={},\n",
      "                                                       max_prompt_length=None,\n",
      "                                                       max_completion_length=None,\n",
      "                                                       num_generations=None,\n",
      "                                                       temperature=0.9,\n",
      "                                                       remove_unused_columns=False,\n",
      "                                                       repetition_penalty=1.0,\n",
      "                                                       use_vllm=False,\n",
      "                                                       vllm_mode=None,\n",
      "                                                       vllm_gpu_memory_utilization=0.9,\n",
      "                                                       vllm_dtype=None,\n",
      "                                                       vllm_max_model_len=None,\n",
      "                                                       epsilon=0.2,\n",
      "                                                       log_completions=False),\n",
      "                                       log_level='info',\n",
      "                                       dep_log_level='warning',\n",
      "                                       enable_wandb=False,\n",
      "                                       enable_mlflow=False,\n",
      "                                       enable_tensorboard=True,\n",
      "                                       logging_strategy='steps',\n",
      "                                       logging_dir=None,\n",
      "                                       logging_steps=10,\n",
      "                                       logging_first_step=False,\n",
      "                                       eval_strategy='no',\n",
      "                                       eval_steps=500,\n",
      "                                       learning_rate=0.001,\n",
      "                                       lr_scheduler_type='linear',\n",
      "                                       lr_scheduler_kwargs={},\n",
      "                                       warmup_ratio=0.1,\n",
      "                                       warmup_steps=None,\n",
      "                                       optimizer='adamw_torch_fused',\n",
      "                                       weight_decay=0.01,\n",
      "                                       adam_beta1=0.9,\n",
      "                                       adam_beta2=0.999,\n",
      "                                       adam_epsilon=1e-08,\n",
      "                                       sgd_momentum=0.0,\n",
      "                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,\n",
      "                                       compile=False,\n",
      "                                       include_performance_metrics=False,\n",
      "                                       include_alternative_mfu_metrics=False,\n",
      "                                       log_model_summary=False,\n",
      "                                       resume_from_checkpoint=None,\n",
      "                                       try_resume_from_last_checkpoint=False,\n",
      "                                       dataloader_num_workers=2,\n",
      "                                       dataloader_persistent_workers=False,\n",
      "                                       dataloader_prefetch_factor=32,\n",
      "                                       dataloader_main_process_only=None,\n",
      "                                       ddp_find_unused_parameters=False,\n",
      "                                       max_grad_norm=1.0,\n",
      "                                       trainer_kwargs={},\n",
      "                                       verl_config_overrides={},\n",
      "                                       profiler=ProfilerParams(save_dir=None,\n",
      "                                                               enable_cpu_profiling=False,\n",
      "                                                               enable_cuda_profiling=False,\n",
      "                                                               record_shapes=False,\n",
      "                                                               profile_memory=False,\n",
      "                                                               with_stack=False,\n",
      "                                                               with_flops=False,\n",
      "                                                               with_modules=False,\n",
      "                                                               row_limit=50,\n",
      "                                                               schedule=ProfilerScheduleParams(enable_schedule=False,\n",
      "                                                                                               wait=0,\n",
      "                                                                                               warmup=1,\n",
      "                                                                                               active=3,\n",
      "                                                                                               repeat=1,\n",
      "                                                                                               skip_first=1)),\n",
      "                                       telemetry=TelemetryParams(telemetry_dir='telemetry',\n",
      "                                                                 collect_telemetry_for_all_ranks=False,\n",
      "                                                                 track_gpu_temperature=False),\n",
      "                                       empty_device_cache_steps=1,\n",
      "                                       nccl_default_timeout_minutes=None,\n",
      "                                       label_ignore_index=None),\n",
      "               peft=PeftParams(lora_r=16,\n",
      "                               lora_alpha=32,\n",
      "                               lora_dropout=0.0,\n",
      "                               lora_target_modules=['q_proj',\n",
      "                                                    'k_proj',\n",
      "                                                    'v_proj',\n",
      "                                                    'o_proj',\n",
      "                                                    'gate_proj',\n",
      "                                                    'up_proj',\n",
      "                                                    'down_proj'],\n",
      "                               lora_modules_to_save=None,\n",
      "                               lora_bias='none',\n",
      "                               lora_init_weights=<LoraWeightInitialization.DEFAULT: 'default'>,\n",
      "                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
      "                               q_lora=False,\n",
      "                               q_lora_bits=4,\n",
      "                               bnb_4bit_quant_type='fp4',\n",
      "                               llm_int8_skip_modules=None,\n",
      "                               use_bnb_nested_quant=False,\n",
      "                               bnb_4bit_quant_storage='uint8',\n",
      "                               bnb_4bit_compute_dtype='float32',\n",
      "                               peft_save_mode=<PeftSaveMode.ADAPTER_ONLY: 'adapter_only'>),\n",
      "               fsdp=FSDPParams(enable_fsdp=False,\n",
      "                               sharding_strategy=<ShardingStrategy.FULL_SHARD: 'FULL_SHARD'>,\n",
      "                               cpu_offload=False,\n",
      "                               mixed_precision=None,\n",
      "                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,\n",
      "                               forward_prefetch=False,\n",
      "                               use_orig_params=None,\n",
      "                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,\n",
      "                               auto_wrap_policy=<AutoWrapPolicy.NO_WRAP: 'NO_WRAP'>,\n",
      "                               min_num_params=100000,\n",
      "                               transformer_layer_cls=None,\n",
      "                               sync_module_states=True))\n",
      "[2025-06-27 00:17:10,361][oumi][rank0][pid:1073734][MainThread][INFO]][models.py:506] Using the model's built-in chat template for model 'HuggingFaceTB/SmolLM2-135M-Instruct'.\n",
      "[2025-06-27 00:17:10,362][oumi][rank0][pid:1073734][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: PromptResponseDataset)... dataset_name: 'O1-OPEN/OpenO1-SFT'\n",
      "[2025-06-27 00:17:10,364][oumi][rank0][pid:1073734][MainThread][WARNING]][base_sft_dataset.py:275] Response template '<|im_start|>assistant\n",
      "' contains leading or trailing whitespaces. These will be ignored.\n",
      "[2025-06-27 00:17:10,366][oumi][rank0][pid:1073734][MainThread][WARNING]][base_sft_dataset.py:291] Instruction template '<|im_start|>user\n",
      "' contains leading or trailing whitespaces. These will be ignored.\n",
      "[2025-06-27 00:17:10,791][oumi][rank0][pid:1073734][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: train\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 372897013\n",
      "\tDownload size: 383545217\n",
      "\tSize: 756442230 bytes\n",
      "\tRows: 77685\n",
      "\tColumns: ['instruction', 'output']\n",
      "[2025-06-27 00:17:11,388][oumi][rank0][pid:1073734][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (77685, 2). Columns:\n",
      "instruction    object\n",
      "output         object\n",
      "dtype: object\n",
      "[2025-06-27 00:17:11,469][oumi][rank0][pid:1073734][MainThread][INFO]][base_map_dataset.py:312] PromptResponseDataset: features=dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "[2025-06-27 00:17:12,142][oumi][rank0][pid:1073734][MainThread][INFO]][base_map_dataset.py:376] Finished transforming dataset (PromptResponseDataset)! Speed: 115440.36 examples/sec. Examples: 77685. Duration: 0.7 sec. Transform workers: 1.\n",
      "[2025-06-27 00:17:12,163][oumi][rank0][pid:1073734][MainThread][INFO]][models.py:228] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
      "[2025-06-27 00:17:12,190][oumi][rank0][pid:1073734][MainThread][INFO]][models.py:300] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for \n",
      "storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory`\n",
      "in to a higher value to use more memory (at your own risk).\n",
      "[2025-06-27 00:17:12,540][oumi][rank0][pid:1073734][MainThread][INFO]][train.py:421] Building PEFT model...\n",
      "[2025-06-27 00:17:12,743][oumi][rank0][pid:1073734][MainThread][INFO]][torch_utils.py:289] \n",
      "Model Parameters Summary:\n",
      "ðŸ”¢ Total     parameters: 139,399,488\n",
      "ðŸ”— Embedding parameters: 28,311,552\n",
      "ðŸŽ¯ Trainable parameters: 4,884,480\n",
      "ðŸ”’ Frozen    parameters: 134,515,008 (96.50%)\n",
      "\n",
      "[2025-06-27 00:17:12,744][oumi][rank0][pid:1073734][MainThread][INFO]][train.py:444] Skipping dataset preparation for TRL_SFT trainer since the dataset is already processed.\n",
      "[2025-06-27 00:17:12,992][oumi][rank0][pid:1073734][MainThread][INFO]][torch_profiler_utils.py:164] PROF: Torch Profiler disabled!\n",
      "[2025-06-27 00:17:13,013][oumi][rank0][pid:1073734][MainThread][INFO]][training.py:62] SFTConfig(output_dir='finetuning_tutorial/output',\n",
      "          overwrite_output_dir=False,\n",
      "          do_train=False,\n",
      "          do_eval=False,\n",
      "          do_predict=False,\n",
      "          eval_strategy=<IntervalStrategy.NO: 'no'>,\n",
      "          prediction_loss_only=False,\n",
      "          per_device_train_batch_size=2,\n",
      "          per_device_eval_batch_size=8,\n",
      "          per_gpu_train_batch_size=None,\n",
      "          per_gpu_eval_batch_size=None,\n",
      "          gradient_accumulation_steps=8,\n",
      "          eval_accumulation_steps=None,\n",
      "          eval_delay=0,\n",
      "          torch_empty_cache_steps=1,\n",
      "          learning_rate=0.001,\n",
      "          weight_decay=0.01,\n",
      "          adam_beta1=0.9,\n",
      "          adam_beta2=0.999,\n",
      "          adam_epsilon=1e-08,\n",
      "          max_grad_norm=1.0,\n",
      "          num_train_epochs=3,\n",
      "          max_steps=10,\n",
      "          lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>,\n",
      "          lr_scheduler_kwargs={},\n",
      "          warmup_ratio=0.1,\n",
      "          warmup_steps=0,\n",
      "          log_level='warning',\n",
      "          log_level_replica='warning',\n",
      "          log_on_each_node=True,\n",
      "          logging_dir='finetuning_tutorial/output/runs/Jun27_00-17-12_oumi-compute001',\n",
      "          logging_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
      "          logging_first_step=False,\n",
      "          logging_steps=10,\n",
      "          logging_nan_inf_filter=True,\n",
      "          save_strategy=<SaveStrategy.NO: 'no'>,\n",
      "          save_steps=0,\n",
      "          save_total_limit=None,\n",
      "          save_safetensors=True,\n",
      "          save_on_each_node=False,\n",
      "          save_only_model=False,\n",
      "          restore_callback_states_from_checkpoint=False,\n",
      "          no_cuda=False,\n",
      "          use_cpu=False,\n",
      "          use_mps_device=False,\n",
      "          seed=42,\n",
      "          data_seed=42,\n",
      "          jit_mode_eval=False,\n",
      "          use_ipex=False,\n",
      "          bf16=False,\n",
      "          fp16=False,\n",
      "          fp16_opt_level='O1',\n",
      "          half_precision_backend='auto',\n",
      "          bf16_full_eval=False,\n",
      "          fp16_full_eval=False,\n",
      "          tf32=None,\n",
      "          local_rank=0,\n",
      "          ddp_backend=None,\n",
      "          tpu_num_cores=None,\n",
      "          tpu_metrics_debug=False,\n",
      "          debug=[],\n",
      "          dataloader_drop_last=False,\n",
      "          eval_steps=500,\n",
      "          dataloader_num_workers=2,\n",
      "          dataloader_prefetch_factor=32,\n",
      "          past_index=-1,\n",
      "          run_name='finetuning_tutorial/output',\n",
      "          disable_tqdm=False,\n",
      "          remove_unused_columns=True,\n",
      "          label_names=None,\n",
      "          load_best_model_at_end=False,\n",
      "          metric_for_best_model=None,\n",
      "          greater_is_better=None,\n",
      "          ignore_data_skip=False,\n",
      "          fsdp=[],\n",
      "          fsdp_min_num_params=0,\n",
      "          fsdp_config={'min_num_params': 0,\n",
      "                       'xla': False,\n",
      "                       'xla_fsdp_grad_ckpt': False,\n",
      "                       'xla_fsdp_v2': False},\n",
      "          tp_size=0,\n",
      "          fsdp_transformer_layer_cls_to_wrap=None,\n",
      "          accelerator_config=AcceleratorConfig(split_batches=False,\n",
      "                                               dispatch_batches=None,\n",
      "                                               even_batches=True,\n",
      "                                               use_seedable_sampler=True,\n",
      "                                               non_blocking=False,\n",
      "                                               gradient_accumulation_kwargs=None,\n",
      "                                               use_configured_state=False),\n",
      "          deepspeed=None,\n",
      "          label_smoothing_factor=0.0,\n",
      "          optim=<OptimizerNames.ADAMW_TORCH_FUSED: 'adamw_torch_fused'>,\n",
      "          optim_args=None,\n",
      "          adafactor=False,\n",
      "          group_by_length=False,\n",
      "          length_column_name='length',\n",
      "          report_to=['tensorboard'],\n",
      "          ddp_find_unused_parameters=False,\n",
      "          ddp_bucket_cap_mb=None,\n",
      "          ddp_broadcast_buffers=None,\n",
      "          dataloader_pin_memory=True,\n",
      "          dataloader_persistent_workers=False,\n",
      "          skip_memory_metrics=True,\n",
      "          use_legacy_prediction_loop=False,\n",
      "          push_to_hub=False,\n",
      "          resume_from_checkpoint=None,\n",
      "          hub_model_id=None,\n",
      "          hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>,\n",
      "          hub_token=None,\n",
      "          hub_private_repo=None,\n",
      "          hub_always_push=False,\n",
      "          gradient_checkpointing=True,\n",
      "          gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "          include_inputs_for_metrics=False,\n",
      "          include_for_metrics=[],\n",
      "          eval_do_concat_batches=True,\n",
      "          fp16_backend='auto',\n",
      "          push_to_hub_model_id=None,\n",
      "          push_to_hub_organization=None,\n",
      "          push_to_hub_token=None,\n",
      "          mp_parameters='',\n",
      "          auto_find_batch_size=False,\n",
      "          full_determinism=False,\n",
      "          torchdynamo=None,\n",
      "          ray_scope='last',\n",
      "          ddp_timeout=1800,\n",
      "          torch_compile=False,\n",
      "          torch_compile_backend=None,\n",
      "          torch_compile_mode=None,\n",
      "          include_tokens_per_second=False,\n",
      "          include_num_input_tokens_seen=False,\n",
      "          neftune_noise_alpha=None,\n",
      "          optim_target_modules=None,\n",
      "          batch_eval_metrics=False,\n",
      "          eval_on_start=False,\n",
      "          use_liger_kernel=False,\n",
      "          eval_use_gather_object=False,\n",
      "          average_tokens_across_devices=False,\n",
      "          model_init_kwargs=None,\n",
      "          dataset_text_field='text',\n",
      "          dataset_kwargs={'skip_prepare_dataset': True},\n",
      "          dataset_num_proc=None,\n",
      "          eos_token=None,\n",
      "          pad_token=None,\n",
      "          max_length=1024,\n",
      "          packing=False,\n",
      "          padding_free=False,\n",
      "          pad_to_multiple_of=None,\n",
      "          eval_packing=None,\n",
      "          completion_only_loss=None,\n",
      "          activation_offloading=False,\n",
      "          max_seq_length=None)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since \n",
      "`PeftModel` hides base models input arguments, if label_names is not given, \n",
      "label_names can't be set automatically within `Trainer`. Note that empty \n",
      "label_names list will be used instead.\n",
      "[2025-06-27 00:17:13,037][oumi][rank0][pid:1073734][MainThread][INFO]][device_utils.py:297] GPU Metrics Before Training: GPU runtime info: NVidiaGpuRuntimeInfo(device_index=0, device_count=1, used_memory_mb=2519.0, temperature=32, fan_speed=None, fan_speeds=None, power_usage_watts=119.044, power_limit_watts=700.0, gpu_utilization=0, memory_utilization=0, performance_state=0, clock_speed_graphics=1980, clock_speed_sm=1980, clock_speed_memory=2619).\n",
      "[2025-06-27 00:17:13,037][oumi][rank0][pid:1073734][MainThread][INFO]][train.py:511] Training init time: 2.880s\n",
      "[2025-06-27 00:17:13,039][oumi][rank0][pid:1073734][MainThread][INFO]][train.py:512] Starting training... (TrainerType.TRL_SFT, transformers: 4.51.3)\n",
      "{'loss': 1.3551, 'grad_norm': 0.08240135759115219, 'learning_rate': 0.0001111111111111111, 'num_tokens': 203364.0, 'mean_token_accuracy': 0.6809639662504197, 'epoch': 0.02}\n",
      "{'train_runtime': 18.4585, 'train_samples_per_second': 8.668, 'train_steps_per_second': 0.542, 'train_loss': 1.355122947692871, 'epoch': 0.02}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:18<00:00,  1.85s/it]\n",
      "[2025-06-27 00:17:31,780][oumi][rank0][pid:1073734][MainThread][INFO]][train.py:519] Training is Complete.\n",
      "[2025-06-27 00:17:31,781][oumi][rank0][pid:1073734][MainThread][INFO]][device_utils.py:297] GPU Metrics After Training: GPU runtime info: NVidiaGpuRuntimeInfo(device_index=0, device_count=1, used_memory_mb=5369.0, temperature=38, fan_speed=None, fan_speeds=None, power_usage_watts=236.356, power_limit_watts=700.0, gpu_utilization=73, memory_utilization=19, performance_state=0, clock_speed_graphics=1980, clock_speed_sm=1980, clock_speed_memory=2619).\n",
      "[2025-06-27 00:17:31,781][oumi][rank0][pid:1073734][MainThread][INFO]][torch_utils.py:136] Peak GPU memory usage: 5.54 GB\n",
      "[2025-06-27 00:17:31,783][oumi][rank0][pid:1073734][MainThread][INFO]][train.py:526] Saving final state...\n",
      "[2025-06-27 00:17:31,789][oumi][rank0][pid:1073734][MainThread][INFO]][train.py:531] Saving final model...\n",
      "[2025-06-27 00:17:31,974][oumi][rank0][pid:1073734][MainThread][INFO]][hf_trainer.py:124] Model has been saved at finetuning_tutorial/output\n",
      "[2025-06-27 00:17:31,974][oumi][rank0][pid:1073734][MainThread][INFO]][train.py:216] \n",
      "\n",
      "Â» We're always looking for feedback. What's one thing we can improve? https://oumi.ai/feedback\n",
      "\u001b[0m[2025-06-27 00:17:33,638][oumi][rank0][pid:1073724][MainThread][INFO]][distributed_run.py:327] Successfully completed! (Rank: 0. Duration: 35.4 sec)\n"
     ]
    }
   ],
   "source": [
    "!oumi distributed torchrun -m oumi train -c \"$tutorial_dir/train.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "As an example, let's create an evaluation configuration file!\n",
    "\n",
    "**Note:** Since we've finetuned our model to produce thoughts before answering, it's very likely to do worse on most evals out-of-the-box.\n",
    "\n",
    "Many evals do not allow models to decode and thus don't take advantage of things like inference-time reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting finetuning_tutorial/eval.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/eval.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"finetuning_tutorial/output\"\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "\n",
    "tasks:\n",
    "  - evaluation_backend: lm_harness\n",
    "    task_name: mmlu_college_computer_science\n",
    "\n",
    "output_dir: \"finetuning_tutorial/output/evaluation\"\n",
    "generation:\n",
    "  batch_size: null # This will let LM HARNESS find the maximum possible batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m   ____  _    _ __  __ _____\u001b[0m\n",
      "\u001b[32m  / __ \\| |  | |  \\/  |_   _|\u001b[0m\n",
      "\u001b[32m | |  | | |  | | \\  / | | |\u001b[0m\n",
      "\u001b[32m | |  | | |  | | |\\/| | | |\u001b[0m\n",
      "\u001b[32m | |__| | |__| | |  | |_| |_\u001b[0m\n",
      "\u001b[32m  \\____/ \\____/|_|  |_|_____|\u001b[0m\n",
      "\n",
      "\u001b[2K\u001b[32mâ ‡\u001b[0m \u001b[32mLoading configuration...\u001b[0m0m\n",
      "\u001b[1A\u001b[2K[2025-06-27 00:17:42,116][oumi][rank0][pid:1074030][MainThread][INFO]][model_params.py:248] Found LoRA adapter at finetuning_tutorial/output, setting `adapter_model` to `model_name`.\n",
      "[2025-06-27 00:17:42,118][oumi][rank0][pid:1074030][MainThread][INFO]][model_params.py:265] Setting `model_name` to HuggingFaceTB/SmolLM2-135M-Instruct found in adapter config.\n",
      "\u001b[2KINFO 06-27 00:17:47 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[2K\u001b[32mâ ‹\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-06-27 00:17:48,726][oumi][rank0][pid:1074030][MainThread][WARNING]][lm_harness.py:311] Since you have GPU support, it is highly recommended that you set the `inference_engine` to `VLLM`, instead of the `NATIVE`, for faster evaluation.\n",
      "\u001b[2K\u001b[32mâ ¼\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-06-27 00:18:05,799][oumi][rank0][pid:1074030][MainThread][INFO]][lm_harness.py:331] \tLM Harness `task_params`:\n",
      "LMHarnessTaskParams(evaluation_backend='lm_harness',\n",
      "                    task_name='mmlu_college_computer_science',\n",
      "                    num_samples=None,\n",
      "                    log_samples=False,\n",
      "                    eval_kwargs={},\n",
      "                    evaluation_platform='',\n",
      "                    num_fewshot=None)\n",
      "[2025-06-27 00:18:05,800][oumi][rank0][pid:1074030][MainThread][INFO]][lm_harness.py:332] \tLM Harness `task_dict`:\n",
      "{'mmlu_college_computer_science': ConfigurableTask(task_name=mmlu_college_computer_science,output_type=multiple_choice,num_fewshot=None,num_samples=100)}\n",
      "[2025-06-27 00:18:05,800][oumi][rank0][pid:1074030][MainThread][INFO]][lm_harness.py:344] \tLM Harness `model_params`:\n",
      "{'batch_size': 1,\n",
      " 'device': 'cuda:0',\n",
      " 'device_map': 'auto',\n",
      " 'dtype': torch.bfloat16,\n",
      " 'max_batch_size': None,\n",
      " 'max_length': None,\n",
      " 'parallelize': False,\n",
      " 'peft': 'finetuning_tutorial/output',\n",
      " 'pretrained': 'HuggingFaceTB/SmolLM2-135M-Instruct',\n",
      " 'trust_remote_code': False}\n",
      "\u001b[2K\u001b[32mâ §\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-06-27 00:18:06,803][oumi][rank0][pid:1074030][MainThread][INFO]][lm_harness.py:348] Starting evaluation...\n",
      "\u001b[2K  \u001b[1;36m0\u001b[0m%|                                                   | \u001b[1;36m0\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<?, ?it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2K\u001b[1;36m100\u001b[0m%|#######################################| \u001b[1;36m100\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m1200.\u001b[0m24it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2Kmâ ‡\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:   \u001b[1;36m0\u001b[0m%|                   | \u001b[1;36m0\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<?, ?it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:   \u001b[1;36m0\u001b[0m%|           | \u001b[1;36m1\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m02:24\u001b[0m,  \u001b[1;36m2.\u001b[0m76it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:   \u001b[1;36m4\u001b[0m%|\u001b[1;36m4\u001b[0m         | \u001b[1;36m17\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m00:08\u001b[0m, \u001b[1;36m43.\u001b[0m65it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:   \u001b[1;36m8\u001b[0m%|\u001b[1;36m8\u001b[0m         | \u001b[1;36m33\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m00:05\u001b[0m, \u001b[1;36m70.\u001b[0m46it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m11\u001b[0m%|#\u001b[1;36m1\u001b[0m        | \u001b[1;36m45\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m00:04\u001b[0m, \u001b[1;36m80.\u001b[0m54it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m14\u001b[0m%|#\u001b[1;36m4\u001b[0m        | \u001b[1;36m57\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m00:03\u001b[0m, \u001b[1;36m89.\u001b[0m29it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m18\u001b[0m%|#\u001b[1;36m6\u001b[0m       | \u001b[1;36m73\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m00:03\u001b[0m, \u001b[1;36m100.\u001b[0m93it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m22\u001b[0m%|##       | \u001b[1;36m89\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:01\u001b[0m<\u001b[1;92m00:02\u001b[0m, \u001b[1;36m110.\u001b[0m12it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m26\u001b[0m%|##\u001b[1;36m1\u001b[0m     | \u001b[1;36m105\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:01\u001b[0m<\u001b[1;92m00:02\u001b[0m, \u001b[1;36m116.\u001b[0m16it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m30\u001b[0m%|##\u001b[1;36m4\u001b[0m     | \u001b[1;36m121\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:01\u001b[0m<\u001b[1;92m00:02\u001b[0m, \u001b[1;36m119.\u001b[0m98it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m34\u001b[0m%|##\u001b[1;36m7\u001b[0m     | \u001b[1;36m137\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:01\u001b[0m<\u001b[1;92m00:02\u001b[0m, \u001b[1;36m123.\u001b[0m46it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m38\u001b[0m%|###     | \u001b[1;36m150\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:01\u001b[0m<\u001b[1;92m00:02\u001b[0m, \u001b[1;36m111.\u001b[0m55it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m40\u001b[0m%|###\u001b[1;36m2\u001b[0m    | \u001b[1;36m162\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:01\u001b[0m<\u001b[1;92m00:02\u001b[0m, \u001b[1;36m100.\u001b[0m69it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m44\u001b[0m%|###\u001b[1;36m5\u001b[0m    | \u001b[1;36m177\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:01\u001b[0m<\u001b[1;92m00:02\u001b[0m, \u001b[1;36m106.\u001b[0m92it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m48\u001b[0m%|###\u001b[1;36m8\u001b[0m    | \u001b[1;36m193\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:02\u001b[0m<\u001b[1;92m00:01\u001b[0m, \u001b[1;36m112.\u001b[0m98it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m52\u001b[0m%|####\u001b[1;36m1\u001b[0m   | \u001b[1;36m209\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:02\u001b[0m<\u001b[1;92m00:01\u001b[0m, \u001b[1;36m118.\u001b[0m04it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m56\u001b[0m%|####\u001b[1;36m5\u001b[0m   | \u001b[1;36m225\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:02\u001b[0m<\u001b[1;92m00:01\u001b[0m, \u001b[1;36m120.\u001b[0m75it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m60\u001b[0m%|####\u001b[1;36m8\u001b[0m   | \u001b[1;36m241\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:02\u001b[0m<\u001b[1;92m00:01\u001b[0m, \u001b[1;36m123.\u001b[0m46it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m64\u001b[0m%|#####\u001b[1;36m1\u001b[0m  | \u001b[1;36m257\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:02\u001b[0m<\u001b[1;92m00:01\u001b[0m, \u001b[1;36m125.\u001b[0m66it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m68\u001b[0m%|#####\u001b[1;36m4\u001b[0m  | \u001b[1;36m273\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:02\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m127.\u001b[0m02it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m72\u001b[0m%|#####\u001b[1;36m7\u001b[0m  | \u001b[1;36m289\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:02\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m127.\u001b[0m21it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m76\u001b[0m%|######\u001b[1;36m1\u001b[0m | \u001b[1;36m305\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:02\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m128.\u001b[0m14it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m80\u001b[0m%|######\u001b[1;36m4\u001b[0m | \u001b[1;36m321\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:02\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m128.\u001b[0m08it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m84\u001b[0m%|######\u001b[1;36m7\u001b[0m | \u001b[1;36m337\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:03\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m128.\u001b[0m31it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m88\u001b[0m%|####### | \u001b[1;36m350\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:03\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m124.\u001b[0m50it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m91\u001b[0m%|#######\u001b[1;36m3\u001b[0m| \u001b[1;36m365\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:03\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m122.\u001b[0m61it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m95\u001b[0m%|#######\u001b[1;36m6\u001b[0m| \u001b[1;36m381\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:03\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m124.\u001b[0m48it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests:  \u001b[1;36m99\u001b[0m%|#######\u001b[1;36m9\u001b[0m| \u001b[1;36m397\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:03\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m126.\u001b[0m64it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2KRunning loglikelihood requests: \u001b[1;36m100\u001b[0m%|########| \u001b[1;36m400\u001b[0m/\u001b[1;36m400\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:03\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m110.\u001b[0m99it/s\u001b[1m]\u001b[0m\n",
      "\u001b[2Kmâ ´\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\n",
      "\u001b[2K\u001b[32mâ ´\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-06-27 00:18:10,639][oumi][rank0][pid:1074030][MainThread][INFO]][lm_harness.py:366] mmlu_college_computer_science's metric dict is {'acc,none': 0.25,\n",
      " 'acc_stderr,none': 0.04351941398892446,\n",
      " 'alias': 'college_computer_science'}\n",
      "\u001b[2K\u001b[32mâ ´\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-06-27 00:18:15,433][oumi][rank0][pid:1074030][MainThread][WARNING]][serialization_utils.py:47] Non-serializable value `LMHarnessTaskParams(evaluation_backend='lm_harness', task_name='mmlu_college_computer_science', num_samples=None, log_samples=False, eval_kwargs={}, evaluation_platform='', num_fewshot=None)` of type `<class 'oumi.core.configs.params.evaluation_params.LMHarnessTaskParams'>`.\n",
      "[2025-06-27 00:18:15,433][oumi][rank0][pid:1074030][MainThread][WARNING]][serialization_utils.py:47] Non-serializable value `ConfigurableTask(task_name=mmlu_college_computer_science,output_type=multiple_choice,num_fewshot=None,num_samples=100)` of type `<class 'lm_eval.api.task.ConfigurableTask'>`.\n",
      "\u001b[2K\u001b[32mâ ¹\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\n",
      "\u001b[1A\u001b[2K\u001b[1;35m                    Evaluation Results                    \u001b[0m\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”“\n",
      "â”ƒ\u001b[1m \u001b[0m\u001b[1mBenchmark               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mMetric\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mScore \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mStd Error\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
      "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”©\n",
      "â”‚\u001b[36m \u001b[0m\u001b[36mcollege_computer_science\u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mAcc   \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32m25.00%\u001b[0m\u001b[32m \u001b[0mâ”‚\u001b[2m \u001b[0m\u001b[2mÂ±4.35%   \u001b[0m\u001b[2m \u001b[0mâ”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "!oumi evaluate -c \"$tutorial_dir/eval.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Use the Fine-tuned Model\n",
    "\n",
    "Once we're happy with the results, we can serve the fine-tuned model for interactive inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting finetuning_tutorial/trained_infer.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/trained_infer.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "  adapter_model: \"finetuning_tutorial/output\"\n",
    "  trust_remote_code: true\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 2048\n",
    "  batch_size: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-27 00:18:17,710][oumi][rank0][pid:1073141][MainThread][WARNING]][infer.py:33] No inference engine specified. Using the default 'native' engine.\n",
      "[2025-06-27 00:18:17,711][oumi][rank0][pid:1073141][MainThread][INFO]][models.py:228] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
      "[2025-06-27 00:18:17,712][oumi][rank0][pid:1073141][MainThread][INFO]][models.py:300] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-27 00:18:17,946][oumi][rank0][pid:1073141][MainThread][INFO]][models.py:281] Loading PEFT adapter from: finetuning_tutorial/output ...\n",
      "[2025-06-27 00:18:18,277][oumi][rank0][pid:1073141][MainThread][INFO]][models.py:506] Using the model's built-in chat template for model 'HuggingFaceTB/SmolLM2-135M-Instruct'.\n",
      "[2025-06-27 00:18:18,281][oumi][rank0][pid:1073141][MainThread][INFO]][native_text_inference_engine.py:151] Setting EOS token id to `2`\n",
      "conversation_id='4db64724-2cb8-5c32-afe3-d81708f55a8f' messages=[USER: Write a Python function to implement the quicksort algorithm. Please include comments explaining each step., ASSISTANT: ```python\n",
      "import math\n",
      "\n",
      "def quicksort(arr):\n",
      "    \"\"\"\n",
      "    Quicksort algorithm implementation.\n",
      "\n",
      "    Args:\n",
      "        arr (list): The input list to be sorted.\n",
      "\n",
      "    Returns:\n",
      "        list: The sorted list.\n",
      "\n",
      "    Raises:\n",
      "        TypeError: If the input is not a list.\n",
      "    \"\"\"\n",
      "    # Check if the input is a list\n",
      "    if not isinstance(arr, list):\n",
      "        raise TypeError(\"Input must be a list\")\n",
      "\n",
      "    # Initialize the pivot element\n",
      "    pivot = arr[0]\n",
      "\n",
      "    # Divide the list into two halves\n",
      "    left = [x for x in arr if x < pivot]\n",
      "    right = [x for x in arr if x > pivot]\n",
      "\n",
      "    # Recursively sort the two halves\n",
      "    if len(left) < 2:\n",
      "        return left\n",
      "\n",
      "    # Recursively sort the two halves\n",
      "    left_half = quicksort(left)\n",
      "    right_half = quicksort(right)\n",
      "\n",
      "    # Merge the two sorted halves\n",
      "    return [x for x in left_half if x < pivot] + [x for x in right_half if x > pivot]\n",
      "\n",
      "# Example usage:\n",
      "arr = [64, 34, 25, 12, 22, 11, 90]\n",
      "sorted_arr = quicksort(arr)\n",
      "print(sorted_arr)\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. **Input Validation**: The function checks if the input is a list. If not, it raises a `TypeError` with a descriptive message.\n",
      "\n",
      "2. **Initialization**: The function initializes the pivot element as the first element in the list.\n",
      "\n",
      "3. **Divide the List**: The function recursively divides the list into two halves using a list comprehension.\n",
      "\n",
      "4. **Sorting**: The function sorts the two halves using the `quicksort` function.\n",
      "\n",
      "5. **Recursion**: The function recursively sorts the two halves using the `quicksort` function.\n",
      "\n",
      "6. **Merging the Two Sorted halves**: The function merges the two sorted halves into a single sorted list.\n",
      "\n",
      "7. **Returning the Sorted List**: The function returns the sorted list.\n",
      "\n",
      "**Example Usage:**\n",
      "\n",
      "```python\n",
      "arr = [64, 34, 25, 12, 22, 11, 90]\n",
      "sorted_arr = quicksort(arr)\n",
      "print(sorted_arr)\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "\n",
      "```\n",
      "[11, 12, 22, 25, 34, 64, 90]\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* The `quicksort` function is used to recursively sort the two halves of the list.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `quicksort` function.\n",
      "* The `quicksort` function recursively sorts the two halves using the `qu] metadata={}\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.configs import InferenceConfig\n",
    "from oumi.infer import infer\n",
    "\n",
    "config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"trained_infer.yaml\"))\n",
    "\n",
    "input_text = (\n",
    "    \"Write a Python function to implement the quicksort algorithm. \"\n",
    "    \"Please include comments explaining each step.\"\n",
    ")\n",
    "\n",
    "results = infer(config=config, inputs=[input_text])\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
