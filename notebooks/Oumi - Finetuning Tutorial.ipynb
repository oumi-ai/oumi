{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Overview\n",
    "\n",
    "In this tutorial, we'll LoRA tune a large language model to improve its ability to generate and explain complex python code. \n",
    "\n",
    "We'll use the Oumi framework to streamline the process and achieve high-quality results.\n",
    "\n",
    "We'll cover the following topics:\n",
    "1. Prerequisites\n",
    "2. Data Preparation & Sanity Checks\n",
    "3. Training Config Preparation\n",
    "4. Launching Training\n",
    "5. Monitoring Progress\n",
    "6. Evaluation\n",
    "7. Analysing Results\n",
    "8. Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "## Oumi Installation\n",
    "First, let's install Oumi. You can find detailed instructions [here](https://github.com/oumi-ai/oumi/blob/main/README.md), but it should be as simple as:\n",
    "\n",
    "```bash\n",
    "pip install -e \".[gpu]\"  # if you have an nvidia or AMD GPU\n",
    "# OR\n",
    "pip install -e \".\"  # if you don't have a GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our working directory\n",
    "For our experiments, we'll use the following folder to save the model, training artifacts, and our working configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"finetuning_tutorial\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "\n",
    "We'll need to set the following environment variables:\n",
    "- [Optional] HF_TOKEN: Your [HuggingFace](https://huggingface.co/docs/hub/en/security-tokens) token, in case you want to access a private model.\n",
    "- [Optional] WANDB_API_KEY: Your [wandb](https://wandb.ai) token, in case you want to log your experiments to wandb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "\n",
    "## Data Preparation\n",
    "Let's start by checking out our datasets, and seeing what the data looks like. The Alpaca dataset includes a variety of tasks, including code generation and explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.builders import build_tokenizer\n",
    "from oumi.core.configs import ModelParams\n",
    "from oumi.datasets import AlpacaDataset\n",
    "\n",
    "# Initialize the dataset\n",
    "tokenizer = build_tokenizer(ModelParams(model_name=\"Qwen/Qwen2-1.5B-Instruct\"))\n",
    "dataset = AlpacaDataset(tokenizer=tokenizer)\n",
    "\n",
    "# Print a few examples\n",
    "for i in range(3):\n",
    "    conversation = dataset.conversation(i)\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    for message in conversation.messages:\n",
    "        print(f\"{message.role}: {message.content[:100]}...\")  # Truncate for brevity\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "For code generation, we want a model with strong general language understanding and coding capabilities. \n",
    "\n",
    "We also want a model that is small enough to train and run on a single GPU.\n",
    "\n",
    "Some good options include:\n",
    "- [\"microsoft/Phi-3-mini-128k-instruct\"](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)\n",
    "- [\"google/gemma-2b\"](https://huggingface.co/google/gemma-2b)\n",
    "- [\"Qwen/Qwen2-1.5B-Instruct\"](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)\n",
    "\n",
    "\n",
    "For this tutorial, we'll use \"Qwen/Qwen2-1.5B-Instruct\" as our base model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Responses\n",
    "\n",
    "Let's see how our model performs on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/infer.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "  trust_remote_code: true\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  device_map: \"auto\"\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 512\n",
    "  batch_size: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.core.configs import InferenceConfig\n",
    "from oumi.infer import infer\n",
    "\n",
    "config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"infer.yaml\"))\n",
    "\n",
    "input_text = (\n",
    "    \"Write a Python function to implement the quicksort algorithm. \"\n",
    "    \"Please include comments explaining each step.\"\n",
    ")\n",
    "\n",
    "results = infer(config=config, inputs=[input_text])\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our training experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a YAML file for our training config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/train.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "  trust_remote_code: true\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  device_map: \"auto\"\n",
    "\n",
    "data:\n",
    "  train:\n",
    "    datasets:\n",
    "      - dataset_name: \"tatsu-lab/alpaca\"\n",
    "        split: \"train\"\n",
    "    target_col: \"text\"\n",
    "\n",
    "training:\n",
    "  output_dir: \"finetuning_tutorial/output\"\n",
    "  per_device_train_batch_size: 2\n",
    "  gradient_accumulation_steps: 8\n",
    "  max_steps: 50\n",
    "  learning_rate: 1e-5\n",
    "  lr_scheduler_type: \"cosine\"\n",
    "  warmup_steps: 200\n",
    "  logging_steps: 10\n",
    "  save_steps: 200\n",
    "  eval_steps: 200\n",
    "\n",
    "  use_peft: true\n",
    "  trainer_type: \"TRL_SFT\"\n",
    "\n",
    "peft:\n",
    "  lora_r: 16\n",
    "  lora_alpha: 32\n",
    "  lora_dropout: 0.05\n",
    "  lora_target_modules:\n",
    "    - \"q_proj\"\n",
    "    - \"k_proj\"\n",
    "    - \"v_proj\"\n",
    "    - \"o_proj\"\n",
    "    - \"gate_proj\"\n",
    "    - \"up_proj\"\n",
    "    - \"down_proj\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "This will start the fine-tuning process using the Oumi framework. The process will take a few hours, depending on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!oumi train -c \"$tutorial_dir/train.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "Let's create an evaluation configuration file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/eval.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"finetuning_tutorial/output\"\n",
    "  trust_remote_code: true\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  device_map: \"auto\"\n",
    "\n",
    "tasks:\n",
    "  - evaluation_platform: lm_harness\n",
    "    task_name: mmlu\n",
    "    eval_kwargs:\n",
    "      num_fewshot: 0\n",
    "\n",
    "output_dir: \".finetuning_tutorial/output/evaluation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!oumi evaluate -c \"$tutorial_dir/eval.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Use the Fine-tuned Model\n",
    "\n",
    "Once we're happy with the results, we can serve the fine-tuned model for interactive inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/trained_infer.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"finetuning_tutorial/output\"\n",
    "  trust_remote_code: true\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  device_map: \"auto\"\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 512\n",
    "  batch_size: 1\n",
    "\n",
    "engine: \"NATIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.core.configs import InferenceConfig\n",
    "from oumi.infer import infer\n",
    "\n",
    "config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"trained_infer.yaml\"))\n",
    "\n",
    "input_text = (\n",
    "    \"Write a Python function to implement the quicksort algorithm. \"\n",
    "    \"Please include comments explaining each step.\"\n",
    ")\n",
    "\n",
    "results = infer(config=config, inputs=[input_text])\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
