{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from huggingface_hub import login\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from lema.builders import (\n",
    "    build_dataset,\n",
    "    build_model,\n",
    "    build_peft_model,\n",
    "    build_tokenizer,\n",
    "    build_trainer,\n",
    ")\n",
    "from lema.core.types import TrainingConfig\n",
    "from lema.utils.saver import save_model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/optas/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "access_token = os.environ.get(\"HF_TOKEN\")\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingParams(optimizer='adamw_torch', use_peft=True, trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>, enable_gradient_checkpointing=True, output_dir='output/zephyr.7b.sft', per_device_train_batch_size=1, per_device_eval_batch_size=8, gradient_accumulation_steps=1, max_steps=-1, num_train_epochs=1, run_name='default', log_level='info', dep_log_level='warning', enable_wandb=False, enable_tensorboard=True, logging_strategy='steps', logging_dir='output/runs', logging_steps=50, learning_rate=2e-05, lr_scheduler_type='cosine', warmup_ratio=0.1, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, gradient_checkpointing_kwargs={'use_reentrant': False})\n",
      "PeftParams(lora_r=16, lora_alpha=16, lora_dropout=0.05, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], lora_bias='none', lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, q_lora=False, q_lora_bits=4)\n"
     ]
    }
   ],
   "source": [
    "config_filename = \"../configs/lema/zephyr.7b.sft.yaml\"\n",
    "base_config = OmegaConf.structured(TrainingConfig)\n",
    "file_config = TrainingConfig.from_yaml(config_filename)\n",
    "config = OmegaConf.merge(base_config, file_config)\n",
    "config: TrainingConfig = OmegaConf.to_object(config)\n",
    "print(config.training)\n",
    "print(config.peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO-finalize in config file\n",
    "config.peft.q_lora = False\n",
    "config.training.per_device_train_batch_size = 1\n",
    "config.training.max_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/optas/miniconda3/envs/lema/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-v0.1', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = build_tokenizer(config)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.model_max_length 2048\n",
      "tokenizer pad_token/eos_token </s> </s>\n",
      "tokenizer.padding_side left\n",
      "tokenizer.chat_template {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n"
     ]
    }
   ],
   "source": [
    "# # Set reasonable default for models without max length\n",
    "# if tokenizer.model_max_length > 100_000: # shall this condition be checked for diff.\n",
    "#  Zephyr models? Now is not.\n",
    "#     tokenizer.model_max_length = 2048\n",
    "\n",
    "print(\"tokenizer.model_max_length\", tokenizer.model_max_length)\n",
    "print(\"tokenizer pad_token/eos_token\", tokenizer.pad_token, tokenizer.eos_token)\n",
    "print(\"tokenizer.padding_side\", tokenizer.padding_side)\n",
    "print(\"tokenizer.chat_template\", tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fb8d6994b94849be8725baf4e80d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/207865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207865\n",
      "1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'prompt_id', 'messages', 'text'],\n",
       "    num_rows: 1024\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data & preprocessing\n",
    "dataset = build_dataset(\n",
    "    dataset_config=config.data,\n",
    "    tokenizer=tokenizer,\n",
    "    **config.data.preprocessing_function_kwargs,\n",
    ")\n",
    "\n",
    "\n",
    "if True:\n",
    "    import numpy as np  # hack\n",
    "\n",
    "    print(len(dataset))\n",
    "    np.random.seed(1234)\n",
    "    ridx = np.random.choice(len(dataset), 1024, replace=False)\n",
    "    dataset = dataset.select(ridx)\n",
    "    print(len(dataset))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - update our code base if we use optimum\n",
    "# Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file. # noqa\n",
    "# WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed. # TODO update in main repo # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7892f02bee3347ecafcb4d67f39431ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider adding special tokens\n",
    "# tokenizer.additional_special_tokens  #'<|assistant|>', <|system|>\n",
    "# tokenizer.encode(\"<|system|>\")  # We already wrap <bos> and <eos>\n",
    "# # in the chat template\n",
    "# # add_special_tokens=\n",
    "# tokenizer.encode(\"|system|\")\n",
    "\n",
    "# # For ChatML we need to add special tokens and resize the embedding layer\n",
    "# if \"<|im_start|>\" in tokenizer.chat_template and \"gemma-tokenizer-chatml\" not in tokenizer.name_or_path: # noqa\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs) # noqa\n",
    "#     model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "#     model_kwargs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.training.use_peft:\n",
    "    model = build_peft_model(\n",
    "        model, config.training.enable_gradient_checkpointing, config.peft\n",
    "    )\n",
    "\n",
    "if config.training.enable_gradient_checkpointing:\n",
    "    model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO update if need be for accelerator\n",
    "trainer_cls = build_trainer(config.training.trainer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585ef69ae072463cb37000eb23d4e4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/optas/miniconda3/envs/lema/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = trainer_cls(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=config.training.to_hf(),\n",
    "    train_dataset=dataset,\n",
    "    **config.data.trainer_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|system|>\\n</s>\\n<|user|>\\nHow many miles of coastline are there in Point Reyes National Seashore?</s>\\n<|assistant|>\\nThere are approximately 80 miles of coastline in Point Reyes National Seashore.</s>\\n<|user|>\\nCan you recommend any specific beaches or hiking trails along the coastline in Point Reyes National Seashore?</s>\\n<|assistant|>\\nYes, here are some recommended beaches and hiking trails along the coastline in Point Reyes National Seashore:\\n\\nBeaches:\\n1. Limantour Beach - a beautiful and wide beach with soft sand perfect for sunbathing, picnicking, and walking.\\n2. Drake's Beach - a popular beach for families, with picnic areas, restrooms, and a visitor center nearby.\\n3. Point Reyes Beach - a long and wild beach with rolling waves and stunning views, perfect for surfing or fishing.\\n4. South Beach - a remote and secluded beach with plenty of wildlife, including harbor seals and sea lions.\\n\\nHiking Trails:\\n1. Tomales Point Trail - a scenic hike along the coast that offers great views of wildlife, including tule elk and migrating whales.\\n2. Bear Valley Trail - a popular trail that leads to a beautiful waterfall and offers stunning views of the coastline and rolling hills.\\n3. Alamere Falls Trail - a challenging trail that leads to a stunning waterfall that falls directly into the ocean.\\n4. Chimney Rock Trail - a short and easy trail that offers panoramic views of the coast and plenty of opportunities for whale watching.</s>\\n<|user|>\\nWow, those all sound amazing! Which one do you think offers the best chance to spot some seals or sea lions?</s>\\n<|assistant|>\\nThe South Beach in Point Reyes National Seashore is known for its large population of harbor seals, elephant seals, and sea lions. From South Beach, you can hike along the Tomales Point Trail to reach the Point Reyes Headlands which offers opportunities to spot these marine mammals during the winter and spring months. Chimney Rock Trail is also a great option to spot harbor seals during the pupping season in the spring.</s>\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1ad73fc479436f828591b84b6ec88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 150.9662, 'train_samples_per_second': 0.013, 'train_steps_per_second': 0.013, 'train_loss': 1.116136908531189, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=1.116136908531189, metrics={'train_runtime': 150.9662, 'train_samples_per_second': 0.013, 'train_steps_per_second': 0.013, 'total_flos': 109778152931328.0, 'train_loss': 1.116136908531189, 'epoch': 0.001953125})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-30 18:56:33,607][lema][INFO][saver.py:25] Model has been saved at output/zephyr.7b.sft.\n"
     ]
    }
   ],
   "source": [
    "# Save final checkpoint & training state\n",
    "trainer.save_state()\n",
    "\n",
    "save_model(\n",
    "    config=config,\n",
    "    trainer=trainer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lema",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
