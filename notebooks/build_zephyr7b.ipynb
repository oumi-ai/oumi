{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - add accelerate, bitsandbytes in toml\n",
    "# TODO - in build_model do we really want to load the full model first,\n",
    "#         and then convert it to quant?\n",
    "\n",
    "## See again:\n",
    "# UserWarning: You passed a tokenizer with `padding_side` not equal to `right`\n",
    "# to the SFTTrainer. This might lead to some unexpected behaviour due to overflow\n",
    "# issues when training a model in half-precision. You might\n",
    "# consider adding `tokenizer.padding_side = 'right'` to your code.\n",
    "\n",
    "# TODO - update our code base if we use optimum (build_model)\n",
    "# Using `disable_exllama` is deprecated and will be removed in version 4.37.\n",
    "# Use `use_exllama` instead and specify the version with `exllama_config`.\n",
    "# The value of `use_exllama` will be overwritten by `disable_exllama` passed\n",
    "# in `GPTQConfig` or stored in your config file. # noqa\n",
    "# WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.\n",
    "# # TODO update in main repo # noqa\n",
    "\n",
    "\n",
    "# TODO Consider adding special tokens like '<|assistant|>', '<|system|>'\n",
    "# via tokenizer.additional_special_tokens -- need to check Mistral\n",
    "\n",
    "# from alignment team:\n",
    "# tokenizer.encode(\"<|system|>\")  # We already wrap <bos> and <eos>\n",
    "# # in the chat template\n",
    "\n",
    "# Future TODO.\n",
    "# # For ChatML we need to add special tokens and resize the embedding layer\n",
    "# if \"<|im_start|>\" in tokenizer.chat_template and \"gemma-tokenizer-chatml\" not in tokenizer.name_or_path: # noqa\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs) # noqa\n",
    "#     model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "#     model_kwargs = None\n",
    "\n",
    "\n",
    "# if tokenizer.model_max_length > 100_000: # shall this condition be checked for diff.\n",
    "#  Zephyr models? Now is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from lema.builders import (\n",
    "    build_dataset,\n",
    "    build_model,\n",
    "    build_peft_model,\n",
    "    build_tokenizer,\n",
    "    build_trainer,\n",
    ")\n",
    "from lema.core.types import TrainingConfig\n",
    "from lema.utils.saver import save_model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# access_token = os.environ.get(\"HF_TOKEN\")\n",
    "access_token = \"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\"  # in notebook\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingParams(optimizer='adamw_torch', use_peft=True, trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>, enable_gradient_checkpointing=True, output_dir='output/zephyr-7b-sft-qlora', per_device_train_batch_size=16, per_device_eval_batch_size=8, gradient_accumulation_steps=2, max_steps=-1, num_train_epochs=1, run_name='default', log_level='info', dep_log_level='warning', enable_wandb=False, enable_tensorboard=True, logging_strategy='steps', logging_dir='output/runs', logging_steps=50, learning_rate=0.0002, lr_scheduler_type='cosine', warmup_ratio=0.1, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, gradient_checkpointing_kwargs={'use_reentrant': False}, packing=True, include_performance_metrics=None)\n",
      "PeftParams(lora_r=16, lora_alpha=16, lora_dropout=0.05, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], lora_modules_to_save=None, lora_bias='none', lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, q_lora=True, q_lora_bits=4)\n"
     ]
    }
   ],
   "source": [
    "config_filename = \"../configs/lema/zephyr.7b/sft/config_qlora.yaml\"\n",
    "base_config = OmegaConf.structured(TrainingConfig)\n",
    "file_config = TrainingConfig.from_yaml(config_filename)\n",
    "config = OmegaConf.merge(base_config, file_config)\n",
    "config: TrainingConfig = OmegaConf.to_object(config)\n",
    "print(config.training)\n",
    "print(config.peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-06 16:44:50,439][lema][WARNING][models.py:118] <pad> token not found: setting <pad> with <eos>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-v0.1', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = build_tokenizer(config.model)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.model_max_length 2048\n",
      "tokenizer pad_token/eos_token </s> </s>\n",
      "tokenizer.padding_side left\n",
      "tokenizer.chat_template {% if messages[0]['role'] == 'system' %}{% set offset = 1 %}{% else %}{% set offset = 0 %}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == offset) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{{ '<|' + message['role'] + '|>\\n' + message['content'] | trim + eos_token + '\\n' }}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizer.model_max_length\", tokenizer.model_max_length)\n",
    "print(\"tokenizer pad_token/eos_token\", tokenizer.pad_token, tokenizer.eos_token)\n",
    "print(\"tokenizer.padding_side\", tokenizer.padding_side)\n",
    "print(\"tokenizer.chat_template\", tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data & preprocessing\n",
    "dataset = build_dataset(config, tokenizer)\n",
    "\n",
    "\n",
    "if not config.data.stream:\n",
    "    import numpy as np  # hack to subsample\n",
    "\n",
    "    print(len(dataset))\n",
    "    np.random.seed(1234)\n",
    "    ridx = np.random.choice(len(dataset), 1024, replace=False)\n",
    "    dataset = dataset.select(ridx)\n",
    "    print(len(dataset))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-06 16:44:53,759][lema][INFO][models.py:55] Building model using device_map: auto...\n",
      "/root/miniconda3/envs/lema/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cad960f45f46e1b51053f5afe037ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(config)\n",
    "\n",
    "if config.training.use_peft:\n",
    "    model = build_peft_model(\n",
    "        model, config.training.enable_gradient_checkpointing, config.peft\n",
    "    )\n",
    "\n",
    "if config.training.enable_gradient_checkpointing:\n",
    "    model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_cls = build_trainer(config.training.trainer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainer_cls(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=config.training.to_hf(),\n",
    "    train_dataset=dataset,\n",
    "    **config.data.trainer_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.all_special_tokens\n",
    "# print(tokenizer.encode(\"|system|\"))\n",
    "# dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final checkpoint & training state\n",
    "trainer.save_state()\n",
    "\n",
    "save_model(\n",
    "    config=config,\n",
    "    trainer=trainer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lema",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
