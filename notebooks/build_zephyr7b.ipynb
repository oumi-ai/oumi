{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from huggingface_hub import login\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from lema.builders import (\n",
    "    build_dataset,\n",
    "    build_model,\n",
    "    build_peft_model,\n",
    "    build_tokenizer,\n",
    "    build_trainer,\n",
    ")\n",
    "from lema.core.types import TrainingConfig\n",
    "from lema.utils.saver import save_model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/optas/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "access_token = os.environ.get(\"HF_TOKEN\")\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingParams(optimizer='adamw_torch', use_peft=True, trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>, enable_gradient_checkpointing=True, output_dir='output/zephyr.7b.sft', per_device_train_batch_size=1, per_device_eval_batch_size=8, gradient_accumulation_steps=1, max_steps=2, num_train_epochs=1, run_name='default', log_level='info', dep_log_level='warning', enable_wandb=False, enable_tensorboard=True, logging_strategy='steps', logging_dir='output/runs', logging_steps=50, learning_rate=2e-05, lr_scheduler_type='cosine', warmup_ratio=0.1, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, gradient_checkpointing_kwargs={'use_reentrant': False}, include_performance_metrics=None)\n",
      "PeftParams(lora_r=16, lora_alpha=16, lora_dropout=0.05, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], lora_modules_to_save=None, lora_bias='none', lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, q_lora=False, q_lora_bits=4)\n"
     ]
    }
   ],
   "source": [
    "config_filename = \"../configs/lema/zephyr.7b.sft.yaml\"\n",
    "base_config = OmegaConf.structured(TrainingConfig)\n",
    "file_config = TrainingConfig.from_yaml(config_filename)\n",
    "config = OmegaConf.merge(base_config, file_config)\n",
    "config: TrainingConfig = OmegaConf.to_object(config)\n",
    "print(config.training)\n",
    "print(config.peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO-finalize in config file\n",
    "config.peft.q_lora = False\n",
    "config.training.per_device_train_batch_size = 1\n",
    "config.training.max_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/optas/miniconda3/envs/lema/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[2024-06-06 00:11:40,071][lema][WARNING][models.py:120] <pad> token not found: setting <pad> with <eos>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-v0.1', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = build_tokenizer(config.model)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.model_max_length 2048\n",
      "tokenizer pad_token/eos_token </s> </s>\n",
      "tokenizer.padding_side left\n",
      "tokenizer.chat_template {% if messages[0]['role'] == 'system' %}{% set offset = 1 %}{% else %}{% set offset = 0 %}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == offset) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{{ '<|' + message['role'] + '|>\\n' + message['content'] | trim + eos_token + '\\n' }}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "# if tokenizer.model_max_length > 100_000: # shall this condition be checked for diff.\n",
    "#  Zephyr models? Now is not.\n",
    "\n",
    "print(\"tokenizer.model_max_length\", tokenizer.model_max_length)\n",
    "print(\"tokenizer pad_token/eos_token\", tokenizer.pad_token, tokenizer.eos_token)\n",
    "print(\"tokenizer.padding_side\", tokenizer.padding_side)\n",
    "print(\"tokenizer.chat_template\", tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_dataset() missing 1 required positional argument: 'data_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data & preprocessing\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m build_dataset(dataset_config\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdata, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m  \u001b[38;5;66;03m# hack to subsample\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: build_dataset() missing 1 required positional argument: 'data_params'"
     ]
    }
   ],
   "source": [
    "# Load data & preprocessing\n",
    "dataset = build_dataset(data_params=config.data, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "if True:\n",
    "    import numpy as np  # hack to subsample\n",
    "\n",
    "    print(len(dataset))\n",
    "    np.random.seed(1234)\n",
    "    ridx = np.random.choice(len(dataset), 1024, replace=False)\n",
    "    dataset = dataset.select(ridx)\n",
    "    print(len(dataset))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - update our code base if we use optimum\n",
    "# Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file. # noqa\n",
    "# WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed. # TODO update in main repo # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.all_special_tokens\n",
    "tokenizer.encode(\"|system|\")\n",
    "\n",
    "# TODO Consider adding special tokens like '<|assistant|>', '<|system|>'\n",
    "# via tokenizer.additional_special_tokens -- need to check Mistral\n",
    "\n",
    "# from alignment team:\n",
    "# tokenizer.encode(\"<|system|>\")  # We already wrap <bos> and <eos>\n",
    "# # in the chat template\n",
    "\n",
    "# Future TODO.\n",
    "# # For ChatML we need to add special tokens and resize the embedding layer\n",
    "# if \"<|im_start|>\" in tokenizer.chat_template and \"gemma-tokenizer-chatml\" not in tokenizer.name_or_path: # noqa\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs) # noqa\n",
    "#     model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "#     model_kwargs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.training.use_peft:\n",
    "    model = build_peft_model(\n",
    "        model, config.training.enable_gradient_checkpointing, config.peft\n",
    "    )\n",
    "\n",
    "if config.training.enable_gradient_checkpointing:\n",
    "    model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO update if need be for accelerator\n",
    "trainer_cls = build_trainer(config.training.trainer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainer_cls(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=config.training.to_hf(),\n",
    "    train_dataset=dataset,\n",
    "    **config.data.trainer_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final checkpoint & training state\n",
    "trainer.save_state()\n",
    "\n",
    "save_model(\n",
    "    config=config,\n",
    "    trainer=trainer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lema",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
