{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from lema.builders import (\n",
    "    build_model,\n",
    "    build_peft_model,\n",
    "    build_tokenizer,\n",
    "    build_trainer,\n",
    ")\n",
    "from lema.core.types import TrainingConfig\n",
    "from lema.datasets.ultrachat_200k import apply_chat_template\n",
    "from lema.utils.saver import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = os.environ.get(\"HF_TOKEN\")\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filename = \"../configs/lema/zephyr.7b.sft.yaml\"\n",
    "base_config = OmegaConf.structured(TrainingConfig)\n",
    "file_config = TrainingConfig.from_yaml(config_filename)\n",
    "config = OmegaConf.merge(base_config, file_config)\n",
    "config: TrainingConfig = OmegaConf.to_object(config)\n",
    "print(config.training)\n",
    "print(config.peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO-finalize in config file\n",
    "config.peft.q_lora = False\n",
    "config.training.per_device_train_batch_size = 1\n",
    "config.training.max_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = build_tokenizer(config)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set reasonable default for models without max length\n",
    "# if tokenizer.model_max_length > 100_000: # shall this condition be checked for diff.\n",
    "#  Zephyr models? Now is not.\n",
    "#     tokenizer.model_max_length = 2048\n",
    "\n",
    "print(\"tokenizer.model_max_length\", tokenizer.model_max_length)\n",
    "print(\"tokenizer pad_token/eos_token\", tokenizer.pad_token, tokenizer.eos_token)\n",
    "print(\"tokenizer.padding_side\", tokenizer.padding_side)\n",
    "print(\"tokenizer.chat_template\", tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-sample dataset\n",
    "dataset = load_dataset(config.data.dataset_name, split=config.data.split)\n",
    "print(len(dataset))\n",
    "np.random.seed(1234)\n",
    "ridx = np.random.choice(len(dataset), 1024, replace=False)\n",
    "dataset = dataset.select(ridx)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"task\": \"sft\",\n",
    "        # \"Whether to automatically insert an empty system message\n",
    "        # as the first message if `system` is mentioned\n",
    "        # in the chat template.\"\n",
    "        \"auto_insert_empty_system_msg\": True,\n",
    "    },\n",
    "    remove_columns=[],\n",
    "    desc=\"Applying chat template\",\n",
    "    **config.data.preprocessing_function_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - update our code base if we use optimum\n",
    "# Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file. # noqa\n",
    "# WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed. # TODO update in main repo # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider adding special tokens\n",
    "# tokenizer.additional_special_tokens  #'<|assistant|>', <|system|>\n",
    "# tokenizer.encode(\"<|system|>\")  # We already wrap <bos> and <eos>\n",
    "# # in the chat template\n",
    "# # add_special_tokens=\n",
    "# tokenizer.encode(\"|system|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.training.use_peft:\n",
    "    model = build_peft_model(\n",
    "        model, config.training.enable_gradient_checkpointing, config.peft\n",
    "    )\n",
    "\n",
    "if config.training.enable_gradient_checkpointing:\n",
    "    model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO update if need be for accelerator\n",
    "trainer_cls = build_trainer(config.training.trainer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainer_cls(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=config.training.to_hf(),\n",
    "    train_dataset=dataset,\n",
    "    **config.data.trainer_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final checkpoint & training state\n",
    "trainer.save_state()\n",
    "\n",
    "save_model(\n",
    "    config=config,\n",
    "    trainer=trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing_fn = build_prompt_generation_fn(preprocessing_function_name, tokenizer)\n",
    "# dataset = dataset.map(preprocessing_fn, batched=True, **kwargs)\n",
    "# dataset = dataset.map(preprocessing_fn, batched=True)\n",
    "\n",
    "# # For ChatML we need to add special tokens and resize the embedding layer\n",
    "# if \"<|im_start|>\" in tokenizer.chat_template and \"gemma-tokenizer-chatml\" not in tokenizer.name_or_path: # noqa\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs) # noqa\n",
    "#     model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "#     model_kwargs = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lema",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
