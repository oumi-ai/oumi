{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Open Unified Machine Intelligence (OUMI)! We created this platform to democratize the development of large models for the open source world. We strongly believe that together, as a community, we can push the boundaries of AI.\n",
    "\n",
    "This tutorial will give you a brief overview of OUMI's core functionality. We'll cover:\n",
    "\n",
    "1. Training a model\n",
    "1. Model Inference\n",
    "1. Evaluating a model against common benchmarks\n",
    "1. Launching jobs\n",
    "1. Customizing datasets and clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "## OUMI Installation\n",
    "First, let's install OUMI. You can find detailed instructions [here](https://github.com/oumi-ai/oumi/blob/main/README.md), but it should be as simple as:\n",
    "\n",
    "```bash\n",
    "pip install -e \".[dev,train]\"\n",
    "```\n",
    "\n",
    "## Creating our working directory\n",
    "For our experiments, we'll use the following folder to save the model, training artifacts, and our working configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"tour_tutorial\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUMI supports training both custom and out-of-the-box models. Want to try out a model on HuggingFace? We can do that. Want to train your own custom Pytorch model? We've got you covered there too.\n",
    "\n",
    "## A quick demo\n",
    "\n",
    "Let's try training a pre-existing model on HuggingFace. We'll use GPT2 as it's small and trains quickly.\n",
    "\n",
    "OUMI uses [training configuration files](https://learning-machines.ai/docs/latest/apidoc/oumi.core.configs.html#oumi.core.configs.TrainingConfig) to specify training parameters. We've already created a training config for GPT2--let's give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tour_tutorial/train.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/train.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"gpt2\" # 124M params\n",
    "  model_max_length: 128\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  load_pretrained_weights: False\n",
    "  trust_remote_code: True\n",
    "  model_kwargs:\n",
    "    disable_dropout: True\n",
    "\n",
    "data:\n",
    "  train:\n",
    "    datasets:\n",
    "      - dataset_name: \"HuggingFaceFW/fineweb-edu\"\n",
    "        subset: \"sample-10BT\"\n",
    "        split: \"train\"\n",
    "    stream: True\n",
    "    pack: True\n",
    "    target_col: \"text\"\n",
    "\n",
    "training:\n",
    "  trainer_type: TRL_SFT\n",
    "  per_device_train_batch_size: 2\n",
    "  max_steps: 10\n",
    "\n",
    "  enable_gradient_checkpointing: False\n",
    "  gradient_checkpointing_kwargs:\n",
    "    use_reentrant: False\n",
    "\n",
    "  learning_rate: 6.0e-04\n",
    "  lr_scheduler_type: \"cosine_with_min_lr\"\n",
    "  lr_scheduler_kwargs:\n",
    "    min_lr_rate: 0.1\n",
    "  warmup_steps: 715\n",
    "  adam_beta1: 0.9\n",
    "  adam_beta2: 0.95\n",
    "  weight_decay: 0.1\n",
    "\n",
    "  run_name: \"gpt2_pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-19 09:56:43,046][oumi][rank0][pid:231378][MainThread][INFO]][train.py:124] Creating training.output_dir: tour_tutorial/output...\n",
      "[2024-09-19 09:56:43,047][oumi][rank0][pid:231378][MainThread][INFO]][train.py:126] Created training.output_dir absolute path: /home/user/oumi/notebooks/tour_tutorial/output\n",
      "[2024-09-19 09:56:43,048][oumi][rank0][pid:231378][MainThread][INFO]][train.py:124] Creating training.telemetry_dir: tour_tutorial/output/telemetry...\n",
      "[2024-09-19 09:56:43,048][oumi][rank0][pid:231378][MainThread][INFO]][train.py:126] Created training.telemetry_dir absolute path: /home/user/oumi/notebooks/tour_tutorial/output/telemetry\n",
      "[2024-09-19 09:56:43,049][oumi][rank0][pid:231378][MainThread][INFO]][torch_utils.py:48] Torch version: 2.4.1+cu121. NumPy version: 2.1.1\n",
      "[2024-09-19 09:56:43,050][oumi][rank0][pid:231378][MainThread][INFO]][torch_utils.py:59] CUDA version: 12.1 CuDNN version: 90.1.0\n",
      "[2024-09-19 09:56:43,939][oumi][rank0][pid:231378][MainThread][INFO]][torch_utils.py:93] CPU cores: 24 CUDA devices: 1\n",
      "device(0)='NVIDIA GeForce RTX 3090' Capability: (8, 6) Memory: [Total: 24.0GiB Free: 22.22GiB Allocated: 0.0GiB Cached: 0.0GiB]\n",
      "[2024-09-19 09:56:43,941][oumi][rank0][pid:231378][MainThread][INFO]][torch_utils.py:43] TrainingConfig: TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='HuggingFaceFW/fineweb-edu',\n",
      "                                                                                subset='sample-10BT',\n",
      "                                                                                split='train',\n",
      "                                                                                dataset_kwargs={},\n",
      "                                                                                sample_count=None,\n",
      "                                                                                mixture_proportion=None,\n",
      "                                                                                shuffle=False,\n",
      "                                                                                seed=None,\n",
      "                                                                                shuffle_buffer_size=1000,\n",
      "                                                                                trust_remote_code=False,\n",
      "                                                                                preprocessing_function_name=None,\n",
      "                                                                                preprocessing_function_kwargs={'batched': False})],\n",
      "                                                        pack=True,\n",
      "                                                        stream=True,\n",
      "                                                        target_col='text',\n",
      "                                                        mixture_strategy='first_exhausted',\n",
      "                                                        seed=None,\n",
      "                                                        experimental_use_async_dataset=False,\n",
      "                                                        experimental_use_torch_datapipes=False),\n",
      "                               test=DatasetSplitParams(datasets=[],\n",
      "                                                       pack=False,\n",
      "                                                       stream=False,\n",
      "                                                       target_col=None,\n",
      "                                                       mixture_strategy='first_exhausted',\n",
      "                                                       seed=None,\n",
      "                                                       experimental_use_async_dataset=False,\n",
      "                                                       experimental_use_torch_datapipes=False),\n",
      "                               validation=DatasetSplitParams(datasets=[],\n",
      "                                                             pack=False,\n",
      "                                                             stream=False,\n",
      "                                                             target_col=None,\n",
      "                                                             mixture_strategy='first_exhausted',\n",
      "                                                             seed=None,\n",
      "                                                             experimental_use_async_dataset=False,\n",
      "                                                             experimental_use_torch_datapipes=False)),\n",
      "               model=ModelParams(model_name='gpt2',\n",
      "                                 adapter_model=None,\n",
      "                                 tokenizer_name=None,\n",
      "                                 model_max_length=128,\n",
      "                                 load_pretrained_weights=False,\n",
      "                                 trust_remote_code=True,\n",
      "                                 torch_dtype_str='bfloat16',\n",
      "                                 compile=False,\n",
      "                                 chat_template=None,\n",
      "                                 attn_implementation=None,\n",
      "                                 device_map='auto',\n",
      "                                 model_kwargs={'disable_dropout': True},\n",
      "                                 enable_liger_kernel=False,\n",
      "                                 shard_for_eval=False,\n",
      "                                 freeze_layers=[]),\n",
      "               training=TrainingParams(use_peft=False,\n",
      "                                       trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>,\n",
      "                                       enable_gradient_checkpointing=False,\n",
      "                                       gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "                                       output_dir='tour_tutorial/output',\n",
      "                                       per_device_train_batch_size=2,\n",
      "                                       per_device_eval_batch_size=8,\n",
      "                                       gradient_accumulation_steps=1,\n",
      "                                       max_steps=10,\n",
      "                                       num_train_epochs=3,\n",
      "                                       save_epoch=False,\n",
      "                                       save_steps=100,\n",
      "                                       save_final_model=True,\n",
      "                                       seed=42,\n",
      "                                       run_name='gpt2_pt',\n",
      "                                       metrics_function=None,\n",
      "                                       log_level='info',\n",
      "                                       dep_log_level='warning',\n",
      "                                       enable_wandb=False,\n",
      "                                       enable_tensorboard=True,\n",
      "                                       logging_strategy='steps',\n",
      "                                       logging_dir='output/runs',\n",
      "                                       logging_steps=50,\n",
      "                                       logging_first_step=False,\n",
      "                                       eval_strategy='no',\n",
      "                                       eval_steps=50,\n",
      "                                       learning_rate=0.0006,\n",
      "                                       lr_scheduler_type='cosine_with_min_lr',\n",
      "                                       lr_scheduler_kwargs={'min_lr_rate': 0.1},\n",
      "                                       warmup_ratio=None,\n",
      "                                       warmup_steps=715,\n",
      "                                       optimizer='adamw_torch',\n",
      "                                       weight_decay=0.1,\n",
      "                                       adam_beta1=0.9,\n",
      "                                       adam_beta2=0.95,\n",
      "                                       adam_epsilon=1e-08,\n",
      "                                       sgd_momentum=0.9,\n",
      "                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,\n",
      "                                       compile=False,\n",
      "                                       include_performance_metrics=False,\n",
      "                                       include_alternative_mfu_metrics=False,\n",
      "                                       log_model_summary=False,\n",
      "                                       resume_from_checkpoint=None,\n",
      "                                       try_resume_from_last_checkpoint=False,\n",
      "                                       dataloader_num_workers=0,\n",
      "                                       dataloader_prefetch_factor=None,\n",
      "                                       dataloader_main_process_only=None,\n",
      "                                       ddp_find_unused_parameters=None,\n",
      "                                       max_grad_norm=1.0,\n",
      "                                       trainer_kwargs={'dataset_text_field': 'text',\n",
      "                                                       'max_seq_length': 128},\n",
      "                                       profiler=ProfilerParams(save_dir=None,\n",
      "                                                               enable_cpu_profiling=False,\n",
      "                                                               enable_cuda_profiling=False,\n",
      "                                                               record_shapes=False,\n",
      "                                                               profile_memory=False,\n",
      "                                                               with_stack=False,\n",
      "                                                               with_flops=False,\n",
      "                                                               with_modules=False,\n",
      "                                                               row_limit=50,\n",
      "                                                               schedule=ProfilerScheduleParams(enable_schedule=False,\n",
      "                                                                                               wait=0,\n",
      "                                                                                               warmup=1,\n",
      "                                                                                               active=3,\n",
      "                                                                                               repeat=1,\n",
      "                                                                                               skip_first=1)),\n",
      "                                       telemetry=TelemetryParams(telemetry_dir='telemetry',\n",
      "                                                                 collect_telemetry_for_all_ranks=False,\n",
      "                                                                 track_gpu_temperature=False),\n",
      "                                       empty_device_cache_steps=None,\n",
      "                                       nccl_default_timeout_minutes=None),\n",
      "               peft=PeftParams(lora_r=16,\n",
      "                               lora_alpha=16,\n",
      "                               lora_dropout=0.05,\n",
      "                               lora_target_modules=None,\n",
      "                               lora_modules_to_save=None,\n",
      "                               lora_bias='none',\n",
      "                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
      "                               q_lora=False,\n",
      "                               q_lora_bits=4,\n",
      "                               bnb_4bit_quant_type='fp4',\n",
      "                               use_bnb_nested_quant=False,\n",
      "                               bnb_4bit_quant_storage='uint8'),\n",
      "               fsdp=FSDPParams(enable_fsdp=False,\n",
      "                               sharding_strategy=<ShardingStrategy.FULL_SHARD: 'FULL_SHARD'>,\n",
      "                               cpu_offload=False,\n",
      "                               mixed_precision=None,\n",
      "                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,\n",
      "                               forward_prefetch=False,\n",
      "                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,\n",
      "                               auto_wrap_policy=<AutoWrapPolicy.SIZE_BASED: 'SIZE_BASED'>,\n",
      "                               min_num_params=100000,\n",
      "                               transformer_layer_cls=None,\n",
      "                               sync_module_states=True))\n",
      "[2024-09-19 09:56:44,533][oumi][rank0][pid:231378][MainThread][WARNING]][models.py:274] <pad> token not found: setting <pad> with <eos>.\n",
      "[2024-09-19 09:56:44,534][oumi][rank0][pid:231378][MainThread][WARNING]][models.py:284] No chat template found for tokenizer. Please specify a chat template using the `chat_template` field. This will be required in future versions of OUMI.\n",
      "[2024-09-19 09:56:44,535][oumi][rank0][pid:231378][MainThread][INFO]][models.py:144] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
      "[2024-09-19 09:56:44,699][oumi][rank0][pid:231378][MainThread][INFO]][torch_naming_heuristics.py:38] Found these dropout attributes and set their values to 0.0: ['resid_pdrop', 'embd_pdrop', 'attn_pdrop', 'summary_first_dropout']\n",
      "[2024-09-19 09:56:44,700][oumi][rank0][pid:231378][MainThread][INFO]][models.py:240] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c0afd793014e95865e2a6a0b4f3f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/23.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357d1362649e4ed5b4bd1abc62b63461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-19 09:57:04,048][oumi][rank0][pid:231378][MainThread][INFO]][torch_profiler_utils.py:148] PROF: Torch Profiler disabled!\n",
      "[2024-09-19 09:57:04,098][oumi][rank0][pid:231378][MainThread][INFO]][training.py:44] SFTConfig(output_dir='tour_tutorial/output',\n",
      "          overwrite_output_dir=False,\n",
      "          do_train=False,\n",
      "          do_eval=False,\n",
      "          do_predict=False,\n",
      "          eval_strategy=<IntervalStrategy.NO: 'no'>,\n",
      "          prediction_loss_only=False,\n",
      "          per_device_train_batch_size=2,\n",
      "          per_device_eval_batch_size=8,\n",
      "          per_gpu_train_batch_size=None,\n",
      "          per_gpu_eval_batch_size=None,\n",
      "          gradient_accumulation_steps=1,\n",
      "          eval_accumulation_steps=None,\n",
      "          eval_delay=0,\n",
      "          torch_empty_cache_steps=None,\n",
      "          learning_rate=0.0006,\n",
      "          weight_decay=0.1,\n",
      "          adam_beta1=0.9,\n",
      "          adam_beta2=0.95,\n",
      "          adam_epsilon=1e-08,\n",
      "          max_grad_norm=1.0,\n",
      "          num_train_epochs=3,\n",
      "          max_steps=10,\n",
      "          lr_scheduler_type=<SchedulerType.COSINE_WITH_MIN_LR: 'cosine_with_min_lr'>,\n",
      "          lr_scheduler_kwargs={'min_lr_rate': 0.1},\n",
      "          warmup_ratio=0.0,\n",
      "          warmup_steps=715,\n",
      "          log_level='warning',\n",
      "          log_level_replica='warning',\n",
      "          log_on_each_node=True,\n",
      "          logging_dir='output/runs',\n",
      "          logging_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
      "          logging_first_step=False,\n",
      "          logging_steps=50,\n",
      "          logging_nan_inf_filter=True,\n",
      "          save_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
      "          save_steps=100,\n",
      "          save_total_limit=None,\n",
      "          save_safetensors=True,\n",
      "          save_on_each_node=False,\n",
      "          save_only_model=False,\n",
      "          restore_callback_states_from_checkpoint=False,\n",
      "          no_cuda=False,\n",
      "          use_cpu=False,\n",
      "          use_mps_device=False,\n",
      "          seed=42,\n",
      "          data_seed=None,\n",
      "          jit_mode_eval=False,\n",
      "          use_ipex=False,\n",
      "          bf16=False,\n",
      "          fp16=False,\n",
      "          fp16_opt_level='O1',\n",
      "          half_precision_backend='auto',\n",
      "          bf16_full_eval=False,\n",
      "          fp16_full_eval=False,\n",
      "          tf32=None,\n",
      "          local_rank=0,\n",
      "          ddp_backend=None,\n",
      "          tpu_num_cores=None,\n",
      "          tpu_metrics_debug=False,\n",
      "          debug=[],\n",
      "          dataloader_drop_last=False,\n",
      "          eval_steps=50,\n",
      "          dataloader_num_workers=0,\n",
      "          dataloader_prefetch_factor=None,\n",
      "          past_index=-1,\n",
      "          run_name='gpt2_pt',\n",
      "          disable_tqdm=False,\n",
      "          remove_unused_columns=True,\n",
      "          label_names=None,\n",
      "          load_best_model_at_end=False,\n",
      "          metric_for_best_model=None,\n",
      "          greater_is_better=None,\n",
      "          ignore_data_skip=False,\n",
      "          fsdp=[],\n",
      "          fsdp_min_num_params=0,\n",
      "          fsdp_config={'min_num_params': 0,\n",
      "                       'xla': False,\n",
      "                       'xla_fsdp_grad_ckpt': False,\n",
      "                       'xla_fsdp_v2': False},\n",
      "          fsdp_transformer_layer_cls_to_wrap=None,\n",
      "          accelerator_config=AcceleratorConfig(split_batches=False,\n",
      "                                               dispatch_batches=None,\n",
      "                                               even_batches=True,\n",
      "                                               use_seedable_sampler=True,\n",
      "                                               non_blocking=False,\n",
      "                                               gradient_accumulation_kwargs=None,\n",
      "                                               use_configured_state=False),\n",
      "          deepspeed=None,\n",
      "          label_smoothing_factor=0.0,\n",
      "          optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>,\n",
      "          optim_args=None,\n",
      "          adafactor=False,\n",
      "          group_by_length=False,\n",
      "          length_column_name='length',\n",
      "          report_to=['tensorboard'],\n",
      "          ddp_find_unused_parameters=None,\n",
      "          ddp_bucket_cap_mb=None,\n",
      "          ddp_broadcast_buffers=None,\n",
      "          dataloader_pin_memory=True,\n",
      "          dataloader_persistent_workers=False,\n",
      "          skip_memory_metrics=True,\n",
      "          use_legacy_prediction_loop=False,\n",
      "          push_to_hub=False,\n",
      "          resume_from_checkpoint=None,\n",
      "          hub_model_id=None,\n",
      "          hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>,\n",
      "          hub_token=None,\n",
      "          hub_private_repo=False,\n",
      "          hub_always_push=False,\n",
      "          gradient_checkpointing=False,\n",
      "          gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "          include_inputs_for_metrics=False,\n",
      "          eval_do_concat_batches=True,\n",
      "          fp16_backend='auto',\n",
      "          evaluation_strategy=None,\n",
      "          push_to_hub_model_id=None,\n",
      "          push_to_hub_organization=None,\n",
      "          push_to_hub_token=None,\n",
      "          mp_parameters='',\n",
      "          auto_find_batch_size=False,\n",
      "          full_determinism=False,\n",
      "          torchdynamo=None,\n",
      "          ray_scope='last',\n",
      "          ddp_timeout=1800,\n",
      "          torch_compile=False,\n",
      "          torch_compile_backend=None,\n",
      "          torch_compile_mode=None,\n",
      "          dispatch_batches=None,\n",
      "          split_batches=None,\n",
      "          include_tokens_per_second=False,\n",
      "          include_num_input_tokens_seen=False,\n",
      "          neftune_noise_alpha=None,\n",
      "          optim_target_modules=None,\n",
      "          batch_eval_metrics=False,\n",
      "          eval_on_start=False,\n",
      "          eval_use_gather_object=False,\n",
      "          dataset_text_field='text',\n",
      "          packing=False,\n",
      "          max_seq_length=128,\n",
      "          dataset_num_proc=None,\n",
      "          dataset_batch_size=1000,\n",
      "          model_init_kwargs=None,\n",
      "          dataset_kwargs=None,\n",
      "          eval_packing=None,\n",
      "          num_of_sequences=1024,\n",
      "          chars_per_token=3.6,\n",
      "          use_liger=False)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "[2024-09-19 09:57:04,255][oumi][rank0][pid:231378][MainThread][INFO]][debugging_utils.py:74] Max Memory Usage Before Training: GPU memory occupied: 2215.0 MiB.\n",
      "[2024-09-19 09:57:04,262][oumi][rank0][pid:231378][MainThread][INFO]][debugging_utils.py:106] Device Temperature Before Training: GPU temperature: 37.0 C.\n",
      "[2024-09-19 09:57:04,263][oumi][rank0][pid:231378][MainThread][INFO]][train.py:308] Training init time: 21.216314554214478s\n",
      "[2024-09-19 09:57:04,263][oumi][rank0][pid:231378][MainThread][INFO]][train.py:309] Starting training...\n",
      "[2024-09-19 09:57:04,264][oumi][rank0][pid:231378][MainThread][INFO]][hf_trainer.py:18] Starting training with transformers==4.43.4...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (845 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00, Epoch 1/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-19 09:57:07,739][oumi][rank0][pid:231378][MainThread][INFO]][train.py:312] Training is Complete.\n",
      "[2024-09-19 09:57:07,740][oumi][rank0][pid:231378][MainThread][INFO]][debugging_utils.py:74] Max Memory Usage After Training: GPU memory occupied: 3687.0 MiB.\n",
      "[2024-09-19 09:57:07,744][oumi][rank0][pid:231378][MainThread][INFO]][debugging_utils.py:106] Device Temperature After Training: GPU temperature: 39.0 C.\n",
      "[2024-09-19 09:57:07,745][oumi][rank0][pid:231378][MainThread][INFO]][train.py:319] Saving final state...\n",
      "[2024-09-19 09:57:07,746][oumi][rank0][pid:231378][MainThread][INFO]][train.py:322] Saving final model...\n",
      "[2024-09-19 09:57:07,958][oumi][rank0][pid:231378][MainThread][INFO]][hf_trainer.py:56] Model has been saved at tour_tutorial/output.\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.configs import TrainingConfig\n",
    "from oumi.train import train\n",
    "\n",
    "config = TrainingConfig.from_yaml(str(Path(tutorial_dir) / \"train.yaml\"))\n",
    "config.training.output_dir = str(Path(tutorial_dir) / \"output\")\n",
    "\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you've trained your first model using OUMI!\n",
    "\n",
    "You can also train your own custom Pytorch model. We cover that in depth in our [Finetuning Tutorial](https://github.com/oumi-ai/oumi/blob/main/notebooks/OUMI%20-%20Finetuning%20Tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference\n",
    "\n",
    "Now that you've trained a model, let's run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tour_tutorial/train_inference_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/train_inference_config.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"tour_tutorial/output\"\n",
    "  trust_remote_code: true\n",
    "  torch_dtype_str: \"half\"\n",
    "  device_map: \"auto\"\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 128\n",
    "  batch_size: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-19 10:04:03,323][oumi][rank0][pid:231378][MainThread][INFO]][models.py:144] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
      "[2024-09-19 10:04:03,324][oumi][rank0][pid:231378][MainThread][INFO]][models.py:240] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
      "2024-09-19:10:04:03,340 INFO     [modeling.py:1086] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "[2024-09-19 10:04:03,750][oumi][rank0][pid:231378][MainThread][WARNING]][models.py:284] No chat template found for tokenizer. Please specify a chat template using the `chat_template` field. This will be required in future versions of OUMI.\n",
      "Generating Model Responses:   0%|                                                                 | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Model Responses: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " stream stream stream stream stream stream stream stream stream stream stream stream stream stream stream stream stream stream hepat hepat hepat hepat hepat hepat hepat hepat hepat hepat hepat hepat hepat hepat hepat hepat hepat hepat hepat circulate circulate circulate circulate circulate circulate circulate circulate circulate rapist rapist rapist Mult Mult Mult Mult Mult combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination combination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.configs import InferenceConfig\n",
    "from oumi.infer import infer\n",
    "\n",
    "config = InferenceConfig.from_yaml(\n",
    "    str(Path(tutorial_dir) / \"train_inference_config.yaml\")\n",
    ")\n",
    "\n",
    "input_text = (\n",
    "    \"Remember that we didn't train for long, so the results might not be great.\"\n",
    ")\n",
    "\n",
    "results = infer(config.model, config.generation, [input_text])\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run inference using the pretrained model by slightly tweaking our config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-19 10:03:55,553][oumi][rank0][pid:231378][MainThread][INFO]][models.py:144] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
      "[2024-09-19 10:03:55,651][oumi][rank0][pid:231378][MainThread][INFO]][models.py:240] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
      "2024-09-19:10:03:55,666 INFO     [modeling.py:1086] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "[2024-09-19 10:03:56,319][oumi][rank0][pid:231378][MainThread][WARNING]][models.py:274] <pad> token not found: setting <pad> with <eos>.\n",
      "[2024-09-19 10:03:56,320][oumi][rank0][pid:231378][MainThread][WARNING]][models.py:284] No chat template found for tokenizer. Please specify a chat template using the `chat_template` field. This will be required in future versions of OUMI.\n",
      "Generating Model Responses:   0%|                                                                 | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Generating Model Responses: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first time I saw the new \"The Walking Dead\" trailer, I was so excited. I was so excited to see the first trailer for the upcoming season of the AMC series. I was so excited to see the first trailer for the upcoming season of the AMC series.\n",
      "\n",
      "I was so excited to see the first trailer for the upcoming season of the AMC series. I was so excited to see the first trailer for the upcoming season of the AMC series.\n",
      "\n",
      "I was so excited to see the first trailer for the upcoming season of the AMC series. I was so excited to see the first trailer for the upcoming season of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pretrained_config = InferenceConfig.from_yaml(\n",
    "    str(Path(tutorial_dir) / \"train_inference_config.yaml\")\n",
    ")\n",
    "pretrained_config.model.model_name = \"gpt2\"\n",
    "\n",
    "input_text = \"Input for the pretrained model: What is your name? \"\n",
    "\n",
    "results = infer(pretrained_config.model, pretrained_config.generation, [input_text])\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a model against common benchmarks\n",
    "\n",
    "You can use OUMI to evaluate pretrained and tuned models against standard benchmarks. For example, let's evaluate the pretrained version of our GPT2 model against `Hellaswag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tour_tutorial/eval.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/eval.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"gpt2\"\n",
    "  trust_remote_code: True\n",
    "\n",
    "data:\n",
    "  datasets:\n",
    "    - dataset_name: \"hellaswag\"\n",
    "\n",
    "generation:\n",
    "  batch_size: 0  # This will let LM HARNESS decide.\n",
    "\n",
    "evaluation_framework: LM_HARNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19:10:04:21,328 INFO     [evaluator.py:152] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-09-19:10:04:21,328 INFO     [evaluator.py:176] Initializing hf model, with arguments: {'pretrained': 'gpt2', 'trust_remote_code': True, 'parallelize': False}\n",
      "2024-09-19:10:04:21,329 INFO     [huggingface.py:170] Using device 'cuda:0'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45c634137184f829e62c56bfcfff04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hellaswag.py:   0%|          | 0.00/4.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7951c2ff4c104f018dde915e7aea6486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/2.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f5c5cc1c6b47839e79497b756bc261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d42e356264448aca884d067555bd663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/47.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41823591f2c743d49c6663f51176de75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed07f21554414f2489ca7c43f96b175c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94939164d80450a9f2df17934d00380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/39905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcafeeaab95a48a789a0536a69d71bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50dd2639436c4dc89d4a4fa73ff1e5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f5b21e739840f595d73abacdcf25c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0193d87109142a3994ee63a1198d7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19:10:04:44,461 INFO     [evaluator.py:261] Setting fewshot random generator seed to 1234\n",
      "2024-09-19:10:04:44,462 INFO     [task.py:411] Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10042/10042 [00:01<00:00, 6011.29it/s]\n",
      "2024-09-19:10:04:46,709 INFO     [evaluator.py:438] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|████████████████████████████████████████████| 40168/40168 [03:17<00:00, 203.64it/s]\n",
      "[2024-09-19 10:08:17,897][oumi][rank0][pid:231378][MainThread][INFO]][evaluate.py:199] hellaswag's metric dictionary is {'acc,none': 0.2894841665006971,\n",
      " 'acc_norm,none': 0.3110934076877116,\n",
      " 'acc_norm_stderr,none': 0.004619948037222892,\n",
      " 'acc_stderr,none': 0.004525960965551725,\n",
      " 'alias': 'hellaswag',\n",
      " 'elapsed_time_sec': 236.5706021785736}\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.configs import EvaluationConfig\n",
    "from oumi.evaluate import evaluate\n",
    "\n",
    "eval_config = EvaluationConfig.from_yaml(str(Path(tutorial_dir) / \"eval.yaml\"))\n",
    "# Uncomment the following line to run evals against the V1 HuggingFace Leaderboard.\n",
    "# This may take a while.\n",
    "# eval_config.data.datasets[0].dataset_name = \"huggingface_leaderboard_v1\"\n",
    "\n",
    "evaluate(eval_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching Jobs\n",
    "\n",
    "Often times you'll need to run various tasks (training, evaluation, etc) on remote hardware that's better suited for the task. OUMI can handle this for you by launching jobs on various compute clusters. For more information about running jobs, see our [Running Jobs Remotely tutorial](https://github.com/oumi-ai/oumi/blob/main/notebooks/OUMI%20-%20Running%20Jobs%20Remotely.ipynb). For running jobs on custom clusters, see our [Launching Jobs on Custom Clusters tutorial](https://github.com/oumi-ai/oumi/blob/main/notebooks/OUMI%20-%20Launching%20Jobs%20on%20Custom%20Clusters.ipynb).\n",
    "\n",
    "\n",
    "Today, OUMI supports running jobs on several cloud provider platforms.\n",
    "\n",
    "For the latest list, we can run the `which_cloud` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tour_tutorial/job.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/job.yaml\n",
    "\n",
    "name: hello-world\n",
    "resources:\n",
    "  cloud: local\n",
    "\n",
    "# Upload working directory to remote.\n",
    "working_dir: .\n",
    "\n",
    "envs:\n",
    "  TEST_ENV_VARIABLE: '\"Hello, World!\"'\n",
    "  OUMI_LOGGING_DIR: \"tour_tutorial/logs\"\n",
    "\n",
    "\n",
    "run: |\n",
    "  set -e  # Exit if any command failed.\n",
    "\n",
    "  echo \"$TEST_ENV_VARIABLE\"\n",
    "  oumi-train -c tour_tutorial/train.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported Clouds in OUMI:\n",
      "local\n",
      "polaris\n",
      "runpod\n",
      "gcp\n",
      "lambda\n"
     ]
    }
   ],
   "source": [
    "import oumi.launcher as launcher\n",
    "\n",
    "print(\"Supported Clouds in OUMI:\")\n",
    "for cloud in launcher.which_clouds():\n",
    "    print(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a simple \"Hello World\" job locally to demonstrate how to use the OUMI job launcher. This job will echo `Hello World`, then run the same GPT2 training job executed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job is running...\n",
      "Job is running...\n",
      "Job is running...\n",
      "Job is running...\n",
      "Job is running...\n",
      "Job is running...\n",
      "Job is running...\n",
      "Job is done!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "job_config = launcher.JobConfig.from_yaml(str(Path(tutorial_dir) / \"job.yaml\"))\n",
    "cluster, job = launcher.up(job_config, cluster_name=None)\n",
    "\n",
    "while job and not job.done:\n",
    "    print(\"Job is running...\")\n",
    "    time.sleep(5)\n",
    "    job = cluster.get_job(job.id)\n",
    "print(\"Job is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job created logs under `/tour_tutorial/logs`. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file: tour_tutorial/logs/2024_09_19_10_10_14_180_0.stdout\n",
      "\"Hello, World!\"\n",
      "{'train_runtime': 2.4145, 'train_samples_per_second': 8.283, 'train_steps_per_second': 4.142, 'train_loss': 10.95625, 'epoch': 1.0}\n",
      "\n",
      "Log file: tour_tutorial/logs/2024_09_19_10_10_14_180_0.stderr\n",
      "[2024-09-19 10:10:17,059][oumi][rank0][pid:235563][MainThread][INFO]][train.py:176] Setting random seed to 42 on rank 0.\n",
      "[2024-09-19 10:10:17,060][oumi][rank0][pid:235563][MainThread][INFO]][train.py:124] Creating training.telemetry_dir: output/telemetry...\n",
      "[2024-09-19 10:10:17,060][oumi][rank0][pid:235563][MainThread][INFO]][train.py:126] Created training.telemetry_dir absolute path: /home/user/oumi/notebooks/output/telemetry\n",
      "[2024-09-19 10:10:17,060][oumi][rank0][pid:235563][MainThread][INFO]][torch_utils.py:48] Torch version: 2.4.1+cu121. NumPy version: 2.1.1\n",
      "[2024-09-19 10:10:17,061][oumi][rank0][pid:235563][MainThread][INFO]][torch_utils.py:59] CUDA version: 12.1 CuDNN version: 90.1.0\n",
      "[2024-09-19 10:10:17,061][oumi][rank0][pid:235563][MainThread][INFO]][torch_utils.py:93] CPU cores: 24 CUDA devices: 1\n",
      "device(0)='NVIDIA GeForce RTX 3090' Capability: (8, 6) Memory: [Total: 24.0GiB Free: 22.77GiB Allocated: 0.0GiB Cached: 0.0GiB]\n",
      "[2024-09-19 10:10:17,062][oumi][rank0][pid:235563][MainThread][INFO]][torch_utils.py:43] TrainingConfig: TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='HuggingFaceFW/fineweb-edu',\n",
      "                                                                                subset='sample-10BT',\n",
      "                                                                                split='train',\n",
      "                                                                                dataset_kwargs={},\n",
      "                                                                                sample_count=None,\n",
      "                                                                                mixture_proportion=None,\n",
      "                                                                                shuffle=False,\n",
      "                                                                                seed=None,\n",
      "                                                                                shuffle_buffer_size=1000,\n",
      "                                                                                trust_remote_code=False,\n",
      "                                                                                preprocessing_function_name=None,\n",
      "                                                                                preprocessing_function_kwargs={'batched': False})],\n",
      "                                                        pack=True,\n",
      "                                                        stream=True,\n",
      "                                                        target_col='text',\n",
      "                                                        mixture_strategy='first_exhausted',\n",
      "                                                        seed=None,\n",
      "                                                        experimental_use_async_dataset=False,\n",
      "                                                        experimental_use_torch_datapipes=False),\n",
      "                               test=DatasetSplitParams(datasets=[],\n",
      "                                                       pack=False,\n",
      "                                                       stream=False,\n",
      "                                                       target_col=None,\n",
      "                                                       mixture_strategy='first_exhausted',\n",
      "                                                       seed=None,\n",
      "                                                       experimental_use_async_dataset=False,\n",
      "                                                       experimental_use_torch_datapipes=False),\n",
      "                               validation=DatasetSplitParams(datasets=[],\n",
      "                                                             pack=False,\n",
      "                                                             stream=False,\n",
      "                                                             target_col=None,\n",
      "                                                             mixture_strategy='first_exhausted',\n",
      "                                                             seed=None,\n",
      "                                                             experimental_use_async_dataset=False,\n",
      "                                                             experimental_use_torch_datapipes=False)),\n",
      "               model=ModelParams(model_name='gpt2',\n",
      "                                 adapter_model=None,\n",
      "                                 tokenizer_name=None,\n",
      "                                 model_max_length=128,\n",
      "                                 load_pretrained_weights=False,\n",
      "                                 trust_remote_code=True,\n",
      "                                 torch_dtype_str='bfloat16',\n",
      "                                 compile=False,\n",
      "                                 chat_template=None,\n",
      "                                 attn_implementation=None,\n",
      "                                 device_map='auto',\n",
      "                                 model_kwargs={'disable_dropout': True},\n",
      "                                 enable_liger_kernel=False,\n",
      "                                 shard_for_eval=False,\n",
      "                                 freeze_layers=[]),\n",
      "               training=TrainingParams(use_peft=False,\n",
      "                                       trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>,\n",
      "                                       enable_gradient_checkpointing=False,\n",
      "                                       gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "                                       output_dir='output',\n",
      "                                       per_device_train_batch_size=2,\n",
      "                                       per_device_eval_batch_size=8,\n",
      "                                       gradient_accumulation_steps=1,\n",
      "                                       max_steps=10,\n",
      "                                       num_train_epochs=3,\n",
      "                                       save_epoch=False,\n",
      "                                       save_steps=100,\n",
      "                                       save_final_model=True,\n",
      "                                       seed=42,\n",
      "                                       run_name='gpt2_pt',\n",
      "                                       metrics_function=None,\n",
      "                                       log_level='info',\n",
      "                                       dep_log_level='warning',\n",
      "                                       enable_wandb=False,\n",
      "                                       enable_tensorboard=True,\n",
      "                                       logging_strategy='steps',\n",
      "                                       logging_dir='output/runs',\n",
      "                                       logging_steps=50,\n",
      "                                       logging_first_step=False,\n",
      "                                       eval_strategy='no',\n",
      "                                       eval_steps=50,\n",
      "                                       learning_rate=0.0006,\n",
      "                                       lr_scheduler_type='cosine_with_min_lr',\n",
      "                                       lr_scheduler_kwargs={'min_lr_rate': 0.1},\n",
      "                                       warmup_ratio=None,\n",
      "                                       warmup_steps=715,\n",
      "                                       optimizer='adamw_torch',\n",
      "                                       weight_decay=0.1,\n",
      "                                       adam_beta1=0.9,\n",
      "                                       adam_beta2=0.95,\n",
      "                                       adam_epsilon=1e-08,\n",
      "                                       sgd_momentum=0.9,\n",
      "                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,\n",
      "                                       compile=False,\n",
      "                                       include_performance_metrics=False,\n",
      "                                       include_alternative_mfu_metrics=False,\n",
      "                                       log_model_summary=False,\n",
      "                                       resume_from_checkpoint=None,\n",
      "                                       try_resume_from_last_checkpoint=False,\n",
      "                                       dataloader_num_workers=0,\n",
      "                                       dataloader_prefetch_factor=None,\n",
      "                                       dataloader_main_process_only=None,\n",
      "                                       ddp_find_unused_parameters=None,\n",
      "                                       max_grad_norm=1.0,\n",
      "                                       trainer_kwargs={'dataset_text_field': 'text',\n",
      "                                                       'max_seq_length': 128},\n",
      "                                       profiler=ProfilerParams(save_dir=None,\n",
      "                                                               enable_cpu_profiling=False,\n",
      "                                                               enable_cuda_profiling=False,\n",
      "                                                               record_shapes=False,\n",
      "                                                               profile_memory=False,\n",
      "                                                               with_stack=False,\n",
      "                                                               with_flops=False,\n",
      "                                                               with_modules=False,\n",
      "                                                               row_limit=50,\n",
      "                                                               schedule=ProfilerScheduleParams(enable_schedule=False,\n",
      "                                                                                               wait=0,\n",
      "                                                                                               warmup=1,\n",
      "                                                                                               active=3,\n",
      "                                                                                               repeat=1,\n",
      "                                                                                               skip_first=1)),\n",
      "                                       telemetry=TelemetryParams(telemetry_dir='telemetry',\n",
      "                                                                 collect_telemetry_for_all_ranks=False,\n",
      "                                                                 track_gpu_temperature=False),\n",
      "                                       empty_device_cache_steps=None,\n",
      "                                       nccl_default_timeout_minutes=None),\n",
      "               peft=PeftParams(lora_r=16,\n",
      "                               lora_alpha=16,\n",
      "                               lora_dropout=0.05,\n",
      "                               lora_target_modules=None,\n",
      "                               lora_modules_to_save=None,\n",
      "                               lora_bias='none',\n",
      "                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
      "                               q_lora=False,\n",
      "                               q_lora_bits=4,\n",
      "                               bnb_4bit_quant_type='fp4',\n",
      "                               use_bnb_nested_quant=False,\n",
      "                               bnb_4bit_quant_storage='uint8'),\n",
      "               fsdp=FSDPParams(enable_fsdp=False,\n",
      "                               sharding_strategy=<ShardingStrategy.FULL_SHARD: 'FULL_SHARD'>,\n",
      "                               cpu_offload=False,\n",
      "                               mixed_precision=None,\n",
      "                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,\n",
      "                               forward_prefetch=False,\n",
      "                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,\n",
      "                               auto_wrap_policy=<AutoWrapPolicy.SIZE_BASED: 'SIZE_BASED'>,\n",
      "                               min_num_params=100000,\n",
      "                               transformer_layer_cls=None,\n",
      "                               sync_module_states=True))\n",
      "[2024-09-19 10:10:17,339][oumi][rank0][pid:235563][MainThread][WARNING]][models.py:274] <pad> token not found: setting <pad> with <eos>.\n",
      "[2024-09-19 10:10:17,339][oumi][rank0][pid:235563][MainThread][WARNING]][models.py:284] No chat template found for tokenizer. Please specify a chat template using the `chat_template` field. This will be required in future versions of OUMI.\n",
      "[2024-09-19 10:10:17,339][oumi][rank0][pid:235563][MainThread][INFO]][models.py:144] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
      "[2024-09-19 10:10:17,448][oumi][rank0][pid:235563][MainThread][INFO]][torch_naming_heuristics.py:38] Found these dropout attributes and set their values to 0.0: ['resid_pdrop', 'embd_pdrop', 'attn_pdrop', 'summary_first_dropout']\n",
      "[2024-09-19 10:10:17,448][oumi][rank0][pid:235563][MainThread][INFO]][models.py:240] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
      "[2024-09-19 10:10:36,432][oumi][rank0][pid:235563][MainThread][INFO]][torch_profiler_utils.py:148] PROF: Torch Profiler disabled!\n",
      "[2024-09-19 10:10:36,472][oumi][rank0][pid:235563][MainThread][INFO]][training.py:44] SFTConfig(output_dir='output',\n",
      "          overwrite_output_dir=False,\n",
      "          do_train=False,\n",
      "          do_eval=False,\n",
      "          do_predict=False,\n",
      "          eval_strategy=<IntervalStrategy.NO: 'no'>,\n",
      "          prediction_loss_only=False,\n",
      "          per_device_train_batch_size=2,\n",
      "          per_device_eval_batch_size=8,\n",
      "          per_gpu_train_batch_size=None,\n",
      "          per_gpu_eval_batch_size=None,\n",
      "          gradient_accumulation_steps=1,\n",
      "          eval_accumulation_steps=None,\n",
      "          eval_delay=0,\n",
      "          torch_empty_cache_steps=None,\n",
      "          learning_rate=0.0006,\n",
      "          weight_decay=0.1,\n",
      "          adam_beta1=0.9,\n",
      "          adam_beta2=0.95,\n",
      "          adam_epsilon=1e-08,\n",
      "          max_grad_norm=1.0,\n",
      "          num_train_epochs=3,\n",
      "          max_steps=10,\n",
      "          lr_scheduler_type=<SchedulerType.COSINE_WITH_MIN_LR: 'cosine_with_min_lr'>,\n",
      "          lr_scheduler_kwargs={'min_lr_rate': 0.1},\n",
      "          warmup_ratio=0.0,\n",
      "          warmup_steps=715,\n",
      "          log_level='warning',\n",
      "          log_level_replica='warning',\n",
      "          log_on_each_node=True,\n",
      "          logging_dir='output/runs',\n",
      "          logging_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
      "          logging_first_step=False,\n",
      "          logging_steps=50,\n",
      "          logging_nan_inf_filter=True,\n",
      "          save_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
      "          save_steps=100,\n",
      "          save_total_limit=None,\n",
      "          save_safetensors=True,\n",
      "          save_on_each_node=False,\n",
      "          save_only_model=False,\n",
      "          restore_callback_states_from_checkpoint=False,\n",
      "          no_cuda=False,\n",
      "          use_cpu=False,\n",
      "          use_mps_device=False,\n",
      "          seed=42,\n",
      "          data_seed=None,\n",
      "          jit_mode_eval=False,\n",
      "          use_ipex=False,\n",
      "          bf16=False,\n",
      "          fp16=False,\n",
      "          fp16_opt_level='O1',\n",
      "          half_precision_backend='auto',\n",
      "          bf16_full_eval=False,\n",
      "          fp16_full_eval=False,\n",
      "          tf32=None,\n",
      "          local_rank=0,\n",
      "          ddp_backend=None,\n",
      "          tpu_num_cores=None,\n",
      "          tpu_metrics_debug=False,\n",
      "          debug=[],\n",
      "          dataloader_drop_last=False,\n",
      "          eval_steps=50,\n",
      "          dataloader_num_workers=0,\n",
      "          dataloader_prefetch_factor=None,\n",
      "          past_index=-1,\n",
      "          run_name='gpt2_pt',\n",
      "          disable_tqdm=False,\n",
      "          remove_unused_columns=True,\n",
      "          label_names=None,\n",
      "          load_best_model_at_end=False,\n",
      "          metric_for_best_model=None,\n",
      "          greater_is_better=None,\n",
      "          ignore_data_skip=False,\n",
      "          fsdp=[],\n",
      "          fsdp_min_num_params=0,\n",
      "          fsdp_config={'min_num_params': 0,\n",
      "                       'xla': False,\n",
      "                       'xla_fsdp_grad_ckpt': False,\n",
      "                       'xla_fsdp_v2': False},\n",
      "          fsdp_transformer_layer_cls_to_wrap=None,\n",
      "          accelerator_config=AcceleratorConfig(split_batches=False,\n",
      "                                               dispatch_batches=None,\n",
      "                                               even_batches=True,\n",
      "                                               use_seedable_sampler=True,\n",
      "                                               non_blocking=False,\n",
      "                                               gradient_accumulation_kwargs=None,\n",
      "                                               use_configured_state=False),\n",
      "          deepspeed=None,\n",
      "          label_smoothing_factor=0.0,\n",
      "          optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>,\n",
      "          optim_args=None,\n",
      "          adafactor=False,\n",
      "          group_by_length=False,\n",
      "          length_column_name='length',\n",
      "          report_to=['tensorboard'],\n",
      "          ddp_find_unused_parameters=None,\n",
      "          ddp_bucket_cap_mb=None,\n",
      "          ddp_broadcast_buffers=None,\n",
      "          dataloader_pin_memory=True,\n",
      "          dataloader_persistent_workers=False,\n",
      "          skip_memory_metrics=True,\n",
      "          use_legacy_prediction_loop=False,\n",
      "          push_to_hub=False,\n",
      "          resume_from_checkpoint=None,\n",
      "          hub_model_id=None,\n",
      "          hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>,\n",
      "          hub_token=None,\n",
      "          hub_private_repo=False,\n",
      "          hub_always_push=False,\n",
      "          gradient_checkpointing=False,\n",
      "          gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "          include_inputs_for_metrics=False,\n",
      "          eval_do_concat_batches=True,\n",
      "          fp16_backend='auto',\n",
      "          evaluation_strategy=None,\n",
      "          push_to_hub_model_id=None,\n",
      "          push_to_hub_organization=None,\n",
      "          push_to_hub_token=None,\n",
      "          mp_parameters='',\n",
      "          auto_find_batch_size=False,\n",
      "          full_determinism=False,\n",
      "          torchdynamo=None,\n",
      "          ray_scope='last',\n",
      "          ddp_timeout=1800,\n",
      "          torch_compile=False,\n",
      "          torch_compile_backend=None,\n",
      "          torch_compile_mode=None,\n",
      "          dispatch_batches=None,\n",
      "          split_batches=None,\n",
      "          include_tokens_per_second=False,\n",
      "          include_num_input_tokens_seen=False,\n",
      "          neftune_noise_alpha=None,\n",
      "          optim_target_modules=None,\n",
      "          batch_eval_metrics=False,\n",
      "          eval_on_start=False,\n",
      "          eval_use_gather_object=False,\n",
      "          dataset_text_field='text',\n",
      "          packing=False,\n",
      "          max_seq_length=128,\n",
      "          dataset_num_proc=None,\n",
      "          dataset_batch_size=1000,\n",
      "          model_init_kwargs=None,\n",
      "          dataset_kwargs=None,\n",
      "          eval_packing=None,\n",
      "          num_of_sequences=1024,\n",
      "          chars_per_token=3.6,\n",
      "          use_liger=False)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "[2024-09-19 10:10:36,616][oumi][rank0][pid:235563][MainThread][INFO]][debugging_utils.py:74] Max Memory Usage Before Training: GPU memory occupied: 3749.0 MiB.\n",
      "[2024-09-19 10:10:36,623][oumi][rank0][pid:235563][MainThread][INFO]][debugging_utils.py:106] Device Temperature Before Training: GPU temperature: 37.0 C.\n",
      "[2024-09-19 10:10:36,623][oumi][rank0][pid:235563][MainThread][INFO]][train.py:308] Training init time: 19.563547611236572s\n",
      "[2024-09-19 10:10:36,623][oumi][rank0][pid:235563][MainThread][INFO]][train.py:309] Starting training...\n",
      "[2024-09-19 10:10:36,623][oumi][rank0][pid:235563][MainThread][INFO]][hf_trainer.py:18] Starting training with transformers==4.43.4...\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (845 > 128). Running this sequence through the model will result in indexing errors\n",
      "\n",
      " 10%|█         | 1/10 [00:01<00:15,  1.69s/it]\n",
      " 60%|██████    | 6/10 [00:01<00:00,  4.45it/s]\n",
      "                                              \n",
      "\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.45it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.15it/s]\n",
      "[2024-09-19 10:10:39,180][oumi][rank0][pid:235563][MainThread][INFO]][train.py:312] Training is Complete.\n",
      "[2024-09-19 10:10:39,181][oumi][rank0][pid:235563][MainThread][INFO]][debugging_utils.py:74] Max Memory Usage After Training: GPU memory occupied: 4912.0 MiB.\n",
      "[2024-09-19 10:10:39,185][oumi][rank0][pid:235563][MainThread][INFO]][debugging_utils.py:106] Device Temperature After Training: GPU temperature: 39.0 C.\n",
      "[2024-09-19 10:10:39,185][oumi][rank0][pid:235563][MainThread][INFO]][train.py:319] Saving final state...\n",
      "[2024-09-19 10:10:39,185][oumi][rank0][pid:235563][MainThread][INFO]][train.py:322] Saving final model...\n",
      "[2024-09-19 10:10:39,392][oumi][rank0][pid:235563][MainThread][INFO]][hf_trainer.py:56] Model has been saved at output.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_dir = Path(tutorial_dir) / \"logs\"\n",
    "for log_file in logs_dir.iterdir():\n",
    "    print(f\"Log file: {log_file}\")\n",
    "    with open(log_file, \"r\") as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing datasets and clusters\n",
    "\n",
    "OUMI offers rich customization that allows users to build custom solutions on top of our existing building blocks. Several of OUMI's primary resources (Datasets, Clouds, etc) leverage the OUMI Registry when invoked.\n",
    "\n",
    "This registry allows users to build custom classes that function as drop-in replacements for core functionality.\n",
    "\n",
    "For more details on registering custom datasets, see the [demo here](https://github.com/oumi-ai/oumi/blob/main/USAGE.md#6-custom-datasets).\n",
    "\n",
    "For a tutorial on writing a custom cloud/cluster for running jobs, see the [tutorial here](https://github.com/oumi-ai/oumi/blob/main/notebooks/OUMI%20-%20Launching%20Jobs%20on%20Custom%20Clusters.ipynb).\n",
    "\n",
    "You can find further information about the required registry decorators [here](https://learning-machines.ai/docs/latest/apidoc/oumi.core.html#oumi.core.registry.register_cloud_builder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's next?\n",
    "\n",
    "Now that you've completed our tour, you're ready to tackle our other [notebook guides](https://github.com/oumi-ai/oumi/tree/main/notebooks). \n",
    "\n",
    "Make sure you also take a look at our [Usage guide](https://github.com/oumi-ai/oumi/blob/main/USAGE.md) for an overview of our CLI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
