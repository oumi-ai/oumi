{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"align-center\">\n",
    "<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n",
    "\n",
    "[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n",
    "[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n",
    "[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n",
    "</div>\n",
    "\n",
    "üëã Welcome to Open Universal Machine Intelligence (Oumi)!\n",
    "\n",
    "üöÄ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](https://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n",
    "\n",
    "ü§ù Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n",
    "\n",
    "‚≠ê If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Inference Engine\n",
    "\n",
    "This notebook demonstrates how to use the `VLLMInferenceEngine` class for inference with Llama 3.3 70B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "## Machine Requirements\n",
    "\n",
    "‚ùó**NOTICE:** This notebook doesn't run on Colab because the GPU is too old to be supported by vLLM.\n",
    "\n",
    "It is recommended to run this notebook on a machine with GPU support, as vLLM is mainly intended to run on GPUs. Llama 3.3 70B requires 140GB VRAM to serve, though we also provide examples below for inference with Llama 3.1 8B, Llama 3.2 1B, and quantized Llama 3.3 70B that require less memory.\n",
    "\n",
    "If your local machine cannot run this notebook, you can instead run this notebook on a cloud platform. The following demonstrates how to open a VSCode instance backed by a GCP node with 4 A100 GPUs, from which the notebook can be run.\n",
    "\n",
    "```bash\n",
    "# Run on your local machine\n",
    "gcloud auth application-default login  # Authenticate with GCP\n",
    "make gcpcode ARGS=\"--resources.accelerators A100:4\"  # 4 A100-40GB GPUs, enough for 70B model. Can also use 2x \"A100-80GB\"\n",
    "```\n",
    "\n",
    "## Oumi Installation\n",
    "\n",
    "First, let's install Oumi and vLLM. You can find more detailed instructions about Oumi installation [here](https://oumi.ai/docs/en/latest/get_started/installation.html). Here, we include Oumi's GPU dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install oumi[gpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama Access\n",
    "\n",
    "Llama 3.3 70B is a gated model on HuggingFace Hub. To run this notebook, you must first complete the [agreement](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) on HuggingFace, and wait for it to be accepted. Then, specify `HF_TOKEN` below to enable access to the model if it's not already set.\n",
    "\n",
    "Usually, you can get the token by running this command `cat ~/.cache/huggingface/token` on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# if not os.environ.get(\"HF_TOKEN\"):\n",
    "#     # NOTE: Set your Hugging Face token here if not already set.\n",
    "#     os.environ[\"HF_TOKEN\"] = \"<MY_HF_TOKEN>\"\n",
    "# hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "# print(f\"Using HF Token: '{hf_token}'\")\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# This is needed for vLLM to use multiple GPUs in a notebook.\n",
    "# If you're not running in a notebook, you can ignore this.\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download Llama 3.3 70B to your machine before inference, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_transfer in /home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages (0.1.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/home/shanghong/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659\n"
     ]
    }
   ],
   "source": [
    "%pip install hf_transfer\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "! huggingface-cli download meta-llama/Llama-3.1-8B-Instruct --exclude original/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-30 22:44:05 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from oumi.core.configs import InferenceConfig\n",
    "from oumi.core.types import Conversation, Message, Role\n",
    "from oumi.inference import VLLMInferenceEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have multiple GPUs, we can use Ray to parallelize the inference.\n",
    "# This is essential if you're running a model that's too big to fit in a single GPU.\n",
    "\n",
    "import ray\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() >= 2:\n",
    "    ray.shutdown()\n",
    "    ray.init()  # num_gpus=torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the config file\n",
    "\n",
    "Note: in this section we are writing the config file to the current working directory.\n",
    "\n",
    "An alternative option is to initialize the params classes directly: `ModelParams`, `GenerationParams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"vllm_tutorial_llama70b_infer.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vllm_tutorial_llama70b_infer.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile vllm_tutorial_llama70b_infer.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"meta-llama/Llama-3.1-8B-Instruct\"  # 8B model, requires 1x A100-40GB GPUs\n",
    "  # model_name: \"meta-llama/Llama-3.3-70B-Instruct\"  # 70B model, requires 4x A100-40GB GPUs\n",
    "  model_max_length: 512\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  trust_remote_code: True\n",
    "  attn_implementation: \"sdpa\"\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 128\n",
    "  batch_size: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model and the inference engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-30 22:44:19,805][oumi][rank0][pid:1901055][MainThread][WARNING]][models.py:463] Undefined pad token. Setting it to `<|finetune_right_pad_id|>`.\n",
      "[2025-06-30 22:44:19,807][oumi][rank0][pid:1901055][MainThread][INFO]][models.py:506] Using the model's built-in chat template for model 'meta-llama/Llama-3.1-8B-Instruct'.\n",
      "INFO 06-30 22:44:29 [config.py:600] This model supports multiple tasks: {'score', 'classify', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 06-30 22:44:29 [config.py:1600] Defaulting to use mp for distributed inference\n",
      "INFO 06-30 22:44:29 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 06-30 22:44:29 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 06-30 22:44:35 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 06-30 22:44:39 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 06-30 22:44:39 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-30 22:44:39 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_23769b56'), local_subscribe_addr='ipc:///tmp/d996bf54-e650-421b-8bf9-79c50c6633fc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 06-30 22:44:44 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 06-30 22:44:47 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7a56f3f2ff90>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1901757)\u001b[0;0m INFO 06-30 22:44:47 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ae27480a'), local_subscribe_addr='ipc:///tmp/a4aa1e69-6c5c-48f4-b801-4c3aede23960', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 06-30 22:44:53 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 06-30 22:44:56 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f0164dfa950>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1901879)\u001b[0;0m INFO 06-30 22:44:56 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e8af06b8'), local_subscribe_addr='ipc:///tmp/e2831c49-2bd1-49bf-96a4-6a718c103d0a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1901879)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=1901757)\u001b[0;0m INFO 06-30 22:44:57 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1901757)\u001b[0;0m INFO 06-30 22:44:57 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 06-30 22:44:57 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1901879)\u001b[0;0m INFO 06-30 22:44:57 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1901757)\u001b[0;0m INFO 06-30 22:44:58 [custom_all_reduce_utils.py:206] generating GPU P2P access cache in /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1901757)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=1901879)\u001b[0;0m INFO 06-30 22:45:23 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 06-30 22:45:23 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1901757)\u001b[0;0m INFO 06-30 22:45:23 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_a40bb388'), local_subscribe_addr='ipc:///tmp/80211233-105e-4afe-8789-2447add4b03e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1901757)\u001b[0;0m INFO 06-30 22:45:23 [parallel_state.py:957] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1901757)\u001b[0;0m INFO 06-30 22:45:23 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1901757)\u001b[0;0m INFO 06-30 22:45:23 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1901879)\u001b[0;0m INFO 06-30 22:45:23 [parallel_state.py:957] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1901879)\u001b[0;0m INFO 06-30 22:45:23 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1901879)\u001b[0;0m INFO 06-30 22:45:23 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1901757)\u001b[0;0m WARNING 06-30 22:45:23 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1901879)\u001b[0;0m WARNING 06-30 22:45:23 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1901757)\u001b[0;0m INFO 06-30 22:45:23 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1901879)\u001b[0;0m INFO 06-30 22:45:23 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:06<00:18,  6.04s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:12<00:13,  6.52s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:19<00:06,  6.49s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:20<00:00,  4.42s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:20<00:00,  5.15s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1901757)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=1901879)\u001b[0;0m INFO 06-30 22:45:44 [loader.py:447] Loading weights took 20.54 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1901757)\u001b[0;0m INFO 06-30 22:45:44 [loader.py:447] Loading weights took 20.62 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1901757)\u001b[0;0m INFO 06-30 22:45:44 [gpu_model_runner.py:1273] Model loading took 7.5123 GiB and 20.930658 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1901879)\u001b[0;0m INFO 06-30 22:45:44 [gpu_model_runner.py:1273] Model loading took 7.5123 GiB and 20.897024 seconds\n",
      "INFO 06-30 22:45:50 [kv_cache_utils.py:578] GPU KV cache size: 929,840 tokens\n",
      "INFO 06-30 22:45:50 [kv_cache_utils.py:581] Maximum concurrency for 512 tokens per request: 1816.09x\n",
      "INFO 06-30 22:45:50 [kv_cache_utils.py:578] GPU KV cache size: 929,840 tokens\n",
      "INFO 06-30 22:45:50 [kv_cache_utils.py:581] Maximum concurrency for 512 tokens per request: 1816.09x\n",
      "INFO 06-30 22:45:50 [core.py:162] init engine (profile, create kv cache, warmup model) took 5.99 seconds\n",
      "CPU times: user 1.59 s, sys: 234 ms, total: 1.82 s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Download, and load the model in memory\n",
    "# This may take a while, depending on your internet speed.\n",
    "# The inference engine only needs to be loaded once and can be\n",
    "# reused for multiple conversations.\n",
    "\n",
    "config = InferenceConfig.from_yaml(config_path)\n",
    "\n",
    "inference_engine = VLLMInferenceEngine(\n",
    "    config.model,\n",
    "    tensor_parallel_size=torch.cuda.device_count(),  # use all available GPUs\n",
    "    # Enable prefix caching for vLLM.\n",
    "    # This is key for performance when running prompts with a long prefix,\n",
    "    # such as judging or conversations with large system prompts\n",
    "    # or few-shot examples.\n",
    "    enable_prefix_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing our inputs\n",
    "\n",
    "The inference engine expects a list of conversations, where each conversation is a list of messages.\n",
    "\n",
    "See the [Conversation](https://github.com/oumi-ai/oumi/blob/38b3d2b27407be5fc9be5a1dd88f9ad518f3491c/src/oumi/core/types/turn.py#L109) class for more details.\n",
    "\n",
    "Tip: you can visualize how the conversation is rendered as a prompt with the following:\n",
    "\n",
    "```python\n",
    "inference_engine.apply_chat_template(conversation, tokenize=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = [\n",
    "    Conversation(\n",
    "        messages=[\n",
    "            Message(\n",
    "                role=Role.SYSTEM, content=\"Translate the following text into French.\"\n",
    "            ),\n",
    "            Message(role=Role.USER, content=\"Hello, how are you?\"),\n",
    "        ]\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running inference\n",
    "\n",
    "Under the hood, the vLLM engine will batch the conversations to run inference with a high throughput.\n",
    "\n",
    "Make sure to feed all your prompts to the engine at once for maximum throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference for 1 conversations\n",
      "INFO 06-30 22:46:27 [chat_utils.py:396] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "CPU times: user 383 ms, sys: 43.9 ms, total: 426 ms\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(f\"Running inference for {len(conversations)} conversations\")\n",
    "\n",
    "generations = inference_engine.infer(\n",
    "    input=conversations,\n",
    "    inference_config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: Translate the following text into French.\n",
      "USER: Hello, how are you?\n",
      "ASSISTANT: Bonjour, comment allez-vous ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for conversation in generations:\n",
    "    print(repr(conversation))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Running quantized GGUF models\n",
    "\n",
    "You can also run quantized GGUF models, by downloading the model file and passing it to the engine.\n",
    "\n",
    "For example, to run the Llama 3.3 70B model quantized at 4-bit, you can do the following: \n",
    "\n",
    "First, we download the GGUF model file. There are multiple quantization schemes available, here we choose the `Q4_K_S` scheme which is 4-bit with the `K_S` quantization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo_id = \"bartowski/Llama-3.3-70B-Instruct-GGUF\"\n",
    "filename = \"Llama-3.3-70B-Instruct-Q4_K_S.gguf\"\n",
    "\n",
    "# Will download the model in the current working directory instead of HF_CACHE_DIR\n",
    "model_path = hf_hub_download(repo_id, filename=filename, local_dir=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then update the config file to point to the model we just downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vllm_tutorial_llama70b_infer.yaml\n",
    "\n",
    "model:\n",
    "  # Filepath to the GGUF model, which we just downloaded, see `model_path` output above\n",
    "  model_name: \"Meta-Llama-3.1-70B-Instruct-Q4_K_S.gguf\"  \n",
    "  # GGUF files do not have a config. We need to specify the tokenizer name manually.\n",
    "  tokenizer_name: \"meta-llama/Llama-3.3-70B-Instruct\"  \n",
    "  model_max_length: 512\n",
    "  torch_dtype_str: \"float16\"  # GGUF models require float16\n",
    "  trust_remote_code: True\n",
    "  attn_implementation: \"sdpa\"\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 128\n",
    "  batch_size: 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
