{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"align-center\">\n",
    "<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n",
    "\n",
    "[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n",
    "[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n",
    "[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - Train a Letter Counting Model using GRPO.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>\n",
    "\n",
    "üëã Welcome to Open Universal Machine Intelligence (Oumi)!\n",
    "\n",
    "üöÄ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](https://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n",
    "\n",
    "ü§ù Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n",
    "\n",
    "‚≠ê If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Letter Counting Model using GRPO\n",
    "\n",
    "Welcome to Oumi! In this tutorial notebook, we're going to fine-tune an LLM using Group Relative Policy Optimization (GRPO), a reinforcement learning algorithm. But first, a little (recent) history lesson --\n",
    "\n",
    "In June 2024, a user discovered that ChatGPT had a little problem -- it couldn't correctly answer a simple question, [\"How Many R‚Äôs Are in the Word Strawberry?\"](https://community.openai.com/t/incorrect-count-of-r-characters-in-the-word-strawberry/829618/2)\n",
    "\n",
    "Because of the way LLMs tokenize input strings, counting letters can be pretty tough for them! Fortunately, you (and Oumi!) are here to help.\n",
    "\n",
    "Below, we show you how to employ a custom evaluation function to evaluate popular models on the task of counting letters in words. Then, we will align Llama 3.2 3B to improve its performance on this task.\n",
    "\n",
    "This notebook includes cell outputs, but some irrelevant outputs (ex. install lines, warnings) are modified/removed for readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Machine Requirements\n",
    "\n",
    "This notebook runs both model evaluation and GRPO training, which require 8GB and 40GB VRAM, respectively.\n",
    "\n",
    "‚ùó**NOTICE:** If you're running this notebook on Colab using a T4 GPU, it's not possible to run training due to memory requirements. To run evaluation, some adjustments need to be made as vLLM doesn't support T4 GPUs. This will be explained in the evaluation section.\n",
    "\n",
    "If your local machine cannot run this notebook, you can instead run this notebook on a cloud platform. The following demonstrates how to open a VSCode instance backed by a GCP node with 4 A100 GPUs, from which the notebook can be run. It is possible to run this notebook on just 1 GPU, but you will need make some adjustments to training parameters, which will be explained in the training section.\n",
    "\n",
    "```bash\n",
    "# Run on your local machine\n",
    "gcloud auth application-default login  # Authenticate with GCP\n",
    "make gcpcode ARGS=\"--resources.accelerators A100:4\"\n",
    "```\n",
    "\n",
    "### Oumi Installation\n",
    "\n",
    "First, let's install Oumi and vLLM (part of the `gpu` optional dependencies). You can find more detailed instructions about Oumi installation [here](https://oumi.ai/docs/en/latest/get_started/installation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install oumi[gpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote API Access\n",
    "\n",
    "As part of this notebook, you can evaluate frontier models from Open AI, Google, Anthropic, and Meta on the letter counting task. If you want to evaluate any of these models, set the corresponding fields below. The code is commented out by default to avoid any accidental overwriting of existing variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"  # Set your OpenAI API key here.\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"\"  # Set your Gemini API key here.\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"\"  # Set your Anthropic API key here.\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Set your GCP project id and region, if you want to query Llama 3.1 405B in Vertex.\n",
    "REGION = \"us-central1\"  # Set your GCP region here.\n",
    "PROJECT_ID = \"lema-dev\"  # Set your GCP project id here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Directory\n",
    "\n",
    "Finally, we'll set up a directory to use for this tutorial, and some environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"letter_counting_tutorial\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable warnings from HF.\n",
    "\n",
    "# This is needed for vLLM to use multiple GPUs in a notebook.\n",
    "# If you're not running in a notebook, you can ignore this.\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"oumi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset we'll use for this notebook is `oumi-ai/oumi-letter-count`, which can be found on [HF Datasets](https://huggingface.co/datasets/oumi-ai/oumi-letter-count). Its prompts ask to count the letters in various English words, with metadata in each example containing the correct count. We use the `train` split for training and the `test` split for evaluation. We'll use an Oumi dataset class, `LetterCountGrpoDataset`, to load and preprocess the HF Dataset. The following code displays an example prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-27 23:20:11,839][oumi][rank0][pid:1269100][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: LetterCountGrpoDataset)... dataset_name: 'oumi-ai/oumi-letter-count'\n",
      "[2025-06-27 23:20:12,645][oumi][rank0][pid:1269100][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: validation\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 22894322\n",
      "\tDownload size: 5697295\n",
      "\tSize: 28591617 bytes\n",
      "\tRows: 10000\n",
      "\tColumns: ['conversation_id', 'messages', 'metadata']\n",
      "[2025-06-27 23:20:12,798][oumi][rank0][pid:1269100][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (10000, 3). Columns:\n",
      "conversation_id    object\n",
      "messages           object\n",
      "metadata           object\n",
      "dtype: object\n",
      "--------------------------------------------------------------------------------\n",
      "Sample:\n",
      "{'conversation_id': 'oumi_letter_count_0',\n",
      " 'messages': [{'content': 'Your final answer should be an integer written as '\n",
      "                          'digits and formatted as \"\\\\boxed{your_answer}\". For '\n",
      "                          'example, if the answer is 42, you should output '\n",
      "                          '\"\\\\boxed{42}\".',\n",
      "               'role': 'system'},\n",
      "              {'content': \"Could you determine the count of 'l's in \"\n",
      "                          \"'substantial'?\",\n",
      "               'role': 'user'}],\n",
      " 'metadata': {'letter': 'l',\n",
      "              'letter_count_integer': 1,\n",
      "              'letter_count_string': 'one',\n",
      "              'unformatted_prompt': 'Could you determine the count of '\n",
      "                                    '{letter}s in {word}?',\n",
      "              'word': 'substantial'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from oumi.datasets.grpo.letter_count import LetterCountGrpoDataset\n",
    "\n",
    "dataset = LetterCountGrpoDataset(\n",
    "    dataset=\"oumi-ai/oumi-letter-count-clean\", split=\"validation\"\n",
    ")\n",
    "print(\"-\" * 80)\n",
    "print(\"Sample:\")\n",
    "pprint(dataset.conversation(0).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "First, we'll evaluate how various models perform on the letter counting task. We'll evaluate frontier models by calling their respective remote API, and Llama 3.2 3B by running local inference on it using vLLM.\n",
    "\n",
    "We've already defined a custom evaluation function in Oumi which runs inference on the above dataset, extracts the answer from the model response, and calculates various metrics such as accuracy. This function is defined at `src/oumi/evaluation/registry/count_letters_task.py` ([GitHub link](https://github.com/oumi-ai/oumi/blob/main/src/oumi/evaluation/registry/count_letters_task.py)), and we print its contents below for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@register_evaluation_function(\"count_letters\")\n",
      "def count_letters(\n",
      "    task_params: EvaluationTaskParams,\n",
      "    inference_engine: BaseInferenceEngine,\n",
      ") -> dict[str, Any]:\n",
      "    \"\"\"Custom evaluation function registered as `count_letters`.\"\"\"\n",
      "    dataset = LetterCountGrpoDataset(\n",
      "        dataset=\"oumi-ai/oumi-letter-count-clean\", split=\"test\"\n",
      "    )\n",
      "    # TODO: OPE-1155: Add support for using Oumi dataset code to create the dataset.\n",
      "    # dataset = build_dataset(\"oumi-ai/oumi-letter-count\", tokenizer=None, sample_count=10)  # noqa: E501\n",
      "    num_samples = task_params.num_samples\n",
      "    if num_samples is None:\n",
      "        num_samples = len(dataset)\n",
      "    input_conversations = [dataset.conversation(i) for i in range(num_samples)]\n",
      "    conversations = inference_engine.infer(input_conversations)\n",
      "    logger.info(f\"Finished inference on {len(conversations)} conversations!\")\n",
      "    if len(conversations) > 0:\n",
      "        logger.info(f\"Sample conversation: {conversations[0]}\")\n",
      "\n",
      "    count = 0  # The number of examples with correct answers extracted.\n",
      "    total = 0  # All examples.\n",
      "    valid_count = 0  # The number of examples with valid answers extracted.\n",
      "    for i, conversation in enumerate(conversations):\n",
      "        total += 1\n",
      "        # Grab the model's response\n",
      "        response = conversation.last_message()\n",
      "        # Ignore cases where model didn't respond or it's a multimodal response.\n",
      "        # For now, we focus on text-only responses.\n",
      "        if not response or not isinstance(response.content, str):\n",
      "            continue\n",
      "        # Count the example as correct if the extracted prediction is correct.\n",
      "        prediction = _extract_prediction(response.content)\n",
      "        if prediction is None:\n",
      "            continue\n",
      "        valid_count += 1\n",
      "        if prediction == conversation.metadata[\"letter_count_integer\"]:\n",
      "            count += 1\n",
      "\n",
      "    return {\n",
      "        # Accuracy across all examples.\n",
      "        \"accuracy\": count / total if total > 0 else 0,\n",
      "        # Accuracy when only counting examples with properly extracted answers.\n",
      "        \"properly_extracted_accuracy\": count / valid_count if valid_count > 0 else 0,\n",
      "        \"num_samples\": num_samples,\n",
      "        # These three values sum up to num_samples.\n",
      "        \"num_correct_answers\": count,\n",
      "        \"num_incorrect_answers\": valid_count - count,\n",
      "        \"num_invalid_answers\": total - valid_count,\n",
      "    }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "from oumi.evaluation.registry.count_letters_task import count_letters\n",
    "\n",
    "print(inspect.getsource(count_letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, you can select which models you want to evaluate. You can lower `NUM_SAMPLES`  to reduce cost when calling remote APIs, with the downside of noisier results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 100\n",
    "# We set an environment variable to be used at the end of the Colab.\n",
    "os.environ[\"NUM_SAMPLES\"] = str(NUM_SAMPLES)\n",
    "\n",
    "model_names = [\n",
    "    \"llama_3b\",\n",
    "    # Uncomment any models you wish to evaluate - you can evaluate multiple at once.\n",
    "    # \"gpt_4o\",\n",
    "    # \"gemini_pro\",\n",
    "    # \"llama_405b\",\n",
    "    # \"claude_sonnet\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó**NOTICE:** If running this notebook on Colab, delete the following line: `inference_engine: VLLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting letter_counting_tutorial/llama_3b_eval.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/llama_3b_eval.yaml\n",
    "\n",
    "# We save this config as a YAML file as we'll use it again at the end of the notebook.\n",
    "model:\n",
    "  model_name: \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "  model_max_length: 131072\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  attn_implementation: \"sdpa\"\n",
    "  trust_remote_code: True\n",
    "\n",
    "inference_engine: VLLM\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 2048\n",
    "\n",
    "tasks:\n",
    "  - evaluation_backend: custom\n",
    "    task_name: count_letters\n",
    "\n",
    "output_dir: \"letter_counting_tutorial/evaluation/llama_3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EvaluationConfig for various models.\n",
    "# Note that Llama 3B uses the local VLLM inference engines, while the others use various\n",
    "# remote engines.\n",
    "\n",
    "with open(f\"{tutorial_dir}/llama_3b_eval.yaml\") as f:\n",
    "    llama_3b_yaml = f.read()\n",
    "\n",
    "configs = {\n",
    "    \"llama_3b\": llama_3b_yaml,\n",
    "    \"gpt_4o\": \"\"\"\n",
    "      model:\n",
    "        model_name: \"gpt-4o\"\n",
    "\n",
    "      inference_engine: OPENAI\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_key_env_varname: \"OPENAI_API_KEY\"\n",
    "        max_retries: 3\n",
    "        num_workers: 100\n",
    "        politeness_policy: 60\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: count_letters\n",
    "\n",
    "      output_dir: \"letter_counting_tutorial/evaluation/gpt_4o\"\n",
    "      \"\"\",\n",
    "    \"gemini_pro\": \"\"\"\n",
    "      model:\n",
    "        model_name: \"gemini-2.5-pro-preview-03-25\"\n",
    "\n",
    "      inference_engine: GOOGLE_GEMINI\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_key_env_varname: \"GEMINI_API_KEY\"\n",
    "        max_retries: 3\n",
    "        num_workers: 2\n",
    "        politeness_policy: 60\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: count_letters\n",
    "\n",
    "      output_dir: \"letter_counting_tutorial/evaluation/gemini_pro\"\n",
    "      \"\"\",\n",
    "    \"llama_405b\": f\"\"\"\n",
    "      model:\n",
    "        model_name: \"meta/llama-3.1-405b-instruct-maas\"\n",
    "\n",
    "      inference_engine: GOOGLE_VERTEX\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_url: \"https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/openapi/chat/completions\"\n",
    "        max_retries: 3\n",
    "        num_workers: 10\n",
    "        politeness_policy: 60\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: count_letters\n",
    "\n",
    "      output_dir: \"letter_counting_tutorial/evaluation/llama_405b\"\n",
    "      \"\"\",\n",
    "    \"claude_sonnet\": \"\"\"\n",
    "      model:\n",
    "        model_name: \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "      inference_engine: ANTHROPIC\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_key_env_varname: \"ANTHROPIC_API_KEY\"\n",
    "        max_retries: 3\n",
    "        num_workers: 5\n",
    "        politeness_policy: 65\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: count_letters\n",
    "\n",
    "      output_dir: \"letter_counting_tutorial/evaluation/claude_sonnet\"\n",
    "      \"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-27 23:20:18 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-06-27 23:20:19,647][oumi][rank0][pid:1269100][MainThread][WARNING]][models.py:463] Undefined pad token. Setting it to `<|finetune_right_pad_id|>`.\n",
      "[2025-06-27 23:20:19,649][oumi][rank0][pid:1269100][MainThread][INFO]][models.py:506] Using the model's built-in chat template for model 'meta-llama/Llama-3.2-3B-Instruct'.\n",
      "INFO 06-27 23:20:29 [config.py:600] This model supports multiple tasks: {'score', 'classify', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 06-27 23:20:29 [config.py:1600] Defaulting to use mp for distributed inference\n",
      "INFO 06-27 23:20:29 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 06-27 23:20:29 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 06-27 23:20:35 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 06-27 23:20:38 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 06-27 23:20:38 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-27 23:20:38 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_f443df5e'), local_subscribe_addr='ipc:///tmp/91d64bf9-cb3e-408d-9601-168fb9406a9a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 06-27 23:20:43 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 06-27 23:20:47 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7c58f7b0acd0>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1269596)\u001b[0;0m INFO 06-27 23:20:47 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_426d1138'), local_subscribe_addr='ipc:///tmp/17816bd3-51db-480c-b4e6-df92d55a93d4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 06-27 23:20:53 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 06-27 23:20:56 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x739ee68dd210>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1269689)\u001b[0;0m INFO 06-27 23:20:56 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bd7bd5d1'), local_subscribe_addr='ipc:///tmp/2b7db3e1-0317-4189-8633-c0c037ce6009', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 06-27 23:21:02 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 06-27 23:21:05 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x74f087f03cd0>\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1269822)\u001b[0;0m INFO 06-27 23:21:05 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9f1f262e'), local_subscribe_addr='ipc:///tmp/9f735e8d-daa5-43a0-8957-159d7d11059e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 06-27 23:21:13 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 06-27 23:21:16 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x725a753a5210>\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1269955)\u001b[0;0m INFO 06-27 23:21:16 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cfc80c18'), local_subscribe_addr='ipc:///tmp/2024982a-f61e-4554-be7f-83a71853df2e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1269596)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=1269822)\u001b[0;0m INFO 06-27 23:21:17 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "INFO 06-27 23:21:17 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1269822)\u001b[0;0m INFO 06-27 23:21:17 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1269596)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=1269955)\u001b[0;0m INFO 06-27 23:21:17 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1269955)\u001b[0;0m INFO 06-27 23:21:17 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 06-27 23:21:17 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1269689)\u001b[0;0m INFO 06-27 23:21:17 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1269689)\u001b[0;0m INFO 06-27 23:21:17 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1269689)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=1269822)\u001b[0;0m INFO 06-27 23:21:21 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 06-27 23:21:21 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1269955)\u001b[0;0m INFO 06-27 23:21:21 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1269596)\u001b[0;0m INFO 06-27 23:21:21 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1269596)\u001b[0;0m INFO 06-27 23:21:21 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_63ae90be'), local_subscribe_addr='ipc:///tmp/edb306e6-1c0e-47e6-a604-bc334b0ac028', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1269822)\u001b[0;0m INFO 06-27 23:21:21 [parallel_state.py:957] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1269822)\u001b[0;0m INFO 06-27 23:21:21 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1269596)\u001b[0;0m INFO 06-27 23:21:21 [parallel_state.py:957] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1269596)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=1269955)\u001b[0;0m INFO 06-27 23:21:21 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-27 23:21:21 [parallel_state.py:957] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1269955)\u001b[0;0m INFO 06-27 23:21:21 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1269689)\u001b[0;0m INFO 06-27 23:21:21 [parallel_state.py:957] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1269689)\u001b[0;0m INFO 06-27 23:21:21 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1269689)\u001b[0;0m INFO 06-27 23:21:22 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1269955)\u001b[0;0m INFO 06-27 23:21:22 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1269822)\u001b[0;0m INFO 06-27 23:21:22 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1269596)\u001b[0;0m INFO 06-27 23:21:22 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1269689)\u001b[0;0m WARNING 06-27 23:21:22 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1269955)\u001b[0;0m WARNING 06-27 23:21:22 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1269822)\u001b[0;0m WARNING 06-27 23:21:22 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1269596)\u001b[0;0m WARNING 06-27 23:21:22 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1269596)\u001b[0;0m INFO 06-27 23:21:22 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1269822)\u001b[0;0m INFO 06-27 23:21:22 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1269689)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=1269955)\u001b[0;0m INFO 06-27 23:21:22 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 06-27 23:21:22 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.45it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.20it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.04it/s]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1269596)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=1269596)\u001b[0;0m INFO 06-27 23:21:23 [loader.py:447] Loading weights took 1.00 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1269955)\u001b[0;0m INFO 06-27 23:21:23 [loader.py:447] Loading weights took 1.11 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1269822)\u001b[0;0m INFO 06-27 23:21:24 [loader.py:447] Loading weights took 1.23 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1269689)\u001b[0;0m INFO 06-27 23:21:24 [loader.py:447] Loading weights took 1.22 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1269596)\u001b[0;0m INFO 06-27 23:21:24 [gpu_model_runner.py:1273] Model loading took 1.5341 GiB and 1.438169 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1269955)\u001b[0;0m INFO 06-27 23:21:24 [gpu_model_runner.py:1273] Model loading took 1.5341 GiB and 1.666282 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1269822)\u001b[0;0m INFO 06-27 23:21:24 [gpu_model_runner.py:1273] Model loading took 1.5341 GiB and 1.859815 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1269689)\u001b[0;0m INFO 06-27 23:21:24 [gpu_model_runner.py:1273] Model loading took 1.5341 GiB and 1.957536 seconds\n",
      "INFO 06-27 23:21:32 [kv_cache_utils.py:578] GPU KV cache size: 2,249,856 tokens\n",
      "INFO 06-27 23:21:32 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 17.17x\n",
      "INFO 06-27 23:21:32 [kv_cache_utils.py:578] GPU KV cache size: 2,245,168 tokens\n",
      "INFO 06-27 23:21:32 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 17.13x\n",
      "INFO 06-27 23:21:32 [kv_cache_utils.py:578] GPU KV cache size: 2,245,168 tokens\n",
      "INFO 06-27 23:21:32 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 17.13x\n",
      "INFO 06-27 23:21:32 [kv_cache_utils.py:578] GPU KV cache size: 2,268,576 tokens\n",
      "INFO 06-27 23:21:32 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 17.31x\n",
      "INFO 06-27 23:21:32 [core.py:162] init engine (profile, create kv cache, warmup model) took 7.69 seconds\n",
      "[2025-06-27 23:21:33,816][oumi][rank0][pid:1269100][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: LetterCountGrpoDataset)... dataset_name: 'oumi-ai/oumi-letter-count'\n",
      "[2025-06-27 23:21:34,773][oumi][rank0][pid:1269100][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: test\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 22894322\n",
      "\tDownload size: 5697295\n",
      "\tSize: 28591617 bytes\n",
      "\tRows: 20000\n",
      "\tColumns: ['conversation_id', 'messages', 'metadata']\n",
      "[2025-06-27 23:21:36,015][oumi][rank0][pid:1269100][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (20000, 3). Columns:\n",
      "conversation_id    object\n",
      "messages           object\n",
      "metadata           object\n",
      "dtype: object\n",
      "INFO 06-27 23:21:37 [chat_utils.py:396] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.41it/s, est. speed input: 818.79 toks/s, output: 469.61 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-27 23:21:48,208][oumi][rank0][pid:1269100][MainThread][INFO]][count_letters_task.py:54] Finished inference on 100 conversations!\n",
      "[2025-06-27 23:21:48,211][oumi][rank0][pid:1269100][MainThread][INFO]][count_letters_task.py:56] Sample conversation: conversation_id='oumi_letter_count_0' messages=[SYSTEM: Your final answer should be an integer written as digits and formatted as \"\\boxed{your_answer}\". For example, if the answer is 42, you should output \"\\boxed{42}\"., USER: Look through 'perivaginal' and count the 'n's., ASSISTANT: There are 2 'n's in 'perivaginal'.] metadata={'letter': 'n', 'letter_count_integer': 1, 'letter_count_string': 'one', 'unformatted_prompt': 'Look through {word} and count the {letter}s.', 'word': 'perivaginal'}\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation on all specified models.\n",
    "\n",
    "from oumi.core.configs import EvaluationConfig\n",
    "from oumi.core.evaluation import Evaluator\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    # Create the evaluation config from the YAML string.\n",
    "    config_yaml: str = configs[model_name]\n",
    "    config = EvaluationConfig.from_str(config_yaml)\n",
    "    config.tasks[0].num_samples = NUM_SAMPLES\n",
    "\n",
    "    # Run the evaluation.\n",
    "    evaluator = Evaluator()\n",
    "    evaluator_out = evaluator.evaluate(config)\n",
    "\n",
    "    # # Record the results.\n",
    "    results[model_name] = evaluator_out[0].get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 100\n",
      "--------------------------------------------------------------------------------\n",
      "Model: llama_3b\n",
      "Accuracy: 31.00%\n",
      "Properly Extracted Accuracy: 46.27%\n",
      "Num correct, incorrect, invalid: 31, 36, 33\n"
     ]
    }
   ],
   "source": [
    "# Print results.\n",
    "\n",
    "print(f\"Total samples: {NUM_SAMPLES}\")\n",
    "for model_name, result in results.items():\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {result['accuracy']:.2%}\")\n",
    "    print(f\"Properly Extracted Accuracy: {result['properly_extracted_accuracy']:.2%}\")\n",
    "    correct = result[\"num_correct_answers\"]\n",
    "    incorrect = result[\"num_incorrect_answers\"]\n",
    "    invalid = result[\"num_invalid_answers\"]\n",
    "    print(f\"Num correct, incorrect, invalid: {correct}, {incorrect}, {invalid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO\n",
    "\n",
    "Now, we train Llama 3.2 3B on the task of counting letters using the GRPO algorithm implemented by [HuggingFace's `trl` library](https://huggingface.co/docs/trl/en/index).\n",
    "\n",
    "Note that we can calculate a concrete reward for this task by comparing the answer extracted by the model with the correct answer. In the reward function defined in `src/oumi/datasets/grpo/rewards/count_letters_rewards.py` ([GitHub link](https://github.com/oumi-ai/oumi/blob/main/src/oumi/datasets/grpo/rewards/count_letters_rewards.py)), we calculate the reward to be `-abs(predicted_count - target_count)`. We use simple heuristics to extract the predicted count. The following cell prints out the reward function code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2025 - Oumi\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import re\n",
      "from typing import Any, Optional\n",
      "\n",
      "from oumi.core.registry import RegistryType, register\n",
      "\n",
      "\n",
      "def _extract_prediction(response: str) -> Optional[int]:\n",
      "    r\"\"\"Returns the numeric answer extracted from `\\boxed{...}`, or None otherwise.\"\"\"\n",
      "    regex_result = re.findall(r\"\\\\boxed\\{([-+]?\\d+)\\}\", response)\n",
      "    if not regex_result or len(regex_result) != 1:\n",
      "        return None\n",
      "    number_str = regex_result[0]\n",
      "    # Except clause shouldn't trigger because the regex should only find ints.\n",
      "    try:\n",
      "        return int(number_str)\n",
      "    except ValueError:\n",
      "        return None\n",
      "\n",
      "\n",
      "def compute_letter_count_reward(completion: str, target_count: int) -> float:\n",
      "    \"\"\"Computes the rewards for counting the letters in a string.\n",
      "\n",
      "    Args:\n",
      "        completion: The completion string from the LLM.\n",
      "        target_count: The target count of letters.\n",
      "\n",
      "    Returns:\n",
      "        The reward value.\n",
      "    \"\"\"\n",
      "    count = _extract_prediction(completion)\n",
      "\n",
      "    # Lowest reward goes to unparseable responses\n",
      "    if count is None:\n",
      "        return -3.0\n",
      "\n",
      "    delta = abs(count - target_count)\n",
      "\n",
      "    # Reward scales from [0, -2) as delta increases\n",
      "    # Ensures that \"worse\" answers (where the counts are off by a higher amount) are\n",
      "    # penalized while never reaching -3.0 which is reserved for unparseable answers.\n",
      "    return (1 / (delta + 0.5)) - 2\n",
      "\n",
      "\n",
      "@register(\"count_letters\", RegistryType.REWARD_FUNCTION)\n",
      "def _count_letters(\n",
      "    completions: list[list[dict[str, Any]]],\n",
      "    letter_count: list[int],\n",
      "    **kwargs: dict[str, Any],\n",
      ") -> list[float]:\n",
      "    \"\"\"Custom reward function for counting letters in a string.\n",
      "\n",
      "    For more details on custom reward functions used in trl's GRPOTrainer, see:\n",
      "    https://huggingface.co/docs/trl/main/en/grpo_trainer#using-a-custom-reward-function.\n",
      "\n",
      "    Args:\n",
      "        completions: The list of completions from the LLM.\n",
      "        letter_count: The list of target count of letters.\n",
      "        kwargs: Unused.\n",
      "\n",
      "    Returns:\n",
      "        The reward values for each completion, calculated as the negative of the\n",
      "        absolute difference between the count and the target count. The count is assumed\n",
      "        to be the last group of consecutive digits in the completion string.\n",
      "    \"\"\"\n",
      "    completions_strs = [c[0][\"content\"] for c in completions]\n",
      "    return [\n",
      "        compute_letter_count_reward(c, t)\n",
      "        for c, t in zip(completions_strs, letter_count)\n",
      "    ]\n"
     ]
    }
   ],
   "source": [
    "!cat ../src/oumi/datasets/grpo/rewards/count_letters_rewards.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up to free-up GPU memory used for evaluation above\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Delete the evaluator and collect garbage.\"\"\"\n",
    "    global evaluator\n",
    "    if evaluator:  # type: ignore\n",
    "        del evaluator\n",
    "        evaluator = None\n",
    "    for _ in range(3):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó**NOTICE:** Set `training.enable_wandb` to True if you want to log your training run to Weights and Biases. In addition, you must also log into WandB, ex. by running `wandb login`.\n",
    "\n",
    "‚ùó**NOTICE:** We only train for 2 steps for demonstration purposes. You can increase `max_steps`, or replace it with `num_train_epochs` to set your desired number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting letter_counting_tutorial/grpo_train.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/grpo_train.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "  model_max_length: 8192\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  attn_implementation: \"sdpa\"\n",
    "\n",
    "data:\n",
    "  train:\n",
    "    datasets:\n",
    "      - dataset_name: \"oumi-ai/oumi-letter-count\"\n",
    "        split: \"train\"\n",
    "\n",
    "training:\n",
    "  trainer_type: \"TRL_GRPO\"\n",
    "  save_steps: 500\n",
    "  max_steps: 4 # for demo purposes\n",
    "  per_device_train_batch_size: 2\n",
    "  gradient_accumulation_steps: 1\n",
    "  learning_rate: 5e-7\n",
    "  lr_scheduler_type: \"cosine\"\n",
    "  warmup_steps: 20\n",
    "\n",
    "  reward_functions: [\"count_letters\"]\n",
    "\n",
    "  ddp_find_unused_parameters: False\n",
    "  optimizer: \"adafactor\"\n",
    "  compile: True\n",
    "\n",
    "  grpo:\n",
    "    num_generations: 4\n",
    "    use_vllm: True\n",
    "\n",
    "  dataloader_num_workers: \"auto\"\n",
    "  dataloader_prefetch_factor: 32\n",
    "\n",
    "  logging_steps: 1\n",
    "  output_dir: \"letter_counting_tutorial/llama_3b_grpo\"\n",
    "  # Set this to True if you want to log to Weights and Biases.\n",
    "  enable_wandb: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-27 23:53:57 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m1298016\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "INFO 06-27 23:54:09 [config.py:600] This model supports multiple tasks: {'reward', 'generate', 'embed', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 06-27 23:54:09 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 06-27 23:54:20 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 06-27 23:54:22 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "INFO 06-27 23:54:22 [worker_base.py:589] Injected <class 'trl.scripts.vllm_serve.WeightSyncWorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['close_communicator', 'init_communicator', 'update_named_param']\n",
      "WARNING 06-27 23:54:22 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7cc8d16590d0>\n",
      "INFO 06-27 23:54:23 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 06-27 23:54:23 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-27 23:54:23 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "WARNING 06-27 23:54:23 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-27 23:54:23 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.45it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  2.12it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.98it/s]\n",
      "\n",
      "INFO 06-27 23:54:24 [loader.py:447] Loading weights took 1.02 seconds\n",
      "INFO 06-27 23:54:25 [gpu_model_runner.py:1273] Model loading took 6.0160 GiB and 1.458958 seconds\n",
      "INFO 06-27 23:54:31 [backends.py:416] Using cache directory: /home/shanghong/.cache/vllm/torch_compile_cache/36786aec49/rank_0_0 for vLLM's torch.compile\n",
      "INFO 06-27 23:54:31 [backends.py:426] Dynamo bytecode transform time: 6.80 s\n",
      "INFO 06-27 23:54:35 [backends.py:132] Cache the graph of shape None for later use\n",
      "INFO 06-27 23:54:55 [backends.py:144] Compiling a graph for general shape takes 23.44 s\n",
      "INFO 06-27 23:55:06 [monitor.py:33] torch.compile takes 30.25 s in total\n",
      "INFO 06-27 23:55:07 [kv_cache_utils.py:578] GPU KV cache size: 554,496 tokens\n",
      "INFO 06-27 23:55:07 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 4.23x\n",
      "INFO 06-27 23:55:30 [gpu_model_runner.py:1608] Graph capturing finished in 23 secs, took 0.57 GiB\n",
      "INFO 06-27 23:55:30 [core.py:162] init engine (profile, create kv cache, warmup model) took 65.42 seconds\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
      "^C\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
      "[2025-06-28 00:04:23,930][oumi][rank0][pid:1306989][MainThread][INFO]][distributed_run.py:308] Running the command: ['torchrun', '--nnodes=1', '--node-rank=0', '--nproc-per-node=4', '--master-addr=127.0.0.1', '--master-port=8007', '-m', 'oumi', 'train', '-c', 'letter_counting_tutorial/grpo_train.yaml']\n",
      "W0628 00:04:25.568000 1307033 site-packages/torch/distributed/run.py:792] \n",
      "W0628 00:04:25.568000 1307033 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0628 00:04:25.568000 1307033 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0628 00:04:25.568000 1307033 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "\n",
      "\u001b[32m   ____  _    _ __  __ _____\u001b[0m\n",
      "\u001b[32m  / __ \\| |  | |  \\/  |_   _|\u001b[0m\n",
      "\u001b[32m | |  | | |  | | \\  / | | |\u001b[0m\n",
      "\u001b[32m | |  | | |  | | |\\/| | | |\u001b[0m\n",
      "\u001b[32m | |__| | |__| | |  | |_| |_\u001b[0m\n",
      "\u001b[32m  \\____/ \\____/|_|  |_|_____|\u001b[0m\n",
      "\n",
      "\u001b[2K\u001b[32m‚†á\u001b[0m \u001b[32mLoading configuration...\u001b[0mconfiguration...\u001b[0m\u001b[32m‚†ã\u001b[0m \u001b[32mLoading configuration...\u001b[0m\u001b[32m‚†ã\u001b[0m \u001b[32mLoading configuration...\u001b[0m\u001b[32m‚†ã\u001b[0m \u001b[32mLoading configuration...\u001b[0m"
     ]
    }
   ],
   "source": [
    "!oumi distributed torchrun -m oumi train -c $tutorial_dir/grpo_train.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-28 00:58:23,248\tINFO worker.py:1723 -- Connecting to existing Ray cluster at address: 172.26.135.196:6379...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-28 00:58:28,252 W 1344725 1344725] gcs_rpc_client.h:151: Failed to connect to GCS at address 172.26.135.196:6379 within 5 seconds.\n",
      "[2025-06-28 00:58:58,259 W 1344725 1344725] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n",
      "[2025-06-28 00:59:04,268 W 1344725 1344725] gcs_rpc_client.h:151: Failed to connect to GCS at address 172.26.135.196:6379 within 5 seconds.\n",
      "[2025-06-28 00:59:34,270 W 1344725 1344725] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n"
     ]
    }
   ],
   "source": [
    "# If we have multiple GPUs, we can use Ray to parallelize the inference.\n",
    "# This is essential if you're running a model that's too big to fit in a single GPU.\n",
    "\n",
    "import ray\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() >= 2:\n",
    "    ray.shutdown()\n",
    "    ray.init(address=None)  # num_gpus=torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our Trained Model\n",
    "\n",
    "Let's now evaluate our trained model to see if it improved on the letter counting task. Note that it may not improve much, since we trained it for a relatively short time.\n",
    "\n",
    "Below, we demonstrate an alternative method of running evaluation with the `oumi` CLI. We use the same Llama 3B evaluation config we used above, with the only change being pointing it at the model we just trained.\n",
    "\n",
    "First, we need to reset the notebook to clear variables from our previous vLLM run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m   ____  _    _ __  __ _____\u001b[0m\n",
      "\u001b[32m  / __ \\| |  | |  \\/  |_   _|\u001b[0m\n",
      "\u001b[32m | |  | | |  | | \\  / | | |\u001b[0m\n",
      "\u001b[32m | |  | | |  | | |\\/| | | |\u001b[0m\n",
      "\u001b[32m | |__| | |__| | |  | |_| |_\u001b[0m\n",
      "\u001b[32m  \\____/ \\____/|_|  |_|_____|\u001b[0m\n",
      "\n",
      "\u001b[2K\u001b[32m‚†º\u001b[0m \u001b[32mLoading configuration...\u001b[0m0m\n",
      "\u001b[2KINFO 06-28 00:28:08 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[2K\u001b[32m‚†º\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-06-28 00:28:10,216][oumi][rank0][pid:1330249][MainThread][INFO]][models.py:506] Using the model's built-in chat template for model 'letter_counting_tutorial/llama_3b_grpo'.\n",
      "\u001b[2KINFO 06-28 00:28:19 [config.py:600] This model supports multiple tasks: \n",
      "{'score', 'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "\u001b[2KINFO 06-28 00:28:19 [config.py:1600] Defaulting to use mp for distributed \n",
      "inference\n",
      "\u001b[2KINFO 06-28 00:28:19 [config.py:1780] Chunked prefill is enabled with \n",
      "max_num_batched_tokens=16384.\n",
      "\u001b[2KWARNING 06-28 00:28:19 [cuda.py:96] To see benefits of async output processing, \n",
      "enable CUDA graph. Since, enforce-eager is enabled, async output processor \n",
      "cannot be used\n",
      "\u001b[2KWARNING 06-28 00:28:21 [utils.py:2273] We must use the `spawn` multiprocessing \n",
      "start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See \n",
      "https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multi\n",
      "processing for more information. Reason: CUDA is initialized\n",
      "\u001b[2K\u001b[32m‚†º\u001b[0m \u001b[32mRunning evaluation...\u001b[0mINFO 06-28 00:28:27 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[2K\u001b[32m‚†ß\u001b[0m \u001b[32mRunning evaluation...\u001b[0mINFO 06-28 00:28:30 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='letter_counting_tutorial/llama_3b_grpo', speculative_config=None, tokenizer='letter_counting_tutorial/llama_3b_grpo', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=letter_counting_tutorial/llama_3b_grpo, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 06-28 00:28:30 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-28 00:28:30 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_f22d7ec4'), local_subscribe_addr='ipc:///tmp/9ed18bc7-8d52-4fd1-9307-e4a424b408d1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[2K\u001b[32m‚†π\u001b[0m \u001b[32mRunning evaluation...\u001b[0mINFO 06-28 00:28:36 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[2K\u001b[32m‚†º\u001b[0m \u001b[32mRunning evaluation...\u001b[0mWARNING 06-28 00:28:39 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x746dd5b2f8d0>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1330778)\u001b[0;0m INFO 06-28 00:28:39 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c59e43c4'), local_subscribe_addr='ipc:///tmp/199a5815-6821-4e3c-97cd-74acb4a21f50', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[2K\u001b[32m‚†ô\u001b[0m \u001b[32mRunning evaluation...\u001b[0mINFO 06-28 00:28:46 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[2K\u001b[32m‚†ô\u001b[0m \u001b[32mRunning evaluation...\u001b[0mWARNING 06-28 00:28:49 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x79f66a46b990>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1330901)\u001b[0;0m INFO 06-28 00:28:49 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3a446ddd'), local_subscribe_addr='ipc:///tmp/d644480d-798c-4e14-9f78-fe061dc62e6a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[2K\u001b[32m‚†è\u001b[0m \u001b[32mRunning evaluation...\u001b[0mINFO 06-28 00:28:55 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[2K\u001b[32m‚†á\u001b[0m \u001b[32mRunning evaluation...\u001b[0mWARNING 06-28 00:28:58 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x76c0e0747250>\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1331019)\u001b[0;0m INFO 06-28 00:28:58 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_81c80bec'), local_subscribe_addr='ipc:///tmp/7e2bae84-7eb3-4f63-93cb-9a5544e33f3b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[2K\u001b[32m‚†è\u001b[0m \u001b[32mRunning evaluation...\u001b[0mINFO 06-28 00:29:05 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[2K\u001b[32m‚†∏\u001b[0m \u001b[32mRunning evaluation...\u001b[0mWARNING 06-28 00:29:09 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x715d465fad10>\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1331153)\u001b[0;0m INFO 06-28 00:29:09 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c8e32e89'), local_subscribe_addr='ipc:///tmp/b4ba0ce2-c6fd-4e64-8d6b-1f348cf96ae9', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[2K\u001b[32m‚†ã\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorker rank=1 pid=1330901)\u001b[0;0m INFO 06-28 00:29:09 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1330901)\u001b[0;0m INFO 06-28 00:29:09 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1330778)\u001b[0;0m INFO 06-28 00:29:09 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1330778)\u001b[0;0m INFO 06-28 00:29:09 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1331019)\u001b[0;0m INFO 06-28 00:29:09 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1331019)\u001b[0;0m INFO 06-28 00:29:09 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1331153)\u001b[0;0m INFO 06-28 00:29:09 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1331153)\u001b[0;0m INFO 06-28 00:29:09 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[2K\u001b[32m‚†á\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorker rank=1 pid=1330901)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=1331153)\u001b[0;0m INFO 06-28 00:29:13 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 06-28 00:29:13 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1330778)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=1331019)\u001b[0;0m INFO 06-28 00:29:13 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 06-28 00:29:13 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1330778)\u001b[0;0m INFO 06-28 00:29:13 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_f0995cff'), local_subscribe_addr='ipc:///tmp/04a12686-4f8f-4acf-9b2a-de13bbe856a0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1331019)\u001b[0;0m INFO 06-28 00:29:13 [parallel_state.py:957] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1331019)\u001b[0;0m INFO 06-28 00:29:13 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1330778)\u001b[0;0m INFO 06-28 00:29:13 [parallel_state.py:957] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1330778)\u001b[0;0m INFO 06-28 00:29:13 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1330901)\u001b[0;0m INFO 06-28 00:29:13 [parallel_state.py:957] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1330901)\u001b[0;0m INFO 06-28 00:29:13 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1331153)\u001b[0;0m INFO 06-28 00:29:13 [parallel_state.py:957] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1331153)\u001b[0;0m INFO 06-28 00:29:13 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[2K\u001b[32m‚†¶\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorker rank=0 pid=1330778)\u001b[0;0m INFO 06-28 00:29:14 [gpu_model_runner.py:1258] Starting to load model letter_counting_tutorial/llama_3b_grpo...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1331153)\u001b[0;0m INFO 06-28 00:29:14 [gpu_model_runner.py:1258] Starting to load model letter_counting_tutorial/llama_3b_grpo...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1330901)\u001b[0;0m INFO 06-28 00:29:14 [gpu_model_runner.py:1258] Starting to load model letter_counting_tutorial/llama_3b_grpo...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1331019)\u001b[0;0m INFO 06-28 00:29:14 [gpu_model_runner.py:1258] Starting to load model letter_counting_tutorial/llama_3b_grpo...\n",
      "\u001b[2K\u001b[32m‚†ô\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorker rank=1 pid=1330901)\u001b[0;0m WARNING 06-28 00:29:14 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1331153)\u001b[0;0m WARNING 06-28 00:29:14 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[2K\u001b[32m‚†π\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorker rank=0 pid=1330778)\u001b[0;0m WARNING 06-28 00:29:14 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1331019)\u001b[0;0m WARNING 06-28 00:29:14 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.18it/s][0;0m \n",
      "\u001b[2K\u001b[32m‚†¥\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorker rank=1 pid=1330901)\u001b[0;0m INFO 06-28 00:29:16 [loader.py:447] Loading weights took 1.19 seconds\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.67it/s][0;0m \n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.56it/s]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1330778)\u001b[0;0m \n",
      "\u001b[2K\u001b[32m‚†è\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorker rank=2 pid=1331019)\u001b[0;0m INFO 06-28 00:29:16 [loader.py:447] Loading weights took 1.30 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1330778)\u001b[0;0m INFO 06-28 00:29:16 [loader.py:447] Loading weights took 1.32 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1331153)\u001b[0;0m INFO 06-28 00:29:16 [loader.py:447] Loading weights took 1.40 seconds\n",
      "\u001b[2K\u001b[32m‚†ß\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorker rank=1 pid=1330901)\u001b[0;0m INFO 06-28 00:29:16 [gpu_model_runner.py:1273] Model loading took 1.5341 GiB and 1.587265 seconds\n",
      "\u001b[2K\u001b[32m‚†á\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorker rank=2 pid=1331019)\u001b[0;0m INFO 06-28 00:29:16 [gpu_model_runner.py:1273] Model loading took 1.5341 GiB and 1.763444 seconds\n",
      "\u001b[2K\u001b[32m‚†è\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorker rank=0 pid=1330778)\u001b[0;0m INFO 06-28 00:29:17 [gpu_model_runner.py:1273] Model loading took 1.5341 GiB and 1.794230 seconds\n",
      "\u001b[2K\u001b[32m‚†ã\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorker rank=3 pid=1331153)\u001b[0;0m INFO 06-28 00:29:17 [gpu_model_runner.py:1273] Model loading took 1.5341 GiB and 1.813302 seconds\n",
      "\u001b[2K\u001b[32m‚†¥\u001b[0m \u001b[32mRunning evaluation...\u001b[0mINFO 06-28 00:29:24 [kv_cache_utils.py:578] GPU KV cache size: 2,249,856 tokens\n",
      "INFO 06-28 00:29:24 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 17.17x\n",
      "INFO 06-28 00:29:24 [kv_cache_utils.py:578] GPU KV cache size: 2,245,168 tokens\n",
      "INFO 06-28 00:29:24 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 17.13x\n",
      "INFO 06-28 00:29:24 [kv_cache_utils.py:578] GPU KV cache size: 2,245,168 tokens\n",
      "INFO 06-28 00:29:24 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 17.13x\n",
      "INFO 06-28 00:29:24 [kv_cache_utils.py:578] GPU KV cache size: 2,268,576 tokens\n",
      "INFO 06-28 00:29:24 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 17.31x\n",
      "\u001b[2K\u001b[32m‚†π\u001b[0m \u001b[32mRunning evaluation...\u001b[0mINFO 06-28 00:29:25 [core.py:162] init engine (profile, create kv cache, warmup model) took 8.21 seconds\n",
      "\u001b[2K\u001b[32m‚†è\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-06-28 00:29:26,727][oumi][rank0][pid:1330249][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: LetterCountGrpoDataset)... dataset_name: 'oumi-ai/oumi-letter-count'\n",
      "\u001b[2K\u001b[32m‚†º\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-06-28 00:29:27,888][oumi][rank0][pid:1330249][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: test\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 22894322\n",
      "\tDownload size: 5697295\n",
      "\tSize: 28591617 bytes\n",
      "\tRows: 20000\n",
      "\tColumns: ['conversation_id', 'messages', 'metadata']\n",
      "\u001b[2K\u001b[32m‚†è\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-06-28 00:29:29,042][oumi][rank0][pid:1330249][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (20000, 3). Columns:\n",
      "conversation_id    object\n",
      "messages           object\n",
      "metadata           object\n",
      "dtype: object\n",
      "\u001b[2KINFO 06-28 00:29:30 [chat_utils.py:396] Detected the chat template content \n",
      "format to be 'string'. You can set `--chat-template-content-format` to override \n",
      "this.\n",
      "\u001b[2KProcessed prompts:   \u001b[1;36m0\u001b[0m%| | \u001b[1;36m0\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<?, ?it/s, est. speed input: \u001b[1;36m0.00\u001b[0m toks/s,\n",
      "\u001b[2KProcessed prompts:   \u001b[1;36m1\u001b[0m%| | \u001b[1;36m1\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m01:19\u001b[0m,  \u001b[1;36m1.\u001b[0m25it/s, est. speed input: \u001b[1;36m106\u001b[0m.\n",
      "\u001b[2KProcessed prompts:   \u001b[1;36m4\u001b[0m%| | \u001b[1;36m4\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:01\u001b[0m<\u001b[1;92m00:28\u001b[0m,  \u001b[1;36m3.\u001b[0m33it/s, est. speed input: \u001b[1;36m252\u001b[0m.\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m12\u001b[0m%|\u001b[1;36m1\u001b[0m| \u001b[1;36m12\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:01\u001b[0m<\u001b[1;92m00:07\u001b[0m, \u001b[1;36m11.\u001b[0m28it/s, est. speed input: \u001b[1;36m685\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m25\u001b[0m%|\u001b[1;36m2\u001b[0m| \u001b[1;36m25\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:01\u001b[0m<\u001b[1;92m00:03\u001b[0m, \u001b[1;36m22.\u001b[0m15it/s, est. speed input: \u001b[1;36m122\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m40\u001b[0m%|\u001b[1;36m4\u001b[0m| \u001b[1;36m40\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:01\u001b[0m<\u001b[1;92m00:01\u001b[0m, \u001b[1;36m38.\u001b[0m60it/s, est. speed input: \u001b[1;36m182\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m47\u001b[0m%|\u001b[1;36m4\u001b[0m| \u001b[1;36m47\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:02\u001b[0m<\u001b[1;92m00:02\u001b[0m, \u001b[1;36m25.\u001b[0m07it/s, est. speed input: \u001b[1;36m165\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m53\u001b[0m%|\u001b[1;36m5\u001b[0m| \u001b[1;36m53\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:05\u001b[0m<\u001b[1;92m00:06\u001b[0m,  \u001b[1;36m6.\u001b[0m97it/s, est. speed input: \u001b[1;36m864\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m57\u001b[0m%|\u001b[1;36m5\u001b[0m| \u001b[1;36m57\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:06\u001b[0m<\u001b[1;92m00:07\u001b[0m,  \u001b[1;36m5.\u001b[0m97it/s, est. speed input: \u001b[1;36m771\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m60\u001b[0m%|\u001b[1;36m6\u001b[0m| \u001b[1;36m60\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:06\u001b[0m<\u001b[1;92m00:06\u001b[0m,  \u001b[1;36m6.\u001b[0m65it/s, est. speed input: \u001b[1;36m789\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m63\u001b[0m%|\u001b[1;36m6\u001b[0m| \u001b[1;36m63\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:07\u001b[0m<\u001b[1;92m00:05\u001b[0m,  \u001b[1;36m6.\u001b[0m34it/s, est. speed input: \u001b[1;36m763\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m65\u001b[0m%|\u001b[1;36m6\u001b[0m| \u001b[1;36m65\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:07\u001b[0m<\u001b[1;92m00:05\u001b[0m,  \u001b[1;36m6.\u001b[0m46it/s, est. speed input: \u001b[1;36m758\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m67\u001b[0m%|\u001b[1;36m6\u001b[0m| \u001b[1;36m67\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:07\u001b[0m<\u001b[1;92m00:05\u001b[0m,  \u001b[1;36m5.\u001b[0m98it/s, est. speed input: \u001b[1;36m737\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m71\u001b[0m%|\u001b[1;36m7\u001b[0m| \u001b[1;36m71\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:08\u001b[0m<\u001b[1;92m00:03\u001b[0m,  \u001b[1;36m7.\u001b[0m55it/s, est. speed input: \u001b[1;36m756\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m73\u001b[0m%|\u001b[1;36m7\u001b[0m| \u001b[1;36m73\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:08\u001b[0m<\u001b[1;92m00:04\u001b[0m,  \u001b[1;36m6.\u001b[0m05it/s, est. speed input: \u001b[1;36m724\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m78\u001b[0m%|\u001b[1;36m7\u001b[0m| \u001b[1;36m78\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:09\u001b[0m<\u001b[1;92m00:03\u001b[0m,  \u001b[1;36m7.\u001b[0m21it/s, est. speed input: \u001b[1;36m732\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m79\u001b[0m%|\u001b[1;36m7\u001b[0m| \u001b[1;36m79\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:09\u001b[0m<\u001b[1;92m00:03\u001b[0m,  \u001b[1;36m6.\u001b[0m84it/s, est. speed input: \u001b[1;36m725\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m81\u001b[0m%|\u001b[1;36m8\u001b[0m| \u001b[1;36m81\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:09\u001b[0m<\u001b[1;92m00:02\u001b[0m,  \u001b[1;36m6.\u001b[0m81it/s, est. speed input: \u001b[1;36m721\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m85\u001b[0m%|\u001b[1;36m8\u001b[0m| \u001b[1;36m85\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:10\u001b[0m<\u001b[1;92m00:02\u001b[0m,  \u001b[1;36m6.\u001b[0m93it/s, est. speed input: \u001b[1;36m715\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m86\u001b[0m%|\u001b[1;36m8\u001b[0m| \u001b[1;36m86\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:10\u001b[0m<\u001b[1;92m00:02\u001b[0m,  \u001b[1;36m5.\u001b[0m53it/s, est. speed input: \u001b[1;36m694\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m90\u001b[0m%|\u001b[1;36m9\u001b[0m| \u001b[1;36m90\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:10\u001b[0m<\u001b[1;92m00:01\u001b[0m,  \u001b[1;36m8.\u001b[0m25it/s, est. speed input: \u001b[1;36m716\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m92\u001b[0m%|\u001b[1;36m9\u001b[0m| \u001b[1;36m92\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:11\u001b[0m<\u001b[1;92m00:00\u001b[0m,  \u001b[1;36m8.\u001b[0m97it/s, est. speed input: \u001b[1;36m722\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m94\u001b[0m%|\u001b[1;36m9\u001b[0m| \u001b[1;36m94\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:11\u001b[0m<\u001b[1;92m00:00\u001b[0m,  \u001b[1;36m7.\u001b[0m12it/s, est. speed input: \u001b[1;36m709\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m95\u001b[0m%|\u001b[1;36m9\u001b[0m| \u001b[1;36m95\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:11\u001b[0m<\u001b[1;92m00:00\u001b[0m,  \u001b[1;36m6.\u001b[0m77it/s, est. speed input: \u001b[1;36m705\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m96\u001b[0m%|\u001b[1;36m9\u001b[0m| \u001b[1;36m96\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:11\u001b[0m<\u001b[1;92m00:00\u001b[0m,  \u001b[1;36m6.\u001b[0m86it/s, est. speed input: \u001b[1;36m704\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m97\u001b[0m%|\u001b[1;36m9\u001b[0m| \u001b[1;36m97\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:12\u001b[0m<\u001b[1;92m00:00\u001b[0m,  \u001b[1;36m5.\u001b[0m61it/s, est. speed input: \u001b[1;36m693\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m99\u001b[0m%|\u001b[1;36m9\u001b[0m| \u001b[1;36m99\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:13\u001b[0m<\u001b[1;92m00:00\u001b[0m,  \u001b[1;36m2.\u001b[0m36it/s, est. speed input: \u001b[1;36m621\u001b[0m\n",
      "\u001b[2KProcessed prompts: \u001b[1;36m100\u001b[0m%|#| \u001b[1;36m100\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:14\u001b[0m<\u001b[1;92m00:00\u001b[0m,  \u001b[1;36m1.\u001b[0m93it/s, est. speed input: \u001b[1;36m59\u001b[0m\n",
      "\u001b[2KProcessed prompts: \u001b[1;36m100\u001b[0m%|#| \u001b[1;36m100\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:14\u001b[0m<\u001b[1;92m00:00\u001b[0m,  \u001b[1;36m6.\u001b[0m78it/s, est. speed input: \u001b[1;36m59\u001b[0m\n",
      "\u001b[2Km‚†¥\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\n",
      "\u001b[2K\u001b[32m‚†è\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-06-28 00:29:45,862][oumi][rank0][pid:1330249][MainThread][INFO]][count_letters_task.py:54] Finished inference on 100 conversations!\n",
      "[2025-06-28 00:29:45,863][oumi][rank0][pid:1330249][MainThread][INFO]][count_letters_task.py:56] Sample conversation: conversation_id='oumi_letter_count_0' messages=[SYSTEM: Your final answer should be an integer written as digits and formatted as \"\\boxed{your_answer}\". For example, if the answer is 42, you should output \"\\boxed{42}\"., USER: Look through 'perivaginal' and count the 'n's., ASSISTANT: There are 2 'n's in 'perivaginal'.] metadata={'letter': 'n', 'letter_count_integer': 1, 'letter_count_string': 'one', 'unformatted_prompt': 'Look through {word} and count the {letter}s.', 'word': 'perivaginal'}\n",
      "\u001b[2K\u001b[32m‚†è\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\n",
      "\u001b[1A\u001b[2K\u001b[1;35m                         Evaluation Results                         \u001b[0m\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ\u001b[1m \u001b[0m\u001b[1mBenchmark    \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mMetric                     \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mScore \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mStd Error\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
      "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mAccuracy                   \u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m33.00%\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mProperly Extracted Accuracy\u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m47.14%\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mNum Samples                \u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m100   \u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mNum Correct Answers        \u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m33    \u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mNum Incorrect Answers      \u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m37    \u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mNum Invalid Answers        \u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m30    \u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "!oumi evaluate -c letter_counting_tutorial/llama_3b_eval.yaml \\\n",
    "    --model.model_name \"letter_counting_tutorial/llama_3b_grpo\" \\\n",
    "    --tasks.0.num_samples 100 \\\n",
    "    --output_dir \"letter_counting_tutorial/evaluation/llama_3_grpo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Better Letter Counter\n",
    "\n",
    "Looks like we were able to significantly improve on the performance of Llama-3.2-3B-Instruct:\n",
    "\n",
    "**BEFORE**\n",
    "\n",
    "Accuracy: 31.00%\n",
    "Properly Extracted Accuracy: 46.27%\n",
    "\n",
    "**AFTER**\n",
    "\n",
    "Accuracy: 51.00%\n",
    "Properly Extracted Accuracy: 53.68%\n",
    "\n",
    "A lot of the improvement from using GRPO came because this small LLM learned to better mimic the expected output format of the extractor, but the accuracy for properly extracted samples also improved! This is a great illustration of the kind of task GRPO training excels at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "Now that you know how easy it is to train in Oumi using GRPO, perhaps you'd like to try training on your own data (in a similar data format) -- check out [our docs](https://oumi.ai/docs/en/latest/resources/datasets/sft_datasets.html#using-an-unregistered-dataset-whose-format-is-identical-to-a-registered-dataset) for an easy way to do just that. \n",
    "\n",
    "Have fun!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
