{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"align-center\">\n",
    "<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n",
    "\n",
    "[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n",
    "[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n",
    "[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - Train a Letter Counting Model using GRPO.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>\n",
    "\n",
    "üëã Welcome to Open Universal Machine Intelligence (Oumi)!\n",
    "\n",
    "üöÄ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](https://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n",
    "\n",
    "ü§ù Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n",
    "\n",
    "‚≠ê If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Letter Counting Model using GRPO\n",
    "\n",
    "This notebook delves into a fun, popular question to ask LLMs: \"How Many R‚Äôs Are in the Word Strawberry?\". First, we will use a custom evaluation function to evaluate many popular models on the task of counting letters in words. Then, we will use Group Relative Policy Optimization (GRPO), a reinforcement learning algorithm, to train Llama 3.2 3B to improve its performance on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Machine Requirements\n",
    "\n",
    "This notebook runs both model evaluation and GRPO training, which require 8GB and 40GB VRAM, respectively.\n",
    "\n",
    "‚ùó**NOTICE:** If you're running this notebook on Colab using a T4 GPU, it's not possible to run training due to memory requirements. To run evaluation, some adjustments need to be made as vLLM doesn't support T4 GPUs. This will be explained in the evaluation section.\n",
    "\n",
    "If your local machine cannot run this notebook, you can instead run this notebook on a cloud platform. The following demonstrates how to open a VSCode instance backed by a GCP node with 4 A100 GPUs, from which the notebook can be run. It is possible to run this notebook on just 1 GPU, but you will need make some adjustments to training parameters, which will be explained in the training section.\n",
    "\n",
    "```bash\n",
    "# Run on your local machine\n",
    "gcloud auth application-default login  # Authenticate with GCP\n",
    "make gcpcode ARGS=\"--resources.accelerators A100:4\"\n",
    "```\n",
    "\n",
    "### Oumi Installation\n",
    "\n",
    "First, let's install Oumi and vLLM (part of the `gpu` optional dependencies). You can find more detailed instructions about Oumi installation [here](https://oumi.ai/docs/en/latest/get_started/installation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/oumi-ai/oumi.git\n",
      "  Cloning https://github.com/oumi-ai/oumi.git to /tmp/pip-req-build-2iea6pt1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/oumi-ai/oumi.git /tmp/pip-req-build-2iea6pt1\n",
      "  Resolved https://github.com/oumi-ai/oumi.git to commit 2b62dbb247d11dedeaf3b72a21e64f50722188ef\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate<1.3,>=1.2.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (1.2.1)\n",
      "Requirement already satisfied: aiohttp<3.12,>=3.10 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (3.11.16)\n",
      "Requirement already satisfied: aiofiles<25,>=22.1.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (24.1.0)\n",
      "Requirement already satisfied: aioresponses<0.8,>=0.7.6 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (0.7.8)\n",
      "Requirement already satisfied: backoff<2.3,>=2.2.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (2.2.1)\n",
      "Requirement already satisfied: datasets<3.3,>=3.2.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (3.2.0)\n",
      "Requirement already satisfied: jsonlines in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (4.0.0)\n",
      "Requirement already satisfied: lm_eval<0.5.0,>=0.4.5 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (0.4.8)\n",
      "Requirement already satisfied: mlflow<2.22.0,>=2.21.2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (2.21.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.26.4 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (1.26.4)\n",
      "Requirement already satisfied: omegaconf<2.5,>=2.4.0dev3 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (2.4.0.dev3)\n",
      "Requirement already satisfied: packaging in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (23.1)\n",
      "Requirement already satisfied: pandas<3,>=2.0.3 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (2.2.3)\n",
      "Requirement already satisfied: peft<0.15,>=0.14.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (0.14.0)\n",
      "Requirement already satisfied: pexpect<4.9,>=4.8.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (4.8.0)\n",
      "Requirement already satisfied: pillow<11.2,>=11.1.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (11.1.0)\n",
      "Requirement already satisfied: protobuf>=5.29.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (5.29.4)\n",
      "Requirement already satisfied: pydantic<2.10,>=2.9.2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (2.9.2)\n",
      "Requirement already satisfied: responses<0.26,>=0.25.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (0.25.7)\n",
      "Requirement already satisfied: skypilot<0.8,>=0.7.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (0.7.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (2.18.0)\n",
      "Requirement already satisfied: torch<2.6.0,>=2.5.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (2.5.1)\n",
      "Requirement already satisfied: torchdata<0.10.0,>=0.9.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (0.9.0)\n",
      "Requirement already satisfied: torchvision<0.21,>=0.20.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (0.20.1)\n",
      "Requirement already satisfied: tqdm in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (4.67.1)\n",
      "Requirement already satisfied: transformers<4.52,>=4.51.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (4.51.1)\n",
      "Requirement already satisfied: trl<0.16,>=0.15.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (0.15.2)\n",
      "Requirement already satisfied: typer in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (0.15.2)\n",
      "Requirement already satisfied: typing_extensions in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (4.13.1)\n",
      "Requirement already satisfied: wandb<0.20,>=0.19.3 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from oumi==0.1.12.dev7+g2b62dbb) (0.19.9)\n",
      "Requirement already satisfied: psutil in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from accelerate<1.3,>=1.2.1->oumi==0.1.12.dev7+g2b62dbb) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from accelerate<1.3,>=1.2.1->oumi==0.1.12.dev7+g2b62dbb) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from accelerate<1.3,>=1.2.1->oumi==0.1.12.dev7+g2b62dbb) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from accelerate<1.3,>=1.2.1->oumi==0.1.12.dev7+g2b62dbb) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from aiohttp<3.12,>=3.10->oumi==0.1.12.dev7+g2b62dbb) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from aiohttp<3.12,>=3.10->oumi==0.1.12.dev7+g2b62dbb) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from aiohttp<3.12,>=3.10->oumi==0.1.12.dev7+g2b62dbb) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from aiohttp<3.12,>=3.10->oumi==0.1.12.dev7+g2b62dbb) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from aiohttp<3.12,>=3.10->oumi==0.1.12.dev7+g2b62dbb) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from aiohttp<3.12,>=3.10->oumi==0.1.12.dev7+g2b62dbb) (6.4.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from aiohttp<3.12,>=3.10->oumi==0.1.12.dev7+g2b62dbb) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from aiohttp<3.12,>=3.10->oumi==0.1.12.dev7+g2b62dbb) (1.19.0)\n",
      "Requirement already satisfied: filelock in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from datasets<3.3,>=3.2.0->oumi==0.1.12.dev7+g2b62dbb) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from datasets<3.3,>=3.2.0->oumi==0.1.12.dev7+g2b62dbb) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from datasets<3.3,>=3.2.0->oumi==0.1.12.dev7+g2b62dbb) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from datasets<3.3,>=3.2.0->oumi==0.1.12.dev7+g2b62dbb) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from datasets<3.3,>=3.2.0->oumi==0.1.12.dev7+g2b62dbb) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from datasets<3.3,>=3.2.0->oumi==0.1.12.dev7+g2b62dbb) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<3.3,>=3.2.0->oumi==0.1.12.dev7+g2b62dbb) (2024.9.0)\n",
      "Requirement already satisfied: evaluate in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (0.4.3)\n",
      "Requirement already satisfied: numexpr in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (2.10.2)\n",
      "Requirement already satisfied: pybind11>=2.6.2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (2.13.6)\n",
      "Requirement already satisfied: pytablewriter in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (1.2.1)\n",
      "Requirement already satisfied: rouge-score>=0.0.4 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (0.1.2)\n",
      "Requirement already satisfied: sacrebleu>=1.5.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (1.6.1)\n",
      "Requirement already satisfied: sqlitedict in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (2.1.0)\n",
      "Requirement already satisfied: tqdm-multiprocess in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (0.0.11)\n",
      "Requirement already satisfied: zstandard in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (0.19.0)\n",
      "Requirement already satisfied: word2number in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (1.1)\n",
      "Requirement already satisfied: more_itertools in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (10.6.0)\n",
      "Requirement already satisfied: mlflow-skinny==2.21.3 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (2.21.3)\n",
      "Requirement already satisfied: Flask<4 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (3.1.0)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (3.1.6)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (1.15.2)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (3.4.3)\n",
      "Requirement already satisfied: gunicorn<24 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (23.0.0)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (3.7)\n",
      "Requirement already satisfied: matplotlib<4 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (3.10.1)\n",
      "Requirement already satisfied: scipy<2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (1.15.2)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (2.0.40)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle<4 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (3.1.1)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (0.49.0)\n",
      "Requirement already satisfied: fastapi<1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (0.115.12)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (3.1.44)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (8.6.1)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (1.31.1)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (0.5.3)\n",
      "Requirement already satisfied: uvicorn<1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (0.34.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pandas<3,>=2.0.3->oumi==0.1.12.dev7+g2b62dbb) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pandas<3,>=2.0.3->oumi==0.1.12.dev7+g2b62dbb) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pandas<3,>=2.0.3->oumi==0.1.12.dev7+g2b62dbb) (2024.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pexpect<4.9,>=4.8.0->oumi==0.1.12.dev7+g2b62dbb) (0.7.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pydantic<2.10,>=2.9.2->oumi==0.1.12.dev7+g2b62dbb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pydantic<2.10,>=2.9.2->oumi==0.1.12.dev7+g2b62dbb) (2.23.4)\n",
      "Requirement already satisfied: urllib3<3.0,>=1.25.10 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from responses<0.26,>=0.25.0->oumi==0.1.12.dev7+g2b62dbb) (1.26.18)\n",
      "Requirement already satisfied: wheel in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (0.41.2)\n",
      "Requirement already satisfied: colorama in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (0.4.6)\n",
      "Requirement already satisfied: cryptography in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (41.0.7)\n",
      "Requirement already satisfied: jsonschema in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (4.23.0)\n",
      "Requirement already satisfied: networkx in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (3.4.2)\n",
      "Requirement already satisfied: pendulum in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (3.0.0)\n",
      "Requirement already satisfied: PrettyTable>=2.0.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (3.16.0)\n",
      "Requirement already satisfied: python-dotenv in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (1.1.0)\n",
      "Requirement already satisfied: rich in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (14.0.0)\n",
      "Requirement already satisfied: tabulate in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (0.9.0)\n",
      "Requirement already satisfied: pulp in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (3.1.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18.0->oumi==0.1.12.dev7+g2b62dbb) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18.0->oumi==0.1.12.dev7+g2b62dbb) (1.72.0rc1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18.0->oumi==0.1.12.dev7+g2b62dbb) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18.0->oumi==0.1.12.dev7+g2b62dbb) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18.0->oumi==0.1.12.dev7+g2b62dbb) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18.0->oumi==0.1.12.dev7+g2b62dbb) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch<2.6.0,>=2.5.0->oumi==0.1.12.dev7+g2b62dbb) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch<2.6.0,>=2.5.0->oumi==0.1.12.dev7+g2b62dbb) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch<2.6.0,>=2.5.0->oumi==0.1.12.dev7+g2b62dbb) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch<2.6.0,>=2.5.0->oumi==0.1.12.dev7+g2b62dbb) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch<2.6.0,>=2.5.0->oumi==0.1.12.dev7+g2b62dbb) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch<2.6.0,>=2.5.0->oumi==0.1.12.dev7+g2b62dbb) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch<2.6.0,>=2.5.0->oumi==0.1.12.dev7+g2b62dbb) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch<2.6.0,>=2.5.0->oumi==0.1.12.dev7+g2b62dbb) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch<2.6.0,>=2.5.0->oumi==0.1.12.dev7+g2b62dbb) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch<2.6.0,>=2.5.0->oumi==0.1.12.dev7+g2b62dbb) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch<2.6.0,>=2.5.0->oumi==0.1.12.dev7+g2b62dbb) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch<2.6.0,>=2.5.0->oumi==0.1.12.dev7+g2b62dbb) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch<2.6.0,>=2.5.0->oumi==0.1.12.dev7+g2b62dbb) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch<2.6.0,>=2.5.0->oumi==0.1.12.dev7+g2b62dbb) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from sympy==1.13.1->torch<2.6.0,>=2.5.0->oumi==0.1.12.dev7+g2b62dbb) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from transformers<4.52,>=4.51.0->oumi==0.1.12.dev7+g2b62dbb) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from transformers<4.52,>=4.51.0->oumi==0.1.12.dev7+g2b62dbb) (0.21.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from wandb<0.20,>=0.19.3->oumi==0.1.12.dev7+g2b62dbb) (0.4.0)\n",
      "Requirement already satisfied: platformdirs in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from wandb<0.20,>=0.19.3->oumi==0.1.12.dev7+g2b62dbb) (3.10.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from wandb<0.20,>=0.19.3->oumi==0.1.12.dev7+g2b62dbb) (2.25.1)\n",
      "Requirement already satisfied: setproctitle in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from wandb<0.20,>=0.19.3->oumi==0.1.12.dev7+g2b62dbb) (1.3.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from typer->oumi==0.1.12.dev7+g2b62dbb) (1.5.4)\n",
      "Requirement already satisfied: Mako in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (1.3.9)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from Flask<4->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.9 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from Flask<4->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (1.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (4.0.12)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from graphene<4->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from graphene<4->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from matplotlib<4->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from matplotlib<4->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from matplotlib<4->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from matplotlib<4->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from matplotlib<4->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (3.2.3)\n",
      "Requirement already satisfied: wcwidth in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from PrettyTable>=2.0.0->skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets<3.3,>=3.2.0->oumi==0.1.12.dev7+g2b62dbb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets<3.3,>=3.2.0->oumi==0.1.12.dev7+g2b62dbb) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets<3.3,>=3.2.0->oumi==0.1.12.dev7+g2b62dbb) (2023.11.17)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from rich->skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from rich->skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (2.19.1)\n",
      "Requirement already satisfied: nltk in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (3.9.1)\n",
      "Requirement already satisfied: portalocker in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (3.1.1)\n",
      "Requirement already satisfied: lxml in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (5.3.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (3.6.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (3.1.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from cryptography->skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from jsonschema->skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from jsonschema->skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from jsonschema->skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (0.24.0)\n",
      "Requirement already satisfied: time-machine>=2.6.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pendulum->skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (2.16.0)\n",
      "Requirement already satisfied: DataProperty<2,>=1.1.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pytablewriter->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (1.1.0)\n",
      "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pytablewriter->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (1.1.4)\n",
      "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pytablewriter->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (3.2.3)\n",
      "Requirement already satisfied: tabledata<2,>=1.3.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pytablewriter->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (1.3.4)\n",
      "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pytablewriter->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (0.1.7)\n",
      "Requirement already satisfied: typepy<2,>=1.3.2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (1.3.4)\n",
      "Requirement already satisfied: pycparser in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from cffi>=1.12->cryptography->skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (2.21)\n",
      "Requirement already satisfied: google-auth~=2.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (2.38.0)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from fastapi<1->mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (0.46.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (5.0.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->skypilot<0.8,>=0.7.0->oumi==0.1.12.dev7+g2b62dbb) (0.1.2)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi==0.1.12.dev7+g2b62dbb) (5.2.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (1.2.18)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (0.52b1)\n",
      "Requirement already satisfied: h11>=0.8 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from uvicorn<1->mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (0.14.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (1.17.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (4.9)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (4.9.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.3->mlflow<2.22.0,>=2.21.2->oumi==0.1.12.dev7+g2b62dbb) (0.6.1)\n",
      "Building wheels for collected packages: oumi\n",
      "  Building wheel for oumi (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for oumi: filename=oumi-0.1.12.dev7+g2b62dbb-py3-none-any.whl size=545324 sha256=e6070141d61cadb7a6ceb2c23c5b684a928b48063f177190c6861156792db38f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-h4g__2rg/wheels/ba/74/ba/892fcc8d178577365d58cebdcc694e805c47b498bc53233063\n",
      "Successfully built oumi\n",
      "Installing collected packages: oumi\n",
      "  Attempting uninstall: oumi\n",
      "    Found existing installation: oumi 0.1.12.dev14+gcde64fdd.d20250410\n",
      "    Uninstalling oumi-0.1.12.dev14+gcde64fdd.d20250410:\n",
      "      Successfully uninstalled oumi-0.1.12.dev14+gcde64fdd.d20250410\n",
      "Successfully installed oumi-0.1.12.dev7+g2b62dbb\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting vllm<0.8.0,>=0.7.3\n",
      "  Downloading vllm-0.7.3-cp38-abi3-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: psutil in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (7.0.0)\n",
      "Collecting sentencepiece (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: numpy<2.0.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (1.26.4)\n",
      "Collecting numba==0.60.0 (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (4.67.1)\n",
      "Collecting blake3 (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading blake3-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting py-cpuinfo (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: transformers>=4.48.2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (4.51.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (0.21.1)\n",
      "Requirement already satisfied: protobuf in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (5.29.4)\n",
      "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3) (0.115.12)\n",
      "Requirement already satisfied: aiohttp in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (3.11.16)\n",
      "Collecting openai>=1.52.0 (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading openai-1.72.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic>=2.9 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (2.9.2)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (0.21.1)\n",
      "Requirement already satisfied: pillow in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (11.1.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.9 (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting outlines==0.1.11 (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lark==1.2.2 (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.11 (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading xgrammar-0.1.11-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (4.13.1)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (3.18.0)\n",
      "Collecting partial-json-parser (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (26.4.0)\n",
      "Collecting msgspec (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf==0.10.0 (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: importlib_metadata in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (8.6.1)\n",
      "Collecting mistral_common>=1.5.0 (from mistral_common[opencv]>=1.5.0->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: pyyaml in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (6.0.2)\n",
      "Collecting einops (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.9.1 (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading compressed_tensors-0.9.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting depyf==0.18.0 (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: cloudpickle in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (3.1.1)\n",
      "Collecting ray==2.40.0 (from ray[adag]==2.40.0->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading ray-2.40.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: torch==2.5.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (2.5.1)\n",
      "Collecting torchaudio==2.5.1 (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading torchaudio-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: torchvision==0.20.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from vllm<0.8.0,>=0.7.3) (0.20.1)\n",
      "Collecting xformers==0.0.28.post3 (from vllm<0.8.0,>=0.7.3)\n",
      "  Downloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting astor (from depyf==0.18.0->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: dill in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from depyf==0.18.0->vllm<0.8.0,>=0.7.3) (0.3.8)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba==0.60.0->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting interegular (from outlines==0.1.11->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jinja2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from outlines==0.1.11->vllm<0.8.0,>=0.7.3) (3.1.6)\n",
      "Requirement already satisfied: nest_asyncio in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from outlines==0.1.11->vllm<0.8.0,>=0.7.3) (1.6.0)\n",
      "Collecting diskcache (from outlines==0.1.11->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: referencing in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from outlines==0.1.11->vllm<0.8.0,>=0.7.3) (0.36.2)\n",
      "Requirement already satisfied: jsonschema in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from outlines==0.1.11->vllm<0.8.0,>=0.7.3) (4.23.0)\n",
      "Collecting pycountry (from outlines==0.1.11->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting airportsdata (from outlines==0.1.11->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading outlines_core-0.1.26-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: click>=7.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm<0.8.0,>=0.7.3) (8.1.8)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray==2.40.0->ray[adag]==2.40.0->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: packaging in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm<0.8.0,>=0.7.3) (23.1)\n",
      "Requirement already satisfied: aiosignal in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm<0.8.0,>=0.7.3) (1.3.2)\n",
      "Requirement already satisfied: frozenlist in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm<0.8.0,>=0.7.3) (1.5.0)\n",
      "Collecting cupy-cuda12x (from ray[adag]==2.40.0->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading cupy_cuda12x-13.4.1-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: networkx in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (3.4.2)\n",
      "Requirement already satisfied: fsspec in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from torch==2.5.1->vllm<0.8.0,>=0.7.3) (1.13.1)\n",
      "Requirement already satisfied: pybind11 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from xgrammar==0.1.11->vllm<0.8.0,>=0.7.3) (2.13.6)\n",
      "Requirement already satisfied: pytest in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from xgrammar==0.1.11->vllm<0.8.0,>=0.7.3) (8.3.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.5.1->vllm<0.8.0,>=0.7.3) (1.3.0)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3) (0.46.1)\n",
      "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3) (0.28.1)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3) (0.34.0)\n",
      "Collecting opencv-python-headless>=4.0.0 (from mistral_common[opencv]>=1.5.0->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from openai>=1.52.0->vllm<0.8.0,>=0.7.3) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from openai>=1.52.0->vllm<0.8.0,>=0.7.3) (1.8.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.52.0->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from openai>=1.52.0->vllm<0.8.0,>=0.7.3) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pydantic>=2.9->vllm<0.8.0,>=0.7.3) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pydantic>=2.9->vllm<0.8.0,>=0.7.3) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->vllm<0.8.0,>=0.7.3) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->vllm<0.8.0,>=0.7.3) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->vllm<0.8.0,>=0.7.3) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->vllm<0.8.0,>=0.7.3) (2023.11.17)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from tiktoken>=0.6.0->vllm<0.8.0,>=0.7.3) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from tokenizers>=0.19.1->vllm<0.8.0,>=0.7.3) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from transformers>=4.48.2->vllm<0.8.0,>=0.7.3) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from aiohttp->vllm<0.8.0,>=0.7.3) (2.6.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from aiohttp->vllm<0.8.0,>=0.7.3) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from aiohttp->vllm<0.8.0,>=0.7.3) (25.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from aiohttp->vllm<0.8.0,>=0.7.3) (6.4.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from aiohttp->vllm<0.8.0,>=0.7.3) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from aiohttp->vllm<0.8.0,>=0.7.3) (1.19.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from importlib_metadata->vllm<0.8.0,>=0.7.3) (3.21.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.52.0->vllm<0.8.0,>=0.7.3) (1.2.2)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: typer>=0.12.3 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3) (0.15.2)\n",
      "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading rich_toolkit-0.14.1-py3-none-any.whl.metadata (999 bytes)\n",
      "Requirement already satisfied: httpcore==1.* in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from jinja2->outlines==0.1.11->vllm<0.8.0,>=0.7.3) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from jsonschema->outlines==0.1.11->vllm<0.8.0,>=0.7.3) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from jsonschema->outlines==0.1.11->vllm<0.8.0,>=0.7.3) (0.24.0)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3) (1.1.0)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading watchfiles-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[adag]==2.40.0->vllm<0.8.0,>=0.7.3)\n",
      "  Downloading fastrlock-0.8.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: iniconfig in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pytest->xgrammar==0.1.11->vllm<0.8.0,>=0.7.3) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pytest->xgrammar==0.1.11->vllm<0.8.0,>=0.7.3) (1.5.0)\n",
      "Requirement already satisfied: tomli>=1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from pytest->xgrammar==0.1.11->vllm<0.8.0,>=0.7.3) (2.2.1)\n",
      "Requirement already satisfied: rich>=13.7.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3) (14.0.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/gcpuser/miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm<0.8.0,>=0.7.3) (0.1.2)\n",
      "Downloading vllm-0.7.3-cp38-abi3-manylinux1_x86_64.whl (264.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m264.6/264.6 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading compressed_tensors-0.9.1-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading gguf-0.10.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ray-2.40.0-cp310-cp310-manylinux2014_x86_64.whl (66.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.8/66.8 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.5.1-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m151.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading xgrammar-0.1.11-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (396 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m396.2/396.2 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outlines_core-0.1.26-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m163.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.72.0-py3-none-any.whl (643 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m643.9/643.9 kB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blake3-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m376.4/376.4 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.6/211.6 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
      "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Downloading jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m352.9/352.9 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading airportsdata-20250224-py3-none-any.whl (913 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading cupy_cuda12x-13.4.1-cp310-cp310-manylinux2014_x86_64.whl (104.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.6/104.6 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m161.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastrlock-0.8.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich_toolkit-0.14.1-py3-none-any.whl (24 kB)\n",
      "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m151.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m454.9/454.9 kB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, py-cpuinfo, fastrlock, blake3, websockets, uvloop, python-multipart, pycountry, partial-json-parser, opencv-python-headless, msgspec, msgpack, llvmlite, lark, jiter, interegular, httptools, gguf, einops, dnspython, diskcache, cupy-cuda12x, astor, airportsdata, watchfiles, tiktoken, numba, email-validator, depyf, rich-toolkit, prometheus-fastapi-instrumentator, openai, lm-format-enforcer, xformers, torchaudio, ray, outlines_core, mistral_common, fastapi-cli, xgrammar, outlines, compressed-tensors, vllm\n",
      "Successfully installed airportsdata-20250224 astor-0.8.1 blake3-1.0.4 compressed-tensors-0.9.1 cupy-cuda12x-13.4.1 depyf-0.18.0 diskcache-5.6.3 dnspython-2.7.0 einops-0.8.1 email-validator-2.2.0 fastapi-cli-0.0.7 fastrlock-0.8.3 gguf-0.10.0 httptools-0.6.4 interegular-0.3.3 jiter-0.9.0 lark-1.2.2 llvmlite-0.43.0 lm-format-enforcer-0.10.11 mistral_common-1.5.4 msgpack-1.1.0 msgspec-0.19.0 numba-0.60.0 openai-1.72.0 opencv-python-headless-4.11.0.86 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.1.0 py-cpuinfo-9.0.0 pycountry-24.6.1 python-multipart-0.0.20 ray-2.40.0 rich-toolkit-0.14.1 sentencepiece-0.2.0 tiktoken-0.9.0 torchaudio-2.5.1 uvloop-0.21.0 vllm-0.7.3 watchfiles-1.0.5 websockets-15.0.1 xformers-0.0.28.post3 xgrammar-0.1.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/oumi-ai/oumi.git\n",
    "%pip install \"vllm>=0.7.3,<0.8.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote API Access\n",
    "\n",
    "As part of this notebook, you can evaluate frontier models from Open AI, Google, Anthropic, and Meta on the letter counting task. If you want to evaluate any of these models, set the corresponding fields below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # Set your OpenAI API key here.\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"\"  # Set your Gemini API key here.\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"\"  # Set your Anthropic API key here.\n",
    "\n",
    "# Set your GCP project id and region, if you want to query Llama 3.1 405B in Vertex.\n",
    "REGION = \"\"  # Set your GCP region here.\n",
    "PROJECT_ID = \"\"  # Set your GCP project id here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Directory\n",
    "\n",
    "Finally, we'll set up a directory to use for this tutorial, and some environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"letter_counting_tutorial\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable warnings from HF.\n",
    "\n",
    "# This is needed for vLLM to use multiple GPUs in a notebook.\n",
    "# If you're not running in a notebook, you can ignore this.\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset we'll use for this notebook is `oumi-ai/oumi-letter-count`, which can be found on [HF Datasets](https://huggingface.co/datasets/oumi-ai/oumi-letter-count). Its prompts ask to count the letters in various English words, with metadata in each example containing the correct count. We use the `train` split for training and the `test` split for evaluation. We'll use an Oumi dataset class, `LetterCountGrpoDataset`, to load and preprocess the HF Dataset. The following code displays an example prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-10 09:36:55,268][oumi][rank0][pid:9994][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: LetterCountGrpoDataset)... dataset_name: 'oumi-ai/oumi-letter-count'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d63247b99b49979b2f06e69710911e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/941 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5bdd9ce28b34a518bfd8de1840b61e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/4.47M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8372afeb86e64ac99cd891288b79d8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/400k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5459866a082c4da4b2a6c71028803057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/831k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5941603aee4843c38b52ce603233fd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f585a7b414274a4097f070a226d238b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7599699e46444adf9dbe7b2e9a5f9f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-10 09:37:00,027][oumi][rank0][pid:9994][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: validation\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 22894322\n",
      "\tDownload size: 5697295\n",
      "\tSize: 28591617 bytes\n",
      "\tRows: 10000\n",
      "\tColumns: ['conversation_id', 'messages', 'metadata']\n",
      "[2025-04-10 09:37:00,248][oumi][rank0][pid:9994][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (10000, 3). Columns:\n",
      "conversation_id    object\n",
      "messages           object\n",
      "metadata           object\n",
      "dtype: object\n",
      "--------------------------------------------------------------------------------\n",
      "Sample:\n",
      "{'conversation_id': 'oumi_letter_count_0',\n",
      " 'messages': [{'content': \"Could you determine the count of 'l's in \"\n",
      "                          \"'substantial'?\",\n",
      "               'role': 'user'},\n",
      "              {'content': 'Your final answer should be written as digits and '\n",
      "                          'formatted as \"\\\\boxed{your_answer}\". For example, '\n",
      "                          'if the answer is 42, make sure to output '\n",
      "                          '\"\\\\boxed{42}\".',\n",
      "               'role': 'system'}],\n",
      " 'metadata': {'letter': 'l',\n",
      "              'letter_count_integer': 1,\n",
      "              'letter_count_string': 'one',\n",
      "              'unformatted_prompt': 'Could you determine the count of '\n",
      "                                    '{letter}s in {word}?',\n",
      "              'word': 'substantial'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from oumi.datasets.grpo.letter_count import LetterCountGrpoDataset\n",
    "\n",
    "dataset = LetterCountGrpoDataset(split=\"validation\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Sample:\")\n",
    "pprint(dataset.conversation(0).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "First, we'll evaluate how various models perform on the letter counting task. We'll evaluate frontier models by calling their respective remote API, and Llama 3.2 3B by running local inference on it using vLLM.\n",
    "\n",
    "We've already defined a custom evaluation function in Oumi which runs inference on the above dataset, extracts the answer from the model response, and calculates various metrics such as accuracy. This function is defined at `src/oumi/evaluation/registry/count_letters_task.py` ([GitHub link](https://github.com/oumi-ai/oumi/blob/main/src/oumi/evaluation/registry/count_letters_task.py)), and we print its contents below for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@register_evaluation_function(\"count_letters\")\n",
      "def count_letters(\n",
      "    task_params: EvaluationTaskParams,\n",
      "    inference_engine: BaseInferenceEngine,\n",
      ") -> dict[str, Any]:\n",
      "    \"\"\"Custom evaluation function registered as `count_letters`.\"\"\"\n",
      "    dataset = LetterCountGrpoDataset(split=\"test\")\n",
      "    # TODO: OPE-1155: Add support for using Oumi dataset code to create the dataset.\n",
      "    # dataset = build_dataset(\"oumi-ai/oumi-letter-count\", tokenizer=None, sample_count=10)  # noqa: E501\n",
      "    # dataset = build_dataset(\"oumi-ai/berrybench-v0.1.0\", tokenizer=None, sample_count=10)  # noqa: E501\n",
      "    num_samples = task_params.num_samples\n",
      "    if num_samples is None:\n",
      "        num_samples = len(dataset)\n",
      "    input_conversations = [dataset.conversation(i) for i in range(num_samples)]\n",
      "    conversations = inference_engine.infer(input_conversations)\n",
      "    logger.info(f\"Finished inference on {len(conversations)} conversations!\")\n",
      "    if len(conversations) > 0:\n",
      "        logger.info(f\"Sample conversation: {conversations[0]}\")\n",
      "\n",
      "    count = 0  # The number of examples with correct answers extracted.\n",
      "    total = 0  # All examples.\n",
      "    valid_count = 0  # The number of examples with valid answers extracted.\n",
      "    for i, conversation in enumerate(conversations):\n",
      "        total += 1\n",
      "        # Grab the model's response\n",
      "        response = conversation.last_message()\n",
      "        # Ignore cases where model didn't respond or it's a multimodal response.\n",
      "        # For now, we focus on text-only responses.\n",
      "        if not response or not isinstance(response.content, str):\n",
      "            continue\n",
      "        # Count the example as correct if the extracted prediction is correct.\n",
      "        prediction = _extract_prediction(response.content)\n",
      "        if prediction is None:\n",
      "            continue\n",
      "        valid_count += 1\n",
      "        if prediction == conversation.metadata[\"letter_count_integer\"]:\n",
      "            count += 1\n",
      "\n",
      "    return {\n",
      "        # Accuracy across all examples.\n",
      "        \"accuracy\": count / total,\n",
      "        # Accuracy when only counting examples with properly extracted answers.\n",
      "        \"properly_extracted_accuracy\": count / valid_count,\n",
      "        \"num_samples\": num_samples,\n",
      "        # These three values sum up to num_samples.\n",
      "        \"num_correct_answers\": count,\n",
      "        \"num_incorrect_answers\": valid_count - count,\n",
      "        \"num_invalid_answers\": total - valid_count,\n",
      "    }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "from oumi.evaluation.registry.count_letters_task import count_letters\n",
    "\n",
    "print(inspect.getsource(count_letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, you can select which models you want to evaluate. You can lower `NUM_SAMPLES`  to reduce cost when calling remote APIs, with the downside of noisier results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 100\n",
    "# We set an environment variable to be used at the end of the Colab.\n",
    "os.environ[\"NUM_SAMPLES\"] = str(NUM_SAMPLES)\n",
    "\n",
    "model_names = [\n",
    "    \"llama_3b\",\n",
    "    # Uncomment any models you wish to evaluate - you can evaluate multiple at once.\n",
    "    # \"gpt_4o\",\n",
    "    # \"gemini_pro\",\n",
    "    # \"llama_405b\",\n",
    "    # \"claude_sonnet\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó**NOTICE:** If running this notebook on Colab, delete the following line: `inference_engine: VLLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing letter_counting_tutorial/llama_3b_eval.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/llama_3b_eval.yaml\n",
    "\n",
    "# We save this config as a YAML file as we'll use it again at the end of the notebook.\n",
    "model:\n",
    "  model_name: \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "  model_max_length: 131072\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  attn_implementation: \"sdpa\"\n",
    "  trust_remote_code: True\n",
    "\n",
    "inference_engine: VLLM\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 2048\n",
    "\n",
    "tasks:\n",
    "  - evaluation_backend: custom\n",
    "    task_name: count_letters\n",
    "\n",
    "output_dir: \"letter_counting_tutorial/evaluation/llama_3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EvaluationConfig for various models.\n",
    "# Note that Llama 3B uses the local VLLM inference engines, while the others use various\n",
    "# remote engines.\n",
    "\n",
    "with open(f\"{tutorial_dir}/llama_3b_eval.yaml\") as f:\n",
    "    llama_3b_yaml = f.read()\n",
    "\n",
    "configs = {\n",
    "    \"llama_3b\": llama_3b_yaml,\n",
    "    \"gpt_4o\": \"\"\"\n",
    "      model:\n",
    "        model_name: \"gpt-4o\"\n",
    "\n",
    "      inference_engine: OPENAI\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_key_env_varname: \"OPENAI_API_KEY\"\n",
    "        max_retries: 3\n",
    "        num_workers: 100\n",
    "        politeness_policy: 60\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: count_letters\n",
    "\n",
    "      output_dir: \"letter_counting_tutorial/evaluation/gpt_4o\"\n",
    "      \"\"\",\n",
    "    \"gemini_pro\": \"\"\"\n",
    "      model:\n",
    "        model_name: \"gemini-2.5-pro-preview-03-25\"\n",
    "\n",
    "      inference_engine: GOOGLE_GEMINI\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_key_env_varname: \"GEMINI_API_KEY\"\n",
    "        max_retries: 3\n",
    "        num_workers: 2\n",
    "        politeness_policy: 60\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: count_letters\n",
    "\n",
    "      output_dir: \"letter_counting_tutorial/evaluation/gemini_pro\"\n",
    "      \"\"\",\n",
    "    \"llama_405b\": f\"\"\"\n",
    "      model:\n",
    "        model_name: \"meta/llama-3.1-405b-instruct-maas\"\n",
    "\n",
    "      inference_engine: GOOGLE_VERTEX\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_url: \"https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/openapi/chat/completions\"\n",
    "        max_retries: 3\n",
    "        num_workers: 10\n",
    "        politeness_policy: 60\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: count_letters\n",
    "\n",
    "      output_dir: \"letter_counting_tutorial/evaluation/llama_405b\"\n",
    "      \"\"\",\n",
    "    \"claude_sonnet\": \"\"\"\n",
    "      model:\n",
    "        model_name: \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "      inference_engine: ANTHROPIC\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_key_env_varname: \"ANTHROPIC_API_KEY\"\n",
    "        max_retries: 3\n",
    "        num_workers: 5\n",
    "        politeness_policy: 65\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: count_letters\n",
    "\n",
    "      output_dir: \"letter_counting_tutorial/evaluation/claude_sonnet\"\n",
    "      \"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7f30d79d2b436b9ab93a5b6caf0f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba926c6e6ce343dba805a5d9b385ee19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7dd7c98c3d4a27ac9ea33b0a389119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ca2f4039de423e8e314f30fc346ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-10 09:37:10,370][oumi][rank0][pid:9994][MainThread][WARNING]][models.py:439] Undefined pad token. Setting it to `<|finetune_right_pad_id|>`.\n",
      "[2025-04-10 09:37:10,373][oumi][rank0][pid:9994][MainThread][INFO]][models.py:482] Using the model's built-in chat template for model 'meta-llama/Llama-3.2-3B-Instruct'.\n",
      "INFO 04-10 09:37:10 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 04-10 09:37:19 config.py:549] This model supports multiple tasks: {'embed', 'generate', 'classify', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 04-10 09:37:19 config.py:1382] Defaulting to use mp for distributed inference\n",
      "WARNING 04-10 09:37:19 arg_utils.py:1187] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 04-10 09:37:19 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 04-10 09:37:19 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 04-10 09:37:19 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 04-10 09:37:19 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a65e22de9d04985b4156410636dc219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-10 09:37:20 multiproc_worker_utils.py:300] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 04-10 09:37:20 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 04-10 09:37:21 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 04-10 09:37:26 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 04-10 09:37:26 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 04-10 09:37:26 __init__.py:207] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10493)\u001b[0;0m INFO 04-10 09:37:26 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10492)\u001b[0;0m INFO 04-10 09:37:26 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10494)\u001b[0;0m INFO 04-10 09:37:26 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10492)\u001b[0;0m INFO 04-10 09:37:27 cuda.py:229] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10493)\u001b[0;0m INFO 04-10 09:37:27 cuda.py:229] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10494)\u001b[0;0m INFO 04-10 09:37:27 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 04-10 09:37:29 utils.py:916] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10493)\u001b[0;0m INFO 04-10 09:37:29 utils.py:916] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10494)\u001b[0;0m INFO 04-10 09:37:29 utils.py:916] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10493)\u001b[0;0m INFO 04-10 09:37:29 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10494)\u001b[0;0m INFO 04-10 09:37:29 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10492)\u001b[0;0m INFO 04-10 09:37:29 utils.py:916] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10492)\u001b[0;0m INFO 04-10 09:37:29 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 04-10 09:37:29 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 04-10 09:37:30 custom_all_reduce_utils.py:206] generating GPU P2P access cache in /home/gcpuser/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 04-10 09:37:50 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/gcpuser/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10492)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=10494)\u001b[0;0m INFO 04-10 09:37:50 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/gcpuser/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 04-10 09:37:50 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/gcpuser/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10493)\u001b[0;0m INFO 04-10 09:37:50 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/gcpuser/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 04-10 09:37:50 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_2a3b87a3'), local_subscribe_port=39009, remote_subscribe_port=None)\n",
      "INFO 04-10 09:37:50 model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10492)\u001b[0;0m INFO 04-10 09:37:50 model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10494)\u001b[0;0m INFO 04-10 09:37:50 model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10493)\u001b[0;0m INFO 04-10 09:37:50 model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10494)\u001b[0;0m INFO 04-10 09:37:50 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10492)\u001b[0;0m INFO 04-10 09:37:50 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10493)\u001b[0;0m INFO 04-10 09:37:50 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 04-10 09:37:51 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10494)\u001b[0;0m INFO 04-10 09:38:03 weight_utils.py:270] Time spent downloading weights for meta-llama/Llama-3.2-3B-Instruct: 12.871211 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d460a266ea14fb296923d238785f146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=10494)\u001b[0;0m INFO 04-10 09:38:04 model_runner.py:1115] Loading model weights took 1.5341 GB\n",
      "INFO 04-10 09:38:05 model_runner.py:1115] Loading model weights took 1.5341 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10493)\u001b[0;0m INFO 04-10 09:38:05 model_runner.py:1115] Loading model weights took 1.5341 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10492)\u001b[0;0m INFO 04-10 09:38:05 model_runner.py:1115] Loading model weights took 1.5341 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10492)\u001b[0;0m INFO 04-10 09:38:12 worker.py:267] Memory profiling takes 6.17 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10492)\u001b[0;0m INFO 04-10 09:38:12 worker.py:267] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.90) = 35.45GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10492)\u001b[0;0m INFO 04-10 09:38:12 worker.py:267] model weights take 1.53GiB; non_torch_memory takes 2.10GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 31.58GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10494)\u001b[0;0m INFO 04-10 09:38:12 worker.py:267] Memory profiling takes 6.16 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10494)\u001b[0;0m INFO 04-10 09:38:12 worker.py:267] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.90) = 35.45GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10494)\u001b[0;0m INFO 04-10 09:38:12 worker.py:267] model weights take 1.53GiB; non_torch_memory takes 1.82GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 31.86GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10493)\u001b[0;0m INFO 04-10 09:38:12 worker.py:267] Memory profiling takes 6.17 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10493)\u001b[0;0m INFO 04-10 09:38:12 worker.py:267] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.90) = 35.45GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10493)\u001b[0;0m INFO 04-10 09:38:12 worker.py:267] model weights take 1.53GiB; non_torch_memory takes 2.10GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 31.58GiB.\n",
      "INFO 04-10 09:38:12 worker.py:267] Memory profiling takes 6.27 seconds\n",
      "INFO 04-10 09:38:12 worker.py:267] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.90) = 35.45GiB\n",
      "INFO 04-10 09:38:12 worker.py:267] model weights take 1.53GiB; non_torch_memory takes 2.20GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 30.54GiB.\n",
      "INFO 04-10 09:38:12 executor_base.py:111] # cuda blocks: 71484, # CPU blocks: 9362\n",
      "INFO 04-10 09:38:12 executor_base.py:116] Maximum concurrency for 131072 tokens per request: 8.73x\n",
      "INFO 04-10 09:38:17 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 11.55 seconds\n",
      "[2025-04-10 09:38:20,267][oumi][rank0][pid:9994][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: LetterCountGrpoDataset)... dataset_name: 'oumi-ai/oumi-letter-count'\n",
      "[2025-04-10 09:38:22,067][oumi][rank0][pid:9994][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: test\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 22894322\n",
      "\tDownload size: 5697295\n",
      "\tSize: 28591617 bytes\n",
      "\tRows: 20000\n",
      "\tColumns: ['conversation_id', 'messages', 'metadata']\n",
      "[2025-04-10 09:38:22,768][oumi][rank0][pid:9994][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (20000, 3). Columns:\n",
      "conversation_id    object\n",
      "messages           object\n",
      "metadata           object\n",
      "dtype: object\n",
      "INFO 04-10 09:38:22 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 26.70it/s, est. speed input: 2430.26 toks/s, output: 635.19 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-10 09:38:26,617][oumi][rank0][pid:9994][MainThread][INFO]][count_letters_task.py:53] Finished inference on 100 conversations!\n",
      "[2025-04-10 09:38:26,618][oumi][rank0][pid:9994][MainThread][INFO]][count_letters_task.py:55] Sample conversation: conversation_id='oumi_letter_count_0' messages=[USER: Look through 'perivaginal' and count the 'n's., SYSTEM: Your final answer should be written as digits and formatted as \"\\boxed{your_answer}\". For example, if the answer is 42, make sure to output \"\\boxed{42}\"., ASSISTANT: There are 2 'n's in 'perivaginal'. \n",
      "\n",
      "\\boxed{2}] metadata={'letter': 'n', 'letter_count_integer': 1, 'letter_count_string': 'one', 'unformatted_prompt': 'Look through {word} and count the {letter}s.', 'word': 'perivaginal'}\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation on all specified models.\n",
    "\n",
    "from oumi.core.configs import EvaluationConfig\n",
    "from oumi.core.evaluation import Evaluator\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    # Create the evaluation config from the YAML string.\n",
    "    config_yaml: str = configs[model_name]\n",
    "    config = EvaluationConfig.from_str(config_yaml)\n",
    "    config.tasks[0].num_samples = NUM_SAMPLES\n",
    "\n",
    "    # Run the evaluation.\n",
    "    evaluator = Evaluator()\n",
    "    evaluator_out = evaluator.evaluate(config)\n",
    "\n",
    "    # # Record the results.\n",
    "    results[model_name] = evaluator_out[0].get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 100\n",
      "--------------------------------------------------------------------------------\n",
      "Model: llama_3b\n",
      "Accuracy: 24.00%\n",
      "Num correct, incorrect, invalid: 24, 69, 7\n"
     ]
    }
   ],
   "source": [
    "# Print results.\n",
    "\n",
    "print(f\"Total samples: {NUM_SAMPLES}\")\n",
    "for model_name, result in results.items():\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {result['accuracy']:.2%}\")\n",
    "    correct = result[\"num_correct_answers\"]\n",
    "    incorrect = result[\"num_incorrect_answers\"]\n",
    "    invalid = result[\"num_invalid_answers\"]\n",
    "    print(f\"Num correct, incorrect, invalid: {correct}, {incorrect}, {invalid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO\n",
    "\n",
    "Now, we train Llama 3.2 3B on the task of counting letters using the GRPO algorithm implemented by [HuggingFace's `trl` library](https://huggingface.co/docs/trl/en/index).\n",
    "\n",
    "Note that we can calculate a concrete reward for this task by comparing the answer extracted by the model with the correct answer. In the reward function defined in `src/oumi/datasets/grpo/rewards/count_letters_rewards.py` ([GitHub link](https://github.com/oumi-ai/oumi/blob/main/src/oumi/datasets/grpo/rewards/count_letters_rewards.py)), we calculate the reward to be `-abs(predicted_count - target_count)`. We use simple heuristics to extract the predicted count. The following cell prints out the reward function code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2025 - Oumi\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import re\n",
      "from typing import Any, Optional\n",
      "\n",
      "from oumi.core.registry import RegistryType, register\n",
      "\n",
      "\n",
      "def _extract_prediction(response: str) -> Optional[int]:\n",
      "    r\"\"\"Returns the numeric answer extracted from `\\boxed{...}`, or None otherwise.\"\"\"\n",
      "    regex_result = re.findall(r\"\\\\boxed\\{([-+]?\\d+)\\}\", response)\n",
      "    if not regex_result or len(regex_result) != 1:\n",
      "        return None\n",
      "    number_str = regex_result[0]\n",
      "    # Except clause shouldn't trigger because the regex should only find ints.\n",
      "    try:\n",
      "        return int(number_str)\n",
      "    except ValueError:\n",
      "        return None\n",
      "\n",
      "\n",
      "def compute_letter_count_reward(completion: str, target_count: int) -> float:\n",
      "    \"\"\"Computes the rewards for counting the letters in a string.\n",
      "\n",
      "    The last group of consecutive digits in the completion is assumed to be the letter\n",
      "    count. We're also assuming it's counting the correct letter. The reward is the\n",
      "    negative of the absolute difference between the count and the target count, plus 0.1\n",
      "    if the answer was properly formatted.\n",
      "\n",
      "    For example, for the string \"There are 2 'r's in strawberry\", and the target count\n",
      "    is 3, the reward is -1.\n",
      "\n",
      "    Args:\n",
      "        completion: The completion string from the LLM.\n",
      "        target_count: The target count of letters.\n",
      "\n",
      "    Returns:\n",
      "        The reward value, calculated as the negative of the absolute difference between\n",
      "        the count and the target count. The count is assumed to be the last group of\n",
      "        consecutive digits in the completion string.\n",
      "    \"\"\"\n",
      "    count = _extract_prediction(completion)\n",
      "    formatting_reward = 0.1 if count is not None else 0\n",
      "    if count is None:\n",
      "        count = 0\n",
      "    return -abs(count - target_count) + formatting_reward\n",
      "\n",
      "\n",
      "@register(\"count_letters\", RegistryType.REWARD_FUNCTION)\n",
      "def _count_letters(\n",
      "    completions: list[list[dict[str, Any]]],\n",
      "    letter_count: list[int],\n",
      "    **kwargs: dict[str, Any],\n",
      ") -> list[float]:\n",
      "    \"\"\"Custom reward function for counting letters in a string.\n",
      "\n",
      "    For more details on custom reward functions used in trl's GRPOTrainer, see:\n",
      "    https://huggingface.co/docs/trl/main/en/grpo_trainer#using-a-custom-reward-function.\n",
      "\n",
      "    Args:\n",
      "        completions: The list of completions from the LLM.\n",
      "        letter_count: The list of target count of letters.\n",
      "        kwargs: Unused.\n",
      "\n",
      "    Returns:\n",
      "        The reward values for each completion, calculated as the negative of the\n",
      "        absolute difference between the count and the target count. The count is assumed\n",
      "        to be the last group of consecutive digits in the completion string.\n",
      "    \"\"\"\n",
      "    completions_strs = [c[0][\"content\"] for c in completions]\n",
      "    return [\n",
      "        compute_letter_count_reward(c, t)\n",
      "        for c, t in zip(completions_strs, letter_count)\n",
      "    ]\n"
     ]
    }
   ],
   "source": [
    "!cat ../src/oumi/datasets/grpo/rewards/count_letters_rewards.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-10 09:38:28 multiproc_worker_utils.py:141] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10493)\u001b[0;0m INFO 04-10 09:38:28 multiproc_worker_utils.py:253] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10494)\u001b[0;0m INFO 04-10 09:38:28 multiproc_worker_utils.py:253] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=10492)\u001b[0;0m INFO 04-10 09:38:28 multiproc_worker_utils.py:253] Worker exiting\n"
     ]
    }
   ],
   "source": [
    "# Clean up to free-up GPU memory used for evaluation above\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Delete the evaluator and collect garbage.\"\"\"\n",
    "    global evaluator\n",
    "    if evaluator:  # type: ignore\n",
    "        del evaluator\n",
    "        evaluator = None\n",
    "    for _ in range(3):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó**NOTICE:** Set `training.enable_wandb` to True if you want to log your training run to Weights and Biases. In addition, you must also log into WandB, ex. by running `wandb login`.\n",
    "\n",
    "‚ùó**NOTICE:** The following training config takes ~1.5 hours to run on 4 A100s, as of trl version 0.15.2. You can decrease `max_steps` below for training to run faster. Alternatively, since 500 steps is not enough to see meaningful improvement on this task, you can also increase `max_steps`. Another option is replacing it with `num_train_epochs` to set your desired number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing letter_counting_tutorial/grpo_train.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/grpo_train.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "  model_max_length: 8192\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  attn_implementation: \"sdpa\"\n",
    "\n",
    "data:\n",
    "  train:\n",
    "    datasets:\n",
    "      - dataset_name: \"oumi-ai/oumi-letter-count\"\n",
    "        split: \"train\"\n",
    "\n",
    "training:\n",
    "  trainer_type: \"TRL_GRPO\"\n",
    "  save_steps: 500\n",
    "  max_steps: 500\n",
    "  per_device_train_batch_size: 2\n",
    "  gradient_accumulation_steps: 1\n",
    "  learning_rate: 5e-5\n",
    "  lr_scheduler_type: \"cosine\"\n",
    "  warmup_steps: 20\n",
    "\n",
    "  reward_functions: [\"count_letters\"]\n",
    "\n",
    "  ddp_find_unused_parameters: False\n",
    "  optimizer: \"adafactor\"\n",
    "  compile: True\n",
    "\n",
    "  grpo:\n",
    "    num_generations: 4\n",
    "\n",
    "  dataloader_num_workers: \"auto\"\n",
    "  dataloader_prefetch_factor: 32\n",
    "\n",
    "  logging_steps: 10\n",
    "  output_dir: \"letter_counting_tutorial/llama_3b_grpo\"\n",
    "  # Set this to True if you want to log to Weights and Biases.\n",
    "  enable_wandb: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-10 09:38:32,770][oumi][rank0][pid:10890][MainThread][INFO]][distributed_run.py:276] Running the command: ['torchrun', '--nnodes=1', '--node-rank=0', '--nproc-per-node=4', '--master-addr=127.0.0.1', '--master-port=8007', '-m', 'oumi', 'train', '-c', 'letter_counting_tutorial/grpo_train.yaml']\n",
      "\n",
      "\u001b[32m   ____  _    _ __  __ _____\u001b[0m\n",
      "\u001b[32m  / __ \\| |  | |  \\/  |_   _|\u001b[0m\n",
      "\u001b[32m | |  | | |  | | \\  / | | |\u001b[0m\n",
      "\u001b[32m | |  | | |  | | |\\/| | | |\u001b[0m\n",
      "\u001b[32m | |__| | |__| | |  | |_| |_\u001b[0m\n",
      "\u001b[32m  \\____/ \\____/|_|  |_|_____|\u001b[0m\n",
      "\u001b[2K\u001b[32m‚†¶\u001b[0m \u001b[32mLoading configuration...\u001b[0mconfiguration...\u001b[0m\u001b[32m‚†ã\u001b[0m \u001b[32mLoading configuration...\u001b[0m\u001b[32m‚†ã\u001b[0m \u001b[32mLoading configuration...\u001b[0m\u001b[32m‚†ã\u001b[0m \u001b[32mLoading configuration...\u001b[0m\n",
      "\u001b[2K\u001b[32m‚†¶\u001b[0m \u001b[32mLoading configuration...\u001b[0m\n",
      "\u001b[2K\u001b[32m‚†¶\u001b[0m \u001b[32mLoading configuration...\u001b[0m\n",
      "\u001b[2K\u001b[32m‚†¶\u001b[0m \u001b[32mLoading configuration...\u001b[0m\n",
      "\u001b[1A\u001b[2KIgnored model.model_max_length=8192 parameter for trainer TrainerType.TRL_GRPO.\n",
      "[2025-04-10 09:38:39,338][oumi][rank0][pid:10894][MainThread][WARNING]][training_config.py:114] Ignored model.model_max_length=8192 parameter for trainer TrainerType.TRL_GRPO.\n",
      "Ignored model.model_max_length=8192 parameter for trainer TrainerType.TRL_GRPO.\n",
      "Ignored model.model_max_length=8192 parameter for trainer TrainerType.TRL_GRPO.\n",
      "[2025-04-10 09:38:40,791][oumi][rank0][pid:10894][MainThread][INFO]][distributed.py:561] Setting random seed to 42 on rank 0.\n",
      "[2025-04-10 09:38:44,182][oumi][rank0][pid:10894][MainThread][INFO]][distributed.py:286] Initialized distributed (True): DeviceRankInfo(world_size=4, rank=0, local_world_size=4, local_rank=0)\n",
      "[2025-04-10 09:38:44,182][oumi][rank0][pid:10894][MainThread][INFO]][train.py:112] Creating training.output_dir: letter_counting_tutorial/llama_3b_grpo...\n",
      "[2025-04-10 09:38:44,182][oumi][rank0][pid:10894][MainThread][INFO]][train.py:114] Created training.output_dir absolute path: /home/gcpuser/sky_workdir/notebooks/letter_counting_tutorial/llama_3b_grpo\n",
      "[2025-04-10 09:38:44,182][oumi][rank0][pid:10894][MainThread][INFO]][train.py:112] Creating training.telemetry_dir: letter_counting_tutorial/llama_3b_grpo/telemetry...\n",
      "[2025-04-10 09:38:44,182][oumi][rank0][pid:10894][MainThread][INFO]][train.py:114] Created training.telemetry_dir absolute path: /home/gcpuser/sky_workdir/notebooks/letter_counting_tutorial/llama_3b_grpo/telemetry\n",
      "[2025-04-10 09:38:44,183][oumi][rank0][pid:10894][MainThread][INFO]][torch_utils.py:81] Torch version: 2.5.1+cu124. NumPy version: 1.26.4\n",
      "[2025-04-10 09:38:44,183][oumi][rank0][pid:10894][MainThread][INFO]][torch_utils.py:89] CUDA version: 12.4 \n",
      "[2025-04-10 09:38:44,184][oumi][rank0][pid:10894][MainThread][INFO]][torch_utils.py:92] CuDNN version: 90.1.0\n",
      "[2025-04-10 09:38:44,743][oumi][rank0][pid:10894][MainThread][INFO]][torch_utils.py:125] CPU cores: 48 CUDA devices: 4\n",
      "device(0)='NVIDIA A100-SXM4-40GB' Capability: (8, 0) Memory: [Total: 39.39GiB Free: 34.02GiB Allocated: 0.0GiB Cached: 0.0GiB]\n",
      "device(1)='NVIDIA A100-SXM4-40GB' Capability: (8, 0) Memory: [Total: 39.39GiB Free: 36.85GiB Allocated: 0.0GiB Cached: 0.0GiB]\n",
      "device(2)='NVIDIA A100-SXM4-40GB' Capability: (8, 0) Memory: [Total: 39.39GiB Free: 37.41GiB Allocated: 0.0GiB Cached: 0.0GiB]\n",
      "device(3)='NVIDIA A100-SXM4-40GB' Capability: (8, 0) Memory: [Total: 39.39GiB Free: 37.27GiB Allocated: 0.0GiB Cached: 0.0GiB]\n",
      "[2025-04-10 09:38:44,746][oumi][rank0][pid:10894][MainThread][INFO]][train.py:149] Oumi version: 0.1.12.dev7+g2b62dbb\n",
      "[2025-04-10 09:38:44,750][oumi][rank0][pid:10894][MainThread][INFO]][train.py:151] Git revision hash: None\n",
      "[2025-04-10 09:38:44,752][oumi][rank0][pid:10894][MainThread][INFO]][train.py:152] Git tag: None\n",
      "[2025-04-10 09:38:44,753][oumi][rank0][pid:10894][MainThread][INFO]][train.py:160] Resolved 'training.dataloader_num_workers=auto' to 'training.dataloader_num_workers=8'\n",
      "[2025-04-10 09:38:44,754][oumi][rank0][pid:10894][MainThread][INFO]][train.py:236] TrainingConfig:\n",
      "TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='oumi-ai/oumi-letter-count',\n",
      "                                                                                dataset_path=None,\n",
      "                                                                                subset=None,\n",
      "                                                                                split='train',\n",
      "                                                                                dataset_kwargs={},\n",
      "                                                                                sample_count=None,\n",
      "                                                                                mixture_proportion=None,\n",
      "                                                                                shuffle=False,\n",
      "                                                                                seed=None,\n",
      "                                                                                shuffle_buffer_size=1000,\n",
      "                                                                                trust_remote_code=False,\n",
      "                                                                                transform_num_workers=None)],\n",
      "                                                        collator_name=None,\n",
      "                                                        pack=False,\n",
      "                                                        stream=False,\n",
      "                                                        target_col=None,\n",
      "                                                        mixture_strategy='first_exhausted',\n",
      "                                                        seed=None,\n",
      "                                                        use_async_dataset=False,\n",
      "                                                        use_torchdata=None),\n",
      "                               test=DatasetSplitParams(datasets=[],\n",
      "                                                       collator_name=None,\n",
      "                                                       pack=False,\n",
      "                                                       stream=False,\n",
      "                                                       target_col=None,\n",
      "                                                       mixture_strategy='first_exhausted',\n",
      "                                                       seed=None,\n",
      "                                                       use_async_dataset=False,\n",
      "                                                       use_torchdata=None),\n",
      "                               validation=DatasetSplitParams(datasets=[],\n",
      "                                                             collator_name=None,\n",
      "                                                             pack=False,\n",
      "                                                             stream=False,\n",
      "                                                             target_col=None,\n",
      "                                                             mixture_strategy='first_exhausted',\n",
      "                                                             seed=None,\n",
      "                                                             use_async_dataset=False,\n",
      "                                                             use_torchdata=None)),\n",
      "               model=ModelParams(model_name='meta-llama/Llama-3.2-3B-Instruct',\n",
      "                                 adapter_model=None,\n",
      "                                 tokenizer_name=None,\n",
      "                                 tokenizer_pad_token=None,\n",
      "                                 tokenizer_kwargs={},\n",
      "                                 model_max_length=8192,\n",
      "                                 load_pretrained_weights=True,\n",
      "                                 trust_remote_code=False,\n",
      "                                 torch_dtype_str='bfloat16',\n",
      "                                 compile=False,\n",
      "                                 chat_template=None,\n",
      "                                 attn_implementation='sdpa',\n",
      "                                 device_map='auto',\n",
      "                                 model_kwargs={},\n",
      "                                 enable_liger_kernel=False,\n",
      "                                 shard_for_eval=False,\n",
      "                                 freeze_layers=[]),\n",
      "               training=TrainingParams(use_peft=False,\n",
      "                                       trainer_type=<TrainerType.TRL_GRPO: 'trl_grpo'>,\n",
      "                                       enable_gradient_checkpointing=False,\n",
      "                                       gradient_checkpointing_kwargs={},\n",
      "                                       output_dir='letter_counting_tutorial/llama_3b_grpo',\n",
      "                                       per_device_train_batch_size=2,\n",
      "                                       per_device_eval_batch_size=8,\n",
      "                                       gradient_accumulation_steps=1,\n",
      "                                       max_steps=2,\n",
      "                                       num_train_epochs=3,\n",
      "                                       save_epoch=False,\n",
      "                                       save_steps=500,\n",
      "                                       save_final_model=True,\n",
      "                                       seed=42,\n",
      "                                       data_seed=42,\n",
      "                                       use_deterministic=False,\n",
      "                                       full_determinism=False,\n",
      "                                       run_name=None,\n",
      "                                       metrics_function=None,\n",
      "                                       reward_functions=['count_letters'],\n",
      "                                       grpo=GrpoParams(model_init_kwargs={},\n",
      "                                                       max_prompt_length=None,\n",
      "                                                       max_completion_length=None,\n",
      "                                                       num_generations=4,\n",
      "                                                       temperature=0.9,\n",
      "                                                       remove_unused_columns=False,\n",
      "                                                       use_vllm=False,\n",
      "                                                       vllm_device=None,\n",
      "                                                       vllm_gpu_memory_utilization=0.9,\n",
      "                                                       vllm_dtype=None,\n",
      "                                                       vllm_max_model_len=None),\n",
      "                                       log_level='info',\n",
      "                                       dep_log_level='warning',\n",
      "                                       enable_wandb=False,\n",
      "                                       enable_mlflow=False,\n",
      "                                       enable_tensorboard=True,\n",
      "                                       logging_strategy='steps',\n",
      "                                       logging_dir=None,\n",
      "                                       logging_steps=10,\n",
      "                                       logging_first_step=False,\n",
      "                                       eval_strategy='no',\n",
      "                                       eval_steps=500,\n",
      "                                       learning_rate=5e-05,\n",
      "                                       lr_scheduler_type='cosine',\n",
      "                                       lr_scheduler_kwargs={},\n",
      "                                       warmup_ratio=None,\n",
      "                                       warmup_steps=20,\n",
      "                                       optimizer='adafactor',\n",
      "                                       weight_decay=0.0,\n",
      "                                       adam_beta1=0.9,\n",
      "                                       adam_beta2=0.999,\n",
      "                                       adam_epsilon=1e-08,\n",
      "                                       sgd_momentum=0.0,\n",
      "                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,\n",
      "                                       compile=True,\n",
      "                                       include_performance_metrics=False,\n",
      "                                       include_alternative_mfu_metrics=False,\n",
      "                                       log_model_summary=False,\n",
      "                                       resume_from_checkpoint=None,\n",
      "                                       try_resume_from_last_checkpoint=False,\n",
      "                                       dataloader_num_workers=8,\n",
      "                                       dataloader_persistent_workers=False,\n",
      "                                       dataloader_prefetch_factor=32,\n",
      "                                       dataloader_main_process_only=None,\n",
      "                                       ddp_find_unused_parameters=False,\n",
      "                                       max_grad_norm=1.0,\n",
      "                                       trainer_kwargs={},\n",
      "                                       profiler=ProfilerParams(save_dir=None,\n",
      "                                                               enable_cpu_profiling=False,\n",
      "                                                               enable_cuda_profiling=False,\n",
      "                                                               record_shapes=False,\n",
      "                                                               profile_memory=False,\n",
      "                                                               with_stack=False,\n",
      "                                                               with_flops=False,\n",
      "                                                               with_modules=False,\n",
      "                                                               row_limit=50,\n",
      "                                                               schedule=ProfilerScheduleParams(enable_schedule=False,\n",
      "                                                                                               wait=0,\n",
      "                                                                                               warmup=1,\n",
      "                                                                                               active=3,\n",
      "                                                                                               repeat=1,\n",
      "                                                                                               skip_first=1)),\n",
      "                                       telemetry=TelemetryParams(telemetry_dir='telemetry',\n",
      "                                                                 collect_telemetry_for_all_ranks=False,\n",
      "                                                                 track_gpu_temperature=False),\n",
      "                                       empty_device_cache_steps=None,\n",
      "                                       nccl_default_timeout_minutes=None),\n",
      "               peft=PeftParams(lora_r=8,\n",
      "                               lora_alpha=8,\n",
      "                               lora_dropout=0.0,\n",
      "                               lora_target_modules=None,\n",
      "                               lora_modules_to_save=None,\n",
      "                               lora_bias='none',\n",
      "                               lora_init_weights=<LoraWeightInitialization.DEFAULT: 'default'>,\n",
      "                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
      "                               q_lora=False,\n",
      "                               q_lora_bits=4,\n",
      "                               bnb_4bit_quant_type='fp4',\n",
      "                               use_bnb_nested_quant=False,\n",
      "                               bnb_4bit_quant_storage='uint8',\n",
      "                               bnb_4bit_compute_dtype='float32',\n",
      "                               peft_save_mode=<PeftSaveMode.ADAPTER_ONLY: 'adapter_only'>),\n",
      "               fsdp=FSDPParams(enable_fsdp=False,\n",
      "                               sharding_strategy=<ShardingStrategy.FULL_SHARD: 'FULL_SHARD'>,\n",
      "                               cpu_offload=False,\n",
      "                               mixed_precision=None,\n",
      "                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,\n",
      "                               forward_prefetch=False,\n",
      "                               use_orig_params=None,\n",
      "                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,\n",
      "                               auto_wrap_policy=<AutoWrapPolicy.NO_WRAP: 'NO_WRAP'>,\n",
      "                               min_num_params=100000,\n",
      "                               transformer_layer_cls=None,\n",
      "                               sync_module_states=True))\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s][2025-04-10 09:38:45,562][oumi][rank0][pid:10894][MainThread][WARNING]][models.py:439] Undefined pad token. Setting it to `<|finetune_right_pad_id|>`.\n",
      "[2025-04-10 09:38:45,563][oumi][rank0][pid:10894][MainThread][INFO]][models.py:482] Using the model's built-in chat template for model 'meta-llama/Llama-3.2-3B-Instruct'.\n",
      "[2025-04-10 09:38:45,563][oumi][rank0][pid:10894][MainThread][INFO]][models.py:203] Building model for distributed training (world_size: 4)...\n",
      "[2025-04-10 09:38:45,563][oumi][rank0][pid:10894][MainThread][INFO]][models.py:208] Building model using device_map: cuda:0 (DeviceRankInfo(world_size=4, rank=0, local_world_size=4, local_rank=0))...\n",
      "[2025-04-10 09:38:45,563][oumi][rank0][pid:10894][MainThread][INFO]][models.py:276] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.00s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.11s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.23s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.21s/it]\n",
      "[2025-04-10 09:38:47,784][oumi][rank0][pid:10894][MainThread][INFO]][torch_utils.py:289] \n",
      "Model Parameters Summary:\n",
      "üî¢ Total     parameters: 3,212,749,824\n",
      "üîó Embedding parameters: 394,002,432\n",
      "üéØ Trainable parameters: 3,212,749,824\n",
      "üîí Frozen    parameters: 0 (0.00%)\n",
      "\n",
      "[2025-04-10 09:38:47,784][oumi][rank0][pid:10894][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: LetterCountGrpoDataset)... dataset_name: 'oumi-ai/oumi-letter-count'\n",
      "[2025-04-10 09:38:50,326][oumi][rank0][pid:10894][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: train\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 22894322\n",
      "\tDownload size: 5697295\n",
      "\tSize: 28591617 bytes\n",
      "\tRows: 100000\n",
      "\tColumns: ['conversation_id', 'messages', 'metadata']\n",
      "Generating train split: 8 examples [00:00, 2656.51 examples/s]\n",
      "[2025-04-10 09:38:50,827][oumi][rank0][pid:10894][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (100000, 3). Columns:\n",
      "conversation_id    object\n",
      "messages           object\n",
      "metadata           object\n",
      "dtype: object\n",
      "[2025-04-10 09:38:50,837][oumi][rank0][pid:10894][MainThread][INFO]][base_map_dataset.py:312] LetterCountGrpoDataset: features=dict_keys(['prompt', 'letter_count'])\n",
      "Generating train split: 100000 examples [00:08, 11203.44 examples/s]\n",
      "[2025-04-10 09:39:12,603][oumi][rank0][pid:10894][MainThread][INFO]][base_map_dataset.py:376] Finished transforming dataset (LetterCountGrpoDataset)! Speed: 4594.28 examples/sec. Examples: 100000. Duration: 21.8 sec. Transform workers: 1.\n",
      "[2025-04-10 09:39:13,715][oumi][rank0][pid:10894][MainThread][INFO]][torch_profiler_utils.py:164] PROF: Torch Profiler disabled!\n",
      "[2025-04-10 09:39:13,822][oumi][rank0][pid:10894][MainThread][INFO]][training.py:62] GRPOConfig(output_dir='letter_counting_tutorial/llama_3b_grpo',\n",
      "           overwrite_output_dir=False,\n",
      "           do_train=False,\n",
      "           do_eval=False,\n",
      "           do_predict=False,\n",
      "           eval_strategy=<IntervalStrategy.NO: 'no'>,\n",
      "           prediction_loss_only=False,\n",
      "           per_device_train_batch_size=2,\n",
      "           per_device_eval_batch_size=8,\n",
      "           per_gpu_train_batch_size=None,\n",
      "           per_gpu_eval_batch_size=None,\n",
      "           gradient_accumulation_steps=1,\n",
      "           eval_accumulation_steps=None,\n",
      "           eval_delay=0,\n",
      "           torch_empty_cache_steps=None,\n",
      "           learning_rate=5e-05,\n",
      "           weight_decay=0.0,\n",
      "           adam_beta1=0.9,\n",
      "           adam_beta2=0.999,\n",
      "           adam_epsilon=1e-08,\n",
      "           max_grad_norm=1.0,\n",
      "           num_train_epochs=3,\n",
      "           max_steps=2,\n",
      "           lr_scheduler_type=<SchedulerType.COSINE: 'cosine'>,\n",
      "           lr_scheduler_kwargs={},\n",
      "           warmup_ratio=0.0,\n",
      "           warmup_steps=20,\n",
      "           log_level='warning',\n",
      "           log_level_replica='warning',\n",
      "           log_on_each_node=True,\n",
      "           logging_dir='letter_counting_tutorial/llama_3b_grpo/runs/Apr10_09-39-13_wizeng-dev-9d78-head-1i4810vq-compute',\n",
      "           logging_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
      "           logging_first_step=False,\n",
      "           logging_steps=10,\n",
      "           logging_nan_inf_filter=True,\n",
      "           save_strategy=<SaveStrategy.STEPS: 'steps'>,\n",
      "           save_steps=500,\n",
      "           save_total_limit=None,\n",
      "           save_safetensors=True,\n",
      "           save_on_each_node=False,\n",
      "           save_only_model=False,\n",
      "           restore_callback_states_from_checkpoint=False,\n",
      "           no_cuda=False,\n",
      "           use_cpu=False,\n",
      "           use_mps_device=False,\n",
      "           seed=42,\n",
      "           data_seed=42,\n",
      "           jit_mode_eval=False,\n",
      "           use_ipex=False,\n",
      "           bf16=False,\n",
      "           fp16=False,\n",
      "           fp16_opt_level='O1',\n",
      "           half_precision_backend='auto',\n",
      "           bf16_full_eval=False,\n",
      "           fp16_full_eval=False,\n",
      "           tf32=None,\n",
      "           local_rank=0,\n",
      "           ddp_backend=None,\n",
      "           tpu_num_cores=None,\n",
      "           tpu_metrics_debug=False,\n",
      "           debug=[],\n",
      "           dataloader_drop_last=False,\n",
      "           eval_steps=500,\n",
      "           dataloader_num_workers=8,\n",
      "           dataloader_prefetch_factor=32,\n",
      "           past_index=-1,\n",
      "           run_name='letter_counting_tutorial/llama_3b_grpo',\n",
      "           disable_tqdm=False,\n",
      "           remove_unused_columns=False,\n",
      "           label_names=None,\n",
      "           load_best_model_at_end=False,\n",
      "           metric_for_best_model=None,\n",
      "           greater_is_better=None,\n",
      "           ignore_data_skip=False,\n",
      "           fsdp=[],\n",
      "           fsdp_min_num_params=0,\n",
      "           fsdp_config={'min_num_params': 0,\n",
      "                        'xla': False,\n",
      "                        'xla_fsdp_grad_ckpt': False,\n",
      "                        'xla_fsdp_v2': False},\n",
      "           tp_size=0,\n",
      "           fsdp_transformer_layer_cls_to_wrap=None,\n",
      "           accelerator_config=AcceleratorConfig(split_batches=False,\n",
      "                                                dispatch_batches=None,\n",
      "                                                even_batches=True,\n",
      "                                                use_seedable_sampler=True,\n",
      "                                                non_blocking=False,\n",
      "                                                gradient_accumulation_kwargs=None,\n",
      "                                                use_configured_state=False),\n",
      "           deepspeed=None,\n",
      "           label_smoothing_factor=0.0,\n",
      "           optim=<OptimizerNames.ADAFACTOR: 'adafactor'>,\n",
      "           optim_args=None,\n",
      "           adafactor=False,\n",
      "           group_by_length=False,\n",
      "           length_column_name='length',\n",
      "           report_to=['tensorboard'],\n",
      "           ddp_find_unused_parameters=False,\n",
      "           ddp_bucket_cap_mb=None,\n",
      "           ddp_broadcast_buffers=None,\n",
      "           dataloader_pin_memory=True,\n",
      "           dataloader_persistent_workers=False,\n",
      "           skip_memory_metrics=True,\n",
      "           use_legacy_prediction_loop=False,\n",
      "           push_to_hub=False,\n",
      "           resume_from_checkpoint=None,\n",
      "           hub_model_id=None,\n",
      "           hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>,\n",
      "           hub_token=None,\n",
      "           hub_private_repo=None,\n",
      "           hub_always_push=False,\n",
      "           gradient_checkpointing=False,\n",
      "           gradient_checkpointing_kwargs={},\n",
      "           include_inputs_for_metrics=False,\n",
      "           include_for_metrics=[],\n",
      "           eval_do_concat_batches=True,\n",
      "           fp16_backend='auto',\n",
      "           push_to_hub_model_id=None,\n",
      "           push_to_hub_organization=None,\n",
      "           push_to_hub_token=None,\n",
      "           mp_parameters='',\n",
      "           auto_find_batch_size=False,\n",
      "           full_determinism=False,\n",
      "           torchdynamo=None,\n",
      "           ray_scope='last',\n",
      "           ddp_timeout=1800,\n",
      "           torch_compile=True,\n",
      "           torch_compile_backend='inductor',\n",
      "           torch_compile_mode=None,\n",
      "           include_tokens_per_second=False,\n",
      "           include_num_input_tokens_seen=False,\n",
      "           neftune_noise_alpha=None,\n",
      "           optim_target_modules=None,\n",
      "           batch_eval_metrics=False,\n",
      "           eval_on_start=False,\n",
      "           use_liger_kernel=False,\n",
      "           eval_use_gather_object=False,\n",
      "           average_tokens_across_devices=False,\n",
      "           model_init_kwargs=None,\n",
      "           max_prompt_length=512,\n",
      "           num_generations=4,\n",
      "           temperature=0.9,\n",
      "           max_completion_length=256,\n",
      "           ds3_gather_for_generation=True,\n",
      "           use_vllm=False,\n",
      "           vllm_device='auto',\n",
      "           vllm_gpu_memory_utilization=0.9,\n",
      "           vllm_dtype='auto',\n",
      "           vllm_max_model_len=None,\n",
      "           beta=0.04,\n",
      "           reward_weights=None,\n",
      "           sync_ref_model=False,\n",
      "           ref_model_mixup_alpha=0.9,\n",
      "           ref_model_sync_steps=64,\n",
      "           log_completions=False)\n",
      "[2025-04-10 09:39:14,790][oumi][rank0][pid:10894][MainThread][INFO]][device_utils.py:297] GPU Metrics Before Training: GPU runtime info: None.\n",
      "[2025-04-10 09:39:14,799][oumi][rank0][pid:10894][MainThread][INFO]][train.py:419] Training init time: 31.849s\n",
      "[2025-04-10 09:39:14,800][oumi][rank0][pid:10894][MainThread][INFO]][train.py:420] Starting training... (TrainerType.TRL_GRPO, transformers: 4.51.1)\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]INFO 04-10 09:39:29 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 04-10 09:39:29 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 04-10 09:39:29 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 04-10 09:39:30 __init__.py:207] Automatically detected platform cuda.\n",
      "[rank0]:W0410 09:39:53.974000 10894 \n",
      "site-packages/torch/_logging/_internal.py:1081] [1/0] Profiler function <class \n",
      "'torch.autograd.profiler.record_function'> will be ignored\n",
      "[rank2]:W0410 09:39:53.975000 10896 \n",
      "site-packages/torch/_logging/_internal.py:1081] [1/0] Profiler function <class \n",
      "'torch.autograd.profiler.record_function'> will be ignored\n",
      "[rank3]:W0410 09:39:53.975000 10897 \n",
      "site-packages/torch/_logging/_internal.py:1081] [1/0] Profiler function <class \n",
      "'torch.autograd.profiler.record_function'> will be ignored\n",
      "[rank1]:W0410 09:39:53.975000 10895 \n",
      "site-packages/torch/_logging/_internal.py:1081] [1/0] Profiler function <class \n",
      "'torch.autograd.profiler.record_function'> will be ignored\n",
      "{'train_runtime': 226.3313, 'train_samples_per_second': 0.071, 'train_steps_per_second': 0.009, 'train_loss': -0.5479733943939209, 'rewards/_count_letters': -1.0625, 'reward': -1.0625, 'reward_std': 0.9289332032203674, 'completion_length': 47.8125, 'kl': 0.0001125335693359375, 'epoch': 0.0}\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [03:46<00:00, 113.16s/it]\n",
      "[2025-04-10 09:43:01,562][oumi][rank0][pid:10894][MainThread][INFO]][train.py:427] Training is Complete.\n",
      "[2025-04-10 09:43:01,562][oumi][rank0][pid:10894][MainThread][INFO]][device_utils.py:297] GPU Metrics After Training: GPU runtime info: None.\n",
      "[2025-04-10 09:43:01,562][oumi][rank0][pid:10894][MainThread][INFO]][torch_utils.py:136] Peak GPU memory usage: 29.84 GB\n",
      "[2025-04-10 09:43:01,562][oumi][rank0][pid:10894][MainThread][INFO]][train.py:434] Saving final state...\n",
      "[2025-04-10 09:43:01,563][oumi][rank0][pid:10894][MainThread][INFO]][train.py:439] Saving final model...\n",
      "[2025-04-10 09:43:14,924][oumi][rank0][pid:10894][MainThread][INFO]][hf_trainer.py:116] Model has been saved at letter_counting_tutorial/llama_3b_grpo\n",
      "[2025-04-10 09:43:15,458][oumi][rank0][pid:10894][MainThread][INFO]][train.py:446] \n",
      "\n",
      "¬ª We're always looking for feedback. What's one thing we can improve? https://oumi.ai/feedback\n",
      "[2025-04-10 09:43:23,534][oumi][rank0][pid:10890][MainThread][INFO]][distributed_run.py:295] Successfully completed! (Rank: 0. Duration: 290.8 sec)\n"
     ]
    }
   ],
   "source": [
    "!oumi distributed torchrun -m oumi train -c $tutorial_dir/grpo_train.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our Trained Model\n",
    "\n",
    "Let's now evaluate our trained model to see if it improved on the letter counting task. Note that it may not improve much, since we trained it for a relatively short time.\n",
    "\n",
    "Below, we demonstrate an alternative method of running evaluation with the `oumi` CLI. We use the same Llama 3B evaluation config we used above, with the only change being pointing it at the model we just trained.\n",
    "\n",
    "First, we need to reset the notebook to clear variables from our previous vLLM run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m   ____  _    _ __  __ _____\u001b[0m\n",
      "\u001b[32m  / __ \\| |  | |  \\/  |_   _|\u001b[0m\n",
      "\u001b[32m | |  | | |  | | \\  / | | |\u001b[0m\n",
      "\u001b[32m | |  | | |  | | |\\/| | | |\u001b[0m\n",
      "\u001b[32m | |__| | |__| | |  | |_| |_\u001b[0m\n",
      "\u001b[32m  \\____/ \\____/|_|  |_|_____|\u001b[0m\n",
      "\u001b[2K\u001b[32m‚†¥\u001b[0m \u001b[32mLoading configuration...\u001b[0m0m\n",
      "\u001b[2K\u001b[32m‚†ã\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-04-10 09:47:15,521][oumi][rank0][pid:16694][MainThread][INFO]][models.py:482] Using the model's built-in chat template for model 'letter_counting_tutorial/llama_3b_grpo'.\n",
      "\u001b[2KINFO 04-10 09:47:15 __init__.py:207] Automatically detected platform cuda.\n",
      "\u001b[2KINFO 04-10 09:47:23 config.py:549] This model supports multiple tasks: {'score',\n",
      "'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "\u001b[2KINFO 04-10 09:47:23 config.py:1382] Defaulting to use mp for distributed \n",
      "inference\n",
      "\u001b[2KWARNING 04-10 09:47:23 arg_utils.py:1187] Chunked prefill is enabled by default \n",
      "for models with max_model_len > 32K. Currently, chunked prefill might not work \n",
      "with some features or models. If you encounter any issues, please disable \n",
      "chunked prefill by setting --enable-chunked-prefill=False.\n",
      "\u001b[2KINFO 04-10 09:47:23 config.py:1555] Chunked prefill is enabled with \n",
      "max_num_batched_tokens=2048.\n",
      "\u001b[2KWARNING 04-10 09:47:23 cuda.py:95] To see benefits of async output processing, \n",
      "enable CUDA graph. Since, enforce-eager is enabled, async output processor \n",
      "cannot be used\n",
      "\u001b[2KWARNING 04-10 09:47:23 config.py:685] Async output processing is not supported \n",
      "on the current platform type cuda.\n",
      "\u001b[2KINFO 04-10 09:47:23 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) \n",
      "with config: model='letter_counting_tutorial/llama_3b_grpo', \n",
      "speculative_config=None, tokenizer='letter_counting_tutorial/llama_3b_grpo', \n",
      "skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, \n",
      "override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, \n",
      "dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, \n",
      "tensor_parallel_size=4, pipeline_parallel_size=1, \n",
      "disable_custom_all_reduce=False, quantization=None, enforce_eager=True, \n",
      "kv_cache_dtype=auto,  device_config=cuda, \n",
      "decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), \n",
      "observability_config=ObservabilityConfig(otlp_traces_endpoint=None, \n",
      "collect_model_forward_time=False, collect_model_execute_time=False), seed=0, \n",
      "served_model_name=letter_counting_tutorial/llama_3b_grpo, num_scheduler_steps=1,\n",
      "multi_step_stream_outputs=True, enable_prefix_caching=True, \n",
      "chunked_prefill_enabled=True, use_async_output_proc=False, \n",
      "disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, \n",
      "pooler_config=None, \n",
      "compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_siz\n",
      "es\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "\u001b[2KINFO 04-10 09:47:25 cuda.py:229] Using Flash Attention backend.\n",
      "\u001b[2K\u001b[32m‚†ß\u001b[0m \u001b[32mRunning evaluation...\u001b[0mINFO 04-10 09:47:29 __init__.py:207] Automatically detected platform cuda.\n",
      "\u001b[2K\u001b[32m‚†á\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorkerProcess pid=16719)\u001b[0;0m INFO 04-10 09:47:29 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 04-10 09:47:29 __init__.py:207] Automatically detected platform cuda.\n",
      "\u001b[2K\u001b[32m‚†è\u001b[0m \u001b[32mRunning evaluation...\u001b[0mINFO 04-10 09:47:29 __init__.py:207] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16717)\u001b[0;0m INFO 04-10 09:47:29 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[2K\u001b[32m‚†ã\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorkerProcess pid=16718)\u001b[0;0m INFO 04-10 09:47:29 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[2K\u001b[32m‚†∏\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorkerProcess pid=16719)\u001b[0;0m INFO 04-10 09:47:31 cuda.py:229] Using Flash Attention backend.\n",
      "\u001b[2K\u001b[32m‚†º\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorkerProcess pid=16717)\u001b[0;0m INFO 04-10 09:47:31 cuda.py:229] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16718)\u001b[0;0m INFO 04-10 09:47:31 cuda.py:229] Using Flash Attention backend.\n",
      "\u001b[2K\u001b[32m‚†è\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorkerProcess pid=16718)\u001b[0;0m INFO 04-10 09:47:32 utils.py:916] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16719)\u001b[0;0m INFO 04-10 09:47:32 utils.py:916] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16717)\u001b[0;0m INFO 04-10 09:47:32 utils.py:916] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16718)\u001b[0;0m INFO 04-10 09:47:32 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16719)\u001b[0;0m INFO 04-10 09:47:32 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16717)\u001b[0;0m INFO 04-10 09:47:32 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[2KINFO 04-10 09:47:32 utils.py:916] Found nccl from library libnccl.so.2\n",
      "\u001b[2KINFO 04-10 09:47:32 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[2K\u001b[32m‚†ô\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorkerProcess pid=16719)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=16717)\u001b[0;0m INFO 04-10 09:47:33 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/gcpuser/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 04-10 09:47:33 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/gcpuser/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16718)\u001b[0;0m INFO 04-10 09:47:33 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/gcpuser/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[2KINFO 04-10 09:47:33 custom_all_reduce_utils.py:244] reading GPU P2P access cache\n",
      "from /home/gcpuser/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[2KINFO 04-10 09:47:33 shm_broadcast.py:258] vLLM message queue communication \n",
      "handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], \n",
      "buffer_handle=(3, 4194304, 6, 'psm_3013b587'), local_subscribe_port=47309, \n",
      "remote_subscribe_port=None)\n",
      "\u001b[2KINFO 04-10 09:47:33 model_runner.py:1110] Starting to load model \n",
      "letter_counting_tutorial/llama_3b_grpo...\n",
      "\u001b[32m‚†∏\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorkerProcess pid=16718)\u001b[0;0m INFO 04-10 09:47:33 model_runner.py:1110] Starting to load model letter_counting_tutorial/llama_3b_grpo...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16717)\u001b[0;0m INFO 04-10 09:47:33 model_runner.py:1110] Starting to load model letter_counting_tutorial/llama_3b_grpo...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16719)\u001b[0;0m INFO 04-10 09:47:33 model_runner.py:1110] Starting to load model letter_counting_tutorial/llama_3b_grpo...\n",
      "\u001b[2KLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[2KLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  \n",
      "1.75it/s]\n",
      "\u001b[2KLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  \n",
      "2.72it/s]\n",
      "\u001b[2KLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  \n",
      "2.51it/s]\n",
      "\u001b[2Km‚†¥\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\n",
      "\u001b[32m‚†¥\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorkerProcess pid=16719)\u001b[0;0m INFO 04-10 09:47:34 model_runner.py:1115] Loading model weights took 1.5341 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16717)\u001b[0;0m INFO 04-10 09:47:34 model_runner.py:1115] Loading model weights took 1.5341 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16718)\u001b[0;0m INFO 04-10 09:47:34 model_runner.py:1115] Loading model weights took 1.5341 GB\n",
      "\u001b[2KINFO 04-10 09:47:34 model_runner.py:1115] Loading model weights took 1.5341 GB\n",
      "\u001b[2K\u001b[32m‚†∏\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorkerProcess pid=16719)\u001b[0;0m INFO 04-10 09:47:39 worker.py:267] Memory profiling takes 4.30 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16719)\u001b[0;0m INFO 04-10 09:47:39 worker.py:267] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.90) = 35.45GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16719)\u001b[0;0m INFO 04-10 09:47:39 worker.py:267] model weights take 1.53GiB; non_torch_memory takes 1.82GiB; PyTorch activation peak memory takes 0.07GiB; the rest of the memory reserved for KV Cache is 32.03GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16718)\u001b[0;0m INFO 04-10 09:47:39 worker.py:267] Memory profiling takes 4.30 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16718)\u001b[0;0m INFO 04-10 09:47:39 worker.py:267] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.90) = 35.45GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16718)\u001b[0;0m INFO 04-10 09:47:39 worker.py:267] model weights take 1.53GiB; non_torch_memory takes 2.10GiB; PyTorch activation peak memory takes 0.07GiB; the rest of the memory reserved for KV Cache is 31.74GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16717)\u001b[0;0m INFO 04-10 09:47:39 worker.py:267] Memory profiling takes 4.31 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16717)\u001b[0;0m INFO 04-10 09:47:39 worker.py:267] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.90) = 35.45GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16717)\u001b[0;0m INFO 04-10 09:47:39 worker.py:267] model weights take 1.53GiB; non_torch_memory takes 2.10GiB; PyTorch activation peak memory takes 0.07GiB; the rest of the memory reserved for KV Cache is 31.74GiB.\n",
      "\u001b[2K\u001b[32m‚†á\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\n",
      "\n",
      "INFO 04-10 09:47:39 worker.py:267] model weights take 1.53GiB; non_torch_memory \n",
      "takes 2.20GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the \n",
      "memory reserved for KV Cache is 30.54GiB.\n",
      "\u001b[2KINFO 04-10 09:47:39 executor_base.py:111] # cuda blocks: 71484, # CPU blocks: \n",
      "9362\n",
      "\u001b[2KINFO 04-10 09:47:39 executor_base.py:116] Maximum concurrency for 131072 tokens \n",
      "per request: 8.73x\n",
      "\u001b[2KINFO 04-10 09:47:44 llm_engine.py:436] init engine (profile, create kv cache, \n",
      "warmup model) took 9.57 seconds\n",
      "\u001b[2K\u001b[32m‚†π\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-04-10 09:47:46,962][oumi][rank0][pid:16694][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: LetterCountGrpoDataset)... dataset_name: 'oumi-ai/oumi-letter-count'\n",
      "\u001b[2K\u001b[32m‚†ã\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-04-10 09:47:49,180][oumi][rank0][pid:16694][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: test\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 22894322\n",
      "\tDownload size: 5697295\n",
      "\tSize: 28591617 bytes\n",
      "\tRows: 20000\n",
      "\tColumns: ['conversation_id', 'messages', 'metadata']\n",
      "\u001b[2K\u001b[32m‚†è\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-04-10 09:47:49,820][oumi][rank0][pid:16694][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (20000, 3). Columns:\n",
      "conversation_id    object\n",
      "messages           object\n",
      "metadata           object\n",
      "dtype: object\n",
      "\u001b[2KINFO 04-10 09:47:49 chat_utils.py:332] Detected the chat template content format\n",
      "to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "\u001b[2KProcessed prompts:   \u001b[1;36m0\u001b[0m%| | \u001b[1;36m0\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<?, ?it/s, est. speed input: \u001b[1;36m0.00\u001b[0m toks/s,\n",
      "\u001b[2KProcessed prompts:   \u001b[1;36m1\u001b[0m%| | \u001b[1;36m1\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m00:43\u001b[0m,  \u001b[1;36m2.\u001b[0m26it/s, est. speed input: \u001b[1;36m205\u001b[0m.\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m15\u001b[0m%|\u001b[1;36m1\u001b[0m| \u001b[1;36m15\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m00:02\u001b[0m, \u001b[1;36m35.\u001b[0m16it/s, est. speed input: \u001b[1;36m245\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m23\u001b[0m%|\u001b[1;36m2\u001b[0m| \u001b[1;36m23\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m00:02\u001b[0m, \u001b[1;36m29.\u001b[0m31it/s, est. speed input: \u001b[1;36m235\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m41\u001b[0m%|\u001b[1;36m4\u001b[0m| \u001b[1;36m41\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m00:01\u001b[0m, \u001b[1;36m56.\u001b[0m64it/s, est. speed input: \u001b[1;36m375\u001b[0m\n",
      "\u001b[2KProcessed prompts:  \u001b[1;36m86\u001b[0m%|\u001b[1;36m8\u001b[0m| \u001b[1;36m86\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:01\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m133.\u001b[0m82it/s, est. speed input: \u001b[1;36m70\u001b[0m\n",
      "\u001b[2KProcessed prompts: \u001b[1;36m100\u001b[0m%|#| \u001b[1;36m100\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:03\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m28.\u001b[0m84it/s, est. speed input: \u001b[1;36m26\u001b[0m\n",
      "\u001b[2Km‚†∏\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\n",
      "\u001b[32m‚†∏\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-04-10 09:47:53,387][oumi][rank0][pid:16694][MainThread][INFO]][count_letters_task.py:53] Finished inference on 100 conversations!\n",
      "[2025-04-10 09:47:53,387][oumi][rank0][pid:16694][MainThread][INFO]][count_letters_task.py:55] Sample conversation: conversation_id='oumi_letter_count_0' messages=[USER: Look through 'perivaginal' and count the 'n's., SYSTEM: Your final answer should be written as digits and formatted as \"\\boxed{your_answer}\". For example, if the answer is 42, make sure to output \"\\boxed{42}\"., ASSISTANT: There are 2 'n's in 'perivaginal'. \n",
      "\n",
      "\\boxed{2}] metadata={'letter': 'n', 'letter_count_integer': 1, 'letter_count_string': 'one', 'unformatted_prompt': 'Look through {word} and count the {letter}s.', 'word': 'perivaginal'}\n",
      "\u001b[2KINFO 04-10 09:47:53 multiproc_worker_utils.py:141] Terminating local vLLM worker\n",
      "processes\n",
      "\u001b[32m‚†ß\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorkerProcess pid=16717)\u001b[0;0m INFO 04-10 09:47:53 multiproc_worker_utils.py:253] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16718)\u001b[0;0m INFO 04-10 09:47:53 multiproc_worker_utils.py:253] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16719)\u001b[0;0m INFO 04-10 09:47:53 multiproc_worker_utils.py:253] Worker exiting\n",
      "\u001b[2K\u001b[32m‚†ß\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\n",
      "\u001b[1A\u001b[2K\u001b[1;35m                         Evaluation Results                         \u001b[0m\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ\u001b[1m \u001b[0m\u001b[1mBenchmark    \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mMetric                     \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mScore \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mStd Error\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
      "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mAccuracy                   \u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m28.00%\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mProperly Extracted Accuracy\u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m31.46%\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mNum Samples                \u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m100.00\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mNum Correct Answers        \u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m28.00 \u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mNum Incorrect Answers      \u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m61.00 \u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mNum Invalid Answers        \u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m11.00 \u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "[rank0]:[W410 09:47:58.432638348 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!oumi evaluate -c letter_counting_tutorial/llama_3b_eval.yaml \\\n",
    "    --model.model_name \"letter_counting_tutorial/llama_3b_grpo\" \\\n",
    "    --tasks.0.num_samples $NUM_SAMPLES \\\n",
    "    --output_dir \"letter_counting_tutorial/evaluation/llama_3_grpo\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
