{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"align-center\">\n",
    "<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n",
    "\n",
    "[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n",
    "[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n",
    "[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - Train a Letter Counting Model using GRPO.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>\n",
    "\n",
    "üëã Welcome to Open Universal Machine Intelligence (Oumi)!\n",
    "\n",
    "üöÄ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](https://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n",
    "\n",
    "ü§ù Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n",
    "\n",
    "‚≠ê If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Letter Counting Model using GRPO\n",
    "\n",
    "Welcome to Oumi! In this tutorial notebook, we're going to fine-tune an LLM using Group Relative Policy Optimization (GRPO), a reinforcement learning algorithm. But first, a little (recent) history lesson --\n",
    "\n",
    "In June 2024, a user discovered that ChatGPT had a little problem -- it couldn't correctly answer a simple question, [\"How Many R‚Äôs Are in the Word Strawberry?\"](https://community.openai.com/t/incorrect-count-of-r-characters-in-the-word-strawberry/829618/2)\n",
    "\n",
    "Because of the way LLMs tokenize input strings, counting letters can be pretty tough for them! Fortunately, you (and Oumi!) are here to help.\n",
    "\n",
    "Below, we show you how to employ a custom evaluation function to evaluate popular models on the task of counting letters in words. Then, we will align Llama 3.2 3B to improve its performance on this task.\n",
    "\n",
    "This notebook includes cell outputs, but some irrelevant outputs (ex. install lines, warnings) are modified/removed for readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Machine Requirements\n",
    "\n",
    "This notebook runs both model evaluation and GRPO training, which require 8GB and 40GB VRAM, respectively.\n",
    "\n",
    "‚ùó**NOTICE:** If you're running this notebook on Colab using a T4 GPU, it's not possible to run training due to memory requirements. To run evaluation, some adjustments need to be made as vLLM doesn't support T4 GPUs. This will be explained in the evaluation section.\n",
    "\n",
    "If your local machine cannot run this notebook, you can instead run this notebook on a cloud platform. The following demonstrates how to open a VSCode instance backed by a GCP node with 4 A100 GPUs, from which the notebook can be run. It is possible to run this notebook on just 1 GPU, but you will need make some adjustments to training parameters, which will be explained in the training section.\n",
    "\n",
    "```bash\n",
    "# Run on your local machine\n",
    "gcloud auth application-default login  # Authenticate with GCP\n",
    "make gcpcode ARGS=\"--resources.accelerators A100:4\"\n",
    "```\n",
    "\n",
    "### Oumi Installation\n",
    "\n",
    "First, let's install Oumi and vLLM (part of the `gpu` optional dependencies). You can find more detailed instructions about Oumi installation [here](https://oumi.ai/docs/en/latest/get_started/installation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install oumi[gpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote API Access\n",
    "\n",
    "As part of this notebook, you can evaluate frontier models from Open AI, Google, Anthropic, and Meta on the letter counting task. If you want to evaluate any of these models, set the corresponding fields below. The code is commented out by default to avoid any accidental overwriting of existing variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"  # Set your OpenAI API key here.\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"\"  # Set your Gemini API key here.\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"\"  # Set your Anthropic API key here.\n",
    "\n",
    "# Set your GCP project id and region, if you want to query Llama 3.1 405B in Vertex.\n",
    "REGION = \"\"  # Set your GCP region here.\n",
    "PROJECT_ID = \"\"  # Set your GCP project id here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Directory\n",
    "\n",
    "Finally, we'll set up a directory to use for this tutorial, and some environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"letter_counting_tutorial\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable warnings from HF.\n",
    "\n",
    "# This is needed for vLLM to use multiple GPUs in a notebook.\n",
    "# If you're not running in a notebook, you can ignore this.\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset we'll use for this notebook is `oumi-ai/oumi-letter-count`, which can be found on [HF Datasets](https://huggingface.co/datasets/oumi-ai/oumi-letter-count). Its prompts ask to count the letters in various English words, with metadata in each example containing the correct count. We use the `train` split for training and the `test` split for evaluation. We'll use an Oumi dataset class, `LetterCountGrpoDataset`, to load and preprocess the HF Dataset. The following code displays an example prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-27 22:34:28,696][oumi][rank0][pid:1235199][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: LetterCountGrpoDataset)... dataset_name: 'oumi-ai/oumi-letter-count'\n",
      "[2025-06-27 22:34:29,359][oumi][rank0][pid:1235199][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: validation\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 22894322\n",
      "\tDownload size: 5697295\n",
      "\tSize: 28591617 bytes\n",
      "\tRows: 10000\n",
      "\tColumns: ['conversation_id', 'messages', 'metadata']\n",
      "[2025-06-27 22:34:29,513][oumi][rank0][pid:1235199][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (10000, 3). Columns:\n",
      "conversation_id    object\n",
      "messages           object\n",
      "metadata           object\n",
      "dtype: object\n",
      "--------------------------------------------------------------------------------\n",
      "Sample:\n",
      "{'conversation_id': 'oumi_letter_count_0',\n",
      " 'messages': [{'content': 'Your final answer should be an integer written as '\n",
      "                          'digits and formatted as \"\\\\boxed{your_answer}\". For '\n",
      "                          'example, if the answer is 42, you should output '\n",
      "                          '\"\\\\boxed{42}\".',\n",
      "               'role': 'system'},\n",
      "              {'content': \"Could you determine the count of 'l's in \"\n",
      "                          \"'substantial'?\",\n",
      "               'role': 'user'}],\n",
      " 'metadata': {'letter': 'l',\n",
      "              'letter_count_integer': 1,\n",
      "              'letter_count_string': 'one',\n",
      "              'unformatted_prompt': 'Could you determine the count of '\n",
      "                                    '{letter}s in {word}?',\n",
      "              'word': 'substantial'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from oumi.datasets.grpo.letter_count import LetterCountGrpoDataset\n",
    "\n",
    "dataset = LetterCountGrpoDataset(\n",
    "    dataset=\"oumi-ai/oumi-letter-count-clean\", split=\"validation\"\n",
    ")\n",
    "print(\"-\" * 80)\n",
    "print(\"Sample:\")\n",
    "pprint(dataset.conversation(0).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "First, we'll evaluate how various models perform on the letter counting task. We'll evaluate frontier models by calling their respective remote API, and Llama 3.2 3B by running local inference on it using vLLM.\n",
    "\n",
    "We've already defined a custom evaluation function in Oumi which runs inference on the above dataset, extracts the answer from the model response, and calculates various metrics such as accuracy. This function is defined at `src/oumi/evaluation/registry/count_letters_task.py` ([GitHub link](https://github.com/oumi-ai/oumi/blob/main/src/oumi/evaluation/registry/count_letters_task.py)), and we print its contents below for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@register_evaluation_function(\"count_letters\")\n",
      "def count_letters(\n",
      "    task_params: EvaluationTaskParams,\n",
      "    inference_engine: BaseInferenceEngine,\n",
      ") -> dict[str, Any]:\n",
      "    \"\"\"Custom evaluation function registered as `count_letters`.\"\"\"\n",
      "    dataset = LetterCountGrpoDataset(\n",
      "        dataset=\"oumi-ai/oumi-letter-count-clean\", split=\"test\"\n",
      "    )\n",
      "    # TODO: OPE-1155: Add support for using Oumi dataset code to create the dataset.\n",
      "    # dataset = build_dataset(\"oumi-ai/oumi-letter-count\", tokenizer=None, sample_count=10)  # noqa: E501\n",
      "    num_samples = task_params.num_samples\n",
      "    if num_samples is None:\n",
      "        num_samples = len(dataset)\n",
      "    input_conversations = [dataset.conversation(i) for i in range(num_samples)]\n",
      "    conversations = inference_engine.infer(input_conversations)\n",
      "    logger.info(f\"Finished inference on {len(conversations)} conversations!\")\n",
      "    if len(conversations) > 0:\n",
      "        logger.info(f\"Sample conversation: {conversations[0]}\")\n",
      "\n",
      "    count = 0  # The number of examples with correct answers extracted.\n",
      "    total = 0  # All examples.\n",
      "    valid_count = 0  # The number of examples with valid answers extracted.\n",
      "    for i, conversation in enumerate(conversations):\n",
      "        total += 1\n",
      "        # Grab the model's response\n",
      "        response = conversation.last_message()\n",
      "        # Ignore cases where model didn't respond or it's a multimodal response.\n",
      "        # For now, we focus on text-only responses.\n",
      "        if not response or not isinstance(response.content, str):\n",
      "            continue\n",
      "        # Count the example as correct if the extracted prediction is correct.\n",
      "        prediction = _extract_prediction(response.content)\n",
      "        if prediction is None:\n",
      "            continue\n",
      "        valid_count += 1\n",
      "        if prediction == conversation.metadata[\"letter_count_integer\"]:\n",
      "            count += 1\n",
      "\n",
      "    return {\n",
      "        # Accuracy across all examples.\n",
      "        \"accuracy\": count / total if total > 0 else 0,\n",
      "        # Accuracy when only counting examples with properly extracted answers.\n",
      "        \"properly_extracted_accuracy\": count / valid_count if valid_count > 0 else 0,\n",
      "        \"num_samples\": num_samples,\n",
      "        # These three values sum up to num_samples.\n",
      "        \"num_correct_answers\": count,\n",
      "        \"num_incorrect_answers\": valid_count - count,\n",
      "        \"num_invalid_answers\": total - valid_count,\n",
      "    }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "from oumi.evaluation.registry.count_letters_task import count_letters\n",
    "\n",
    "print(inspect.getsource(count_letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, you can select which models you want to evaluate. You can lower `NUM_SAMPLES`  to reduce cost when calling remote APIs, with the downside of noisier results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 100\n",
    "# We set an environment variable to be used at the end of the Colab.\n",
    "os.environ[\"NUM_SAMPLES\"] = str(NUM_SAMPLES)\n",
    "\n",
    "model_names = [\n",
    "    \"llama_3b\",\n",
    "    # Uncomment any models you wish to evaluate - you can evaluate multiple at once.\n",
    "    # \"gpt_4o\",\n",
    "    # \"gemini_pro\",\n",
    "    # \"llama_405b\",\n",
    "    # \"claude_sonnet\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó**NOTICE:** If running this notebook on Colab, delete the following line: `inference_engine: VLLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting letter_counting_tutorial/llama_3b_eval.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/llama_3b_eval.yaml\n",
    "\n",
    "# We save this config as a YAML file as we'll use it again at the end of the notebook.\n",
    "model:\n",
    "  model_name: \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "  model_max_length: 131072\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  attn_implementation: \"sdpa\"\n",
    "  trust_remote_code: True\n",
    "\n",
    "inference_engine: VLLM\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 2048\n",
    "\n",
    "tasks:\n",
    "  - evaluation_backend: custom\n",
    "    task_name: count_letters\n",
    "\n",
    "output_dir: \"letter_counting_tutorial/evaluation/llama_3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EvaluationConfig for various models.\n",
    "# Note that Llama 3B uses the local VLLM inference engines, while the others use various\n",
    "# remote engines.\n",
    "\n",
    "with open(f\"{tutorial_dir}/llama_3b_eval.yaml\") as f:\n",
    "    llama_3b_yaml = f.read()\n",
    "\n",
    "configs = {\n",
    "    \"llama_3b\": llama_3b_yaml,\n",
    "    \"gpt_4o\": \"\"\"\n",
    "      model:\n",
    "        model_name: \"gpt-4o\"\n",
    "\n",
    "      inference_engine: OPENAI\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_key_env_varname: \"OPENAI_API_KEY\"\n",
    "        max_retries: 3\n",
    "        num_workers: 100\n",
    "        politeness_policy: 60\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: count_letters\n",
    "\n",
    "      output_dir: \"letter_counting_tutorial/evaluation/gpt_4o\"\n",
    "      \"\"\",\n",
    "    \"gemini_pro\": \"\"\"\n",
    "      model:\n",
    "        model_name: \"gemini-2.5-pro-preview-03-25\"\n",
    "\n",
    "      inference_engine: GOOGLE_GEMINI\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_key_env_varname: \"GEMINI_API_KEY\"\n",
    "        max_retries: 3\n",
    "        num_workers: 2\n",
    "        politeness_policy: 60\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: count_letters\n",
    "\n",
    "      output_dir: \"letter_counting_tutorial/evaluation/gemini_pro\"\n",
    "      \"\"\",\n",
    "    \"llama_405b\": f\"\"\"\n",
    "      model:\n",
    "        model_name: \"meta/llama-3.1-405b-instruct-maas\"\n",
    "\n",
    "      inference_engine: GOOGLE_VERTEX\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_url: \"https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/openapi/chat/completions\"\n",
    "        max_retries: 3\n",
    "        num_workers: 10\n",
    "        politeness_policy: 60\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: count_letters\n",
    "\n",
    "      output_dir: \"letter_counting_tutorial/evaluation/llama_405b\"\n",
    "      \"\"\",\n",
    "    \"claude_sonnet\": \"\"\"\n",
    "      model:\n",
    "        model_name: \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "      inference_engine: ANTHROPIC\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_key_env_varname: \"ANTHROPIC_API_KEY\"\n",
    "        max_retries: 3\n",
    "        num_workers: 5\n",
    "        politeness_policy: 65\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: count_letters\n",
    "\n",
    "      output_dir: \"letter_counting_tutorial/evaluation/claude_sonnet\"\n",
    "      \"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-27 22:35:05 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b0d59452834f9892b9dc0262d279ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e25f06e676421cb24cbcc4f5660252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1676a93d28984092aadce51dad0b5724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01df6577ae6c4f15baf9f0d30b12f1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-27 22:35:06,663][oumi][rank0][pid:1235199][MainThread][WARNING]][models.py:463] Undefined pad token. Setting it to `<|finetune_right_pad_id|>`.\n",
      "[2025-06-27 22:35:06,665][oumi][rank0][pid:1235199][MainThread][INFO]][models.py:506] Using the model's built-in chat template for model 'meta-llama/Llama-3.2-3B-Instruct'.\n",
      "INFO 06-27 22:35:16 [config.py:600] This model supports multiple tasks: {'embed', 'classify', 'reward', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 06-27 22:35:16 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 06-27 22:35:16 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58b38b119e74529a10c86de3daec0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-27 22:35:23 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 06-27 22:35:26 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 06-27 22:35:28 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x71113ca66490>\n",
      "INFO 06-27 22:35:28 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 06-27 22:35:28 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-27 22:35:28 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "WARNING 06-27 22:35:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-27 22:35:29 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 06-27 22:35:51 [weight_utils.py:281] Time spent downloading weights for meta-llama/Llama-3.2-3B-Instruct: 22.828389 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:07<00:07,  7.16s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:09<00:00,  4.30s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:09<00:00,  4.73s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-27 22:36:01 [loader.py:447] Loading weights took 9.47 seconds\n",
      "INFO 06-27 22:36:01 [gpu_model_runner.py:1273] Model loading took 6.0160 GiB and 32.757754 seconds\n",
      "INFO 06-27 22:36:02 [kv_cache_utils.py:578] GPU KV cache size: 559,296 tokens\n",
      "INFO 06-27 22:36:02 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 4.27x\n",
      "INFO 06-27 22:36:02 [core.py:162] init engine (profile, create kv cache, warmup model) took 0.66 seconds\n",
      "[2025-06-27 22:36:02,400][oumi][rank0][pid:1235199][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: LetterCountGrpoDataset)... dataset_name: 'oumi-ai/oumi-letter-count'\n",
      "[2025-06-27 22:36:03,029][oumi][rank0][pid:1235199][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: test\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 22894322\n",
      "\tDownload size: 5697295\n",
      "\tSize: 28591617 bytes\n",
      "\tRows: 20000\n",
      "\tColumns: ['conversation_id', 'messages', 'metadata']\n",
      "[2025-06-27 22:36:03,347][oumi][rank0][pid:1235199][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (20000, 3). Columns:\n",
      "conversation_id    object\n",
      "messages           object\n",
      "metadata           object\n",
      "dtype: object\n",
      "INFO 06-27 22:36:03 [chat_utils.py:396] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 85.50it/s, est. speed input: 7441.08 toks/s, output: 4241.24 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-27 22:36:05,185][oumi][rank0][pid:1235199][MainThread][INFO]][count_letters_task.py:54] Finished inference on 100 conversations!\n",
      "[2025-06-27 22:36:05,186][oumi][rank0][pid:1235199][MainThread][INFO]][count_letters_task.py:56] Sample conversation: conversation_id='oumi_letter_count_0' messages=[SYSTEM: Your final answer should be an integer written as digits and formatted as \"\\boxed{your_answer}\". For example, if the answer is 42, you should output \"\\boxed{42}\"., USER: Look through 'perivaginal' and count the 'n's., ASSISTANT: There are 2 'n's in 'perivaginal'.] metadata={'letter': 'n', 'letter_count_integer': 1, 'letter_count_string': 'one', 'unformatted_prompt': 'Look through {word} and count the {letter}s.', 'word': 'perivaginal'}\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation on all specified models.\n",
    "\n",
    "from oumi.core.configs import EvaluationConfig\n",
    "from oumi.core.evaluation import Evaluator\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    # Create the evaluation config from the YAML string.\n",
    "    config_yaml: str = configs[model_name]\n",
    "    config = EvaluationConfig.from_str(config_yaml)\n",
    "    config.tasks[0].num_samples = NUM_SAMPLES\n",
    "\n",
    "    # Run the evaluation.\n",
    "    evaluator = Evaluator()\n",
    "    evaluator_out = evaluator.evaluate(config)\n",
    "\n",
    "    # # Record the results.\n",
    "    results[model_name] = evaluator_out[0].get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 100\n",
      "--------------------------------------------------------------------------------\n",
      "Model: llama_3b\n",
      "Accuracy: 30.00%\n",
      "Properly Extracted Accuracy: 44.78%\n",
      "Num correct, incorrect, invalid: 30, 37, 33\n"
     ]
    }
   ],
   "source": [
    "# Print results.\n",
    "\n",
    "print(f\"Total samples: {NUM_SAMPLES}\")\n",
    "for model_name, result in results.items():\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {result['accuracy']:.2%}\")\n",
    "    print(f\"Properly Extracted Accuracy: {result['properly_extracted_accuracy']:.2%}\")\n",
    "    correct = result[\"num_correct_answers\"]\n",
    "    incorrect = result[\"num_incorrect_answers\"]\n",
    "    invalid = result[\"num_invalid_answers\"]\n",
    "    print(f\"Num correct, incorrect, invalid: {correct}, {incorrect}, {invalid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO\n",
    "\n",
    "Now, we train Llama 3.2 3B on the task of counting letters using the GRPO algorithm implemented by [HuggingFace's `trl` library](https://huggingface.co/docs/trl/en/index).\n",
    "\n",
    "Note that we can calculate a concrete reward for this task by comparing the answer extracted by the model with the correct answer. In the reward function defined in `src/oumi/datasets/grpo/rewards/count_letters_rewards.py` ([GitHub link](https://github.com/oumi-ai/oumi/blob/main/src/oumi/datasets/grpo/rewards/count_letters_rewards.py)), we calculate the reward to be `-abs(predicted_count - target_count)`. We use simple heuristics to extract the predicted count. The following cell prints out the reward function code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2025 - Oumi\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import re\n",
      "from typing import Any, Optional\n",
      "\n",
      "from oumi.core.registry import RegistryType, register\n",
      "\n",
      "\n",
      "def _extract_prediction(response: str) -> Optional[int]:\n",
      "    r\"\"\"Returns the numeric answer extracted from `\\boxed{...}`, or None otherwise.\"\"\"\n",
      "    regex_result = re.findall(r\"\\\\boxed\\{([-+]?\\d+)\\}\", response)\n",
      "    if not regex_result or len(regex_result) != 1:\n",
      "        return None\n",
      "    number_str = regex_result[0]\n",
      "    # Except clause shouldn't trigger because the regex should only find ints.\n",
      "    try:\n",
      "        return int(number_str)\n",
      "    except ValueError:\n",
      "        return None\n",
      "\n",
      "\n",
      "def compute_letter_count_reward(completion: str, target_count: int) -> float:\n",
      "    \"\"\"Computes the rewards for counting the letters in a string.\n",
      "\n",
      "    Args:\n",
      "        completion: The completion string from the LLM.\n",
      "        target_count: The target count of letters.\n",
      "\n",
      "    Returns:\n",
      "        The reward value.\n",
      "    \"\"\"\n",
      "    count = _extract_prediction(completion)\n",
      "\n",
      "    # Lowest reward goes to unparseable responses\n",
      "    if count is None:\n",
      "        return -3.0\n",
      "\n",
      "    delta = abs(count - target_count)\n",
      "\n",
      "    # Reward scales from [0, -2) as delta increases\n",
      "    # Ensures that \"worse\" answers (where the counts are off by a higher amount) are\n",
      "    # penalized while never reaching -3.0 which is reserved for unparseable answers.\n",
      "    return (1 / (delta + 0.5)) - 2\n",
      "\n",
      "\n",
      "@register(\"count_letters\", RegistryType.REWARD_FUNCTION)\n",
      "def _count_letters(\n",
      "    completions: list[list[dict[str, Any]]],\n",
      "    letter_count: list[int],\n",
      "    **kwargs: dict[str, Any],\n",
      ") -> list[float]:\n",
      "    \"\"\"Custom reward function for counting letters in a string.\n",
      "\n",
      "    For more details on custom reward functions used in trl's GRPOTrainer, see:\n",
      "    https://huggingface.co/docs/trl/main/en/grpo_trainer#using-a-custom-reward-function.\n",
      "\n",
      "    Args:\n",
      "        completions: The list of completions from the LLM.\n",
      "        letter_count: The list of target count of letters.\n",
      "        kwargs: Unused.\n",
      "\n",
      "    Returns:\n",
      "        The reward values for each completion, calculated as the negative of the\n",
      "        absolute difference between the count and the target count. The count is assumed\n",
      "        to be the last group of consecutive digits in the completion string.\n",
      "    \"\"\"\n",
      "    completions_strs = [c[0][\"content\"] for c in completions]\n",
      "    return [\n",
      "        compute_letter_count_reward(c, t)\n",
      "        for c, t in zip(completions_strs, letter_count)\n",
      "    ]\n"
     ]
    }
   ],
   "source": [
    "!cat ../src/oumi/datasets/grpo/rewards/count_letters_rewards.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W627 22:36:22.951844850 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Clean up to free-up GPU memory used for evaluation above\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Delete the evaluator and collect garbage.\"\"\"\n",
    "    global evaluator\n",
    "    if evaluator:  # type: ignore\n",
    "        del evaluator\n",
    "        evaluator = None\n",
    "    for _ in range(3):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó**NOTICE:** Set `training.enable_wandb` to True if you want to log your training run to Weights and Biases. In addition, you must also log into WandB, ex. by running `wandb login`.\n",
    "\n",
    "‚ùó**NOTICE:** We only train for 2 steps for demonstration purposes. You can increase `max_steps`, or replace it with `num_train_epochs` to set your desired number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting letter_counting_tutorial/grpo_train.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/grpo_train.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "  model_max_length: 8192\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  attn_implementation: \"sdpa\"\n",
    "\n",
    "data:\n",
    "  train:\n",
    "    datasets:\n",
    "      - dataset_name: \"oumi-ai/oumi-letter-count\"\n",
    "        split: \"train\"\n",
    "\n",
    "training:\n",
    "  trainer_type: \"TRL_GRPO\"\n",
    "  save_steps: 500\n",
    "  # max_steps: 500\n",
    "  per_device_train_batch_size: 2\n",
    "  gradient_accumulation_steps: 1\n",
    "  learning_rate: 5e-7\n",
    "  lr_scheduler_type: \"cosine\"\n",
    "  warmup_steps: 20\n",
    "\n",
    "  reward_functions: [\"count_letters\"]\n",
    "\n",
    "  ddp_find_unused_parameters: False\n",
    "  optimizer: \"adafactor\"\n",
    "  compile: True\n",
    "\n",
    "  grpo:\n",
    "    num_generations: 4\n",
    "\n",
    "  dataloader_num_workers: \"auto\"\n",
    "  dataloader_prefetch_factor: 32\n",
    "\n",
    "  logging_steps: 10\n",
    "  output_dir: \"letter_counting_tutorial/llama_3b_grpo\"\n",
    "  # Set this to True if you want to log to Weights and Biases.\n",
    "  enable_wandb: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-27 22:38:26,830][oumi][rank0][pid:1236605][MainThread][INFO]][distributed_run.py:308] Running the command: ['oumi', 'train', '-c', 'letter_counting_tutorial/grpo_train.yaml']\n",
      "\n",
      "\u001b[32m   ____  _    _ __  __ _____\u001b[0m\n",
      "\u001b[32m  / __ \\| |  | |  \\/  |_   _|\u001b[0m\n",
      "\u001b[32m | |  | | |  | | \\  / | | |\u001b[0m\n",
      "\u001b[32m | |  | | |  | | |\\/| | | |\u001b[0m\n",
      "\u001b[32m | |__| | |__| | |  | |_| |_\u001b[0m\n",
      "\u001b[32m  \\____/ \\____/|_|  |_|_____|\u001b[0m\n",
      "\n",
      "\u001b[2K\u001b[32m‚†è\u001b[0m \u001b[32mLoading configuration...\u001b[0m0m\n",
      "\u001b[1A\u001b[2K[2025-06-27 22:38:35,359][oumi][rank0][pid:1236624][MainThread][WARNING]][training_config.py:115] Ignored model.model_max_length=8192 parameter for trainer TrainerType.TRL_GRPO.\n",
      "[2025-06-27 22:38:35,630][oumi][rank0][pid:1236624][MainThread][INFO]][distributed.py:578] Setting random seed to 42 on rank 0.\n",
      "INFO:datasets:PyTorch version 2.6.0 available.\n",
      "[2025-06-27 22:38:41,463][oumi][rank0][pid:1236624][MainThread][INFO]][torch_utils.py:81] Torch version: 2.6.0+cu124. NumPy version: 1.26.4\n",
      "[2025-06-27 22:38:41,463][oumi][rank0][pid:1236624][MainThread][INFO]][torch_utils.py:89] CUDA version: 12.4 \n",
      "[2025-06-27 22:38:41,463][oumi][rank0][pid:1236624][MainThread][INFO]][torch_utils.py:92] CuDNN version: 90.8.0\n",
      "[2025-06-27 22:38:41,464][oumi][rank0][pid:1236624][MainThread][INFO]][torch_utils.py:125] CPU cores: 208 CUDA devices: 1\n",
      "device(0)='NVIDIA H100 80GB HBM3' Capability: (9, 0) Memory: [Total: 79.19GiB Free: 78.17GiB Allocated: 0.0GiB Cached: 0.0GiB]\n",
      "[2025-06-27 22:38:41,469][oumi][rank0][pid:1236624][MainThread][INFO]][train.py:153] Oumi version: 0.2.1.dev3+g91dd651c\n",
      "[2025-06-27 22:38:41,474][oumi][rank0][pid:1236624][MainThread][INFO]][train.py:155] Git revision hash: 91dd651cca462cdcd8641e5c7501deaa54e98cf5\n",
      "[2025-06-27 22:38:41,487][oumi][rank0][pid:1236624][MainThread][INFO]][train.py:156] Git tag: None\n",
      "[2025-06-27 22:38:41,490][oumi][rank0][pid:1236624][MainThread][INFO]][train.py:164] Resolved 'training.dataloader_num_workers=auto' to 'training.dataloader_num_workers=2'\n",
      "[2025-06-27 22:38:41,490][oumi][rank0][pid:1236624][MainThread][INFO]][train.py:281] TrainingConfig:\n",
      "TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='oumi-ai/oumi-letter-count',\n",
      "                                                                                dataset_path=None,\n",
      "                                                                                subset=None,\n",
      "                                                                                split='train',\n",
      "                                                                                dataset_kwargs={},\n",
      "                                                                                sample_count=None,\n",
      "                                                                                mixture_proportion=None,\n",
      "                                                                                shuffle=False,\n",
      "                                                                                seed=None,\n",
      "                                                                                shuffle_buffer_size=1000,\n",
      "                                                                                trust_remote_code=False,\n",
      "                                                                                transform_num_workers=None)],\n",
      "                                                        collator_name=None,\n",
      "                                                        collator_kwargs={},\n",
      "                                                        pack=False,\n",
      "                                                        stream=False,\n",
      "                                                        target_col=None,\n",
      "                                                        mixture_strategy='first_exhausted',\n",
      "                                                        seed=None,\n",
      "                                                        use_async_dataset=False,\n",
      "                                                        use_torchdata=None),\n",
      "                               test=DatasetSplitParams(datasets=[],\n",
      "                                                       collator_name=None,\n",
      "                                                       collator_kwargs={},\n",
      "                                                       pack=False,\n",
      "                                                       stream=False,\n",
      "                                                       target_col=None,\n",
      "                                                       mixture_strategy='first_exhausted',\n",
      "                                                       seed=None,\n",
      "                                                       use_async_dataset=False,\n",
      "                                                       use_torchdata=None),\n",
      "                               validation=DatasetSplitParams(datasets=[],\n",
      "                                                             collator_name=None,\n",
      "                                                             collator_kwargs={},\n",
      "                                                             pack=False,\n",
      "                                                             stream=False,\n",
      "                                                             target_col=None,\n",
      "                                                             mixture_strategy='first_exhausted',\n",
      "                                                             seed=None,\n",
      "                                                             use_async_dataset=False,\n",
      "                                                             use_torchdata=None)),\n",
      "               model=ModelParams(model_name='meta-llama/Llama-3.2-3B-Instruct',\n",
      "                                 adapter_model=None,\n",
      "                                 tokenizer_name=None,\n",
      "                                 tokenizer_pad_token=None,\n",
      "                                 tokenizer_kwargs={},\n",
      "                                 processor_kwargs={},\n",
      "                                 model_max_length=8192,\n",
      "                                 load_pretrained_weights=True,\n",
      "                                 trust_remote_code=False,\n",
      "                                 torch_dtype_str='bfloat16',\n",
      "                                 compile=False,\n",
      "                                 chat_template=None,\n",
      "                                 attn_implementation='sdpa',\n",
      "                                 device_map='auto',\n",
      "                                 model_kwargs={},\n",
      "                                 enable_liger_kernel=False,\n",
      "                                 shard_for_eval=False,\n",
      "                                 freeze_layers=[],\n",
      "                                 model_revision=None),\n",
      "               training=TrainingParams(use_peft=False,\n",
      "                                       trainer_type=<TrainerType.TRL_GRPO: 'trl_grpo'>,\n",
      "                                       enable_gradient_checkpointing=False,\n",
      "                                       gradient_checkpointing_kwargs={},\n",
      "                                       output_dir='letter_counting_tutorial/llama_3b_grpo',\n",
      "                                       per_device_train_batch_size=2,\n",
      "                                       per_device_eval_batch_size=8,\n",
      "                                       gradient_accumulation_steps=1,\n",
      "                                       max_steps=-1,\n",
      "                                       num_train_epochs=3,\n",
      "                                       save_epoch=False,\n",
      "                                       save_steps=500,\n",
      "                                       save_final_model=True,\n",
      "                                       seed=42,\n",
      "                                       data_seed=42,\n",
      "                                       use_deterministic=False,\n",
      "                                       full_determinism=False,\n",
      "                                       run_name=None,\n",
      "                                       metrics_function=None,\n",
      "                                       reward_functions=['count_letters'],\n",
      "                                       grpo=GrpoParams(model_init_kwargs={},\n",
      "                                                       max_prompt_length=None,\n",
      "                                                       max_completion_length=None,\n",
      "                                                       num_generations=2,\n",
      "                                                       temperature=0.9,\n",
      "                                                       remove_unused_columns=False,\n",
      "                                                       repetition_penalty=1.0,\n",
      "                                                       use_vllm=False,\n",
      "                                                       vllm_mode=None,\n",
      "                                                       vllm_gpu_memory_utilization=0.9,\n",
      "                                                       vllm_dtype=None,\n",
      "                                                       vllm_max_model_len=None,\n",
      "                                                       epsilon=0.2,\n",
      "                                                       log_completions=False),\n",
      "                                       log_level='info',\n",
      "                                       dep_log_level='warning',\n",
      "                                       enable_wandb=True,\n",
      "                                       enable_mlflow=False,\n",
      "                                       enable_tensorboard=True,\n",
      "                                       logging_strategy='steps',\n",
      "                                       logging_dir=None,\n",
      "                                       logging_steps=10,\n",
      "                                       logging_first_step=False,\n",
      "                                       eval_strategy='no',\n",
      "                                       eval_steps=500,\n",
      "                                       learning_rate=5e-07,\n",
      "                                       lr_scheduler_type='cosine',\n",
      "                                       lr_scheduler_kwargs={},\n",
      "                                       warmup_ratio=None,\n",
      "                                       warmup_steps=20,\n",
      "                                       optimizer='adafactor',\n",
      "                                       weight_decay=0.0,\n",
      "                                       adam_beta1=0.9,\n",
      "                                       adam_beta2=0.999,\n",
      "                                       adam_epsilon=1e-08,\n",
      "                                       sgd_momentum=0.0,\n",
      "                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,\n",
      "                                       compile=True,\n",
      "                                       include_performance_metrics=False,\n",
      "                                       include_alternative_mfu_metrics=False,\n",
      "                                       log_model_summary=False,\n",
      "                                       resume_from_checkpoint=None,\n",
      "                                       try_resume_from_last_checkpoint=False,\n",
      "                                       dataloader_num_workers=2,\n",
      "                                       dataloader_persistent_workers=False,\n",
      "                                       dataloader_prefetch_factor=32,\n",
      "                                       dataloader_main_process_only=None,\n",
      "                                       ddp_find_unused_parameters=False,\n",
      "                                       max_grad_norm=1.0,\n",
      "                                       trainer_kwargs={},\n",
      "                                       verl_config_overrides={},\n",
      "                                       profiler=ProfilerParams(save_dir=None,\n",
      "                                                               enable_cpu_profiling=False,\n",
      "                                                               enable_cuda_profiling=False,\n",
      "                                                               record_shapes=False,\n",
      "                                                               profile_memory=False,\n",
      "                                                               with_stack=False,\n",
      "                                                               with_flops=False,\n",
      "                                                               with_modules=False,\n",
      "                                                               row_limit=50,\n",
      "                                                               schedule=ProfilerScheduleParams(enable_schedule=False,\n",
      "                                                                                               wait=0,\n",
      "                                                                                               warmup=1,\n",
      "                                                                                               active=3,\n",
      "                                                                                               repeat=1,\n",
      "                                                                                               skip_first=1)),\n",
      "                                       telemetry=TelemetryParams(telemetry_dir='telemetry',\n",
      "                                                                 collect_telemetry_for_all_ranks=False,\n",
      "                                                                 track_gpu_temperature=False),\n",
      "                                       empty_device_cache_steps=None,\n",
      "                                       nccl_default_timeout_minutes=None,\n",
      "                                       label_ignore_index=None),\n",
      "               peft=PeftParams(lora_r=8,\n",
      "                               lora_alpha=8,\n",
      "                               lora_dropout=0.0,\n",
      "                               lora_target_modules=None,\n",
      "                               lora_modules_to_save=None,\n",
      "                               lora_bias='none',\n",
      "                               lora_init_weights=<LoraWeightInitialization.DEFAULT: 'default'>,\n",
      "                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
      "                               q_lora=False,\n",
      "                               q_lora_bits=4,\n",
      "                               bnb_4bit_quant_type='fp4',\n",
      "                               llm_int8_skip_modules=None,\n",
      "                               use_bnb_nested_quant=False,\n",
      "                               bnb_4bit_quant_storage='uint8',\n",
      "                               bnb_4bit_compute_dtype='float32',\n",
      "                               peft_save_mode=<PeftSaveMode.ADAPTER_ONLY: 'adapter_only'>),\n",
      "               fsdp=FSDPParams(enable_fsdp=False,\n",
      "                               sharding_strategy=<ShardingStrategy.FULL_SHARD: 'FULL_SHARD'>,\n",
      "                               cpu_offload=False,\n",
      "                               mixed_precision=None,\n",
      "                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,\n",
      "                               forward_prefetch=False,\n",
      "                               use_orig_params=None,\n",
      "                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,\n",
      "                               auto_wrap_policy=<AutoWrapPolicy.NO_WRAP: 'NO_WRAP'>,\n",
      "                               min_num_params=100000,\n",
      "                               transformer_layer_cls=None,\n",
      "                               sync_module_states=True))\n",
      "[2025-06-27 22:38:41,850][oumi][rank0][pid:1236624][MainThread][WARNING]][models.py:463] Undefined pad token. Setting it to `<|finetune_right_pad_id|>`.\n",
      "[2025-06-27 22:38:41,851][oumi][rank0][pid:1236624][MainThread][INFO]][models.py:506] Using the model's built-in chat template for model 'meta-llama/Llama-3.2-3B-Instruct'.\n",
      "[2025-06-27 22:38:41,852][oumi][rank0][pid:1236624][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: LetterCountGrpoDataset)... dataset_name: 'oumi-ai/oumi-letter-count'\n",
      "[2025-06-27 22:38:42,380][oumi][rank0][pid:1236624][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: train\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 22894322\n",
      "\tDownload size: 5697295\n",
      "\tSize: 28591617 bytes\n",
      "\tRows: 100000\n",
      "\tColumns: ['conversation_id', 'messages', 'metadata']\n",
      "[2025-06-27 22:38:42,814][oumi][rank0][pid:1236624][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (100000, 3). Columns:\n",
      "conversation_id    object\n",
      "messages           object\n",
      "metadata           object\n",
      "dtype: object\n",
      "[2025-06-27 22:38:42,849][oumi][rank0][pid:1236624][MainThread][INFO]][base_map_dataset.py:312] LetterCountGrpoDataset: features=dict_keys(['prompt', 'letter_count'])\n",
      "[2025-06-27 22:38:48,263][oumi][rank0][pid:1236624][MainThread][INFO]][base_map_dataset.py:376] Finished transforming dataset (LetterCountGrpoDataset)! Speed: 18470.86 examples/sec. Examples: 100000. Duration: 5.4 sec. Transform workers: 1.\n",
      "[2025-06-27 22:38:48,281][oumi][rank0][pid:1236624][MainThread][INFO]][models.py:228] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
      "[2025-06-27 22:38:48,304][oumi][rank0][pid:1236624][MainThread][INFO]][models.py:300] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for \n",
      "storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory`\n",
      "in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.02it/s]\n",
      "[2025-06-27 22:38:49,471][oumi][rank0][pid:1236624][MainThread][INFO]][torch_utils.py:289] \n",
      "Model Parameters Summary:\n",
      "üî¢ Total     parameters: 3,212,749,824\n",
      "üîó Embedding parameters: 394,002,432\n",
      "üéØ Trainable parameters: 3,212,749,824\n",
      "üîí Frozen    parameters: 0 (0.00%)\n",
      "\n",
      "INFO 06-27 22:38:49 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-06-27 22:38:51,770][oumi][rank0][pid:1236624][MainThread][INFO]][torch_profiler_utils.py:164] PROF: Torch Profiler disabled!\n",
      "[2025-06-27 22:38:51,792][oumi][rank0][pid:1236624][MainThread][INFO]][training.py:62] GRPOConfig(output_dir='letter_counting_tutorial/llama_3b_grpo',\n",
      "           overwrite_output_dir=False,\n",
      "           do_train=False,\n",
      "           do_eval=False,\n",
      "           do_predict=False,\n",
      "           eval_strategy=<IntervalStrategy.NO: 'no'>,\n",
      "           prediction_loss_only=False,\n",
      "           per_device_train_batch_size=2,\n",
      "           per_device_eval_batch_size=8,\n",
      "           per_gpu_train_batch_size=None,\n",
      "           per_gpu_eval_batch_size=None,\n",
      "           gradient_accumulation_steps=1,\n",
      "           eval_accumulation_steps=None,\n",
      "           eval_delay=0,\n",
      "           torch_empty_cache_steps=None,\n",
      "           learning_rate=5e-07,\n",
      "           weight_decay=0.0,\n",
      "           adam_beta1=0.9,\n",
      "           adam_beta2=0.999,\n",
      "           adam_epsilon=1e-08,\n",
      "           max_grad_norm=1.0,\n",
      "           num_train_epochs=3,\n",
      "           max_steps=-1,\n",
      "           lr_scheduler_type=<SchedulerType.COSINE: 'cosine'>,\n",
      "           lr_scheduler_kwargs={},\n",
      "           warmup_ratio=0.0,\n",
      "           warmup_steps=20,\n",
      "           log_level='warning',\n",
      "           log_level_replica='warning',\n",
      "           log_on_each_node=True,\n",
      "           logging_dir='letter_counting_tutorial/llama_3b_grpo/runs/Jun27_22-38-51_oumi-compute001',\n",
      "           logging_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
      "           logging_first_step=False,\n",
      "           logging_steps=10,\n",
      "           logging_nan_inf_filter=True,\n",
      "           save_strategy=<SaveStrategy.STEPS: 'steps'>,\n",
      "           save_steps=500,\n",
      "           save_total_limit=None,\n",
      "           save_safetensors=True,\n",
      "           save_on_each_node=False,\n",
      "           save_only_model=False,\n",
      "           restore_callback_states_from_checkpoint=False,\n",
      "           no_cuda=False,\n",
      "           use_cpu=False,\n",
      "           use_mps_device=False,\n",
      "           seed=42,\n",
      "           data_seed=42,\n",
      "           jit_mode_eval=False,\n",
      "           use_ipex=False,\n",
      "           bf16=False,\n",
      "           fp16=False,\n",
      "           fp16_opt_level='O1',\n",
      "           half_precision_backend='auto',\n",
      "           bf16_full_eval=False,\n",
      "           fp16_full_eval=False,\n",
      "           tf32=None,\n",
      "           local_rank=0,\n",
      "           ddp_backend=None,\n",
      "           tpu_num_cores=None,\n",
      "           tpu_metrics_debug=False,\n",
      "           debug=[],\n",
      "           dataloader_drop_last=False,\n",
      "           eval_steps=500,\n",
      "           dataloader_num_workers=2,\n",
      "           dataloader_prefetch_factor=32,\n",
      "           past_index=-1,\n",
      "           run_name='letter_counting_tutorial/llama_3b_grpo',\n",
      "           disable_tqdm=False,\n",
      "           remove_unused_columns=False,\n",
      "           label_names=None,\n",
      "           load_best_model_at_end=False,\n",
      "           metric_for_best_model=None,\n",
      "           greater_is_better=None,\n",
      "           ignore_data_skip=False,\n",
      "           fsdp=[],\n",
      "           fsdp_min_num_params=0,\n",
      "           fsdp_config={'min_num_params': 0,\n",
      "                        'xla': False,\n",
      "                        'xla_fsdp_grad_ckpt': False,\n",
      "                        'xla_fsdp_v2': False},\n",
      "           tp_size=0,\n",
      "           fsdp_transformer_layer_cls_to_wrap=None,\n",
      "           accelerator_config=AcceleratorConfig(split_batches=False,\n",
      "                                                dispatch_batches=None,\n",
      "                                                even_batches=True,\n",
      "                                                use_seedable_sampler=True,\n",
      "                                                non_blocking=False,\n",
      "                                                gradient_accumulation_kwargs=None,\n",
      "                                                use_configured_state=False),\n",
      "           deepspeed=None,\n",
      "           label_smoothing_factor=0.0,\n",
      "           optim=<OptimizerNames.ADAFACTOR: 'adafactor'>,\n",
      "           optim_args=None,\n",
      "           adafactor=False,\n",
      "           group_by_length=False,\n",
      "           length_column_name='length',\n",
      "           report_to=['wandb', 'tensorboard'],\n",
      "           ddp_find_unused_parameters=False,\n",
      "           ddp_bucket_cap_mb=None,\n",
      "           ddp_broadcast_buffers=None,\n",
      "           dataloader_pin_memory=True,\n",
      "           dataloader_persistent_workers=False,\n",
      "           skip_memory_metrics=True,\n",
      "           use_legacy_prediction_loop=False,\n",
      "           push_to_hub=False,\n",
      "           resume_from_checkpoint=None,\n",
      "           hub_model_id=None,\n",
      "           hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>,\n",
      "           hub_token=None,\n",
      "           hub_private_repo=None,\n",
      "           hub_always_push=False,\n",
      "           gradient_checkpointing=False,\n",
      "           gradient_checkpointing_kwargs={},\n",
      "           include_inputs_for_metrics=False,\n",
      "           include_for_metrics=[],\n",
      "           eval_do_concat_batches=True,\n",
      "           fp16_backend='auto',\n",
      "           push_to_hub_model_id=None,\n",
      "           push_to_hub_organization=None,\n",
      "           push_to_hub_token=None,\n",
      "           mp_parameters='',\n",
      "           auto_find_batch_size=False,\n",
      "           full_determinism=False,\n",
      "           torchdynamo=None,\n",
      "           ray_scope='last',\n",
      "           ddp_timeout=1800,\n",
      "           torch_compile=True,\n",
      "           torch_compile_backend='inductor',\n",
      "           torch_compile_mode=None,\n",
      "           include_tokens_per_second=False,\n",
      "           include_num_input_tokens_seen=False,\n",
      "           neftune_noise_alpha=None,\n",
      "           optim_target_modules=None,\n",
      "           batch_eval_metrics=False,\n",
      "           eval_on_start=False,\n",
      "           use_liger_kernel=False,\n",
      "           eval_use_gather_object=False,\n",
      "           average_tokens_across_devices=False,\n",
      "           model_init_kwargs=None,\n",
      "           disable_dropout=False,\n",
      "           max_prompt_length=512,\n",
      "           num_generations=2,\n",
      "           max_completion_length=256,\n",
      "           ds3_gather_for_generation=True,\n",
      "           shuffle_dataset=True,\n",
      "           generation_batch_size=2,\n",
      "           steps_per_generation=1,\n",
      "           temperature=0.9,\n",
      "           top_p=1.0,\n",
      "           top_k=None,\n",
      "           min_p=None,\n",
      "           repetition_penalty=1.0,\n",
      "           cache_implementation=None,\n",
      "           use_vllm=False,\n",
      "           vllm_server_base_url=None,\n",
      "           vllm_mode='server',\n",
      "           vllm_guided_decoding_regex=None,\n",
      "           vllm_server_host='0.0.0.0',\n",
      "           vllm_server_port=8000,\n",
      "           vllm_server_timeout=240.0,\n",
      "           vllm_gpu_memory_utilization=0.3,\n",
      "           vllm_tensor_parallel_size=1,\n",
      "           beta=0.04,\n",
      "           num_iterations=1,\n",
      "           epsilon=0.2,\n",
      "           delta=None,\n",
      "           epsilon_high=None,\n",
      "           reward_weights=None,\n",
      "           scale_rewards=True,\n",
      "           loss_type='bnpo',\n",
      "           mask_truncated_completions=False,\n",
      "           sync_ref_model=False,\n",
      "           ref_model_mixup_alpha=0.6,\n",
      "           ref_model_sync_steps=512,\n",
      "           use_liger_loss=False,\n",
      "           log_completions=False,\n",
      "           num_completions_to_print=None,\n",
      "           wandb_log_unique_prompts=False)\n",
      "[2025-06-27 22:38:51,857][oumi][rank0][pid:1236624][MainThread][INFO]][device_utils.py:297] GPU Metrics Before Training: GPU runtime info: NVidiaGpuRuntimeInfo(device_index=0, device_count=1, used_memory_mb=13817.0, temperature=31, fan_speed=None, fan_speeds=None, power_usage_watts=119.669, power_limit_watts=700.0, gpu_utilization=0, memory_utilization=0, performance_state=0, clock_speed_graphics=1980, clock_speed_sm=1980, clock_speed_memory=2619).\n",
      "[2025-06-27 22:38:51,857][oumi][rank0][pid:1236624][MainThread][INFO]][train.py:511] Training init time: 10.398s\n",
      "[2025-06-27 22:38:51,860][oumi][rank0][pid:1236624][MainThread][INFO]][train.py:512] Starting training... (TrainerType.TRL_GRPO, transformers: 4.51.3)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshanghong_sim\u001b[0m (\u001b[33mshanghongsim\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.12rc1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/shanghong/oumi/notebooks/wandb/run-20250627_223852-ydcovnqj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mletter_counting_tutorial/llama_3b_grpo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/shanghongsim/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/shanghongsim/huggingface/runs/ydcovnqj\u001b[0m\n",
      "  0%|                                                | 0/300000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "!oumi distributed torchrun -m oumi train -c $tutorial_dir/grpo_train.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our Trained Model\n",
    "\n",
    "Let's now evaluate our trained model to see if it improved on the letter counting task. Note that it may not improve much, since we trained it for a relatively short time.\n",
    "\n",
    "Below, we demonstrate an alternative method of running evaluation with the `oumi` CLI. We use the same Llama 3B evaluation config we used above, with the only change being pointing it at the model we just trained.\n",
    "\n",
    "First, we need to reset the notebook to clear variables from our previous vLLM run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m   ____  _    _ __  __ _____\u001b[0m\n",
      "\u001b[32m  / __ \\| |  | |  \\/  |_   _|\u001b[0m\n",
      "\u001b[32m | |  | | |  | | \\  / | | |\u001b[0m\n",
      "\u001b[32m | |  | | |  | | |\\/| | | |\u001b[0m\n",
      "\u001b[32m | |__| | |__| | |  | |_| |_\u001b[0m\n",
      "\u001b[32m  \\____/ \\____/|_|  |_|_____|\u001b[0m\n",
      "\u001b[2K\u001b[32m‚†¥\u001b[0m \u001b[32mLoading configuration...\u001b[0m0m\n",
      "\u001b[2K\u001b[32m‚†ã\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-04-10 09:47:15,521][oumi][rank0][pid:16694][MainThread][INFO]][models.py:482] Using the model's built-in chat template for model 'letter_counting_tutorial/llama_3b_grpo'.\n",
      "\u001b[2KINFO 04-10 09:47:15 __init__.py:207] Automatically detected platform cuda.\n",
      "\u001b[2KINFO 04-10 09:47:33 model_runner.py:1110] Starting to load model \n",
      "letter_counting_tutorial/llama_3b_grpo...\n",
      "\u001b[32m‚†∏\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorkerProcess pid=16718)\u001b[0;0m INFO 04-10 09:47:33 model_runner.py:1110] Starting to load model letter_counting_tutorial/llama_3b_grpo...\n",
      "\u001b[32m‚†¥\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorkerProcess pid=16719)\u001b[0;0m INFO 04-10 09:47:34 model_runner.py:1115] Loading model weights took 1.5341 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=16718)\u001b[0;0m INFO 04-10 09:47:34 model_runner.py:1115] Loading model weights took 1.5341 GB\n",
      "\n",
      "\u001b[2K\u001b[32m‚†π\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-04-10 09:47:46,962][oumi][rank0][pid:16694][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: LetterCountGrpoDataset)... dataset_name: 'oumi-ai/oumi-letter-count'\n",
      "\u001b[2K\u001b[32m‚†ã\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-04-10 09:47:49,180][oumi][rank0][pid:16694][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: test\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 22894322\n",
      "\tDownload size: 5697295\n",
      "\tSize: 28591617 bytes\n",
      "\tRows: 20000\n",
      "\tColumns: ['conversation_id', 'messages', 'metadata']\n",
      "\u001b[2K\u001b[32m‚†è\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-04-10 09:47:49,820][oumi][rank0][pid:16694][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (20000, 3). Columns:\n",
      "conversation_id    object\n",
      "messages           object\n",
      "metadata           object\n",
      "dtype: object\n",
      "\u001b[2KINFO 04-10 09:47:49 chat_utils.py:332] Detected the chat template content format\n",
      "to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "\u001b[2KProcessed prompts: \u001b[1;36m100\u001b[0m%|#| \u001b[1;36m100\u001b[0m/\u001b[1;36m100\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:03\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m28.\u001b[0m84it/s, est. speed input: \u001b[1;36m26\u001b[0m\n",
      "\u001b[32m‚†∏\u001b[0m \u001b[32mRunning evaluation...\u001b[0m[2025-04-10 09:47:53,387][oumi][rank0][pid:16694][MainThread][INFO]][count_letters_task.py:53] Finished inference on 100 conversations!\n",
      "[2025-04-10 09:47:53,387][oumi][rank0][pid:16694][MainThread][INFO]][count_letters_task.py:55] Sample conversation: conversation_id='oumi_letter_count_0' messages=[USER: Look through 'perivaginal' and count the 'n's., SYSTEM: Your final answer should be written as digits and formatted as \"\\boxed{your_answer}\". For example, if the answer is 42, make sure to output \"\\boxed{42}\"., ASSISTANT: There are 2 'n's in 'perivaginal'. \n",
      "\n",
      "\\boxed{2}] metadata={'letter': 'n', 'letter_count_integer': 1, 'letter_count_string': 'one', 'unformatted_prompt': 'Look through {word} and count the {letter}s.', 'word': 'perivaginal'}\n",
      "\u001b[2KINFO 04-10 09:47:53 multiproc_worker_utils.py:141] Terminating local vLLM worker\n",
      "processes\n",
      "\u001b[32m‚†ß\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\u001b[1;36m(VllmWorkerProcess pid=16717)\u001b[0;0m INFO 04-10 09:47:53 multiproc_worker_utils.py:253] Worker exiting\n",
      "\u001b[2K\u001b[32m‚†ß\u001b[0m \u001b[32mRunning evaluation...\u001b[0m\n",
      "\u001b[1A\u001b[2K\u001b[1;35m                         Evaluation Results                         \u001b[0m\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ\u001b[1m \u001b[0m\u001b[1mBenchmark    \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mMetric                     \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mScore \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mStd Error\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
      "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mAccuracy                   \u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m51.00%\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mProperly Extracted Accuracy\u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m53.68%\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mNum Samples                \u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m100   \u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mNum Correct Answers        \u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m51    \u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mNum Incorrect Answers      \u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m44    \u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mcount_letters\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33mNum Invalid Answers        \u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32m5     \u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[2m \u001b[0m\u001b[2m-        \u001b[0m\u001b[2m \u001b[0m‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "[rank0]:[W410 09:47:58.432638348 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!oumi evaluate -c letter_counting_tutorial/llama_3b_eval.yaml \\\n",
    "    --model.model_name \"letter_counting_tutorial/llama_3b_grpo\" \\\n",
    "    --tasks.0.num_samples $NUM_SAMPLES \\\n",
    "    --output_dir \"letter_counting_tutorial/evaluation/llama_3_grpo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Better Letter Counter\n",
    "\n",
    "Looks like we were able to significantly improve on the performance of Llama-3.2-3B-Instruct:\n",
    "\n",
    "**BEFORE**\n",
    "\n",
    "Accuracy: 31.00%\n",
    "Properly Extracted Accuracy: 46.27%\n",
    "\n",
    "**AFTER**\n",
    "\n",
    "Accuracy: 51.00%\n",
    "Properly Extracted Accuracy: 53.68%\n",
    "\n",
    "A lot of the improvement from using GRPO came because this small LLM learned to better mimic the expected output format of the extractor, but the accuracy for properly extracted samples also improved! This is a great illustration of the kind of task GRPO training excels at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "Now that you know how easy it is to train in Oumi using GRPO, perhaps you'd like to try training on your own data (in a similar data format) -- check out [our docs](https://oumi.ai/docs/en/latest/resources/datasets/sft_datasets.html#using-an-unregistered-dataset-whose-format-is-identical-to-a-registered-dataset) for an easy way to do just that. \n",
    "\n",
    "Have fun!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
