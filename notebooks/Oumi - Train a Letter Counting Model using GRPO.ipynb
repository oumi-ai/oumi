{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"align-center\">\n",
    "<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n",
    "\n",
    "[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n",
    "[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n",
    "[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n",
    "</div>\n",
    "\n",
    "üëã Welcome to Open Universal Machine Intelligence (Oumi)!\n",
    "\n",
    "üöÄ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](https://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n",
    "\n",
    "ü§ù Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n",
    "\n",
    "‚≠ê If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Letter Counting Model using GRPO\n",
    "\n",
    "This notebook focuses on the popular LLM prompt: \"How Many R‚Äôs Are in the Word Strawberry?\". First, we will use a custom evaluation function to evaluate many popular models on the task of counting letters in words. Then, we will use GRPO to train a model to improve its performance on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Machine Requirements\n",
    "\n",
    "‚ùó**NOTICE:** This notebook doesn't run on Colab due to memory requirements.\n",
    "\n",
    "It is recommended to run this notebook on a machine which has a GPU with at least TODO GB VRAM. If your local machine cannot run this notebook, you can instead run this notebook on a cloud platform. The following demonstrates how to open a VSCode instance backed by a GCP node with 4 A100 GPUs, from which the notebook can be run.\n",
    "\n",
    "```bash\n",
    "# Run on your local machine\n",
    "gcloud auth application-default login  # Authenticate with GCP\n",
    "make gcpcode ARGS=\"--resources.accelerators A100:4\"\n",
    "```\n",
    "\n",
    "### Oumi Installation\n",
    "\n",
    "First, let's install Oumi and vLLM. You can find more detailed instructions about Oumi installation [here](https://oumi.ai/docs/en/latest/get_started/installation.html). Here, we include Oumi's GPU dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install oumi[gpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Keys\n",
    "\n",
    "As part of this notebook, you can evaluate frontier models from Open AI, Google, and Anthropic on the letter counting task. If you want to evaluate any of these models, set the corresponding fields below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # Set your OpenAI API key here.\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"\"  # Set your Gemini API key here.\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"\"  # Set your  Anthropic API key here.\n",
    "\n",
    "# Set your GCP project id and region, to be able to query Llama 3.1 405B in Vertex.\n",
    "REGION = \"\"  # Set your GCP region here.\n",
    "PROJECT_ID = \"\"  # Set your GCP project id here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Directory\n",
    "\n",
    "Next, we'll set up a directory to use for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"letter_counting_tutorial\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable warnings from HF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-08 21:32:11,092][oumi][rank0][pid:10161][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: LetterCountGrpoDataset)... dataset_name: 'oumi-ai/oumi-letter-count'\n",
      "[2025-04-08 21:32:14,480][oumi][rank0][pid:10161][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: validation\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 22894322\n",
      "\tDownload size: 5697295\n",
      "\tSize: 28591617 bytes\n",
      "\tRows: 10000\n",
      "\tColumns: ['conversation_id', 'messages', 'metadata']\n",
      "[2025-04-08 21:32:14,598][oumi][rank0][pid:10161][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (10000, 3). Columns:\n",
      "conversation_id    object\n",
      "messages           object\n",
      "metadata           object\n",
      "dtype: object\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluation example:\n",
      "{'conversation_id': 'oumi_letter_count_0',\n",
      " 'messages': [{'content': \"Could you determine the count of 'l's in \"\n",
      "                          \"'substantial'?\",\n",
      "               'role': 'user'},\n",
      "              {'content': 'Your final answer should be written as digits and '\n",
      "                          'formatted as \"\\\\boxed{your_answer}\". For example, '\n",
      "                          'if the answer is 42, make sure to output '\n",
      "                          '\"\\\\boxed{42}\".',\n",
      "               'role': 'system'}],\n",
      " 'metadata': {'letter': 'l',\n",
      "              'letter_count_integer': 1,\n",
      "              'letter_count_string': 'one',\n",
      "              'unformatted_prompt': 'Could you determine the count of '\n",
      "                                    '{letter}s in {word}?',\n",
      "              'word': 'substantial'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from oumi.datasets.grpo.letter_count import LetterCountGrpoDataset\n",
    "\n",
    "dataset = LetterCountGrpoDataset(split=\"validation\")\n",
    "print(\"-\" * 80)\n",
    "# print(\"Raw example:\")\n",
    "# pprint(dataset.raw(0).to_dict())\n",
    "print(\"Evaluation example:\")\n",
    "pprint(dataset.conversation(0).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "See custom evaluation function at `src/oumi/evaluation/registry/count_letters_task.py`.\n",
    "TODO: Can we print its contents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 5\n",
    "# NUM_SAMPLES = 100\n",
    "\n",
    "model_names = [\n",
    "    \"llama_3b\",\n",
    "    # Uncomment any models you wish to evaluate - you can evaluate multiple at once.\n",
    "    # \"gpt_4o\",\n",
    "    # \"o1_preview\",\n",
    "    # \"gemini_pro\",\n",
    "    # \"llama_405b\",\n",
    "    # \"claude_sonnet\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"llama_3b\": \"\"\"\n",
    "      model:\n",
    "        model_name: \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "        # model_name: \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "        model_max_length: 131072\n",
    "        torch_dtype_str: \"bfloat16\"\n",
    "        attn_implementation: \"sdpa\"\n",
    "        trust_remote_code: True\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 2048\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: count_letters\n",
    "\n",
    "      # inference_engine: VLLM\n",
    "      output_dir: \"letter_counting_tutorial/evaluation\"\n",
    "      \"\"\",\n",
    "    \"gpt_4o\": \"\"\"\n",
    "      model:\n",
    "        model_name: \"gpt-4o\"\n",
    "\n",
    "      inference_engine: OPENAI\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_key_env_varname: \"OPENAI_API_KEY\"\n",
    "        max_retries: 3\n",
    "        num_workers: 100\n",
    "        politeness_policy: 60\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: hallucination_classification\n",
    "      \"\"\",\n",
    "    \"o1_preview\": \"\"\"\n",
    "      model:\n",
    "        model_name: \"o1-preview\"\n",
    "\n",
    "      inference_engine: OPENAI\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_key_env_varname: \"OPENAI_API_KEY\"\n",
    "        max_retries: 3\n",
    "        num_workers: 100\n",
    "        politeness_policy: 60\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 1.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: hallucination_classification\n",
    "      \"\"\",\n",
    "    \"gemini_pro\": \"\"\"\n",
    "      model:\n",
    "        model_name: \"gemini-2.5-pro-preview-03-25\"\n",
    "\n",
    "      inference_engine: GOOGLE_GEMINI\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_key_env_varname: \"GEMINI_API_KEY\"\n",
    "        max_retries: 3\n",
    "        num_workers: 2\n",
    "        politeness_policy: 60\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: hallucination_classification\n",
    "      \"\"\",\n",
    "    \"llama_405b\": f\"\"\"\n",
    "      model:\n",
    "        model_name: \"meta/llama-3.1-405b-instruct-maas\"\n",
    "\n",
    "      inference_engine: GOOGLE_VERTEX\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_url: \"https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/openapi/chat/completions\"\n",
    "        max_retries: 3\n",
    "        num_workers: 10\n",
    "        politeness_policy: 60\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: hallucination_classification\n",
    "      \"\"\",\n",
    "    \"claude_sonnet\": \"\"\"\n",
    "      model:\n",
    "        model_name: \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "      inference_engine: ANTHROPIC\n",
    "\n",
    "      inference_remote_params:\n",
    "        api_key_env_varname: \"ANTHROPIC_API_KEY\"\n",
    "        max_retries: 3\n",
    "        num_workers: 5\n",
    "        politeness_policy: 65\n",
    "        connection_timeout: 300\n",
    "\n",
    "      generation:\n",
    "        max_new_tokens: 8192\n",
    "        temperature: 0.0\n",
    "\n",
    "      tasks:\n",
    "        - evaluation_backend: custom\n",
    "          task_name: hallucination_classification\n",
    "      \"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.core.configs import EvaluationConfig\n",
    "from oumi.core.evaluation import Evaluator\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    # Create the evaluation config from the YAML string.\n",
    "    config_yaml: str = configs[model_name]\n",
    "    config = EvaluationConfig.from_str(config_yaml)\n",
    "    config.tasks[0].num_samples = NUM_SAMPLES\n",
    "\n",
    "    # Run the evaluation.\n",
    "    evaluator = Evaluator()\n",
    "    evaluator_out = evaluator.evaluate(config)\n",
    "\n",
    "    # # Record the results.\n",
    "    results[model_name] = evaluator_out[0].get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total samples: {NUM_SAMPLES}\")\n",
    "for model_name, result in results.items():\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {result['accuracy']}\")\n",
    "    correct = result[\"num_correct_answers\"]\n",
    "    incorrect = result[\"num_incorrect_answers\"]\n",
    "    invalid = result[\"num_invalid_answers\"]\n",
    "    print(f\"Num correct, incorrect, invalid: {correct}, {incorrect}, {invalid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO\n",
    "\n",
    "Set `training.enable_wandb` to True if you want to log your training run to Weights and Biases. In addition, you must also log into WandB, ex. by running `wandb login`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/train.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "  model_max_length: 8192\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  attn_implementation: \"sdpa\"\n",
    "\n",
    "data:\n",
    "  train:\n",
    "    datasets:\n",
    "      - dataset_name: \"oumi-ai/oumi-letter-count\"\n",
    "        split: \"train\"\n",
    "\n",
    "training:\n",
    "  trainer_type: \"TRL_GRPO\"\n",
    "  save_steps: 500\n",
    "  max_steps: 500\n",
    "  per_device_train_batch_size: 2\n",
    "  gradient_accumulation_steps: 1\n",
    "  learning_rate: 5e-5\n",
    "\n",
    "  reward_functions: [\"count_letters\"]\n",
    "\n",
    "  ddp_find_unused_parameters: False\n",
    "  optimizer: \"adafactor\"\n",
    "  compile: True\n",
    "\n",
    "  grpo:\n",
    "    num_generations: 4\n",
    "\n",
    "  dataloader_num_workers: \"auto\"\n",
    "  dataloader_prefetch_factor: 32\n",
    "\n",
    "  logging_steps: 10\n",
    "  output_dir: \"letter_counting_tutorial/llama_3b_grpo\"\n",
    "  # Set this to True if you want to log to Weights and Biases.\n",
    "  enable_wandb: False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our Trained Model\n",
    "\n",
    "Let's now evaluate our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the evaluation config from the YAML string.\n",
    "config_yaml: str = configs[\"llama_3b\"]\n",
    "config = EvaluationConfig.from_str(config_yaml)\n",
    "config.tasks[0].num_samples = NUM_SAMPLES\n",
    "config.model.model_name = \"letter_counting_tutorial/llama_3b_grpo\"\n",
    "\n",
    "# Run the evaluation.\n",
    "evaluator = Evaluator()\n",
    "evaluator_out = evaluator.evaluate(config)\n",
    "\n",
    "# # Record the results.\n",
    "trained_model_results = evaluator_out[0].get_results()\n",
    "\n",
    "print(f\"Accuracy: {trained_model_results['accuracy']}\")\n",
    "correct = trained_model_results[\"num_correct_answers\"]\n",
    "incorrect = trained_model_results[\"num_incorrect_answers\"]\n",
    "invalid = trained_model_results[\"num_invalid_answers\"]\n",
    "print(f\"Num correct, incorrect, invalid: {correct}, {incorrect}, {invalid}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
