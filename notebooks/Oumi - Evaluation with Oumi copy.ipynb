{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"align-center\">\n",
    "<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n",
    "\n",
    "[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n",
    "[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n",
    "[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - Evaluation with Oumi.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>\n",
    "\n",
    "üëã Welcome to Open Universal Machine Intelligence (Oumi)!\n",
    "\n",
    "üöÄ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](https://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n",
    "\n",
    "ü§ù Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n",
    "\n",
    "‚≠ê If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation with Oumi\n",
    "\n",
    "This notebook provides a guide for running end-to-end evaluations on your trained model using Oumi. Specifically, it explores how the performance of LLaMA models evolves as we scale from 1B to 3B and 8B parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Environment\n",
    "\n",
    "‚ùó**NOTICE:** We recommend running this notebook on a GPU. If running on Google Colab, you can use the free T4 GPU runtime (Colab Menu: `Runtime` -> `Change runtime type`). Llama 8B is too large to fit on the T4 GPU for inference. You can remove it from the list of models to evaluate, or evaluate the SmolLM family of models instead of Llama in the `Experimental Setup` section.\n",
    "\n",
    "### Oumi Installation\n",
    "\n",
    "First, let's install Oumi, AlpacaEval, and vLLM. You can find more detailed instructions about Oumi installation [here](https://oumi.ai/docs/en/latest/get_started/installation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install oumi alpaca_eval vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama Access\n",
    "\n",
    "Llama models are gated on HuggingFace Hub. To run this notebook, you must first complete the agreements for Llama [3.1](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) and [3.2](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) on HuggingFace, and wait for them to be accepted. Then, specify `HF_TOKEN` below to enable access to the model if it's not already set.\n",
    "\n",
    "Usually, you can get the token by running this command `cat ~/.cache/huggingface/token` on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# if not os.environ.get(\"HF_TOKEN\"):\n",
    "#     # NOTE: Set your Hugging Face token here if not already set.\n",
    "#     os.environ[\"HF_TOKEN\"] = \"<MY_HF_TOKEN>\"\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "# print(f\"Using HF Token: '{hf_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Access\n",
    "\n",
    "AlpacaEval 2.0 calculates win rates by comparing a model's responses to reference responses. This process requires an annotator, with the default being [GPT-4 Turbo](https://github.com/tatsu-lab/alpaca_eval?tab=readme-ov-file#alpacaeval-20). To access the latest GPT-4 models, an OpenAI API key is necessary. You can find instructions for creating an OpenAI account and generating an API key on [OpenAI's quickstart webpage](https://platform.openai.com/docs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"OPENAI_API_KEY\"] = \"<MY_OPENAI_TOKEN>\"  # Specify your OpenAI API key here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>‚ö†Ô∏è Cost considerations</b>: The cost of running a standard AlpacaEval 2.0 evaluation and annotating 805 examples with GPT-4 Turbo is <b>$3.50</b>. You need to evaluate on the full dataset to reproduce the results presented in this notebook. However, if you are only interested in experimenting with the API, you can limit the evaluation to a smaller number of examples. For instance, annotating just 3 examples will cost less than <b>0.5¬¢</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ALPACA_EXAMPLES = 3  # Replace with None for full dataset evaluation.\n",
    "NUM_MMLU_EXAMPLES = 10  # Replace with None for full dataset evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Directory Setup\n",
    "\n",
    "Finally, we will create a directory for the tutorial to store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"./output/evaluation_tutorial/TEST_OUMI\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "In this experiment, we aim to compare the performance of models as we scale their size. For this purpose, we have selected LLaMA models with `1B`, `3B`, and `8B` parameters. In the code snippet below, you can specify any model hosted on HuggingFace, provide a path to a local directory containing your model, or use any other model format supported by Oumi inference. Additionally, note that we restrict the maximum number of tokens for each model to `8192` in order to optimize both cost and resource usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = [\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "]\n",
    "MODEL_MAX_TOKENS = 8192\n",
    "\n",
    "# Alternative models to evaluate on Colab.\n",
    "# MODEL_NAMES = [\n",
    "#     \"HuggingFaceTB/SmolLM-135M-Instruct\",\n",
    "#     \"HuggingFaceTB/SmolLM-360M-Instruct\",\n",
    "#     \"HuggingFaceTB/SmolLM-1.7B-Instruct\",\n",
    "# ]\n",
    "# MODEL_MAX_TOKENS = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct a comprehensive evaluation of a model's performance, it is essential to assess it across a [diverse set of benchmarks](https://oumi.ai/docs/en/latest/user_guides/evaluate/standardized_benchmarks.html#trade-offs). At a minimum, a [standardized benchmark](https://oumi.ai/docs/en/latest/user_guides/evaluate/standardized_benchmarks.html) is required to evaluate the model's knowledge and reasoning capabilities, while a [generative benchmark](https://oumi.ai/docs/en/latest/user_guides/evaluate/generative_benchmarks.html) is necessary to assess the quality of responses and its ability to follow instructions. In this evaluation, we select [MMLU Pro](https://arxiv.org/abs/2406.01574) for its focus on challenging, reasoning-intensive knowledge tasks and [AlpacaEval 2.0](https://arxiv.org/abs/2404.04475) for assessing the overall quality of the generated responses. To customize the evaluation tasks, please see [our documentation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluation_config.html#configuration-options). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.core.configs import EvaluationTaskParams\n",
    "\n",
    "TASKS = [\n",
    "    EvaluationTaskParams(\n",
    "        evaluation_backend=\"lm_harness\",\n",
    "        task_name=\"leaderboard_mmlu_pro\",\n",
    "        num_samples=NUM_MMLU_EXAMPLES,\n",
    "    ),\n",
    "    EvaluationTaskParams(\n",
    "        evaluation_backend=\"alpaca_eval\",\n",
    "        num_samples=NUM_ALPACA_EXAMPLES,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To perform evaluations with Oumi, we define an [EvaluationConfig](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluation_config.html) for each model with the tasks discussed above, along with the relevant [model](https://github.com/oumi-ai/oumi/blob/main/src/oumi/core/configs/params/model_params.py) and [generation](https://github.com/oumi-ai/oumi/blob/main/src/oumi/core/configs/params/generation_params.py) parameters. If the system is equipped with a GPU, we strongly recommend configuring the [inference engine](https://oumi.ai/docs/en/latest/api/oumi.core.configs.html#oumi.core.configs.InferenceEngineType) to `VLLM` for optimal performance.\n",
    "\n",
    "The code snippet below illustrates how to run evaluations for our three models, storing the key metrics for each model in lists. Specifically, the length-controlled win rate for Alpaca is saved in `alpaca_lcwr`, while the accuracy for MMLU Pro is stored in `mmlu_acc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-01 18:17:00 [__init__.py:239] Automatically detected platform cuda.\n",
      "Evaluating meta-llama/Llama-3.2-1B-Instruct...\n",
      "[2025-07-01 18:17:14,205][oumi][rank0][pid:2181942][MainThread][INFO]][lm_harness.py:331] \tLM Harness `task_params`:\n",
      "LMHarnessTaskParams(evaluation_backend='lm_harness',\n",
      "                    task_name='leaderboard_mmlu_pro',\n",
      "                    num_samples=10,\n",
      "                    log_samples=False,\n",
      "                    eval_kwargs={},\n",
      "                    evaluation_platform='',\n",
      "                    num_fewshot=None)\n",
      "[2025-07-01 18:17:14,207][oumi][rank0][pid:2181942][MainThread][INFO]][lm_harness.py:332] \tLM Harness `task_dict`:\n",
      "{'leaderboard_mmlu_pro': ConfigurableTask(task_name=leaderboard_mmlu_pro,output_type=multiple_choice,num_fewshot=None,num_samples=12032)}\n",
      "[2025-07-01 18:17:14,210][oumi][rank0][pid:2181942][MainThread][INFO]][lm_harness.py:344] \tLM Harness `model_params`:\n",
      "{'batch_size': 1,\n",
      " 'device': 'cuda:0',\n",
      " 'dtype': torch.bfloat16,\n",
      " 'max_batch_size': None,\n",
      " 'max_length': 8192,\n",
      " 'pretrained': 'meta-llama/Llama-3.2-1B-Instruct',\n",
      " 'trust_remote_code': False}\n",
      "INFO 07-01 18:17:23 [config.py:600] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "INFO 07-01 18:17:23 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 07-01 18:17:24 [utils.py:2273] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 07-01 18:17:29 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 07-01 18:17:31 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 07-01 18:17:32 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7e005348b3d0>\n",
      "INFO 07-01 18:17:32 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 07-01 18:17:32 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-01 18:17:32 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.2-1B-Instruct...\n",
      "WARNING 07-01 18:17:32 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-01 18:17:33 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 07-01 18:17:33 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.74it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.74it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-01 18:17:33 [loader.py:447] Loading weights took 0.39 seconds\n",
      "INFO 07-01 18:17:33 [gpu_model_runner.py:1273] Model loading took 2.3185 GiB and 0.717626 seconds\n",
      "INFO 07-01 18:17:38 [backends.py:416] Using cache directory: /home/shanghong/.cache/vllm/torch_compile_cache/f218a09398/rank_0_0 for vLLM's torch.compile\n",
      "INFO 07-01 18:17:38 [backends.py:426] Dynamo bytecode transform time: 4.55 s\n",
      "INFO 07-01 18:17:38 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 07-01 18:17:41 [monitor.py:33] torch.compile takes 4.55 s in total\n",
      "INFO 07-01 18:17:41 [kv_cache_utils.py:578] GPU KV cache size: 2,081,040 tokens\n",
      "INFO 07-01 18:17:41 [kv_cache_utils.py:581] Maximum concurrency for 8,192 tokens per request: 254.03x\n",
      "INFO 07-01 18:17:57 [gpu_model_runner.py:1608] Graph capturing finished in 15 secs, took 0.42 GiB\n",
      "INFO 07-01 18:17:57 [core.py:162] init engine (profile, create kv cache, warmup model) took 23.36 seconds\n",
      "[2025-07-01 18:17:57,476][oumi][rank0][pid:2181942][MainThread][INFO]][lm_harness.py:348] Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lm_eval.api.task:Building contexts for leaderboard_mmlu_pro on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 27906.21it/s]\n",
      "INFO:lm_eval.evaluator:Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:00<00:00, 182.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 18:17:58,063][oumi][rank0][pid:2181942][MainThread][INFO]][lm_harness.py:366] leaderboard_mmlu_pro's metric dict is {'acc,none': 0.0, 'acc_stderr,none': 0.0, 'alias': 'leaderboard_mmlu_pro'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W701 18:18:02.176686034 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 18:18:04,043][oumi][rank0][pid:2181942][MainThread][WARNING]][serialization_utils.py:47] Non-serializable value `LMHarnessTaskParams(evaluation_backend='lm_harness', task_name='leaderboard_mmlu_pro', num_samples=10, log_samples=False, eval_kwargs={}, evaluation_platform='', num_fewshot=None)` of type `<class 'oumi.core.configs.params.evaluation_params.LMHarnessTaskParams'>`.\n",
      "[2025-07-01 18:18:04,045][oumi][rank0][pid:2181942][MainThread][WARNING]][serialization_utils.py:47] Non-serializable value `ConfigurableTask(task_name=leaderboard_mmlu_pro,output_type=multiple_choice,num_fewshot=None,num_samples=12032)` of type `<class 'lm_eval.api.task.ConfigurableTask'>`.\n",
      "[2025-07-01 18:18:04,755][oumi][rank0][pid:2181942][MainThread][WARNING]][models.py:463] Undefined pad token. Setting it to `<|finetune_right_pad_id|>`.\n",
      "[2025-07-01 18:18:04,757][oumi][rank0][pid:2181942][MainThread][INFO]][models.py:506] Using the model's built-in chat template for model 'meta-llama/Llama-3.2-1B-Instruct'.\n",
      "INFO 07-01 18:18:04 [config.py:600] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "INFO 07-01 18:18:04 [config.py:1600] Defaulting to use mp for distributed inference\n",
      "INFO 07-01 18:18:04 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 07-01 18:18:04 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-01 18:18:11 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 07-01 18:18:14 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 07-01 18:18:14 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 07-01 18:18:14 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_4835807d'), local_subscribe_addr='ipc:///tmp/7c6488f3-06dc-4314-9a64-431d3dec2073', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-01 18:18:19 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 07-01 18:18:22 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x774f28242d10>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2183355)\u001b[0;0m INFO 07-01 18:18:22 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b7c3cceb'), local_subscribe_addr='ipc:///tmp/13656a73-8427-4321-9357-99c0384b2b18', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-01 18:18:26 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 07-01 18:18:29 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7601142c5b50>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2183475)\u001b[0;0m INFO 07-01 18:18:29 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_49b60d29'), local_subscribe_addr='ipc:///tmp/8e8a243e-7613-40f0-a42a-7aa7cd4a28e4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2183475)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=2183355)\u001b[0;0m INFO 07-01 18:18:30 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2183355)\u001b[0;0m INFO 07-01 18:18:30 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 07-01 18:18:30 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2183475)\u001b[0;0m INFO 07-01 18:18:30 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2183475)\u001b[0;0m INFO 07-01 18:18:31 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2183355)\u001b[0;0m INFO 07-01 18:18:31 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2183355)\u001b[0;0m INFO 07-01 18:18:31 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_ee500267'), local_subscribe_addr='ipc:///tmp/16e88875-e0b9-4898-b85b-2deaba395bb0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2183355)\u001b[0;0m INFO 07-01 18:18:31 [parallel_state.py:957] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2183355)\u001b[0;0m INFO 07-01 18:18:31 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2183475)\u001b[0;0m INFO 07-01 18:18:31 [parallel_state.py:957] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2183475)\u001b[0;0m INFO 07-01 18:18:31 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2183355)\u001b[0;0m INFO 07-01 18:18:31 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.2-1B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2183475)\u001b[0;0m INFO 07-01 18:18:31 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.2-1B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2183355)\u001b[0;0m WARNING 07-01 18:18:32 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2183475)\u001b[0;0m WARNING 07-01 18:18:32 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2183355)\u001b[0;0m INFO 07-01 18:18:32 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2183475)\u001b[0;0m INFO 07-01 18:18:32 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2183355)\u001b[0;0m INFO 07-01 18:18:32 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=2183475)\u001b[0;0m INFO 07-01 18:18:32 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.68it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.66it/s]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2183355)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=2183355)\u001b[0;0m INFO 07-01 18:18:32 [loader.py:447] Loading weights took 0.30 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2183475)\u001b[0;0m INFO 07-01 18:18:32 [loader.py:447] Loading weights took 0.36 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2183355)\u001b[0;0m INFO 07-01 18:18:32 [gpu_model_runner.py:1273] Model loading took 1.1667 GiB and 0.559896 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2183475)\u001b[0;0m INFO 07-01 18:18:32 [gpu_model_runner.py:1273] Model loading took 1.1667 GiB and 0.829921 seconds\n",
      "INFO 07-01 18:18:38 [kv_cache_utils.py:578] GPU KV cache size: 4,140,512 tokens\n",
      "INFO 07-01 18:18:38 [kv_cache_utils.py:581] Maximum concurrency for 8,192 tokens per request: 505.43x\n",
      "INFO 07-01 18:18:38 [kv_cache_utils.py:578] GPU KV cache size: 4,140,512 tokens\n",
      "INFO 07-01 18:18:38 [kv_cache_utils.py:581] Maximum concurrency for 8,192 tokens per request: 505.43x\n",
      "INFO 07-01 18:18:38 [core.py:162] init engine (profile, create kv cache, warmup model) took 5.81 seconds\n",
      "Python executable: /home/shanghong/miniconda3/envs/oumi/bin/python\n",
      "sys.path: ['/home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/ray/thirdparty_files', '/home/shanghong/miniconda3/envs/oumi/lib/python311.zip', '/home/shanghong/miniconda3/envs/oumi/lib/python3.11', '/home/shanghong/miniconda3/envs/oumi/lib/python3.11/lib-dynload', '', '/home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages', '/home/shanghong/oumi/src', '/tmp/tmp91n8vu4b', '/home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/setuptools/_vendor']\n",
      "[2025-07-01 18:18:39,914][oumi][rank0][pid:2181942][MainThread][INFO]][alpaca_eval.py:97] Loading the `tatsu-lab/alpaca_eval` dataset.\n",
      "[2025-07-01 18:18:39,917][oumi][rank0][pid:2181942][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: AlpacaEvalDataset)... dataset_name: 'tatsu-lab/alpaca_eval'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd1b942ea2141eab3e825f54aa8aec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/30.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068553ae045244fc972cc2989d42b181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "alpaca_eval.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9022523d84442aa9acb25187ac259a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "alpaca_eval.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shanghong/.cache/huggingface/hub/datasets--tatsu-lab--alpaca_eval/snapshots/2edc6fad8be6b14ea7230aabfd08188da6b8b814/alpaca_eval.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f179a3f637c1431fab205d6e947cbca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating eval split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 18:18:41,051][oumi][rank0][pid:2181942][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: eval\n",
      "\tVersion: 1.0.0\n",
      "\tDataset size: 554496\n",
      "\tDownload size: 620778\n",
      "\tSize: 1175274 bytes\n",
      "\tRows: 805\n",
      "\tColumns: ['instruction', 'output', 'generator', 'dataset']\n",
      "[2025-07-01 18:18:41,700][oumi][rank0][pid:2181942][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (805, 4). Columns:\n",
      "instruction    object\n",
      "output         object\n",
      "generator      object\n",
      "dataset        object\n",
      "dtype: object\n",
      "[2025-07-01 18:18:41,742][oumi][rank0][pid:2181942][MainThread][INFO]][alpaca_eval.py:106] \tAlpacaEval inference `model_params`:\n",
      "ModelParams(model_name='meta-llama/Llama-3.2-1B-Instruct',\n",
      "            adapter_model=None,\n",
      "            tokenizer_name=None,\n",
      "            tokenizer_pad_token=None,\n",
      "            tokenizer_kwargs={},\n",
      "            processor_kwargs={},\n",
      "            model_max_length=8192,\n",
      "            load_pretrained_weights=True,\n",
      "            trust_remote_code=False,\n",
      "            torch_dtype_str='bfloat16',\n",
      "            compile=False,\n",
      "            chat_template=None,\n",
      "            attn_implementation=None,\n",
      "            device_map='auto',\n",
      "            model_kwargs={},\n",
      "            enable_liger_kernel=False,\n",
      "            shard_for_eval=False,\n",
      "            freeze_layers=[],\n",
      "            model_revision=None)\n",
      "\tAlpacaEval inference `generation_params`:\n",
      "GenerationParams(max_new_tokens=8192,\n",
      "                 batch_size=1,\n",
      "                 exclude_prompt_from_response=True,\n",
      "                 seed=None,\n",
      "                 temperature=0.0,\n",
      "                 top_p=1.0,\n",
      "                 frequency_penalty=0.0,\n",
      "                 presence_penalty=0.0,\n",
      "                 stop_strings=None,\n",
      "                 stop_token_ids=None,\n",
      "                 logit_bias={},\n",
      "                 min_p=0.0,\n",
      "                 use_cache=False,\n",
      "                 num_beams=1,\n",
      "                 use_sampling=False,\n",
      "                 guided_decoding=None)\n",
      "INFO 07-01 18:18:42 [chat_utils.py:396] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:25<00:00,  8.51s/it, est. speed input: 6.47 toks/s, output: 90.59 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 18:19:08,145][oumi][rank0][pid:2181942][MainThread][INFO]][alpaca_eval.py:121] Running AlpacaEval annotation.\n",
      "[2025-07-01 18:19:08,149][oumi][rank0][pid:2181942][MainThread][INFO]][alpaca_eval.py:122] \tAlpacaEval `task_params`:\n",
      "AlpacaEvalTaskParams(evaluation_backend='alpaca_eval',\n",
      "                     task_name=None,\n",
      "                     num_samples=3,\n",
      "                     log_samples=False,\n",
      "                     eval_kwargs={},\n",
      "                     evaluation_platform='',\n",
      "                     version=2.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132906a6d8364de4a29e6d3c456f2e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "alpaca_eval_gpt4_baseline.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shanghong/.cache/huggingface/hub/datasets--tatsu-lab--alpaca_eval/snapshots/2edc6fad8be6b14ea7230aabfd08188da6b8b814/alpaca_eval_gpt4_baseline.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9882dd316b24ef39a9fbb268eadd6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating eval split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating the 20250701_181839 outputs.\n",
      "WARNING:root:model_outputs and reference_outputs have different lengths, so we cannot shuffle before taking the first max_instances.\n",
      "INFO:root:Creating the annotator from `weighted_alpaca_eval_gpt4_turbo`.\n",
      "INFO:root:Saving annotations to `/home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json`.\n",
      "Annotation chunk:   0%|          | 0/1 [00:00<?, ?it/s]INFO:root:Annotating 3 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 3 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "WARNING:root:/home/shanghong/miniconda3/envs/oumi/lib/python3.11/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/home/shanghong/miniconda3/envs/oumi/lib/python3.11/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   0%|          | 0/3 [00:00<?, ?it/s]WARNING:root:/home/shanghong/miniconda3/envs/oumi/lib/python3.11/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "prompt_batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.21s/it]\n",
      "INFO:root:Completed 3 examples in 3.7 seconds.\n",
      "INFO:root:Saving all annotations to /home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "Annotation chunk: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.75s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879153095669487ea06dd39b1f337149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "df_gamed.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saving all results to results/20250701_181839/weighted_alpaca_eval_gpt4_turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 18:19:13,095][oumi][rank0][pid:2181942][MainThread][INFO]][alpaca_eval.py:145] AlpacaEval's metric dict is {'avg_length': 3131,\n",
      " 'discrete_win_rate': 0.0,\n",
      " 'lc_standard_error': 0.00788971350387147,\n",
      " 'length_controlled_winrate': 0.030004701563040123,\n",
      " 'mode': 'community',\n",
      " 'n_draws': 0,\n",
      " 'n_total': 3,\n",
      " 'n_wins': 0,\n",
      " 'n_wins_base': 3,\n",
      " 'standard_error': 0.041278944510150066,\n",
      " 'win_rate': 0.04327652582661508}.\n",
      "Evaluating meta-llama/Llama-3.2-3B-Instruct...\n",
      "[2025-07-01 18:19:30,096][oumi][rank0][pid:2181942][MainThread][INFO]][lm_harness.py:331] \tLM Harness `task_params`:\n",
      "LMHarnessTaskParams(evaluation_backend='lm_harness',\n",
      "                    task_name='leaderboard_mmlu_pro',\n",
      "                    num_samples=10,\n",
      "                    log_samples=False,\n",
      "                    eval_kwargs={},\n",
      "                    evaluation_platform='',\n",
      "                    num_fewshot=None)\n",
      "[2025-07-01 18:19:30,097][oumi][rank0][pid:2181942][MainThread][INFO]][lm_harness.py:332] \tLM Harness `task_dict`:\n",
      "{'leaderboard_mmlu_pro': ConfigurableTask(task_name=leaderboard_mmlu_pro,output_type=multiple_choice,num_fewshot=None,num_samples=12032)}\n",
      "[2025-07-01 18:19:30,098][oumi][rank0][pid:2181942][MainThread][INFO]][lm_harness.py:344] \tLM Harness `model_params`:\n",
      "{'batch_size': 1,\n",
      " 'device': 'cuda:0',\n",
      " 'dtype': torch.bfloat16,\n",
      " 'max_batch_size': None,\n",
      " 'max_length': 8192,\n",
      " 'pretrained': 'meta-llama/Llama-3.2-3B-Instruct',\n",
      " 'trust_remote_code': False}\n",
      "INFO 07-01 18:19:30 [config.py:600] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "INFO 07-01 18:19:30 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 07-01 18:19:38 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 07-01 18:19:43 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 07-01 18:19:44 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x72db22bdbcd0>\n",
      "INFO 07-01 18:19:46 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 07-01 18:19:46 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-01 18:19:46 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "WARNING 07-01 18:19:46 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-01 18:19:46 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.00it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.36it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-01 18:19:48 [loader.py:447] Loading weights took 1.50 seconds\n",
      "INFO 07-01 18:19:48 [gpu_model_runner.py:1273] Model loading took 6.0160 GiB and 2.004073 seconds\n",
      "INFO 07-01 18:19:58 [backends.py:416] Using cache directory: /home/shanghong/.cache/vllm/torch_compile_cache/36786aec49/rank_0_0 for vLLM's torch.compile\n",
      "INFO 07-01 18:19:58 [backends.py:426] Dynamo bytecode transform time: 9.48 s\n",
      "INFO 07-01 18:19:58 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 07-01 18:20:04 [monitor.py:33] torch.compile takes 9.48 s in total\n",
      "INFO 07-01 18:20:04 [kv_cache_utils.py:578] GPU KV cache size: 559,552 tokens\n",
      "INFO 07-01 18:20:04 [kv_cache_utils.py:581] Maximum concurrency for 8,192 tokens per request: 68.30x\n",
      "INFO 07-01 18:20:20 [gpu_model_runner.py:1608] Graph capturing finished in 15 secs, took 0.57 GiB\n",
      "INFO 07-01 18:20:20 [core.py:162] init engine (profile, create kv cache, warmup model) took 31.57 seconds\n",
      "[2025-07-01 18:20:20,660][oumi][rank0][pid:2181942][MainThread][INFO]][lm_harness.py:348] Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lm_eval.api.task:Building contexts for leaderboard_mmlu_pro on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 23160.15it/s]\n",
      "INFO:lm_eval.evaluator:Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:00<00:00, 109.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 18:20:21,606][oumi][rank0][pid:2181942][MainThread][INFO]][lm_harness.py:366] leaderboard_mmlu_pro's metric dict is {'acc,none': 0.3,\n",
      " 'acc_stderr,none': 0.15275252316519464,\n",
      " 'alias': 'leaderboard_mmlu_pro'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W701 18:20:26.242481799 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 18:20:28,158][oumi][rank0][pid:2181942][MainThread][WARNING]][serialization_utils.py:47] Non-serializable value `LMHarnessTaskParams(evaluation_backend='lm_harness', task_name='leaderboard_mmlu_pro', num_samples=10, log_samples=False, eval_kwargs={}, evaluation_platform='', num_fewshot=None)` of type `<class 'oumi.core.configs.params.evaluation_params.LMHarnessTaskParams'>`.\n",
      "[2025-07-01 18:20:28,159][oumi][rank0][pid:2181942][MainThread][WARNING]][serialization_utils.py:47] Non-serializable value `ConfigurableTask(task_name=leaderboard_mmlu_pro,output_type=multiple_choice,num_fewshot=None,num_samples=12032)` of type `<class 'lm_eval.api.task.ConfigurableTask'>`.\n",
      "[2025-07-01 18:20:28,894][oumi][rank0][pid:2181942][MainThread][WARNING]][models.py:463] Undefined pad token. Setting it to `<|finetune_right_pad_id|>`.\n",
      "[2025-07-01 18:20:28,896][oumi][rank0][pid:2181942][MainThread][INFO]][models.py:506] Using the model's built-in chat template for model 'meta-llama/Llama-3.2-3B-Instruct'.\n",
      "INFO 07-01 18:20:29 [config.py:600] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "INFO 07-01 18:20:29 [config.py:1600] Defaulting to use mp for distributed inference\n",
      "INFO 07-01 18:20:29 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 07-01 18:20:29 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-01 18:20:35 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 07-01 18:20:37 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 07-01 18:20:37 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 07-01 18:20:37 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_a8d846c8'), local_subscribe_addr='ipc:///tmp/f09d569c-4dfe-4229-ad6e-4b53b3364fa0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-01 18:20:44 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 07-01 18:20:47 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x799a9d4e72d0>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2185435)\u001b[0;0m INFO 07-01 18:20:47 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_694a9b70'), local_subscribe_addr='ipc:///tmp/3b3d1e57-925c-4f45-a956-babfc22797a8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-01 18:20:53 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 07-01 18:20:56 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7e2628e5b290>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2185576)\u001b[0;0m INFO 07-01 18:20:56 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b27721b0'), local_subscribe_addr='ipc:///tmp/7eb21046-9b7f-4e3d-9fae-0c4e7b14d0df', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2185576)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=2185435)\u001b[0;0m INFO 07-01 18:20:56 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2185435)\u001b[0;0m INFO 07-01 18:20:56 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2185576)\u001b[0;0m INFO 07-01 18:20:56 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 07-01 18:20:56 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2185435)\u001b[0;0m INFO 07-01 18:20:58 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2185576)\u001b[0;0m INFO 07-01 18:20:58 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2185435)\u001b[0;0m INFO 07-01 18:20:58 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_13f0e4b4'), local_subscribe_addr='ipc:///tmp/646c742c-24a7-4e06-aada-5a23cdefbdb4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2185435)\u001b[0;0m INFO 07-01 18:20:58 [parallel_state.py:957] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2185435)\u001b[0;0m INFO 07-01 18:20:58 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2185576)\u001b[0;0m INFO 07-01 18:20:58 [parallel_state.py:957] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2185576)\u001b[0;0m INFO 07-01 18:20:58 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2185435)\u001b[0;0m INFO 07-01 18:20:58 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2185576)\u001b[0;0m INFO 07-01 18:20:58 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2185576)\u001b[0;0m WARNING 07-01 18:20:58 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2185435)\u001b[0;0m WARNING 07-01 18:20:58 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2185576)\u001b[0;0m INFO 07-01 18:20:58 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2185435)\u001b[0;0m INFO 07-01 18:20:58 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=2185576)\u001b[0;0m INFO 07-01 18:21:00 [loader.py:447] Loading weights took 1.51 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.50s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.01s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.09s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2185435)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=2185435)\u001b[0;0m INFO 07-01 18:21:01 [loader.py:447] Loading weights took 2.20 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2185576)\u001b[0;0m INFO 07-01 18:21:01 [gpu_model_runner.py:1273] Model loading took 3.0512 GiB and 2.093744 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2185435)\u001b[0;0m INFO 07-01 18:21:01 [gpu_model_runner.py:1273] Model loading took 3.0512 GiB and 2.902773 seconds\n",
      "INFO 07-01 18:21:06 [kv_cache_utils.py:578] GPU KV cache size: 1,146,528 tokens\n",
      "INFO 07-01 18:21:06 [kv_cache_utils.py:581] Maximum concurrency for 8,192 tokens per request: 139.96x\n",
      "INFO 07-01 18:21:06 [kv_cache_utils.py:578] GPU KV cache size: 1,146,528 tokens\n",
      "INFO 07-01 18:21:06 [kv_cache_utils.py:581] Maximum concurrency for 8,192 tokens per request: 139.96x\n",
      "INFO 07-01 18:21:06 [core.py:162] init engine (profile, create kv cache, warmup model) took 5.28 seconds\n",
      "Python executable: /home/shanghong/miniconda3/envs/oumi/bin/python\n",
      "sys.path: ['/home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/ray/thirdparty_files', '/home/shanghong/miniconda3/envs/oumi/lib/python311.zip', '/home/shanghong/miniconda3/envs/oumi/lib/python3.11', '/home/shanghong/miniconda3/envs/oumi/lib/python3.11/lib-dynload', '', '/home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages', '/home/shanghong/oumi/src', '/tmp/tmp91n8vu4b', '/home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/setuptools/_vendor', '/home/shanghong/.cache/huggingface/modules']\n",
      "[2025-07-01 18:21:07,217][oumi][rank0][pid:2181942][MainThread][INFO]][alpaca_eval.py:97] Loading the `tatsu-lab/alpaca_eval` dataset.\n",
      "[2025-07-01 18:21:07,219][oumi][rank0][pid:2181942][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: AlpacaEvalDataset)... dataset_name: 'tatsu-lab/alpaca_eval'\n",
      "[2025-07-01 18:21:07,418][oumi][rank0][pid:2181942][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: eval\n",
      "\tVersion: 1.0.0\n",
      "\tDataset size: 554496\n",
      "\tDownload size: 620778\n",
      "\tSize: 1175274 bytes\n",
      "\tRows: 805\n",
      "\tColumns: ['instruction', 'output', 'generator', 'dataset']\n",
      "[2025-07-01 18:21:08,284][oumi][rank0][pid:2181942][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (805, 4). Columns:\n",
      "instruction    object\n",
      "output         object\n",
      "generator      object\n",
      "dataset        object\n",
      "dtype: object\n",
      "[2025-07-01 18:21:08,309][oumi][rank0][pid:2181942][MainThread][INFO]][alpaca_eval.py:106] \tAlpacaEval inference `model_params`:\n",
      "ModelParams(model_name='meta-llama/Llama-3.2-3B-Instruct',\n",
      "            adapter_model=None,\n",
      "            tokenizer_name=None,\n",
      "            tokenizer_pad_token=None,\n",
      "            tokenizer_kwargs={},\n",
      "            processor_kwargs={},\n",
      "            model_max_length=8192,\n",
      "            load_pretrained_weights=True,\n",
      "            trust_remote_code=False,\n",
      "            torch_dtype_str='bfloat16',\n",
      "            compile=False,\n",
      "            chat_template=None,\n",
      "            attn_implementation=None,\n",
      "            device_map='auto',\n",
      "            model_kwargs={},\n",
      "            enable_liger_kernel=False,\n",
      "            shard_for_eval=False,\n",
      "            freeze_layers=[],\n",
      "            model_revision=None)\n",
      "\tAlpacaEval inference `generation_params`:\n",
      "GenerationParams(max_new_tokens=8192,\n",
      "                 batch_size=1,\n",
      "                 exclude_prompt_from_response=True,\n",
      "                 seed=None,\n",
      "                 temperature=0.0,\n",
      "                 top_p=1.0,\n",
      "                 frequency_penalty=0.0,\n",
      "                 presence_penalty=0.0,\n",
      "                 stop_strings=None,\n",
      "                 stop_token_ids=None,\n",
      "                 logit_bias={},\n",
      "                 min_p=0.0,\n",
      "                 use_cache=False,\n",
      "                 num_beams=1,\n",
      "                 use_sampling=False,\n",
      "                 guided_decoding=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:32<00:00, 10.96s/it, est. speed input: 5.02 toks/s, output: 57.92 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 18:21:42,088][oumi][rank0][pid:2181942][MainThread][INFO]][alpaca_eval.py:121] Running AlpacaEval annotation.\n",
      "[2025-07-01 18:21:42,094][oumi][rank0][pid:2181942][MainThread][INFO]][alpaca_eval.py:122] \tAlpacaEval `task_params`:\n",
      "AlpacaEvalTaskParams(evaluation_backend='alpaca_eval',\n",
      "                     task_name=None,\n",
      "                     num_samples=3,\n",
      "                     log_samples=False,\n",
      "                     eval_kwargs={},\n",
      "                     evaluation_platform='',\n",
      "                     version=2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating the 20250701_182107 outputs.\n",
      "WARNING:root:model_outputs and reference_outputs have different lengths, so we cannot shuffle before taking the first max_instances.\n",
      "INFO:root:Creating the annotator from `weighted_alpaca_eval_gpt4_turbo`.\n",
      "INFO:root:Saving annotations to `/home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json`.\n",
      "INFO:root:Loading all annotations from /home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "Annotation chunk:   0%|          | 0/1 [00:00<?, ?it/s]INFO:root:Annotating 3 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 3 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "WARNING:root:/home/shanghong/miniconda3/envs/oumi/lib/python3.11/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/home/shanghong/miniconda3/envs/oumi/lib/python3.11/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "WARNING:root:/home/shanghong/miniconda3/envs/oumi/lib/python3.11/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "prompt_batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.58it/s]\n",
      "INFO:root:Completed 3 examples in 1.2 seconds.\n",
      "INFO:root:Saving all annotations to /home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "Annotation chunk: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.31s/it]\n",
      "INFO:root:Saving all results to results/20250701_182107/weighted_alpaca_eval_gpt4_turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 18:21:44,078][oumi][rank0][pid:2181942][MainThread][INFO]][alpaca_eval.py:145] AlpacaEval's metric dict is {'avg_length': 2595,\n",
      " 'discrete_win_rate': 0.0,\n",
      " 'lc_standard_error': 0.011829984072084042,\n",
      " 'length_controlled_winrate': 0.044992813467909884,\n",
      " 'mode': 'community',\n",
      " 'n_draws': 0,\n",
      " 'n_total': 3,\n",
      " 'n_wins': 0,\n",
      " 'n_wins_base': 3,\n",
      " 'standard_error': 0.013888605351796625,\n",
      " 'win_rate': 0.019729389984194746}.\n",
      "Evaluating meta-llama/Llama-3.1-8B-Instruct...\n",
      "[2025-07-01 18:22:02,224][oumi][rank0][pid:2181942][MainThread][INFO]][lm_harness.py:331] \tLM Harness `task_params`:\n",
      "LMHarnessTaskParams(evaluation_backend='lm_harness',\n",
      "                    task_name='leaderboard_mmlu_pro',\n",
      "                    num_samples=10,\n",
      "                    log_samples=False,\n",
      "                    eval_kwargs={},\n",
      "                    evaluation_platform='',\n",
      "                    num_fewshot=None)\n",
      "[2025-07-01 18:22:02,225][oumi][rank0][pid:2181942][MainThread][INFO]][lm_harness.py:332] \tLM Harness `task_dict`:\n",
      "{'leaderboard_mmlu_pro': ConfigurableTask(task_name=leaderboard_mmlu_pro,output_type=multiple_choice,num_fewshot=None,num_samples=12032)}\n",
      "[2025-07-01 18:22:02,227][oumi][rank0][pid:2181942][MainThread][INFO]][lm_harness.py:344] \tLM Harness `model_params`:\n",
      "{'batch_size': 1,\n",
      " 'device': 'cuda:0',\n",
      " 'dtype': torch.bfloat16,\n",
      " 'max_batch_size': None,\n",
      " 'max_length': 8192,\n",
      " 'pretrained': 'meta-llama/Llama-3.1-8B-Instruct',\n",
      " 'trust_remote_code': False}\n",
      "INFO 07-01 18:22:02 [config.py:600] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "INFO 07-01 18:22:02 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 07-01 18:22:09 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 07-01 18:22:12 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 07-01 18:22:12 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x79c0050cdfd0>\n",
      "INFO 07-01 18:22:13 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 07-01 18:22:13 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-01 18:22:13 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "WARNING 07-01 18:22:13 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-01 18:22:13 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.26it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.27it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.79it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.56it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-01 18:22:16 [loader.py:447] Loading weights took 2.57 seconds\n",
      "INFO 07-01 18:22:16 [gpu_model_runner.py:1273] Model loading took 14.9889 GiB and 2.929392 seconds\n",
      "INFO 07-01 18:22:23 [backends.py:416] Using cache directory: /home/shanghong/.cache/vllm/torch_compile_cache/bff1dda4a0/rank_0_0 for vLLM's torch.compile\n",
      "INFO 07-01 18:22:23 [backends.py:426] Dynamo bytecode transform time: 7.35 s\n",
      "INFO 07-01 18:22:27 [backends.py:132] Cache the graph of shape None for later use\n",
      "INFO 07-01 18:22:51 [backends.py:144] Compiling a graph for general shape takes 26.84 s\n",
      "INFO 07-01 18:23:04 [monitor.py:33] torch.compile takes 34.19 s in total\n",
      "INFO 07-01 18:23:05 [kv_cache_utils.py:578] GPU KV cache size: 415,824 tokens\n",
      "INFO 07-01 18:23:05 [kv_cache_utils.py:581] Maximum concurrency for 8,192 tokens per request: 50.76x\n",
      "INFO 07-01 18:23:24 [gpu_model_runner.py:1608] Graph capturing finished in 20 secs, took 0.64 GiB\n",
      "INFO 07-01 18:23:24 [core.py:162] init engine (profile, create kv cache, warmup model) took 68.49 seconds\n",
      "[2025-07-01 18:23:25,360][oumi][rank0][pid:2181942][MainThread][INFO]][lm_harness.py:348] Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lm_eval.api.task:Building contexts for leaderboard_mmlu_pro on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 22345.79it/s]\n",
      "INFO:lm_eval.evaluator:Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:01<00:00, 79.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 18:23:26,646][oumi][rank0][pid:2181942][MainThread][INFO]][lm_harness.py:366] leaderboard_mmlu_pro's metric dict is {'acc,none': 0.3,\n",
      " 'acc_stderr,none': 0.15275252316519464,\n",
      " 'alias': 'leaderboard_mmlu_pro'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W701 18:23:31.140919986 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 18:23:33,298][oumi][rank0][pid:2181942][MainThread][WARNING]][serialization_utils.py:47] Non-serializable value `LMHarnessTaskParams(evaluation_backend='lm_harness', task_name='leaderboard_mmlu_pro', num_samples=10, log_samples=False, eval_kwargs={}, evaluation_platform='', num_fewshot=None)` of type `<class 'oumi.core.configs.params.evaluation_params.LMHarnessTaskParams'>`.\n",
      "[2025-07-01 18:23:33,299][oumi][rank0][pid:2181942][MainThread][WARNING]][serialization_utils.py:47] Non-serializable value `ConfigurableTask(task_name=leaderboard_mmlu_pro,output_type=multiple_choice,num_fewshot=None,num_samples=12032)` of type `<class 'lm_eval.api.task.ConfigurableTask'>`.\n",
      "[2025-07-01 18:23:34,061][oumi][rank0][pid:2181942][MainThread][WARNING]][models.py:463] Undefined pad token. Setting it to `<|finetune_right_pad_id|>`.\n",
      "[2025-07-01 18:23:34,063][oumi][rank0][pid:2181942][MainThread][INFO]][models.py:506] Using the model's built-in chat template for model 'meta-llama/Llama-3.1-8B-Instruct'.\n",
      "INFO 07-01 18:23:34 [config.py:600] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "INFO 07-01 18:23:34 [config.py:1600] Defaulting to use mp for distributed inference\n",
      "INFO 07-01 18:23:34 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 07-01 18:23:34 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-01 18:23:41 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 07-01 18:23:44 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 07-01 18:23:44 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 07-01 18:23:44 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_3d795e93'), local_subscribe_addr='ipc:///tmp/570ac4a9-cd7e-479b-a443-ec0a9a494846', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-01 18:23:49 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 07-01 18:23:52 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7c1bc8dcec50>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2188221)\u001b[0;0m INFO 07-01 18:23:52 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_25d5117e'), local_subscribe_addr='ipc:///tmp/521fc2b8-5b76-4e74-8a33-259071e99971', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-01 18:23:57 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 07-01 18:24:00 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f61da887810>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2188356)\u001b[0;0m INFO 07-01 18:24:00 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d09da612'), local_subscribe_addr='ipc:///tmp/e4786604-26b1-4de4-a7cf-036e9326d7a1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2188221)\u001b[0;0m INFO 07-01 18:24:01 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2188221)\u001b[0;0m INFO 07-01 18:24:01 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2188356)\u001b[0;0m INFO 07-01 18:24:01 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2188356)\u001b[0;0m INFO 07-01 18:24:01 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2188221)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=2188356)\u001b[0;0m INFO 07-01 18:24:02 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 07-01 18:24:02 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shanghong/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2188221)\u001b[0;0m INFO 07-01 18:24:02 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_6f03ed90'), local_subscribe_addr='ipc:///tmp/b80a5a78-008d-499a-9ad2-178ff9744e4c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2188221)\u001b[0;0m INFO 07-01 18:24:02 [parallel_state.py:957] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2188356)\u001b[0;0m INFO 07-01 18:24:02 [parallel_state.py:957] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2188221)\u001b[0;0m INFO 07-01 18:24:02 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2188356)\u001b[0;0m INFO 07-01 18:24:02 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2188221)\u001b[0;0m INFO 07-01 18:24:02 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2188356)\u001b[0;0m INFO 07-01 18:24:02 [gpu_model_runner.py:1258] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2188356)\u001b[0;0m WARNING 07-01 18:24:02 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2188221)\u001b[0;0m WARNING 07-01 18:24:02 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2188356)\u001b[0;0m INFO 07-01 18:24:03 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2188221)\u001b[0;0m INFO 07-01 18:24:03 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.07it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=2188356)\u001b[0;0m INFO 07-01 18:24:06 [loader.py:447] Loading weights took 3.53 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.21s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.24it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.08it/s]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2188221)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=2188221)\u001b[0;0m INFO 07-01 18:24:06 [loader.py:447] Loading weights took 3.73 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2188356)\u001b[0;0m INFO 07-01 18:24:06 [gpu_model_runner.py:1273] Model loading took 7.5123 GiB and 3.920266 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2188221)\u001b[0;0m INFO 07-01 18:24:07 [gpu_model_runner.py:1273] Model loading took 7.5123 GiB and 4.215796 seconds\n",
      "INFO 07-01 18:24:11 [kv_cache_utils.py:578] GPU KV cache size: 929,840 tokens\n",
      "INFO 07-01 18:24:11 [kv_cache_utils.py:581] Maximum concurrency for 8,192 tokens per request: 113.51x\n",
      "INFO 07-01 18:24:11 [kv_cache_utils.py:578] GPU KV cache size: 929,840 tokens\n",
      "INFO 07-01 18:24:11 [kv_cache_utils.py:581] Maximum concurrency for 8,192 tokens per request: 113.51x\n",
      "INFO 07-01 18:24:11 [core.py:162] init engine (profile, create kv cache, warmup model) took 4.67 seconds\n",
      "Python executable: /home/shanghong/miniconda3/envs/oumi/bin/python\n",
      "sys.path: ['/home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/ray/thirdparty_files', '/home/shanghong/miniconda3/envs/oumi/lib/python311.zip', '/home/shanghong/miniconda3/envs/oumi/lib/python3.11', '/home/shanghong/miniconda3/envs/oumi/lib/python3.11/lib-dynload', '', '/home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages', '/home/shanghong/oumi/src', '/tmp/tmp91n8vu4b', '/home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/setuptools/_vendor', '/home/shanghong/.cache/huggingface/modules']\n",
      "[2025-07-01 18:24:12,171][oumi][rank0][pid:2181942][MainThread][INFO]][alpaca_eval.py:97] Loading the `tatsu-lab/alpaca_eval` dataset.\n",
      "[2025-07-01 18:24:12,175][oumi][rank0][pid:2181942][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: AlpacaEvalDataset)... dataset_name: 'tatsu-lab/alpaca_eval'\n",
      "[2025-07-01 18:24:12,343][oumi][rank0][pid:2181942][MainThread][INFO]][base_map_dataset.py:487] Dataset Info:\n",
      "\tSplit: eval\n",
      "\tVersion: 1.0.0\n",
      "\tDataset size: 554496\n",
      "\tDownload size: 620778\n",
      "\tSize: 1175274 bytes\n",
      "\tRows: 805\n",
      "\tColumns: ['instruction', 'output', 'generator', 'dataset']\n",
      "[2025-07-01 18:24:12,880][oumi][rank0][pid:2181942][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (805, 4). Columns:\n",
      "instruction    object\n",
      "output         object\n",
      "generator      object\n",
      "dataset        object\n",
      "dtype: object\n",
      "[2025-07-01 18:24:12,908][oumi][rank0][pid:2181942][MainThread][INFO]][alpaca_eval.py:106] \tAlpacaEval inference `model_params`:\n",
      "ModelParams(model_name='meta-llama/Llama-3.1-8B-Instruct',\n",
      "            adapter_model=None,\n",
      "            tokenizer_name=None,\n",
      "            tokenizer_pad_token=None,\n",
      "            tokenizer_kwargs={},\n",
      "            processor_kwargs={},\n",
      "            model_max_length=8192,\n",
      "            load_pretrained_weights=True,\n",
      "            trust_remote_code=False,\n",
      "            torch_dtype_str='bfloat16',\n",
      "            compile=False,\n",
      "            chat_template=None,\n",
      "            attn_implementation=None,\n",
      "            device_map='auto',\n",
      "            model_kwargs={},\n",
      "            enable_liger_kernel=False,\n",
      "            shard_for_eval=False,\n",
      "            freeze_layers=[],\n",
      "            model_revision=None)\n",
      "\tAlpacaEval inference `generation_params`:\n",
      "GenerationParams(max_new_tokens=8192,\n",
      "                 batch_size=1,\n",
      "                 exclude_prompt_from_response=True,\n",
      "                 seed=None,\n",
      "                 temperature=0.0,\n",
      "                 top_p=1.0,\n",
      "                 frequency_penalty=0.0,\n",
      "                 presence_penalty=0.0,\n",
      "                 stop_strings=None,\n",
      "                 stop_token_ids=None,\n",
      "                 logit_bias={},\n",
      "                 min_p=0.0,\n",
      "                 use_cache=False,\n",
      "                 num_beams=1,\n",
      "                 use_sampling=False,\n",
      "                 guided_decoding=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:06<00:00, 22.20s/it, est. speed input: 2.48 toks/s, output: 45.08 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 18:25:20,388][oumi][rank0][pid:2181942][MainThread][INFO]][alpaca_eval.py:121] Running AlpacaEval annotation.\n",
      "[2025-07-01 18:25:20,389][oumi][rank0][pid:2181942][MainThread][INFO]][alpaca_eval.py:122] \tAlpacaEval `task_params`:\n",
      "AlpacaEvalTaskParams(evaluation_backend='alpaca_eval',\n",
      "                     task_name=None,\n",
      "                     num_samples=3,\n",
      "                     log_samples=False,\n",
      "                     eval_kwargs={},\n",
      "                     evaluation_platform='',\n",
      "                     version=2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating the 20250701_182412 outputs.\n",
      "WARNING:root:model_outputs and reference_outputs have different lengths, so we cannot shuffle before taking the first max_instances.\n",
      "INFO:root:Creating the annotator from `weighted_alpaca_eval_gpt4_turbo`.\n",
      "INFO:root:Saving annotations to `/home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json`.\n",
      "INFO:root:Loading all annotations from /home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "Annotation chunk:   0%|          | 0/1 [00:00<?, ?it/s]INFO:root:Annotating 3 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 3 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "WARNING:root:/home/shanghong/miniconda3/envs/oumi/lib/python3.11/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/home/shanghong/miniconda3/envs/oumi/lib/python3.11/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "WARNING:root:/home/shanghong/miniconda3/envs/oumi/lib/python3.11/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "prompt_batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.95it/s]\n",
      "INFO:root:Completed 3 examples in 1.6 seconds.\n",
      "INFO:root:Saving all annotations to /home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /home/shanghong/miniconda3/envs/oumi/lib/python3.11/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "Annotation chunk: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it]\n",
      "INFO:root:Saving all results to results/20250701_182412/weighted_alpaca_eval_gpt4_turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 18:25:22,792][oumi][rank0][pid:2181942][MainThread][INFO]][alpaca_eval.py:145] AlpacaEval's metric dict is {'avg_length': 4014,\n",
      " 'discrete_win_rate': 33.33333333333333,\n",
      " 'lc_standard_error': 5.696288827948022,\n",
      " 'length_controlled_winrate': 27.061533794693894,\n",
      " 'mode': 'community',\n",
      " 'n_draws': 0,\n",
      " 'n_total': 3,\n",
      " 'n_wins': 1,\n",
      " 'n_wins_base': 2,\n",
      " 'standard_error': 31.605918306173457,\n",
      " 'win_rate': 31.65530398635676}.\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.configs import (\n",
    "    EvaluationConfig,\n",
    "    GenerationParams,\n",
    "    InferenceEngineType,\n",
    "    ModelParams,\n",
    ")\n",
    "from oumi.evaluate import evaluate\n",
    "\n",
    "# Store the results for 1B, 3B, 8B models in the following lists.\n",
    "mmlu_acc = []\n",
    "alpaca_lcwr = []\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    # Define the evaluation configuration.\n",
    "    evaluation_config = EvaluationConfig(\n",
    "        tasks=TASKS,\n",
    "        model=ModelParams(\n",
    "            model_name=model_name,\n",
    "            model_max_length=MODEL_MAX_TOKENS,\n",
    "            torch_dtype_str=\"bfloat16\",\n",
    "        ),\n",
    "        generation=GenerationParams(max_new_tokens=MODEL_MAX_TOKENS),\n",
    "        inference_engine=InferenceEngineType.VLLM,\n",
    "        output_dir=tutorial_dir,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model.\n",
    "    task_results = evaluate(evaluation_config)\n",
    "\n",
    "    # Store the results.\n",
    "    mmlu_acc.append(task_results[0][\"results\"][\"leaderboard_mmlu_pro\"][\"acc,none\"])\n",
    "    alpaca_lcwr.append(\n",
    "        task_results[1][\"results\"][\"alpaca_eval\"][\"length_controlled_winrate\"] / 100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'analyze', 'analyze_evaluators', 'annotators', 'completion_parsers', 'constants', 'decoders', 'evaluate', 'evaluate_from_model', 'load_dotenv', 'main', 'make_leaderboard', 'metrics', 'processors', 'types', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import alpaca_eval\n",
    "\n",
    "print(dir(alpaca_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shanghong/miniconda3/envs/oumi/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function evaluate at 0x71e2b09df880>\n"
     ]
    }
   ],
   "source": [
    "import alpaca_eval\n",
    "\n",
    "print(alpaca_eval.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /home/shanghong/miniconda3/envs/oumi/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"Python executable:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shanghong/miniconda3/envs/oumi/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6.6\n"
     ]
    }
   ],
   "source": [
    "import alpaca_eval\n",
    "\n",
    "print(alpaca_eval.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results\n",
    "\n",
    "The evaluation results are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.3, 0.3]\n",
      "[0.0003000470156304012, 0.00044992813467909887, 0.2706153379469389]\n"
     ]
    }
   ],
   "source": [
    "print(mmlu_acc)\n",
    "print(alpaca_lcwr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When visualizing the results using `matplotlib`'s `pyplot`, we observe a clear trend: both the model's knowledge and reasoning abilities (measured by MMLU Pro) and the quality of its responses (measured by AlpacaEval 2.0) improve as the model size increases. A notable performance gain is observed when scaling from 1B to 3B parameters. However, beyond this, the rate of improvement begins to plateau, indicating diminishing returns with further increases in model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:matplotlib.font_manager:generated new fontManager\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "model_sizes = [1, 3, 8]\n",
    "plt.plot(model_sizes, mmlu_acc, label=\"MMLU Pro (Accuracy)\")\n",
    "plt.plot(model_sizes, alpaca_lcwr, label=\"AlpacaEval 2.0 (LC WR)\")\n",
    "\n",
    "# Show\n",
    "plt.xlabel(\"Model Size (Billion Parameters)\")\n",
    "plt.ylabel(\"Model Performance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
