{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7bYaH10SgtN"
   },
   "source": [
    "# Training CNN on Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkhNGqE1SgtP"
   },
   "source": [
    "Oumi is not limited to LLMs. This example shows how to train a simple ConvNet classifier on a custom dataset cotaining binaray data in Numpy `.npz` file. The dataset is created from the classic MNIST dataset (hand-written digits classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHDr11SqSgtP"
   },
   "source": [
    "# Prerequisites\n",
    "## Oumi Installation\n",
    "First, let's install Oumi. You can find detailed instructions [here](https://github.com/oumi-ai/oumi/blob/main/README.md), but it should be as simple as:\n",
    "\n",
    "```bash\n",
    "pip install -e \".[gpu]\"  # if you have an nvidia or AMD GPU\n",
    "# OR\n",
    "pip install -e \".\"  # if you don't have a GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHDr11SqSgtP"
   },
   "source": [
    "## Environment Setup: Common Imports and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "tutorial_dir = \"cnn_mnist_example\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable warnings from HF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHDr11SqSgtP"
   },
   "source": [
    "# Data\n",
    "## Data Preparation\n",
    "First, let's convert MNIST dataset to `.npz` archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JPmWKRVCSgtP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 70000 examples to '/home/user/oumi/notebooks/cnn_mnist_example/mnist.npz'!\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "labels = []\n",
    "splits = []\n",
    "for train_split in (False, True):\n",
    "    mnist_dataset = torchvision.datasets.MNIST(\n",
    "        root=Path(\"/tmp/mnist_data\"),\n",
    "        train=train_split,\n",
    "        download=True,\n",
    "    )\n",
    "    num_examples = len(mnist_dataset)\n",
    "    images.extend(\n",
    "        [np.asarray(mnist_dataset.data[i], dtype=np.uint8) for i in range(num_examples)]\n",
    "    )\n",
    "    labels.extend([int(mnist_dataset.targets[i]) for i in range(num_examples)])\n",
    "    splits.extend([(\"train\" if train_split else \"test\")] * num_examples)\n",
    "\n",
    "npz_filename = (Path(tutorial_dir) / \"mnist.npz\").absolute()\n",
    "\n",
    "# Normalize and convert [N,W,H] to [N,C,W,H] by adding dummy C=1 (PyTorch convention).\n",
    "images = np.expand_dims((np.stack(images).astype(dtype=np.float32) / 255.0), axis=1)\n",
    "np.savez_compressed(\n",
    "    npz_filename, images=images, labels=np.stack(labels), split=np.stack(splits)\n",
    ")\n",
    "print(f\"Saved {len(labels)} examples to '{npz_filename}'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define Oumi custom dataset that can load MNIST data from `.npz` archive. For more details, refer to: https://oumi.ai/docs/latest/resources/datasets/datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing_extensions import override\n",
    "\n",
    "from oumi.core.datasets import BaseMapDataset\n",
    "from oumi.core.registry import register_dataset\n",
    "\n",
    "\n",
    "@register_dataset(\"npz_file\")\n",
    "class NpzDataset(BaseMapDataset):\n",
    "    \"\"\"Loads dataset from Numpy .npz archive.\"\"\"\n",
    "\n",
    "    default_dataset = \"custom\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dataset_name: Optional[str] = None,\n",
    "        dataset_path: Optional[Union[str, Path]] = None,\n",
    "        split: Optional[str] = None,\n",
    "        npz_split_col: Optional[str] = None,\n",
    "        npz_allow_pickle: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes a new instance of the NpzDataset class.\n",
    "\n",
    "        Args:\n",
    "            dataset_name: Dataset name.\n",
    "            dataset_path: Path to .npz file.\n",
    "            split: Dataset split.\n",
    "            npz_split_col: Name of '.npz' array containing dataset split info.\n",
    "                If unspecified, then the name \"split\" is assumed by default.\n",
    "            npz_allow_pickle: Whether pickle is allowed when loading data\n",
    "                from the npz archive.\n",
    "            **kwargs: Additional arguments to pass to the parent class.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If dataset_path is not provided, or\n",
    "                if .npz file contains data in unexpected format.\n",
    "        \"\"\"\n",
    "        if not dataset_path:\n",
    "            raise ValueError(\"`dataset_path` must be provided\")\n",
    "        super().__init__(\n",
    "            dataset_name=dataset_name,\n",
    "            dataset_path=(str(dataset_path) if dataset_path is not None else None),\n",
    "            split=split,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self._npz_allow_pickle = npz_allow_pickle\n",
    "        self._npz_split_col = npz_split_col\n",
    "\n",
    "        dataset_path = Path(dataset_path)\n",
    "        if not dataset_path.is_file():\n",
    "            raise ValueError(f\"Path is not a file! '{dataset_path}'\")\n",
    "        elif dataset_path.suffix.lower() != \".npz\":\n",
    "            raise ValueError(f\"File extension is not '.npz'! '{dataset_path}'\")\n",
    "\n",
    "        self._data = self._load_data()\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_list(x: np.ndarray) -> list:\n",
    "        # `pd.DataFrame` expects Python lists for columns\n",
    "        # (elements can still be `ndarray`)\n",
    "        if len(x.shape) > 1:\n",
    "            return [x[i, ...] for i in range(x.shape[0])]\n",
    "        return x.tolist()\n",
    "\n",
    "    @override\n",
    "    def _load_data(self) -> pd.DataFrame:\n",
    "        data_dict = {}\n",
    "        if not self.dataset_path:\n",
    "            raise ValueError(\"dataset_path is empty!\")\n",
    "        with np.load(self.dataset_path, allow_pickle=self._npz_allow_pickle) as npzfile:\n",
    "            feature_names = list(sorted(npzfile.files))\n",
    "            if len(feature_names) == 0:\n",
    "                raise ValueError(\n",
    "                    f\"'.npz' archive contains no data! '{self.dataset_path}'\"\n",
    "                )\n",
    "            num_examples = None\n",
    "            for feature_name in feature_names:\n",
    "                col_data = npzfile[feature_name]\n",
    "                assert isinstance(col_data, np.ndarray)\n",
    "                if num_examples is None:\n",
    "                    num_examples = col_data.shape[0]\n",
    "                elif num_examples != col_data.shape[0]:\n",
    "                    raise ValueError(\n",
    "                        \"Inconsistent number of examples for features \"\n",
    "                        f\"'{feature_name}' and '{feature_names[0]}': \"\n",
    "                        f\"{col_data.shape[0]} vs {num_examples}!\"\n",
    "                    )\n",
    "                data_dict[feature_name] = self._to_list(col_data)\n",
    "\n",
    "        dataframe: pd.DataFrame = pd.DataFrame(data_dict)\n",
    "\n",
    "        split_feature_name = (self._npz_split_col or \"split\") if self.split else None\n",
    "        if split_feature_name:\n",
    "            if split_feature_name not in dataframe:\n",
    "                raise ValueError(\n",
    "                    f\"'.npz' doesn't contain data split info: '{split_feature_name}'!\"\n",
    "                )\n",
    "            dataframe = pd.DataFrame(\n",
    "                dataframe[dataframe[split_feature_name] == self.split].drop(\n",
    "                    split_feature_name, axis=1\n",
    "                ),\n",
    "                copy=True,\n",
    "            )\n",
    "        return dataframe\n",
    "\n",
    "    @override\n",
    "    def transform(self, sample: pd.Series) -> dict:\n",
    "        \"\"\"Preprocesses the inputs in the given sample.\"\"\"\n",
    "        return sample.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8PTJuc4SgtQ"
   },
   "source": [
    "# Training a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2_HamuySgtQ"
   },
   "source": [
    "Oumi provides the sample `CnnClassfier` model [[source](https://github.com/oumi-ai/oumi/blob/main/src/oumi/models/cnn_classifier.py)]. Let's use it to train a classifier for MNIST hand-written digits.\n",
    "\n",
    "Oumi uses [training configuration files](https://oumi.ai/docs/latest/api/oumi.core.configs.html#oumi.core.configs.TrainingConfig) to specify training parameters. We've already created a training config for `CnnClassfier`--let's give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "l2SQ9fZiSgtQ"
   },
   "outputs": [],
   "source": [
    "yaml_content = f\"\"\"\n",
    "model:\n",
    "  model_name: \"CnnClassifier\"\n",
    "  torch_dtype_str: \"float32\"\n",
    "  load_pretrained_weights: False\n",
    "  model_kwargs:\n",
    "      image_width: 28   # MNIST images are 28x28 single channel\n",
    "      image_height: 28\n",
    "      in_channels: 1\n",
    "      output_dim: 10    # Number of output classes: 10 digits\n",
    "\n",
    "data:\n",
    "  train:\n",
    "    experimental_use_torch_datapipes: True\n",
    "    datasets:\n",
    "      - dataset_name: \"npz_file\" # Custom dataset defined above for .npz archives\n",
    "        dataset_path: \"{npz_filename}\"\n",
    "        split: \"train\"\n",
    "\n",
    "training:\n",
    "  trainer_type: \"OUMI\"  # For non-transformers, use \"OUMI\" trainer\n",
    "  per_device_train_batch_size: 64\n",
    "  num_train_epochs: 2  # Quick \"mini\" training, for demo purposes only\n",
    "  logging_steps: 500\n",
    "  run_name: \"mnist_cnn_classifier\"\n",
    "  output_dir: \"{tutorial_dir}/output\"\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{tutorial_dir}/train.yaml\", \"w\") as f:\n",
    "    f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2GpQDGG5SgtQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-01-20 18:45:15,743][oumi][rank0][pid:2307328][MainThread][INFO]][torch_utils.py:66] Torch version: 2.4.0+cu121. NumPy version: 1.26.4\n",
      "[2025-01-20 18:45:15,745][oumi][rank0][pid:2307328][MainThread][INFO]][torch_utils.py:72] CUDA version: 12.1 CuDNN version: 90.1.0\n",
      "[2025-01-20 18:45:15,936][oumi][rank0][pid:2307328][MainThread][INFO]][torch_utils.py:106] CPU cores: 24 CUDA devices: 1\n",
      "device(0)='NVIDIA GeForce RTX 3090' Capability: (8, 6) Memory: [Total: 24.0GiB Free: 22.76GiB Allocated: 0.0GiB Cached: 0.0GiB]\n",
      "[2025-01-20 18:45:15,938][oumi][rank0][pid:2307328][MainThread][INFO]][train.py:133] Oumi version: 0.1.3.dev2+g6b9b1a1f.d20250121\n",
      "[2025-01-20 18:45:15,940][oumi][rank0][pid:2307328][MainThread][INFO]][train.py:135] Git revision hash: 6b9b1a1f6bbe787515c6bc6b067ec0d479db5f96\n",
      "[2025-01-20 18:45:15,943][oumi][rank0][pid:2307328][MainThread][INFO]][train.py:136] Git tag: None\n",
      "[2025-01-20 18:45:15,944][oumi][rank0][pid:2307328][MainThread][INFO]][train.py:174] TrainingConfig: TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='npz_file',\n",
      "                                                                                dataset_path='/home/user/oumi/notebooks/cnn_mnist_example/mnist.npz',\n",
      "                                                                                subset=None,\n",
      "                                                                                split='train',\n",
      "                                                                                dataset_kwargs={},\n",
      "                                                                                sample_count=None,\n",
      "                                                                                mixture_proportion=None,\n",
      "                                                                                shuffle=False,\n",
      "                                                                                seed=None,\n",
      "                                                                                shuffle_buffer_size=1000,\n",
      "                                                                                trust_remote_code=False,\n",
      "                                                                                transform_num_workers=None)],\n",
      "                                                        collator_name=None,\n",
      "                                                        pack=False,\n",
      "                                                        stream=False,\n",
      "                                                        target_col=None,\n",
      "                                                        mixture_strategy='first_exhausted',\n",
      "                                                        seed=None,\n",
      "                                                        use_async_dataset=False,\n",
      "                                                        experimental_use_torch_datapipes=True),\n",
      "                               test=DatasetSplitParams(datasets=[],\n",
      "                                                       collator_name=None,\n",
      "                                                       pack=False,\n",
      "                                                       stream=False,\n",
      "                                                       target_col=None,\n",
      "                                                       mixture_strategy='first_exhausted',\n",
      "                                                       seed=None,\n",
      "                                                       use_async_dataset=False,\n",
      "                                                       experimental_use_torch_datapipes=False),\n",
      "                               validation=DatasetSplitParams(datasets=[],\n",
      "                                                             collator_name=None,\n",
      "                                                             pack=False,\n",
      "                                                             stream=False,\n",
      "                                                             target_col=None,\n",
      "                                                             mixture_strategy='first_exhausted',\n",
      "                                                             seed=None,\n",
      "                                                             use_async_dataset=False,\n",
      "                                                             experimental_use_torch_datapipes=False)),\n",
      "               model=ModelParams(model_name='CnnClassifier',\n",
      "                                 adapter_model=None,\n",
      "                                 tokenizer_name=None,\n",
      "                                 tokenizer_pad_token=None,\n",
      "                                 tokenizer_kwargs={},\n",
      "                                 model_max_length=None,\n",
      "                                 load_pretrained_weights=False,\n",
      "                                 trust_remote_code=False,\n",
      "                                 torch_dtype_str='float32',\n",
      "                                 compile=False,\n",
      "                                 chat_template=None,\n",
      "                                 attn_implementation=None,\n",
      "                                 device_map='auto',\n",
      "                                 model_kwargs={'image_height': 28,\n",
      "                                               'image_width': 28,\n",
      "                                               'in_channels': 1,\n",
      "                                               'output_dim': 10},\n",
      "                                 enable_liger_kernel=False,\n",
      "                                 shard_for_eval=False,\n",
      "                                 freeze_layers=[]),\n",
      "               training=TrainingParams(use_peft=False,\n",
      "                                       trainer_type=<TrainerType.OUMI: 'oumi'>,\n",
      "                                       enable_gradient_checkpointing=False,\n",
      "                                       gradient_checkpointing_kwargs={},\n",
      "                                       output_dir='cnn_mnist_example/output',\n",
      "                                       per_device_train_batch_size=64,\n",
      "                                       per_device_eval_batch_size=8,\n",
      "                                       gradient_accumulation_steps=1,\n",
      "                                       max_steps=-1,\n",
      "                                       num_train_epochs=2,\n",
      "                                       save_epoch=False,\n",
      "                                       save_steps=500,\n",
      "                                       save_final_model=True,\n",
      "                                       seed=42,\n",
      "                                       run_name='mnist_cnn_classifier',\n",
      "                                       metrics_function=None,\n",
      "                                       log_level='info',\n",
      "                                       dep_log_level='warning',\n",
      "                                       enable_wandb=False,\n",
      "                                       enable_tensorboard=True,\n",
      "                                       logging_strategy='steps',\n",
      "                                       logging_dir=None,\n",
      "                                       logging_steps=500,\n",
      "                                       logging_first_step=False,\n",
      "                                       eval_strategy='no',\n",
      "                                       eval_steps=500,\n",
      "                                       learning_rate=5e-05,\n",
      "                                       lr_scheduler_type='linear',\n",
      "                                       lr_scheduler_kwargs={},\n",
      "                                       warmup_ratio=None,\n",
      "                                       warmup_steps=None,\n",
      "                                       optimizer='adamw_torch',\n",
      "                                       weight_decay=0.0,\n",
      "                                       adam_beta1=0.9,\n",
      "                                       adam_beta2=0.999,\n",
      "                                       adam_epsilon=1e-08,\n",
      "                                       sgd_momentum=0.0,\n",
      "                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,\n",
      "                                       compile=False,\n",
      "                                       include_performance_metrics=False,\n",
      "                                       include_alternative_mfu_metrics=False,\n",
      "                                       log_model_summary=False,\n",
      "                                       resume_from_checkpoint=None,\n",
      "                                       try_resume_from_last_checkpoint=False,\n",
      "                                       dataloader_num_workers=0,\n",
      "                                       dataloader_prefetch_factor=None,\n",
      "                                       dataloader_main_process_only=None,\n",
      "                                       ddp_find_unused_parameters=None,\n",
      "                                       max_grad_norm=1.0,\n",
      "                                       trainer_kwargs={},\n",
      "                                       profiler=ProfilerParams(save_dir=None,\n",
      "                                                               enable_cpu_profiling=False,\n",
      "                                                               enable_cuda_profiling=False,\n",
      "                                                               record_shapes=False,\n",
      "                                                               profile_memory=False,\n",
      "                                                               with_stack=False,\n",
      "                                                               with_flops=False,\n",
      "                                                               with_modules=False,\n",
      "                                                               row_limit=50,\n",
      "                                                               schedule=ProfilerScheduleParams(enable_schedule=False,\n",
      "                                                                                               wait=0,\n",
      "                                                                                               warmup=1,\n",
      "                                                                                               active=3,\n",
      "                                                                                               repeat=1,\n",
      "                                                                                               skip_first=1)),\n",
      "                                       telemetry=TelemetryParams(telemetry_dir='telemetry',\n",
      "                                                                 collect_telemetry_for_all_ranks=False,\n",
      "                                                                 track_gpu_temperature=False),\n",
      "                                       empty_device_cache_steps=None,\n",
      "                                       nccl_default_timeout_minutes=None),\n",
      "               peft=PeftParams(lora_r=8,\n",
      "                               lora_alpha=8,\n",
      "                               lora_dropout=0.0,\n",
      "                               lora_target_modules=None,\n",
      "                               lora_modules_to_save=None,\n",
      "                               lora_bias='none',\n",
      "                               init_lora_weights=<LoraWeightInitialization.DEFAULT: 'default'>,\n",
      "                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
      "                               q_lora=False,\n",
      "                               q_lora_bits=4,\n",
      "                               bnb_4bit_quant_type='fp4',\n",
      "                               use_bnb_nested_quant=False,\n",
      "                               bnb_4bit_quant_storage='uint8',\n",
      "                               bnb_4bit_compute_dtype='float32',\n",
      "                               peft_save_mode=<PeftSaveMode.ADAPTER_ONLY: 'adapter_only'>),\n",
      "               fsdp=FSDPParams(enable_fsdp=False,\n",
      "                               sharding_strategy=<ShardingStrategy.FULL_SHARD: 'FULL_SHARD'>,\n",
      "                               cpu_offload=False,\n",
      "                               mixed_precision=None,\n",
      "                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,\n",
      "                               forward_prefetch=False,\n",
      "                               use_orig_params=None,\n",
      "                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,\n",
      "                               auto_wrap_policy=<AutoWrapPolicy.NO_WRAP: 'NO_WRAP'>,\n",
      "                               min_num_params=100000,\n",
      "                               transformer_layer_cls=None,\n",
      "                               sync_module_states=True))\n",
      "[2025-01-20 18:45:15,982][oumi][rank0][pid:2307328][MainThread][WARNING]][data.py:50] Using experimental torch datapipes preprocessing pipeline. This is currently in beta and may not be stable.\n",
      "[2025-01-20 18:45:15,983][oumi][rank0][pid:2307328][MainThread][INFO]][base_map_dataset.py:68] Creating map dataset (type: NpzDataset) dataset_name: 'None', dataset_path: '/home/user/oumi/notebooks/cnn_mnist_example/mnist.npz'...\n",
      "/home/user/oumi/src/oumi/builders/training.py:99: UserWarning: OUMI trainer is still in development model. Please use HF trainer for now.\n",
      "  warnings.warn(\n",
      "[2025-01-20 18:45:16,479][oumi][rank0][pid:2307328][MainThread][INFO]][torch_profiler_utils.py:150] PROF: Torch Profiler disabled!\n",
      "[2025-01-20 18:45:16,484][oumi][rank0][pid:2307328][MainThread][INFO]][lr_schedules.py:83] No warmup steps provided. Setting warmup_steps=0.\n",
      "[2025-01-20 18:45:16,484][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:604] Logging to cnn_mnist_example/output\n",
      "[2025-01-20 18:45:16,485][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:604] Logging to tensorboard folder: 'cnn_mnist_example/output/tensorboard'\n",
      "[2025-01-20 18:45:16,503][oumi][rank0][pid:2307328][MainThread][INFO]][device_utils.py:283] GPU Metrics Before Training: GPU runtime info: NVidiaGpuRuntimeInfo(device_index=0, device_count=1, used_memory_mb=1768.0, temperature=39, fan_speed=0, fan_speeds=(0, 0), power_usage_watts=67.161, power_limit_watts=350.0, gpu_utilization=0, memory_utilization=0, performance_state=2, clock_speed_graphics=1695, clock_speed_sm=1695, clock_speed_memory=9501).\n",
      "[2025-01-20 18:45:16,504][oumi][rank0][pid:2307328][MainThread][INFO]][train.py:309] Training init time: 0.761s\n",
      "[2025-01-20 18:45:16,505][oumi][rank0][pid:2307328][MainThread][INFO]][train.py:310] Starting training... (TrainerType.OUMI, transformers: 4.45.2)\n",
      "[2025-01-20 18:45:16,505][oumi][rank0][pid:2307328][MainThread][INFO]][torch_utils.py:218] Trainable params: 228010 || All params: 228010 || Trainable%: 100.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982b9fd0e1ed4098be855e5676814abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-01-20 18:45:19,228][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:604] {'epoch': 0,\n",
      " 'global_step': 500,\n",
      " 'global_steps_per_second': 183.7019607898465,\n",
      " 'learning_rate': 3.670042643923241e-05,\n",
      " 'tokens_per_second': 0.0,\n",
      " 'tokens_per_step_per_gpu': 0.0,\n",
      " 'total_tokens_seen': 0,\n",
      " 'train/loss': 0.9217740297317505}\n",
      "[2025-01-20 18:45:19,231][oumi.telemetry][rank0][pid:2307328][MainThread][INFO]][telemetry.py:332] Telemetry Summary (PUGET-SYSTEMS):\n",
      "Total time: 2.75 seconds\n",
      "\n",
      "CPU Timers:\n",
      "\tfetching batch:\n",
      "\t\tTotal: 1.1436s Mean: 0.0023s Median: 0.0022s\n",
      "\t\tMin: 0.0021s Max: 0.0046s StdDev: 0.0003s\n",
      "\t\tCount: 500.0 Percentage of total time: 41.60%\n",
      "\tcomputing tokens:\n",
      "\t\tTotal: 0.0003s Mean: 0.0000s Median: 0.0000s\n",
      "\t\tMin: 0.0000s Max: 0.0000s StdDev: 0.0000s\n",
      "\t\tCount: 500.0 Percentage of total time: 0.01%\n",
      "\tmoving batch to device:\n",
      "\t\tTotal: 0.0182s Mean: 0.0000s Median: 0.0000s\n",
      "\t\tMin: 0.0000s Max: 0.0001s StdDev: 0.0000s\n",
      "\t\tCount: 500.0 Percentage of total time: 0.66%\n",
      "\tmodel forward:\n",
      "\t\tTotal: 0.3600s Mean: 0.0007s Median: 0.0005s\n",
      "\t\tMin: 0.0005s Max: 0.1127s StdDev: 0.0050s\n",
      "\t\tCount: 500.0 Percentage of total time: 13.10%\n",
      "\tloss backward:\n",
      "\t\tTotal: 0.4000s Mean: 0.0008s Median: 0.0007s\n",
      "\t\tMin: 0.0006s Max: 0.0338s StdDev: 0.0015s\n",
      "\t\tCount: 500.0 Percentage of total time: 14.55%\n",
      "\toptimizer step:\n",
      "\t\tTotal: 0.2459s Mean: 0.0005s Median: 0.0004s\n",
      "\t\tMin: 0.0004s Max: 0.0198s StdDev: 0.0009s\n",
      "\t\tCount: 500.0 Percentage of total time: 8.94%\n",
      "[2025-01-20 18:45:19,497][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:535] Training state saved to cnn_mnist_example/output\n",
      "[2025-01-20 18:45:21,366][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:604] End of epoch\n",
      "[2025-01-20 18:45:21,366][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:604] End of epoch. Global step: 938. Epoch runtime: 4.855359214008786s\n",
      "[2025-01-20 18:45:22,094][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:604] {'epoch': 1,\n",
      " 'global_step': 1000,\n",
      " 'global_steps_per_second': 178.97079035299214,\n",
      " 'learning_rate': 2.3374200426439233e-05,\n",
      " 'tokens_per_second': 0.0,\n",
      " 'tokens_per_step_per_gpu': 0.0,\n",
      " 'total_tokens_seen': 0,\n",
      " 'train/loss': 0.5485665798187256}\n",
      "[2025-01-20 18:45:22,099][oumi.telemetry][rank0][pid:2307328][MainThread][INFO]][telemetry.py:332] Telemetry Summary (PUGET-SYSTEMS):\n",
      "Total time: 5.62 seconds\n",
      "\n",
      "CPU Timers:\n",
      "\tfetching batch:\n",
      "\t\tTotal: 2.2851s Mean: 0.0023s Median: 0.0022s\n",
      "\t\tMin: 0.0007s Max: 0.0073s StdDev: 0.0003s\n",
      "\t\tCount: 1001.0 Percentage of total time: 40.70%\n",
      "\tcomputing tokens:\n",
      "\t\tTotal: 0.0007s Mean: 0.0000s Median: 0.0000s\n",
      "\t\tMin: 0.0000s Max: 0.0000s StdDev: 0.0000s\n",
      "\t\tCount: 1000.0 Percentage of total time: 0.01%\n",
      "\tmoving batch to device:\n",
      "\t\tTotal: 0.0364s Mean: 0.0000s Median: 0.0000s\n",
      "\t\tMin: 0.0000s Max: 0.0001s StdDev: 0.0000s\n",
      "\t\tCount: 1000.0 Percentage of total time: 0.65%\n",
      "\tmodel forward:\n",
      "\t\tTotal: 0.6212s Mean: 0.0006s Median: 0.0005s\n",
      "\t\tMin: 0.0005s Max: 0.1127s StdDev: 0.0036s\n",
      "\t\tCount: 1000.0 Percentage of total time: 11.06%\n",
      "\tloss backward:\n",
      "\t\tTotal: 0.7736s Mean: 0.0008s Median: 0.0007s\n",
      "\t\tMin: 0.0006s Max: 0.0338s StdDev: 0.0011s\n",
      "\t\tCount: 1000.0 Percentage of total time: 13.78%\n",
      "\toptimizer step:\n",
      "\t\tTotal: 0.4741s Mean: 0.0005s Median: 0.0004s\n",
      "\t\tMin: 0.0004s Max: 0.0198s StdDev: 0.0006s\n",
      "\t\tCount: 1000.0 Percentage of total time: 8.44%\n",
      "[2025-01-20 18:45:22,325][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:535] Training state saved to cnn_mnist_example/output\n",
      "[2025-01-20 18:45:24,430][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:604] {'epoch': 1,\n",
      " 'global_step': 1500,\n",
      " 'global_steps_per_second': 189.33543792258084,\n",
      " 'learning_rate': 1.0047974413646056e-05,\n",
      " 'tokens_per_second': 0.0,\n",
      " 'tokens_per_step_per_gpu': 0.0,\n",
      " 'total_tokens_seen': 0,\n",
      " 'train/loss': 0.5729988217353821}\n",
      "[2025-01-20 18:45:24,437][oumi.telemetry][rank0][pid:2307328][MainThread][INFO]][telemetry.py:332] Telemetry Summary (PUGET-SYSTEMS):\n",
      "Total time: 7.95 seconds\n",
      "\n",
      "CPU Timers:\n",
      "\tfetching batch:\n",
      "\t\tTotal: 3.4291s Mean: 0.0023s Median: 0.0022s\n",
      "\t\tMin: 0.0007s Max: 0.0076s StdDev: 0.0003s\n",
      "\t\tCount: 1501.0 Percentage of total time: 43.13%\n",
      "\tcomputing tokens:\n",
      "\t\tTotal: 0.0010s Mean: 0.0000s Median: 0.0000s\n",
      "\t\tMin: 0.0000s Max: 0.0000s StdDev: 0.0000s\n",
      "\t\tCount: 1500.0 Percentage of total time: 0.01%\n",
      "\tmoving batch to device:\n",
      "\t\tTotal: 0.0546s Mean: 0.0000s Median: 0.0000s\n",
      "\t\tMin: 0.0000s Max: 0.0001s StdDev: 0.0000s\n",
      "\t\tCount: 1500.0 Percentage of total time: 0.69%\n",
      "\tmodel forward:\n",
      "\t\tTotal: 0.8679s Mean: 0.0006s Median: 0.0005s\n",
      "\t\tMin: 0.0005s Max: 0.1127s StdDev: 0.0029s\n",
      "\t\tCount: 1500.0 Percentage of total time: 10.92%\n",
      "\tloss backward:\n",
      "\t\tTotal: 1.1440s Mean: 0.0008s Median: 0.0007s\n",
      "\t\tMin: 0.0006s Max: 0.0338s StdDev: 0.0009s\n",
      "\t\tCount: 1500.0 Percentage of total time: 14.39%\n",
      "\toptimizer step:\n",
      "\t\tTotal: 0.6939s Mean: 0.0005s Median: 0.0004s\n",
      "\t\tMin: 0.0004s Max: 0.0198s StdDev: 0.0005s\n",
      "\t\tCount: 1500.0 Percentage of total time: 8.73%\n",
      "[2025-01-20 18:45:24,720][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:535] Training state saved to cnn_mnist_example/output\n",
      "[2025-01-20 18:45:26,270][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:604] End of epoch\n",
      "[2025-01-20 18:45:26,271][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:604] End of epoch. Global step: 1876. Epoch runtime: 4.904201650992036s\n",
      "[2025-01-20 18:45:26,272][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:604] Reached 2 epochs. Training completed.\n",
      "[2025-01-20 18:45:26,273][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:604] Training finished! Global step: 1876 Training runtime: 9.767006382986438s\n",
      "[2025-01-20 18:45:26,273][oumi][rank0][pid:2307328][MainThread][INFO]][train.py:317] Training is Complete.\n",
      "[2025-01-20 18:45:26,286][oumi][rank0][pid:2307328][MainThread][INFO]][device_utils.py:283] GPU Metrics After Training: GPU runtime info: NVidiaGpuRuntimeInfo(device_index=0, device_count=1, used_memory_mb=1927.0, temperature=40, fan_speed=0, fan_speeds=(0, 0), power_usage_watts=89.536, power_limit_watts=350.0, gpu_utilization=67, memory_utilization=11, performance_state=3, clock_speed_graphics=780, clock_speed_sm=780, clock_speed_memory=5001).\n",
      "[2025-01-20 18:45:26,287][oumi][rank0][pid:2307328][MainThread][INFO]][torch_utils.py:117] Peak GPU memory usage: 0.07 GB\n",
      "[2025-01-20 18:45:26,287][oumi][rank0][pid:2307328][MainThread][INFO]][train.py:324] Saving final state...\n",
      "[2025-01-20 18:45:26,526][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:535] Training state saved to cnn_mnist_example/output\n",
      "[2025-01-20 18:45:26,526][oumi][rank0][pid:2307328][MainThread][INFO]][train.py:329] Saving final model...\n",
      "[2025-01-20 18:45:26,529][oumi][rank0][pid:2307328][MainThread][INFO]][oumi_trainer.py:604] Model saved to cnn_mnist_example/output/model.safetensors.\n",
      "[2025-01-20 18:45:26,530][oumi][rank0][pid:2307328][MainThread][INFO]][train.py:336] \n",
      "\n",
      "Â» We're always looking for feedback. What's one thing we can improve? https://oumi.ai/feedback\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.configs import TrainingConfig\n",
    "from oumi.train import train\n",
    "\n",
    "config = TrainingConfig.from_yaml(str(Path(tutorial_dir) / \"train.yaml\"))\n",
    "\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XfsWKEFSgtR"
   },
   "source": [
    "Congratulations, you've trained your first CNN using custom dataset (`numpy` arrays) using Oumi!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
