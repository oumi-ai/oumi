{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Inference, Evaluate (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our working directory\n",
    "\n",
    "For our experiments, we'll use the following folder to save the model, training artifacts, and our working configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Oumi -> return_full_text field so the user can deactivate the model\n",
    "# responding with the input text as well.\n",
    "# see https://huggingface.co/docs/transformers/v4.17.0/main_classes/pipelines\n",
    "\n",
    "## Add reference: https://github.com/QwenLM/Qwen2-VL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gor the following warning:\n",
    "# /home/gcpuser/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21:\n",
    "# TqdmWarning: IProgress not found. Please update jupyter and ipywidgets.\n",
    "# See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "#   from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"vision_language_tutorial\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.builders import build_tokenizer\n",
    "from oumi.core.configs import ModelParams\n",
    "from oumi.datasets.vision_language.vqav2_small import Vqav2SmallDataset\n",
    "\n",
    "# Initialize the dataset, build the tokenizer\n",
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "tokenizer = build_tokenizer(ModelParams(model_name=model_name))\n",
    "dataset = Vqav2SmallDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    processor_name=model_name,\n",
    "    # limit=1000,  # Limit the number of examples to load for demonstration purposes\n",
    ")\n",
    "print(\"Examples included:\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from oumi.core.types.conversation import Type\n",
    "\n",
    "# Print a few examples\n",
    "for i in range(2):\n",
    "    conversation = dataset.conversation(i)\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    for message in conversation.messages:\n",
    "        ## More pythonic way to display image below???\n",
    "        if message.role == \"user\":  # User poses a question, regarding an image\n",
    "            img_content = message.content[0]\n",
    "            assert (\n",
    "                img_content.type == Type.IMAGE_BINARY\n",
    "            ), \"Oumi encodes image content in binary.\"\n",
    "            image = Image.open(io.BytesIO(img_content.binary))\n",
    "            display(image.resize((256, 256)))  # Resize for display\n",
    "\n",
    "        print(f\"{message.role}: {message.content[:100]}...\")  # Truncate for brevity\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to see directly the data\n",
    "dataset.data.head()  # Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/infer.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "  torch_dtype_str: \"bfloat16\" # Assumes your GPU supports bfloat16 (Ampere or newer)\n",
    "  chat_template: \"qwen2-vl-instruct\"\n",
    "  model_max_length: 4096\n",
    "  trust_remote_code: True\n",
    "  \n",
    "generation:\n",
    "  max_new_tokens: 64\n",
    "  batch_size: 1\n",
    "  \n",
    "engine: NATIVE # We are using a native engine for inference, consider VLLM if available for much faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.core.configs import InferenceConfig\n",
    "from oumi.infer import infer\n",
    "\n",
    "config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"infer.yaml\"))\n",
    "\n",
    "# Use the data of the ith conversation as input\n",
    "conversation_id = 1\n",
    "query_img = dataset.conversation(conversation_id).messages[0].image_content_items\n",
    "query_text = dataset.conversation(conversation_id).messages[0].text_content_items\n",
    "\n",
    "print(query_text)\n",
    "\n",
    "results = infer(\n",
    "    config=config,\n",
    "    inputs=[str(query_text[0])],\n",
    "    # inputs=[\"Desrcibe the image\"],\n",
    "    input_image_bytes=query_img[0].binary,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = results[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "# Load the image\n",
    "query_img_bytes = query_img[0].binary\n",
    "image = Image.open(io.BytesIO(query_img_bytes))\n",
    "\n",
    "# Define bounding box coordinates based on the given format\n",
    "top_left = (101, 39)  # (X_top_left, Y_top_left)\n",
    "bottom_right = (341, 694)  # (X_bottom_right, Y_bottom_right)\n",
    "\n",
    "# Draw the bounding box\n",
    "draw = ImageDraw.Draw(image)\n",
    "draw.rectangle([top_left, bottom_right], outline=\"red\", width=1)\n",
    "\n",
    "# Show the image with bounding box\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/train.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  model_max_length: 4096\n",
    "  trust_remote_code: True\n",
    "  attn_implementation: \"sdpa\"\n",
    "  chat_template: \"qwen2-vl-instruct\"\n",
    "  freeze_layers:\n",
    "    - \"visual\"     # Let's train only the language component of the model for faster training\n",
    "\n",
    "data:\n",
    "  train:\n",
    "    collator_name: \"vision_language_with_padding\" # simple padding collator\n",
    "    datasets:\n",
    "      - dataset_name: \"merve/vqav2-small\"\n",
    "        split: \"validation\" # This dataset has only a validation split\n",
    "        shuffle: True\n",
    "        seed: 42\n",
    "        transform_num_workers: \"auto\"\n",
    "        dataset_kwargs:\n",
    "          processor_name: \"Qwen/Qwen2-VL-2B-Instruct\" # i.e., the default for our model\n",
    "          limit: 4096\n",
    "          return_tensors: True      \n",
    "\n",
    "training:\n",
    "  output_dir: \"vision_language_tutorial\"\n",
    "  trainer_type: \"TRL_SFT\"\n",
    "  enable_gradient_checkpointing: True\n",
    "  per_device_train_batch_size: 1 # Must be 1: the model generates variable-sized image features.\n",
    "  gradient_accumulation_steps: 32\n",
    "  \n",
    "  # ***NOTE***\n",
    "  # We set it to 10 steps to first verify that it works\n",
    "  # Swap to num_train_epochs: 1 to get more meaningful results.\n",
    "  # Note: 1 training epoch will take XXX hours on a single A100-40GB GPU.\n",
    "  # max_steps: 20\n",
    "  num_train_epochs: 1\n",
    "\n",
    "  gradient_checkpointing_kwargs:\n",
    "    # Reentrant docs: https://pytorch.org/docs/stable/checkpoint.html#torch.utils.checkpoint.checkpoint\n",
    "    use_reentrant: False\n",
    "  ddp_find_unused_parameters: False\n",
    "  empty_device_cache_steps: 1\n",
    "\n",
    "  optimizer: \"adamw_torch_fused\"\n",
    "  learning_rate: 2e-5\n",
    "  warmup_ratio: 0.03\n",
    "  weight_decay: 0.0\n",
    "  lr_scheduler_type: \"cosine\"\n",
    "\n",
    "  logging_steps: 5\n",
    "  save_steps: 0\n",
    "  dataloader_main_process_only: False\n",
    "  dataloader_num_workers: 2\n",
    "  dataloader_prefetch_factor: 8\n",
    "  include_performance_metrics: True\n",
    "  enable_wandb: True # Set to False if you don't want to use Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!oumi train -c \"$tutorial_dir/train.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Fine-tuned Model\n",
    "\n",
    "Once we're happy with the results, we can serve the fine-tuned model for interactive inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/trained_infer.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"vision_language_tutorial\"  \n",
    "  torch_dtype_str: \"bfloat16\" # Assumes your GPU supports bfloat16 (Ampere or newer)\n",
    "  chat_template: \"qwen2-vl-instruct\"\n",
    "  model_max_length: 4096\n",
    "  trust_remote_code: True\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 64\n",
    "  batch_size: 1\n",
    "  \n",
    "engine: NATIVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"trained_infer.yaml\"))\n",
    "\n",
    "# Use the data of the first conversation as input\n",
    "query_img = dataset.conversation(0).messages[0].image_content_items\n",
    "query_text = dataset.conversation(0).messages[0].text_content_items\n",
    "\n",
    "print(query_text)\n",
    "\n",
    "results = infer(\n",
    "    config=config,\n",
    "    inputs=[str(query_text[0])],\n",
    "    # inputs=[\"Desrcibe the image\"],\n",
    "    input_image_bytes=query_img[0].binary,\n",
    ")\n",
    "\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO. New dataset: Loading the dataset\n",
    "# dataset_id = \"HuggingFaceM4/the_cauldron\"\n",
    "# subset = \"geomverse\"\n",
    "# dataset = load_dataset(dataset_id, subset, split=\"train\")\n",
    "\n",
    "# # Selecting a subset of 3K samples for fine-tuning\n",
    "# dataset = dataset.select(range(3000))\n",
    "# print(f\"Using a sample size of {len(dataset)} for fine-tuning.\")\n",
    "# print(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
