{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Internal Notes: to be deleted.\n",
    "\n",
    "1 TODO: Let's implement a `return_full_text` field so the user can demand a model\n",
    "does not include the the input text as well in its response\n",
    "see https://huggingface.co/docs/transformers/v4.17.0/main_classes/pipelines\n",
    "\n",
    "2 pip installing Oumi with [.gpu] it does not include ipywidgets which disables the monitoring of \n",
    "tqdm inside the notebook and results below in: `TqdmWarning: IProgress not found. Please update jupyter and ipywidgets`\n",
    "Handling it with `!pip install ipywidgets`, TODO: Can we do better?\n",
    "\n",
    "\n",
    "!pip install ipywidgets # Installing ipywidgets for widget visualization -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a Vision-Language Model (Overview)\n",
    "\n",
    "In this tutorial, we'll use LoRA training and SFT to guide a large vision/language model to produce short and concise answer grounded on visual input.\n",
    "\n",
    "Specifically, we'll use the Oumi framework to streamline the process and achieve high-quality results fast.\n",
    "\n",
    "We'll cover the following topics:\n",
    "1. Prerequisites\n",
    "2. Data Preparation & Sanity Checks\n",
    "3. Training Config Preparation\n",
    "4. Launching Training\n",
    "5. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "## Oumi Installation\n",
    "First, let's install Oumi and some additional packages. For this notebook you will need access to at least one NVIDIA or AMD GPU **with ~30GBs of memory**.\n",
    "\n",
    "You can find detailed installation instructions [here](https://github.com/oumi-ai/oumi/blob/main/README.md), but for Oumi it should be as simple as:\n",
    "\n",
    "```bash\n",
    "pip install -e \".[gpu]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally, install the following packages for widget visualization.\n",
    "!pip install ipywidgets\n",
    "\n",
    "# And deactivate the parallelism warning from the tokenizers library.\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Deactivate relevant HF warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our working directory\n",
    "\n",
    "For our experiments, we'll use the following folder to save the model, training artifacts, and our inference and training configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"vision_language_tutorial\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows we use Meta's [Llama-3.2-11B-Vision-Instruct](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/) model.\n",
    "\n",
    "Llama-11B-Vision is a high-performing instruction-tuned multi-modal model, which uses a moderate amount of resources (11B parameters).\n",
    "\n",
    "We will finetune this model with the [vqav2-small](https://huggingface.co/datasets/merve/vqav2-small) dataset which will help the model respond in __a succinct manner__ on visually grounded questions.\n",
    "\n",
    "The principles presented here are generic and \"Oumi-flexible\". \n",
    "\n",
    "To repeat this experiment with other models/data you can simply replace e.g., the `model_name` (a string) with the names of other supported models (see [here](https://oumi.ai/docs/en/latest/resources/models/supported_models.html)) and adapt the configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's initialize our dataset and build a tokenizer and an underlying data processor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.builders import build_tokenizer\n",
    "from oumi.core.configs import ModelParams\n",
    "from oumi.datasets.vision_language.vqav2_small import Vqav2SmallDataset\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "tokenizer = build_tokenizer(ModelParams(model_name=model_name))\n",
    "\n",
    "dataset = Vqav2SmallDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    processor_name=model_name,\n",
    "    limit=1000,  # Limit the number of examples for demonstration purposes (!)\n",
    ")\n",
    "\n",
    "print(\"\\nExamples included:\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's see a few examples to get a feel for the dataset we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from oumi.core.types.conversation import Type\n",
    "\n",
    "num_examples_to_display = 3\n",
    "\n",
    "for i in range(num_examples_to_display):\n",
    "    conversation = dataset.conversation(i)  # Retrieve the i-th example (conversation)\n",
    "\n",
    "    print(f\"Example {i}:\")\n",
    "\n",
    "    for message in conversation.messages:\n",
    "        if message.role == \"user\":  # The `user` poses a question, regarding an image\n",
    "            img_content = message.content[0]\n",
    "            assert (\n",
    "                img_content.type == Type.IMAGE_BINARY\n",
    "            ), \"Oumi encodes image content in binary for VQA-Small.\"\n",
    "\n",
    "            image = Image.open(io.BytesIO(img_content.binary))\n",
    "            image.save(f\"{tutorial_dir}/example_{i}.png\")  # Save the image locally\n",
    "            display(image)\n",
    "\n",
    "        print(f\"{message.role}: {message.content}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above the ground-truth answers are **very short and succinct**, which can be an advantage for scenarios where we want to generate concise answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Furthermore, if you want to see directly the underlying stored data, stored in a\n",
    "# pandas DataFrame, you can do so by running the following command:\n",
    "dataset.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Responses\n",
    "\n",
    "Let's see now how this model performs on a given prompt without any finetuning.\n",
    "- For this we will create and execute and `inference configuration` stored in a YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/infer.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "  torch_dtype_str: \"bfloat16\" # Good choice if you have access to Ampere or newer GPU\n",
    "  chat_template: \"llama3-instruct\"\n",
    "  model_max_length: 1024\n",
    "  trust_remote_code: False # For other models this might need to be set to True\n",
    "  \n",
    "generation:\n",
    "  max_new_tokens: 128\n",
    "  batch_size: 1\n",
    "  \n",
    "engine: NATIVE \n",
    "# Let's use the `native` engine (i.e., the underlying machine's default)\n",
    "# for inference.  \n",
    "# You can also consider VLLM, if are working with GPU for much faster inference. \n",
    "# To install an Oumi tested/compatible version, use:\n",
    "# pip install vllm>=0.6.3,<0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.core.configs import InferenceConfig\n",
    "from oumi.core.types.conversation import Conversation, Message, Role\n",
    "from oumi.inference import NativeTextInferenceEngine\n",
    "\n",
    "# Note: the *first* time you call inference will take a few minutes to download\n",
    "# and cache the model (assuming you do not already have it downloaded locally).\n",
    "inference_config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"infer.yaml\"))\n",
    "inference_engine = NativeTextInferenceEngine(inference_config.model)\n",
    "\n",
    "example = dataset.conversation(1)\n",
    "example = Conversation(messages=example.filter_messages(Role.USER))\n",
    "inference_engine.infer([example], inference_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up to free-up GPU memory used for inference above\n",
    "# Delete the inference_engine and collect garbage\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "\n",
    "del inference_engine\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note. You can do the same inference directly with our CLI (terminal) instead of the\n",
    "# Python API. E.g., uncomment the following line and execute this cell:\n",
    "\n",
    "conversation_id = 1\n",
    "query_text_string = (\n",
    "    dataset.conversation(conversation_id).messages[0].text_content_items[0].content\n",
    ")\n",
    "print(f\"\\n{query_text_string}\")\n",
    "\n",
    "# !echo \"$query_text_string\" | oumi infer -c \"$tutorial_dir/infer.yaml\" -i --image=\"$tutorial_dir/example_1.png\" # noqa: E501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! As you can see by default this model gives quite __verbose__ responses. Can we change this behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our training experiment\n",
    " - Specifically, let's create an execute a YAML file with our _training_ config!\n",
    " - You can find many more details about the listed hyper-parameters in our [docs](https://oumi.ai/docs/en/latest/user_guides/train/training_methods.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/train.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  model_max_length: 1024  \n",
    "  attn_implementation: \"sdpa\"\n",
    "  chat_template: \"llama3-instruct\"\n",
    "  freeze_layers:\n",
    "    - \"visual\"     # Let's finetune only the language component of the model\n",
    "\n",
    "data:\n",
    "  train:\n",
    "    collator_name: \"vision_language_with_padding\" # Simple padding collator\n",
    "    use_torchdata: True\n",
    "\n",
    "    datasets:\n",
    "      - dataset_name: \"merve/vqav2-small\"\n",
    "        split: \"validation\" # This dataset has only a validation split\n",
    "        shuffle: True\n",
    "        seed: 42\n",
    "        transform_num_workers: \"auto\"\n",
    "        dataset_kwargs:\n",
    "          # The default for our model:\n",
    "          processor_name: \"meta-llama/Llama-3.2-11B-Vision-Instruct\"           \n",
    "          limit: 1000 # Again, we downsample to 1000 examples for demonstration \n",
    "                      # purposes only.\n",
    "          return_tensors: True      \n",
    "\n",
    "training:\n",
    "  output_dir: \"vision_language_tutorial\"\n",
    "  trainer_type: \"TRL_SFT\"\n",
    "  enable_gradient_checkpointing: True\n",
    "  # You can decrease the following two params if you run out of memory\n",
    "  per_device_train_batch_size: 2 \n",
    "  gradient_accumulation_steps: 8 # Thus effective batch size is 2x8=16 on a single GPU\n",
    "  use_peft: True\n",
    "  \n",
    "  # **NOTE**\n",
    "  # We set `max_steps` to 10 steps to first verify that training works\n",
    "  # Swap to `num_train_epochs: 1` to get more meaningful results\n",
    "  # (One training epoch will take ~25 mins on a single A100-40GB GPUs)\n",
    "  max_steps: 10\n",
    "  # num_train_epochs: 1\n",
    "\n",
    "  gradient_checkpointing_kwargs:\n",
    "    # Reentrant docs: https://pytorch.org/docs/stable/checkpoint.html#torch.utils.checkpoint.checkpoint\n",
    "    use_reentrant: False\n",
    "  ddp_find_unused_parameters: False\n",
    "  empty_device_cache_steps: 1\n",
    "\n",
    "  optimizer: \"adamw_torch_fused\"\n",
    "  learning_rate: 2e-5\n",
    "  warmup_ratio: 0.03\n",
    "  weight_decay: 0.0\n",
    "  lr_scheduler_type: \"cosine\"\n",
    "\n",
    "  logging_steps: 5\n",
    "  save_steps: 0\n",
    "  dataloader_main_process_only: False\n",
    "  dataloader_num_workers: \"auto\"\n",
    "  dataloader_prefetch_factor: 16\n",
    "  include_performance_metrics: True\n",
    "  enable_wandb: True # Set to False if you don't want to use Weights & Biases\n",
    "  \n",
    "peft: # Our LoRA configuration; we target several layers  \n",
    "  lora_r: 8\n",
    "  lora_alpha: 8\n",
    "  lora_dropout: 0.1\n",
    "  lora_target_modules:\n",
    "    - \"q_proj\"\n",
    "    - \"v_proj\"\n",
    "    - \"o_proj\"\n",
    "    - \"k_proj\"\n",
    "    - \"gate_proj\"\n",
    "    - \"up_proj\"\n",
    "    - \"down_proj\"\n",
    "  lora_init_weights: GAUSSIAN\n",
    "\n",
    "# Below lines are effective if you have access to multiple GPUs\n",
    "# If you do, please uncomment them to train with all available GPUS:\n",
    "\n",
    "# fsdp:\n",
    "#   enable_fsdp: True\n",
    "#   sharding_strategy: \"HYBRID_SHARD\"\n",
    "#   forward_prefetch: True\n",
    "#   auto_wrap_policy: \"TRANSFORMER_BASED_WRAP\"\n",
    "#   transformer_layer_cls: \"MllamaSelfAttentionDecoderLayer,MllamaCrossAttentionDecoderLayer,MllamaVisionEncoderLayer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's launch the training!\n",
    "\n",
    "!oumi train -c \"$tutorial_dir/train.yaml\"\n",
    "\n",
    "# Or, if you have multiple GPUS you want to use:\n",
    "# !oumi distributed torchrun -m oumi train -c \"$tutorial_dir/train.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, let's use the Fine-tuned Model and see the effect of training!\n",
    "\n",
    "Once we're happy with the results, we can serve the fine-tuned model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/trained_infer.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"meta-llama/Llama-3.2-11B-Vision-Instruct\"  \n",
    "  adapter_model: \"vision_language_tutorial\" # Directory with our saved LoRA parameters!\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  chat_template: \"llama3-instruct\"\n",
    "  model_max_length: 1024\n",
    "  trust_remote_code: False\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 256\n",
    "  batch_size: 1\n",
    "  \n",
    "engine: NATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"trained_infer.yaml\"))\n",
    "inference_engine = NativeTextInferenceEngine(config.model)\n",
    "\n",
    "example = dataset.conversation(1)\n",
    "example = Conversation(messages=example.filter_messages(Role.USER))\n",
    "inference_engine.infer([example], inference_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, if you want to test it with your own image/question pair:\n",
    "from oumi.core.types.conversation import ContentItem\n",
    "from oumi.utils.image_utils import load_image_png_bytes_from_path\n",
    "\n",
    "your_image_path = f\"{tutorial_dir}/example_1.png\"  # \"Replace with your image path!\"\n",
    "image_bytes = load_image_png_bytes_from_path(your_image_path)\n",
    "\n",
    "conversation = Conversation(\n",
    "    messages=[\n",
    "        Message(\n",
    "            role=Role.USER,\n",
    "            content=[\n",
    "                ContentItem(type=Type.IMAGE_BINARY, binary=image_bytes),\n",
    "                ContentItem(type=Type.TEXT, content=\"Your question here!\"),\n",
    "            ],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "inference_engine.infer([conversation], config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
