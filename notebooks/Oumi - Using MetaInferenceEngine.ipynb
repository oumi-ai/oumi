{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oumi - Using MetaInferenceEngine\n",
    "\n",
    "The `MetaInferenceEngine` provides a simplified interface for running inference across multiple models and providers.\n",
    "\n",
    "This notebook demonstrates how to use the `MetaInferenceEngine` to run inference with different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API keys (or they'll be loaded from environment variables)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-api-key\"\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"your-google-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Conversation\n",
    "\n",
    "First, let's create a conversation to use with our inference engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from oumi.core.types.conversation import Conversation, Message, Role\n",
    "\n",
    "# Create a conversation with a user prompt\n",
    "conversation = Conversation(messages=[\n",
    "    Message(role=Role.USER, content=\"Explain quantum computing in simple terms.\")\n",
    "])\n",
    "\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the MetaInferenceEngine\n",
    "\n",
    "Now, let's initialize the `MetaInferenceEngine` with some generation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from oumi.inference import MetaInferenceEngine\n",
    "\n",
    "# Initialize the engine with generation parameters\n",
    "engine = MetaInferenceEngine(\n",
    "    temperature=0.7,     # Control randomness (0.0 = deterministic, 1.0 = maximum randomness)\n",
    "    max_tokens=1000,     # Maximum number of tokens to generate (will be converted to max_new_tokens)\n",
    "    top_p=0.95           # Control diversity of generated text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference with OpenAI Models\n",
    "\n",
    "First, let's try running inference with an OpenAI model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    # Run inference with GPT-4\n",
    "    gpt4_response = engine.infer([conversation], model_name=\"gpt-4o\")\n",
    "    \n",
    "    # Print the response\n",
    "    print(\"=== GPT-4o Response ===\\n\")\n",
    "    print(gpt4_response[0].messages[-1].content)\n",
    "except Exception as e:\n",
    "    print(f\"Error running inference with OpenAI: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference with Anthropic Models\n",
    "\n",
    "Now, let's try using an Anthropic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    # Run inference with Claude\n",
    "    claude_response = engine.infer([conversation], model_name=\"claude-3-sonnet\")\n",
    "    \n",
    "    # Print the response\n",
    "    print(\"=== Claude Response ===\\n\")\n",
    "    print(claude_response[0].messages[-1].content)\n",
    "except Exception as e:\n",
    "    print(f\"Error running inference with Anthropic: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference with Gemini Models\n",
    "\n",
    "Let's try a Google Gemini model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    # Run inference with Gemini\n",
    "    gemini_response = engine.infer([conversation], model_name=\"gemini-pro\")\n",
    "    \n",
    "    # Print the response\n",
    "    print(\"=== Gemini Response ===\\n\")\n",
    "    print(gemini_response[0].messages[-1].content)\n",
    "except Exception as e:\n",
    "    print(f\"Error running inference with Gemini: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Fully Qualified Model Names\n",
    "\n",
    "The MetaInferenceEngine supports fully qualified model names in the format `engine/model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    # Using VLLM engine with Llama-3 model\n",
    "    vllm_response = engine.infer([conversation], model_name=\"vllm/llama3.1-8b\")\n",
    "    \n",
    "    # Using Together API with a model\n",
    "    together_response = engine.infer([conversation], model_name=\"together/llama3.1-70b\")\n",
    "    \n",
    "    # Print the responses\n",
    "    print(\"=== VLLM Engine Response ===\\n\")\n",
    "    print(vllm_response[0].messages[-1].content)\n",
    "    \n",
    "    print(\"\\n=== Together Engine Response ===\\n\")\n",
    "    print(together_response[0].messages[-1].content)\n",
    "except Exception as e:\n",
    "    print(f\"Error running inference with qualified names: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CLI Aliases\n",
    "\n",
    "You can also use the CLI aliases defined in Oumi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    # Using a CLI alias for Claude model\n",
    "    claude_response = engine.infer([conversation], model_name=\"claude-3-7-sonnet\")\n",
    "    \n",
    "    # Print the response\n",
    "    print(\"=== Claude 3.7 Sonnet (Alias) Response ===\\n\")\n",
    "    print(claude_response[0].messages[-1].content)\n",
    "except Exception as e:\n",
    "    print(f\"Error running inference with CLI alias: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference with Local Models\n",
    "\n",
    "If you have local models available, you can also use those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    # Run inference with local Llama model\n",
    "    # Note: This requires having the model downloaded locally\n",
    "    llama_response = engine.infer([conversation], model_name=\"meta-llama/Llama-3-8b\")\n",
    "    \n",
    "    # Print the response\n",
    "    print(\"=== Llama Response ===\\n\")\n",
    "    print(llama_response[0].messages[-1].content)\n",
    "except Exception as e:\n",
    "    print(f\"Error running inference with local model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Different Generation Parameters\n",
    "\n",
    "You can create multiple engines with different generation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create an engine with high creativity\n",
    "creative_engine = MetaInferenceEngine(temperature=0.9, top_p=0.98)\n",
    "\n",
    "# Create an engine with low creativity (more focused)\n",
    "focused_engine = MetaInferenceEngine(temperature=0.2, top_p=0.5)\n",
    "\n",
    "# Create a conversation with a creative prompt\n",
    "creative_conversation = Conversation(messages=[\n",
    "    Message(role=Role.USER, content=\"Create a short poem about artificial intelligence.\")\n",
    "])\n",
    "\n",
    "try:\n",
    "    # Get creative response\n",
    "    creative_response = creative_engine.infer([creative_conversation], model_name=\"gpt-4o\")\n",
    "    \n",
    "    # Get focused response\n",
    "    focused_response = focused_engine.infer([creative_conversation], model_name=\"gpt-4o\")\n",
    "    \n",
    "    # Print both responses\n",
    "    print(\"=== Creative (High Temperature) Response ===\\n\")\n",
    "    print(creative_response[0].messages[-1].content)\n",
    "    \n",
    "    print(\"\\n=== Focused (Low Temperature) Response ===\\n\")\n",
    "    print(focused_response[0].messages[-1].content)\n",
    "except Exception as e:\n",
    "    print(f\"Error running inference: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Custom API Keys\n",
    "\n",
    "If you need to use custom API keys for a specific request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    # Using a custom API key for a specific request\n",
    "    response = engine.infer(\n",
    "        [conversation], \n",
    "        model_name=\"gpt-4o\", \n",
    "        remote_params={\"api_key\": os.environ.get(\"OPENAI_API_KEY\")}\n",
    "    )\n",
    "    \n",
    "    print(\"=== Response with Custom API Key ===\\n\")\n",
    "    print(response[0].messages[-1].content)\n",
    "except Exception as e:\n",
    "    print(f\"Error running inference with custom API key: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The `MetaInferenceEngine` provides a simplified interface for working with multiple models and providers. It automatically selects the appropriate engine based on the model name, handles parameter conversion, and provides a consistent interface for all models.\n",
    "\n",
    "Key benefits:\n",
    "- Simplified interface for multiple models\n",
    "- Automatic engine selection\n",
    "- Support for fully qualified model names and CLI aliases\n",
    "- Parameter normalization\n",
    "- Engine caching for better performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}