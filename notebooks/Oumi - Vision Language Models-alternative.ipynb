{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Internal Notes: to be deleted.\n",
    "\n",
    "1 TODO: Let's implement a `return_full_text` field so the user can demand a model\n",
    "does not include the the input text as well in its response\n",
    "see https://huggingface.co/docs/transformers/v4.17.0/main_classes/pipelines\n",
    "\n",
    "2 pip installing Oumi with [.gpu] it does not include ipywidgets which disables the monitoring of \n",
    "tqdm inside the notebook and results below in: `TqdmWarning: IProgress not found. Please update jupyter and ipywidgets`\n",
    "Handling it with `!pip install ipywidgets`, TODO: Can we do better?\n",
    "\n",
    "\n",
    "!pip install ipywidgets # Installing ipywidgets for widget visualization -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a Vision-Language Model (Overview)\n",
    "\n",
    "In this tutorial, we'll use SFT training to guide a large vision/language model to produce short and concise answer grounded on visual input.\n",
    "\n",
    "Specifically, we'll use the Oumi framework to streamline the process and achieve high-quality results fast.\n",
    "\n",
    "We'll cover the following topics:\n",
    "1. Prerequisites\n",
    "2. Data Preparation & Sanity Checks\n",
    "3. Training Config Preparation\n",
    "4. Launching Training\n",
    "5. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "## Oumi Installation\n",
    "First, let's install Oumi and some additional packages. You can find detailed instructions [here](https://github.com/oumi-ai/oumi/blob/main/README.md), but for Oumi it should be as simple as:\n",
    "\n",
    "```bash\n",
    "pip install -e \".[gpu]\"  # if you have an nvidia or AMD GPU\n",
    "# OR\n",
    "pip install -e \".\"  # if you don't have a GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally, install the following packages for widget visualization.\n",
    "!pip install ipywidgets\n",
    "\n",
    "# Also, deactivate the parallelism warning from the tokenizers library.\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # deactivate relevant HF warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our working directory\n",
    "\n",
    "For our experiments, we'll use the following folder to save the model, training artifacts, and our inference and training configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"vision_language_tutorial\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows we use the [Qwen2-VL](https://github.com/QwenLM/Qwen2-VL) model.\n",
    "\n",
    "Qwen2-VL is a high-performing multi-modal model which uses a modest amount of resources (2B parameters).\n",
    "\n",
    "We will finetune this model with the [vqav2-small](https://huggingface.co/datasets/merve/vqav2-small) dataset which will help the model respond in a succinct manner on visually grounded questions.\n",
    "\n",
    "The principles presented here are generic and \"Oumi-flexible\". \n",
    "\n",
    "To repeat the exercise with other models/data you can simply replace e.g., the `model_name` (a string) with the names of other supported models (see [here](https://oumi.ai/docs/latest/resources/models/supported_models.html)) and adapt the configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's initialize our dataset and build a tokenizer and an underlying processor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.builders import build_tokenizer\n",
    "from oumi.core.configs import ModelParams\n",
    "from oumi.datasets.vision_language.vqav2_small import Vqav2SmallDataset\n",
    "\n",
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "tokenizer = build_tokenizer(ModelParams(model_name=model_name))\n",
    "\n",
    "dataset = Vqav2SmallDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    processor_name=model_name,\n",
    "    limit=2000,  # Limit the number of examples for demonstration purposes (!)\n",
    ")\n",
    "\n",
    "print(\"\\nExamples included:\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's see a few examples to get a feel for the dataset we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from oumi.core.types.conversation import Type\n",
    "\n",
    "num_examples_to_display = 2\n",
    "\n",
    "for i in range(num_examples_to_display):\n",
    "    conversation = dataset.conversation(i)  # retrieve the i-th example (conversation)\n",
    "\n",
    "    print(f\"Example {i + 1}:\")\n",
    "\n",
    "    for message in conversation.messages:\n",
    "        if message.role == \"user\":  # The `user` poses a question, regarding an image\n",
    "            img_content = message.content[0]\n",
    "            assert (\n",
    "                img_content.type == Type.IMAGE_BINARY\n",
    "            ), \"Oumi encodes image content in binary for VQA-Small.\"\n",
    "\n",
    "            image = Image.open(io.BytesIO(img_content.binary))\n",
    "            image.save(f\"{tutorial_dir}/example_{i}.jpg\")  # optionally save the image\n",
    "            display(image)\n",
    "\n",
    "        print(f\"{message.role}: {message.content}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above the ground-truth answers are **very short and succinct**, which can be an advantage for scenarios where we want to generate concise answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Furthermore, if you want to see directly the underlying stored data, stored in a\n",
    "# pandas DataFrame, you can do so by running the following command:\n",
    "dataset.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Responses\n",
    "\n",
    "Let's see how our model performs on an example prompt without any finetuning.\n",
    "- For this we will create and execute and `inference configuration` stored in a YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/infer.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "  torch_dtype_str: \"bfloat16\" # good choice if you have access to Ampere or newer GPU\n",
    "  chat_template: \"qwen2-vl-instruct\"\n",
    "  model_max_length: 4096\n",
    "  trust_remote_code: True\n",
    "  \n",
    "generation:\n",
    "  max_new_tokens: 64\n",
    "  batch_size: 1\n",
    "  \n",
    "engine: NATIVE \n",
    "# Let's use the `native` engine (i.e., the underlying machine's default) for inference.  \n",
    "# You can also consider VLLM, if are working with GPU for much faster inference. \n",
    "# To install it with Oumi: \n",
    "# pip install \".[optional]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.core.configs import InferenceConfig\n",
    "from oumi.infer import infer\n",
    "\n",
    "config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"infer.yaml\"))\n",
    "\n",
    "# Use first data example here as input\n",
    "conversation_id = 0\n",
    "query_img = dataset.conversation(conversation_id).messages[0].image_content_items\n",
    "query_text = dataset.conversation(conversation_id).messages[0].text_content_items\n",
    "query_text_string = query_text[0].content\n",
    "print(f\"\\n{query_text_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.inference import NativeTextInferenceEngine\n",
    "\n",
    "inference_engine = NativeTextInferenceEngine(config.model)\n",
    "inference_engine.infer([dataset.conversation(0)], config)  # does not seem to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.utils.image_utils import create_png_bytes_from_image\n",
    "\n",
    "test_image = Image.open(f\"{tutorial_dir}/example_{0}.jpg\")\n",
    "image_bytes = create_png_bytes_from_image(test_image)\n",
    "\n",
    "results = infer(\n",
    "    config=config,\n",
    "    inputs=[query_text_string],\n",
    "    input_image_bytes=image_bytes,  # this works\n",
    "    #     input_image_bytes=query_img[0].binary, # does not seem to work with this\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## temporary-note: # this works too\n",
    "# Note. You can do the same inference directly with our CLI client instead of the\n",
    "# Python API. E.g., uncomment the following line and execute this cell:\n",
    "\n",
    "# !echo \"$query_text_string\" | oumi infer -c \"$tutorial_dir/infer.yaml\" -i --image=\"$tutorial_dir/example_0.jpg\" # noqa: E501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! That's good -- but as you can see by default the model gives quite verbose responses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our training experiment\n",
    " - Specifically, let's create an execute a YAML file with our training config!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/train.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  model_max_length: 4096\n",
    "  trust_remote_code: True\n",
    "  attn_implementation: \"sdpa\"\n",
    "  chat_template: \"qwen2-vl-instruct\"\n",
    "  freeze_layers:\n",
    "    - \"visual\"     # Let's finetune only the language component of the model\n",
    "\n",
    "data:\n",
    "  train:\n",
    "    collator_name: \"vision_language_with_padding\" # simple padding collator\n",
    "    experimental_use_torch_datapipes: True\n",
    "\n",
    "    datasets:\n",
    "      - dataset_name: \"merve/vqav2-small\"\n",
    "        split: \"validation\" # This dataset has only a validation split\n",
    "        shuffle: True\n",
    "        seed: 42\n",
    "        transform_num_workers: \"auto\"\n",
    "        dataset_kwargs:\n",
    "          processor_name: \"Qwen/Qwen2-VL-2B-Instruct\" # i.e., the default for our model\n",
    "          limit: 2000 # Again, we downsample to 2,000 examplesfor demonstration purposes only.\n",
    "          return_tensors: True      \n",
    "\n",
    "training:\n",
    "  output_dir: \"vision_language_tutorial\"\n",
    "  trainer_type: \"TRL_SFT\"\n",
    "  enable_gradient_checkpointing: True\n",
    "  per_device_train_batch_size: 1 # Must be 1: the model generates variable-sized image features.\n",
    "  gradient_accumulation_steps: 32\n",
    "  \n",
    "  # ***NOTE***\n",
    "  # We set `max_steps` to 20 steps to first verify that it works\n",
    "  # Swap to `num_train_epochs: 1` to get more meaningful results.\n",
    "  # Note: 1 training epoch will take ~11 mins on a single A100-40GB GPU and 2,000 examples.\n",
    "  max_steps: 20\n",
    "  # num_train_epochs: 1\n",
    "\n",
    "  gradient_checkpointing_kwargs:\n",
    "    # Reentrant docs: https://pytorch.org/docs/stable/checkpoint.html#torch.utils.checkpoint.checkpoint\n",
    "    use_reentrant: False\n",
    "  ddp_find_unused_parameters: False\n",
    "  empty_device_cache_steps: 1\n",
    "\n",
    "  optimizer: \"adamw_torch_fused\"\n",
    "  learning_rate: 2e-5\n",
    "  warmup_ratio: 0.03\n",
    "  weight_decay: 0.0\n",
    "  lr_scheduler_type: \"cosine\"\n",
    "\n",
    "  logging_steps: 5\n",
    "  save_steps: 0\n",
    "  dataloader_main_process_only: False\n",
    "  dataloader_num_workers: 2\n",
    "  dataloader_prefetch_factor: 8\n",
    "  include_performance_metrics: True\n",
    "  enable_wandb: True # Set to False if you don't want to use Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's launch the training!\n",
    "\n",
    "!oumi train -c \"$tutorial_dir/train.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, let's use the Fine-tuned Model and see the effect of training!\n",
    "\n",
    "Once we're happy with the results, we can serve the fine-tuned model for interactive inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/trained_infer.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"vision_language_tutorial\"  \n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  chat_template: \"qwen2-vl-instruct\"\n",
    "  model_max_length: 4096\n",
    "  trust_remote_code: True\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 256\n",
    "  batch_size: 1\n",
    "  \n",
    "engine: NATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"trained_infer.yaml\"))\n",
    "\n",
    "\n",
    "test_image = Image.open(f\"{tutorial_dir}/example_{0}.jpg\")\n",
    "image_bytes = create_png_bytes_from_image(test_image)\n",
    "\n",
    "results = infer(\n",
    "    config=config,\n",
    "    inputs=[query_text_string],\n",
    "    input_image_bytes=image_bytes,\n",
    ")\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
