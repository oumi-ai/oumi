{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this tutorial, we'll fine-tune a large language model to improve its ability to generate and explain complex python code. \n",
    "\n",
    "We'll use the LeMa framework to streamline the process and achieve high-quality results.\n",
    "\n",
    "We'll cover the following topics:\n",
    "1. Prerequisites\n",
    "2. Data Preparation & Sanity Checks\n",
    "3. Training Config Preparation\n",
    "4. Launching Training\n",
    "5. Monitoring Progress\n",
    "6. Evaluation\n",
    "7. Analysing Results\n",
    "8. Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prerequisites\n",
    "#### 1.1. LeMa Installation\n",
    "First, let's install lema. You can find detailed instructions [here](https://github.com/openlema/lema/blob/main/README.md), but it should be as simple as:\n",
    "\n",
    "```bash\n",
    "pip install -e \".[dev,train]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Creating our working directory\n",
    "For our experiments, we'll use the following folder to save the model, training artifacts, and our working configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"finetuning_tutorial\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Setup the environment\n",
    "\n",
    "We'll need to set the following environment variables:\n",
    "- [Optional] HF_TOKEN: Your [HuggingFace](https://huggingface.co/docs/hub/en/security-tokens) token, in case you want to access a private model.\n",
    "- [Optional] WANDB_API_KEY: Your [wandb](https://wandb.ai) token, in case you want to log your experiments to wandb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Getting Started\n",
    "\n",
    "\n",
    "#### 2.1 Data Preparation\n",
    "Let's start by checking out our datasets, and seeing what the data looks like. The Alpaca dataset includes a variety of tasks, including code generation and explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lema.builders import build_tokenizer\n",
    "from lema.core.types import ModelParams\n",
    "from lema.datasets.alpaca import AlpacaDataset\n",
    "\n",
    "# Initialize the dataset\n",
    "tokenizer = build_tokenizer(ModelParams(model_name=\"gpt2\"))\n",
    "dataset = AlpacaDataset(tokenizer=tokenizer)\n",
    "\n",
    "# Print a few examples\n",
    "for i in range(3):\n",
    "    conversation = dataset.conversation(i)\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    for message in conversation.messages:\n",
    "        print(f\"{message.role}: {message.content[:100]}...\")  # Truncate for brevity\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Model Preparation\n",
    "\n",
    "For code generation, we want a model with strong general language understanding and coding capabilities. \n",
    "\n",
    "We also want a model that is small enough to train and run on a single GPU.\n",
    "\n",
    "Some good options include:\n",
    "- [\"microsoft/Phi-3-mini-128k-instruct\"](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)\n",
    "- [\"google/gemma-2b\"](https://huggingface.co/google/gemma-2b)\n",
    "- [\"Qwen/Qwen2-1.5B-Instruct\"](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)\n",
    "\n",
    "\n",
    "For this tutorial, we'll use \"Qwen/Qwen2-1.5B-Instruct\" as our base model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Initial Model Responses\n",
    "\n",
    "Let's see how our model performs on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/inference_config.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "  trust_remote_code: true\n",
    "  torch_dtype_str: \"half\"\n",
    "  device_map: \"auto\"\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 512\n",
    "  batch_size: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lema.core.types import InferenceConfig\n",
    "from lema.infer import infer\n",
    "\n",
    "config = InferenceConfig.from_yaml(Path(tutorial_dir) / \"inference_config.yaml\")\n",
    "\n",
    "input_text = (\n",
    "    \"Write a Python function to implement the quicksort algorithm. \"\n",
    "    \"Please include comments explaining each step.\"\n",
    ")\n",
    "\n",
    "results = infer(config.model, config.generation, [[input_text]])\n",
    "\n",
    "print(results[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparing our training experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a YAML file with our training config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/train.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "  trust_remote_code: true\n",
    "  torch_dtype_str: \"half\"\n",
    "  device_map: \"auto\"\n",
    "\n",
    "data:\n",
    "  train:\n",
    "    datasets:\n",
    "      - dataset_name: \"tatsu-lab/alpaca\"\n",
    "        split: \"train\"\n",
    "    target_col: \"text\"\n",
    "    \n",
    "\n",
    "training:\n",
    "  output_dir: output\n",
    "  per_device_train_batch_size: 2\n",
    "  gradient_accumulation_steps: 8\n",
    "  max_steps: 10\n",
    "  learning_rate: 1e-5\n",
    "  lr_scheduler_type: \"cosine\"\n",
    "  warmup_steps: 200\n",
    "  logging_steps: 10\n",
    "  save_steps: 200\n",
    "  eval_steps: 200\n",
    "\n",
    "  use_peft: true\n",
    "  trainer_type: \"TRL_SFT\"\n",
    "\n",
    "peft:\n",
    "  lora_r: 16\n",
    "  lora_alpha: 32\n",
    "  lora_dropout: 0.05\n",
    "  lora_target_modules:\n",
    "    - \"q_proj\"\n",
    "    - \"k_proj\"\n",
    "    - \"v_proj\"\n",
    "    - \"o_proj\"\n",
    "    - \"gate_proj\"\n",
    "    - \"up_proj\"\n",
    "    - \"down_proj\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "This will start the fine-tuning process using the LeMa framework. The process will take a few hours, depending on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!lema-train -c \"$tutorial_dir/train.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Monitoring the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model is doing. Things to watch out for:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "Let's create an evaluation configuration file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/eval.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"./output\"\n",
    "  trust_remote_code: true\n",
    "  torch_dtype_str: \"half\"\n",
    "  device_map: \"auto\"\n",
    "\n",
    "data:\n",
    "  datasets:\n",
    "    - dataset_name: \"openai_humaneval\"\n",
    "\n",
    "evaluation_framework: \"lm_harness\"\n",
    "num_shots: 0\n",
    "output_dir: \"./output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lema-evaluate -c \"$tutorial_dir/eval.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Analyze Results and Iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [To be continued]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 8. Use the Fine-tuned Model\n",
    "\n",
    "Once we're happy with the results, we can serve the fine-tuned model for interactive inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/train_inference_config.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"./output\"\n",
    "  trust_remote_code: true\n",
    "  torch_dtype_str: \"half\"\n",
    "  device_map: \"auto\"\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 512\n",
    "  batch_size: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lema.core.types import InferenceConfig\n",
    "from lema.infer import infer\n",
    "\n",
    "config = InferenceConfig.from_yaml(Path(tutorial_dir) / \"train_inference_config.yaml\")\n",
    "\n",
    "input_text = (\n",
    "    \"Write a Python function to implement the quicksort algorithm. \"\n",
    "    \"Please include comments explaining each step.\"\n",
    ")\n",
    "\n",
    "results = infer(config.model, config.generation, [[input_text]])\n",
    "\n",
    "print(results[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lema",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
