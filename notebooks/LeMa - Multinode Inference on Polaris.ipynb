{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many flavors of modern LLMs are prohibitively large to serve on your local hardware. To that end, this tutorial will demonstrate how you can running inference on Llama3.1-70b using the Polaris cluster. As a reminder, Polaris is composed of hundreds of nodes, each composed of 4 x A100-40GB GPUs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeMa Installation\n",
    "First, let's install LeMa. You can find detailed instructions [here](https://github.com/openlema/lema/blob/main/README.md), but it should be as simple as:\n",
    "\n",
    "```bash\n",
    "pip install -e \".[dev,train]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our working directory\n",
    "For this tutorial, we'll use the following folder to save our generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"polaris_inference_tutorial\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our inference pipeline currently expects inputs using OpenAI's chat format. Let's download a dataset from HuggingFace and massage it into the proper format. We'll use a small subset of the `cais/mmlu` as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Optional system context we'll use when creating our dataset\n",
    "system_context = \"You are a helpful AI assistant.\"\n",
    "\n",
    "# This dataset has only 100 examples.\n",
    "dset = datasets.load_dataset(\"cais/mmlu\", \"abstract_algebra\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's massage the data and save it as a JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_location = Path(tutorial_dir) / \"data.jsonl\"\n",
    "with open(str(data_location), \"w\") as f:\n",
    "    for data in dset:\n",
    "        system_message = {\"role\": \"system\", \"content\": system_context}\n",
    "        user_content = \"\\n\".join([data[\"question\"], \"Choices: \", *data[\"choices\"]])\n",
    "        user_prompt = {\"role\": \"user\", \"content\": user_content}\n",
    "        entry = {\"messages\": [system_message, user_prompt]}\n",
    "        print(json.dumps(entry), file=f)\n",
    "    print(f\"Sample entry: {entry}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a predefined job for running inference on Llama3.1-70b. You can find it at `configs/lema/jobs/polaris/vllm.yaml`.\n",
    "\n",
    "This job accepts an input JSONL file for inference and an output directory path for writing our results. Note: your output path must be on one of Polaris' file systems (we recommend `/eagle/community_ai/$USER`)\n",
    "\n",
    " Let's load the config and modify it to reference our input file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lema.launcher as launcher\n",
    "\n",
    "job_name = \"Create_a_display_name_for_your_job\"\n",
    "cloud_name = \"polaris\"\n",
    "polaris_user = \"YOUR_USER_NAME\"\n",
    "\n",
    "# We assume you're running this notebook in the /notebooks directory.\n",
    "# Move up one directory to run the job from the root of the repository.\n",
    "os.chdir(Path(tutorial_dir).absolute().parent.parent)\n",
    "job_path = Path(\".\") / \"configs\" / \"lema\" / \"jobs\" / \"polaris\" / \"vllm.yaml\"\n",
    "\n",
    "job = launcher.JobConfig.from_yaml(str(job_path))\n",
    "job.name = job_name\n",
    "job.resources.cloud = cloud_name\n",
    "job.user = polaris_user\n",
    "job.working_dir = \".\"  # Use the current directory as the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add your input and output folder targets to the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your input path should be a relative path from the working directory.\n",
    "input_path = str(Path(\"notebooks\") / data_location)\n",
    "\n",
    "# Write the output to polaris in a directory named after the job and user.\n",
    "output_path = str(Path(\"/eagle\") / \"community_ai\" / polaris_user / job_name)\n",
    "\n",
    "# Set the input and output paths in the job environment.\n",
    "job.envs[\"LEMA_VLLM_INPUT_PATH\"] = input_path\n",
    "job.envs[\"LEMA_VLLM_OUTPUT_PATH\"] = output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our job set up, we can kick off inference on Polaris!\n",
    "\n",
    "**IMPORTANT** Note that you'll be required to input your polaris credentials twice. Make sure you refresh your credentials between each input or copying your files will fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cluster for Polaris jobs must be of the form `queue_name.user_name`.\n",
    "# The following will use the `debug-scaling` queue.\n",
    "cluster, job_status = launcher.up(job, f\"debug-scaling.{polaris_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to wait for our job to finish. We can check our job's status using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while not job_status.done:\n",
    "    job_status = cluster.get_job(job_status.id)\n",
    "    print(f\"Job status: {job_status}\")\n",
    "    time.sleep(10)\n",
    "\n",
    "print(f\"Job finished with status: {job_status.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your job is done, you can find your inference outputs by SSHing into Polaris and navigating to `/eagle/community_ai/YOUR_USER_NAME/YOUR_JOB_NAME`\n",
    "\n",
    "Run the cell below to find the exact location for your jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"You can find the output of your job at {output_path} on Polaris.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of inference will be a JSONL in the standard openai chat format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Setup\n",
    "\n",
    "Depending on the size of your input, your job may require more time to run. Polaris requires jobs to set a max run time when queued. To adjust this, navigate to our job config and adjust the line containing the `#PBS -l walltime` directive to the time required for your run.\n",
    "\n",
    "For example, the following will configure the job to terminate after 10 minutes of run time:\n",
    "`#PBS -l walltime=00:10:00` \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lema",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
