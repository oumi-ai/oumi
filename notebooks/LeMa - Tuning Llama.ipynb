{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we'll focus on how you can easily tune various flavors of Llama. For simplicity, we'll be using Polaris as the platform for compute.\n",
    "\n",
    "**NOTE:** This tutorial builds off of our [Finetuning Tutorial](https://github.com/openlema/lema/blob/main/notebooks/LeMa%20-%20Finetuning%20Tutorial.ipynb). We recommend starting there first to get a thorough understanding of how tuning works in our library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial assumes:\n",
    "- You have a valid ALCF account with access to Polaris\n",
    "- You're familiar with our tuning flow\n",
    "- You're familiar with how to launch lema workflows on Polaris. [Here's a relevant tutorial](https://github.com/openlema/lema/blob/main/notebooks/LeMa%20-%20Deploying%20a%20Job.ipynb)\n",
    "- You've signed [Llama's agreement on HuggingFace](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Llama\n",
    "\n",
    "We currently have out-of-the-box tuning jobs configured for the following flavors of Llama:\n",
    "\n",
    "- Llama3.1 8b LoRA: [configs/lema/jobs/polaris/llama8b_lora.yaml](https://github.com/openlema/lema/blob/main/configs/lema/jobs/polaris/llama8b_lora.yaml) âœ¨\n",
    "- Llama3.1 8b SFT: [configs/lema/jobs/polaris/llama8b_sft.yaml](https://github.com/openlema/lema/blob/main/configs/lema/jobs/polaris/llama8b_sft.yaml) âœ¨\n",
    "- Llama3.1 70b LoRA: [configs/lema/jobs/polaris/llama70b_lora.yaml](https://github.com/openlema/lema/blob/main/configs/lema/jobs/polaris/llama70b_lora.yaml) âœ¨\n",
    "- Llama3.1 70b SFT â€“ COMING SOON! ðŸš€\n",
    "\n",
    "By default our tuning job will run using the [`yahma/alpaca-cleaned`](https://huggingface.co/datasets/yahma/alpaca-cleaned) dataset. This is configured in the job configs above. We strongly suggest tuning these parameters as needed for your specific run.\n",
    "\n",
    "Before running the job, ensure you've signed Llama's agreement on HuggingFace and have obtained your [HF_TOKEN](https://huggingface.co/docs/huggingface_hub/en/package_reference/environment_variables#hftoken). We'll pass it to our job by appending envs.HF_TOKEN=$HF_TOKEN to our launcher script. This is not currently needed as our script copies the model from Polaris' Eagle file system, but it will be needed to pull the model directly from HuggingFace.\n",
    "\n",
    "The example command below uses the `preemptable` queue, which supports up to 10 nodes and up to 3 days runtime. When debugging, prefer using the `debug-scaling` queue, which usually has less queue time. However, you need to modify the `walltime` in the yaml config above to be <= 1 hour. For more details on available queues, see [ALCF's documentation](https://docs.alcf.anl.gov/polaris/running-jobs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```shell\n",
    "# Replace with your desired config. We use Llama 8B LoRA below.\n",
    "LEMA_CONFIG_PATH=\"configs/lema/jobs/polaris/llama8b_lora.yaml\"\n",
    "# If using debug-scaling queue, make sure that the walltime is <= 1 hr.\n",
    "POLARIS_QUEUE=\"preemptable\"\n",
    "lema-launch -p $LEMA_CONFIG_PATH -c $POLARIS_QUEUE.$ALCF_USER envs.HF_TOKEN=$HF_TOKEN user=$ALCF_USER\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
