{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"align-center\">\n",
    "<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n",
    "\n",
    "[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n",
    "[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n",
    "[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - Finetuning Tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>\n",
    "\n",
    "ðŸ‘‹ Welcome to Open Universal Machine Intelligence (Oumi)!\n",
    "\n",
    "ðŸš€ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](https://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n",
    "\n",
    "ðŸ¤ Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n",
    "\n",
    "â­ If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Overview\n",
    "\n",
    "In this tutorial, weâ€™ll use LoRA to fine-tune a large language model on multi-turn conversational data.\n",
    "\n",
    "We'll use the Oumi framework to streamline the process and achieve high-quality results.\n",
    "\n",
    "Oumi supports [Unsloth](https://unsloth.ai/), a lightweight and efficient patch for [Transformers](https://huggingface.co/docs/transformers/en/index) library that significantly speeds up LoRA training while reducing memory usage.\n",
    "\n",
    "We'll cover the following topics:\n",
    "1. Prerequisites\n",
    "2. Data Preparation & Sanity Checks\n",
    "3. Training Config Preparation\n",
    "4. Launching Training\n",
    "5. Monitoring Progress\n",
    "6. Evaluation\n",
    "7. Analyzing Results\n",
    "8. Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "â—**NOTICE:** We recommend running this notebook on a GPU. If running on Google Colab, you can use the free T4 GPU runtime (Colab Menu: `Runtime` -> `Change runtime type`).\n",
    "\n",
    "First, let's install Oumi. You can find more detailed instructions [here](https://oumi.ai/docs/en/latest/get_started/installation.html). Here, we include Oumi's GPU and Unsloth dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install oumi[gpu,unsloth]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our working directory\n",
    "For our experiments, we'll use the following folder to save the model, training artifacts, and our working configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"finetuning_unsloth_tutorial\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "\n",
    "You may need to set the following environment variables:\n",
    "- [Optional] HF_TOKEN: Your [HuggingFace](https://huggingface.co/docs/hub/en/security-tokens) token, in case you want to access a private model like Llama.\n",
    "- [Optional] WANDB_API_KEY: Your [wandb](https://wandb.ai) token, in case you want to log your experiments to wandb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "\n",
    "## Data Preparation\n",
    "We begin by loading the FineTome-100k dataset from Hugging Face. This dataset contains a variety of multi-turn conversations formatted for fine-tuning language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
    "print(dataset[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we preprocess the dataset to convert the raw conversation structure into a standard format of messages, filtering out any system prompts. This step prepares the data for training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    \"\"\"Preprocess the examples.\"\"\"\n",
    "    preprocessed_example = {\"messages\": []}\n",
    "    for message in example[\"conversations\"]:\n",
    "        if message[\"from\"] != \"system\":\n",
    "            preprocessed_example[\"messages\"].append(\n",
    "                {\n",
    "                    \"role\": \"user\"\n",
    "                    if message[\"from\"] in [\"user\", \"human\", \"input\"]\n",
    "                    else \"assistant\",\n",
    "                    \"content\": message[\"value\"],\n",
    "                }\n",
    "            )\n",
    "    return preprocessed_example\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    preprocess,\n",
    "    num_proc=cpu_count(),\n",
    "    remove_columns=[\"conversations\", \"source\", \"score\"],\n",
    ")\n",
    "print(dataset[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the preprocessed dataset locally for access during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = str(Path(tutorial_dir) / \"mlabonne/FineTome-100k\")\n",
    "dataset.save_to_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HuggingFaceDataset class is used to wrap the preprocessed dataset. It takes care of formatting conversations into a structure suitable for training, including tokenization and role handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.builders import build_tokenizer\n",
    "from oumi.core.configs import ModelParams\n",
    "from oumi.datasets import HuggingFaceDataset\n",
    "\n",
    "# Initialize the dataset\n",
    "tokenizer = build_tokenizer(ModelParams(model_name=\"unsloth/gemma-3-1b-it\"))\n",
    "\n",
    "dataset = HuggingFaceDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    hf_dataset_path=dataset_path,\n",
    "    dataset_path=dataset_path,\n",
    ")\n",
    "\n",
    "# Print a few examples\n",
    "for i in range(4):\n",
    "    conversation = dataset.conversation(i)\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    for message in conversation.messages:\n",
    "        print(f\"{message.role}: {message.content[:100]}...\")  # Truncate for brevity\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "For code generation, we want a model with strong general language understanding and coding capabilities. \n",
    "\n",
    "We also want a model that is small enough to train and run on a single GPU.\n",
    "\n",
    "For this tutorial, we'll use \"unsloth/gemma-3-1b-it\" as our base model.\n",
    "\n",
    "This model is accessible through the Unsloth library, which is seamlessly integrated into Oumi, streamlining our fine-tuning workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Responses\n",
    "\n",
    "Let's see how our model performs on an example prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/infer.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"unsloth/gemma-3-1b-it\"\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 128\n",
    "  batch_size: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.core.configs import InferenceConfig\n",
    "from oumi.infer import infer\n",
    "\n",
    "config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"infer.yaml\"))\n",
    "\n",
    "input_text = (\n",
    "    \"Write a Python function to implement the quicksort algorithm. \"\n",
    "    \"Please include comments explaining each step.\"\n",
    ")\n",
    "\n",
    "results = infer(config=config, inputs=[input_text])\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our training experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a YAML file for our training config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/train.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"unsloth/gemma-3-1b-it\"\n",
    "  model_max_length: 2048\n",
    "  model_kwargs: {\n",
    "    \"unsloth_model\": \"FastModel\",\n",
    "    \"load_in_4bit\": True,\n",
    "    \"load_in_8bit\": False,\n",
    "    \"full_finetuning\": False,\n",
    "    \"peft\": {\n",
    "        \"finetune_vision_layers\": False,\n",
    "        \"finetune_language_layers\": True,\n",
    "        \"finetune_attention_modules\": True,\n",
    "        \"finetune_mlp_modules\": True,\n",
    "        \"random_state\": 3407,\n",
    "    }\n",
    "  }\n",
    "\n",
    "data:\n",
    "  train:\n",
    "    datasets:\n",
    "      - dataset_name: \"HuggingFaceDataset\"\n",
    "        dataset_path: \"finetuning_unsloth_tutorial/mlabonne/FineTome-100k\"\n",
    "        split: \"train\"\n",
    "        dataset_kwargs: {\n",
    "          \"hf_dataset_path\": \"finetuning_unsloth_tutorial/mlabonne/FineTome-100k\",\n",
    "          \"assistant_only\": True,\n",
    "          \"instruction_template\": \"<start_of_turn>user\\n\",\n",
    "          \"response_template\": \"<start_of_turn>model\\n\",\n",
    "        }\n",
    "\n",
    "training:\n",
    "  use_peft: true\n",
    "  trainer_type: \"TRL_SFT\"\n",
    "  per_device_train_batch_size: 1\n",
    "  warmup_steps: 5\n",
    "  max_steps: 5\n",
    "  learning_rate: 2e-4\n",
    "  logging_steps: 1\n",
    "  optimizer: \"adamw_8bit\"\n",
    "  weight_decay: 0.01\n",
    "  lr_scheduler_type: \"linear\"\n",
    "  seed: 3407\n",
    "  dataloader_num_workers: 2\n",
    "  output_dir: \"finetuning_unsloth_tutorial/output\"\n",
    "\n",
    "peft:\n",
    "  lora_r: 8\n",
    "  lora_alpha: 8\n",
    "  lora_dropout: 0\n",
    "  lora_bias: \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "This will start the fine-tuning process using the Oumi framework. Because we set `max_steps: 5`, this should be very quick. The full fine-tuning process may take a few hours, depending on your GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SINGLE GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!oumi train -c \"$tutorial_dir/train.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTI-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!oumi distributed torchrun -m oumi train -c \"$tutorial_dir/train.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "As an example, let's create an evaluation configuration file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/eval.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"finetuning_unsloth_tutorial/output\"\n",
    "\n",
    "tasks:\n",
    "  - evaluation_backend: lm_harness\n",
    "    task_name: mmlu_college_computer_science\n",
    "\n",
    "output_dir: \"finetuning_unsloth_tutorial/output/evaluation\"\n",
    "generation:\n",
    "  batch_size: null # This will let LM HARNESS find the maximum possible batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!oumi evaluate -c \"$tutorial_dir/eval.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Use the Fine-tuned Model\n",
    "\n",
    "Once we're happy with the results, we can serve the fine-tuned model for interactive inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/trained_infer.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"unsloth/gemma-3-1b-it\"\n",
    "  adapter_model: \"finetuning_unsloth_tutorial/output\"\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 2048\n",
    "  batch_size: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.core.configs import InferenceConfig\n",
    "from oumi.infer import infer\n",
    "\n",
    "config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"trained_infer.yaml\"))\n",
    "\n",
    "input_text = (\n",
    "    \"Write a Python function to implement the quicksort algorithm. \"\n",
    "    \"Please include comments explaining each step.\"\n",
    ")\n",
    "\n",
    "results = infer(config=config, inputs=[input_text])\n",
    "\n",
    "print(results[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
