{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"align-center\">\n",
    "<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n",
    "\n",
    "[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n",
    "[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n",
    "[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n",
    "</div>\n",
    "\n",
    "üëã Welcome to Open Universal Machine Intelligence (Oumi)!\n",
    "\n",
    "üöÄ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](https://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n",
    "\n",
    "ü§ù Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n",
    "\n",
    "‚≠ê If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7bYaH10SgtN"
   },
   "source": [
    "# OpenEnv GRPO with trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkhNGqE1SgtP"
   },
   "source": [
    "In this tutorial notebook, we're going to use Oumi to train an agentic model on an [OpenEnv](https://github.com/meta-pytorch/OpenEnv) Echo reinforcement learning (RL) environment with the GRPO algorithm. To achieve this, we use the trl library by Hugging Face with a custom rollout function to interact with the vLLM server and OpenEnv environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHDr11SqSgtP"
   },
   "source": [
    "# üìã Prerequisites\n",
    "\n",
    "‚ùó**NOTICE:** This notebook needs to be running on a machine with at least two GPUs.\n",
    "\n",
    "## Oumi Installation\n",
    "\n",
    "First, let's install the latest versions of Oumi, trl, and OpenEnv. You can find more detailed instructions [here](https://oumi.ai/docs/en/latest/get_started/installation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install uv && uv pip install \"oumi[gpu] @ git+https://github.com/oumi-ai/oumi.git\"\n",
    "!pip install uv && uv pip install -e \"..[gpu]\"\n",
    "!uv pip install git+https://github.com/meta-pytorch/OpenEnv.git\n",
    "!uv pip install git+https://github.com/huggingface/trl.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JPmWKRVCSgtP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"openenv_tutorial\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable warnings from HF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start OpenEnv and vLLM servers\n",
    "\n",
    "We need to run 2 servers in addition to the trl trainer. The OpenEnv server receives actions from the LLM and returns the updated state and reward. The vLLM server is used for inference, and updates it weights over training with the updated model weights from the trainer. We start these with separate subprocesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting openenv_tutorial/start_openenv_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/start_openenv_server.py\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def stream_output(pipe, prefix=\"\"):\n",
    "    \"\"\"Stream output lines from subprocess pipe to stdout.\"\"\"\n",
    "    for line in iter(pipe.readline, \"\"):\n",
    "        print(f\"{prefix}{line}\", end=\"\")\n",
    "    pipe.close()\n",
    "\n",
    "\n",
    "print(\"‚ö° Starting FastAPI server for Echo Environment...\")\n",
    "\n",
    "work_dir = str(Path.cwd().parent.absolute())\n",
    "\n",
    "server_process = subprocess.Popen(\n",
    "    [\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"uvicorn\",\n",
    "        \"envs.echo_env.server.app:app\",\n",
    "        \"--host\",\n",
    "        \"0.0.0.0\",\n",
    "        \"--port\",\n",
    "        \"8001\",\n",
    "    ],\n",
    "    env={**os.environ, \"PYTHONPATH\": f\"{work_dir}/src\"},\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    "    cwd=work_dir,\n",
    ")\n",
    "\n",
    "# Start background threads to stream errors\n",
    "threading.Thread(\n",
    "    target=stream_output, args=(server_process.stderr, \"üî• [stderr] \"), daemon=True\n",
    ").start()\n",
    "\n",
    "print(\"‚è≥ Waiting for server to start...\")\n",
    "time.sleep(5)\n",
    "\n",
    "try:\n",
    "    response = requests.get(\"http://0.0.0.0:8001/health\", timeout=2)\n",
    "    print(\"\\n‚úÖ Echo Environment server is running!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Server failed to start: {e}\")\n",
    "    print(\"\\nüìã Checking error output...\")\n",
    "    server_process.poll()\n",
    "    if server_process.stderr:\n",
    "        stderr = server_process.stderr.read()\n",
    "        if stderr:\n",
    "            print(stderr)\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    input(\"Press Enter to exit...\\n\")\n",
    "finally:\n",
    "    print(\"üõë Stopping server...\")\n",
    "    server_process.terminate()\n",
    "    server_process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Servers started. PIDs: 3616371 3616372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Starting FastAPI server for Echo Environment...\n",
      "‚è≥ Waiting for server to start...\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Start both servers in the background\n",
    "server1 = subprocess.Popen(\n",
    "    [\n",
    "        \"bash\",\n",
    "        \"-c\",\n",
    "        (\n",
    "            \"CUDA_VISIBLE_DEVICES=0 trl vllm-serve \"\n",
    "            \"--model Qwen/Qwen2.5-0.5B-Instruct \"\n",
    "            \"--log-level warning \"\n",
    "            \"--host 0.0.0.0 --port 8000\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "server2 = subprocess.Popen([\"python\", f\"{tutorial_dir}/start_openenv_server.py\"])\n",
    "\n",
    "print(\"Servers started. PIDs:\", server1.pid, server2.pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Server not ready: HTTPConnectionPool(host='0.0.0.0', port=8000): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x725828b5b510>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üî• [stderr] INFO:     Started server process [3616373]\n",
      "üî• [stderr] INFO:     Waiting for application startup.\n",
      "üî• [stderr] INFO:     Application startup complete.\n",
      "üî• [stderr] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n",
      "\n",
      "‚úÖ Echo Environment server is running!\n",
      "Press Enter to exit...\n",
      "‚ùå Server not ready: HTTPConnectionPool(host='0.0.0.0', port=8000): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x725828b61dd0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "‚ùå Server not ready: HTTPConnectionPool(host='0.0.0.0', port=8000): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x725828b68410>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "INFO 10-30 20:09:12 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 10-30 20:09:13 [utils.py:328] non-default args: {'disable_log_stats': True, 'worker_extension_cls': 'trl.scripts.vllm_serve.WeightSyncWorkerExtension', 'model_impl': 'vllm', 'model': 'Qwen/Qwen2.5-0.5B-Instruct'}\n",
      "‚ùå Server not ready: HTTPConnectionPool(host='0.0.0.0', port=8000): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x725828b6a910>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "‚ùå Server not ready: HTTPConnectionPool(host='0.0.0.0', port=8000): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x725828b74b10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-30 20:09:21 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 10-30 20:09:21 [__init__.py:1815] Using max model len 32768\n",
      "INFO 10-30 20:09:22 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "‚ùå Server not ready: HTTPConnectionPool(host='0.0.0.0', port=8000): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x725828b69250>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "‚ùå Server not ready: HTTPConnectionPool(host='0.0.0.0', port=8000): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x725828b63110>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "INFO 10-30 20:09:30 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:31 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:31 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:32 [worker_base.py:595] Injected <class 'trl.scripts.vllm_serve.WeightSyncWorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['close_communicator', 'init_communicator', 'update_named_param']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1030 20:09:32.557912363 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:32 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m WARNING 10-30 20:09:32 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:32 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:33 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:33 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:33 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:33 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.46it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.45it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:33 [default_loader.py:268] Loading weights took 0.17 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:33 [gpu_model_runner.py:2392] Model loading took 0.9266 GiB and 0.518918 seconds\n",
      "‚ùå Server not ready: HTTPConnectionPool(host='0.0.0.0', port=8000): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x725828b5ab10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:37 [backends.py:539] Using cache directory: /home/wizeng/.cache/vllm/torch_compile_cache/5d31f4c583/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:37 [backends.py:550] Dynamo bytecode transform time: 3.41 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.464 s\n",
      "‚ùå Server not ready: HTTPConnectionPool(host='0.0.0.0', port=8000): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x725828b768d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:39 [monitor.py:34] torch.compile takes 3.41 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:40 [gpu_worker.py:298] Available KV cache memory: 64.71 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:40 [kv_cache_utils.py:864] GPU KV cache size: 5,654,128 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:40 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 172.55x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 67/67 [00:01<00:00, 40.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:42 [gpu_model_runner.py:3118] Graph capturing finished in 2 secs, took 0.50 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:42 [gpu_worker.py:391] Free memory on device (78.59/79.19 GiB) on startup. Desired GPU memory utilization is (0.9, 71.27 GiB). Actual usage is 0.93 GiB for weight, 5.57 GiB for peak activation, 0.07 GiB for non-torch memory, and 0.5 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=68779733708` to fit into requested memory, or `--kv-cache-memory=76635612672` to fully utilize gpu memory. Current kv cache memory in use is 69478085324 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:42 [core.py:218] init engine (profile, create kv cache, warmup model) took 8.65 seconds\n",
      "INFO 10-30 20:09:43 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 10-30 20:09:43 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "‚úÖ vLLM server is healthy!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import requests\n",
    "\n",
    "URL = \"http://0.0.0.0:8000/health\"\n",
    "\n",
    "\n",
    "def check_vllm_health():\n",
    "    \"\"\"Checks if the vLLM server is healthy.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(URL, timeout=3)\n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ vLLM server is healthy!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Server responded with {response.status_code}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Server not ready: {e}\")\n",
    "    return False\n",
    "\n",
    "\n",
    "max_retries = 24\n",
    "for attempt in range(1, max_retries + 1):\n",
    "    if check_vllm_health():\n",
    "        break\n",
    "    time.sleep(5)\n",
    "else:\n",
    "    print(f\"‚ùå Failed to start vLLM server after {max_retries} attempts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model!\n",
    "\n",
    "By providing a custom rollout function to interact with the OpenEnv and vLLM servers, we can use trl to do agentic GRPO training. We also need to provide a reward function that processes the reward value output by the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@register(\"echo_env_vllm_rollout\", RegistryType.ROLLOUT_FUNCTION)\n",
      "def echo_env_vllm_rollout(\n",
      "    prompts: list[str], args, processing_class\n",
      ") -> dict[str, list]:\n",
      "    \"\"\"Custom rollout function that generates completions via vLLM server and computes environment rewards.\n",
      "\n",
      "    Args:\n",
      "        prompts: List of prompts to generate from\n",
      "        args: GRPOConfig containing all sampling parameters\n",
      "        processing_class: Tokenizer/processor for decoding completions\n",
      "\n",
      "    Returns:\n",
      "        Dict containing prompt_ids, completion_ids, logprobs, and env_reward\n",
      "    \"\"\"  # noqa: E501\n",
      "    # 1. Generate completions via vLLM inference server (running on port 8000)\n",
      "    payload = {\n",
      "        \"prompts\": prompts,\n",
      "        \"n\": args.num_generations,\n",
      "        \"temperature\": args.temperature,\n",
      "        \"top_p\": args.top_p,\n",
      "        \"top_k\": -1 if args.top_k is None else args.top_k,\n",
      "        \"min_p\": 0.0 if args.min_p is None else args.min_p,\n",
      "        \"max_tokens\": args.max_completion_length,\n",
      "        \"repetition_penalty\": args.repetition_penalty,\n",
      "    }\n",
      "    response = requests.post(\"http://0.0.0.0:8000/generate/\", json=payload)\n",
      "\n",
      "    if response.status_code != 200:\n",
      "        print(f\"Error response: {response.text}\")\n",
      "\n",
      "    response.raise_for_status()\n",
      "    result = response.json()\n",
      "\n",
      "    completions_text = processing_class.batch_decode(\n",
      "        result[\"completion_ids\"], skip_special_tokens=True\n",
      "    )\n",
      "\n",
      "    # 2. Step through the environment to get rewards\n",
      "    client = EchoEnv(base_url=\"http://0.0.0.0:8001\")\n",
      "    env_result = client.reset()\n",
      "    env_rewards = []\n",
      "    for msg in completions_text:\n",
      "        env_result = client.step(EchoAction(message=msg))\n",
      "        env_rewards.append(env_result.reward)\n",
      "\n",
      "    # 3. Add environment rewards as extra field\n",
      "    result[\"env_reward\"] = env_rewards\n",
      "\n",
      "    return result\n"
     ]
    }
   ],
   "source": [
    "!tail -n +27 ../src/oumi/datasets/grpo/rollouts/echo_env_vllm_rollout.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@register(\"env_reward\", RegistryType.REWARD_FUNCTION)\n",
      "def reward_from_env(completions, **kwargs):\n",
      "    \"\"\"Reward function that uses the environment reward.\"\"\"\n",
      "    # Extract environment rewards from kwargs (propagated via extra_fields)\n",
      "    env_rewards = kwargs.get(\"env_reward\", [])\n",
      "    if env_rewards:\n",
      "        return [float(reward) for reward in env_rewards]\n",
      "    else:\n",
      "        # Fallback if env_reward is not available\n",
      "        return [0.0] * len(completions)\n"
     ]
    }
   ],
   "source": [
    "!tail -n +23 ../src/oumi/datasets/grpo/rewards/env_reward.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting openenv_tutorial/grpo_train.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $tutorial_dir/grpo_train.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "  model_max_length: 2048\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  attn_implementation: \"sdpa\"\n",
    "\n",
    "data:\n",
    "  train:\n",
    "    datasets:\n",
    "      - dataset_name: \"trl-lib/ultrafeedback-prompt\"\n",
    "        split: \"train\"\n",
    "        sample_count: 100\n",
    "\n",
    "training:\n",
    "  trainer_type: \"TRL_GRPO\"\n",
    "  per_device_train_batch_size: 8\n",
    "  gradient_accumulation_steps: 4\n",
    "\n",
    "  reward_functions: [\"env_reward\"]\n",
    "\n",
    "  ddp_find_unused_parameters: False\n",
    "  optimizer: \"adamw_torch_fused\"\n",
    "\n",
    "  grpo:\n",
    "    use_vllm: True\n",
    "    rollout_function: \"echo_env_vllm_rollout\"\n",
    "\n",
    "  dataloader_num_workers: \"auto\"\n",
    "  dataloader_prefetch_factor: 32\n",
    "\n",
    "  num_train_epochs: 1\n",
    "  logging_steps: 1\n",
    "  log_model_summary: False\n",
    "  output_dir: \"openenv_tutorial/echo_grpo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m   ____  _    _ __  __ _____\u001b[0m\n",
      "\u001b[32m  / __ \\| |  | |  \\/  |_   _|\u001b[0m\n",
      "\u001b[32m | |  | | |  | | \\  / | | |\u001b[0m\n",
      "\u001b[32m | |  | | |  | | |\\/| | | |\u001b[0m\n",
      "\u001b[32m | |__| | |__| | |  | |_| |_\u001b[0m\n",
      "\u001b[32m  \\____/ \\____/|_|  |_|_____|\u001b[0m\n",
      "\n",
      "\u001b[2K\u001b[32m‚†∏\u001b[0m \u001b[32mLoading configuration...\u001b[0m0m\n",
      "\u001b[1A\u001b[2K\u001b[2;36m[10/30/25 20:09:52]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Ignored             \u001b]8;id=473956;file:///home/wizeng/repos/oumi/src/oumi/core/configs/training_config.py\u001b\\\u001b[2mtraining_config.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=913491;file:///home/wizeng/repos/oumi/src/oumi/core/configs/training_config.py#149\u001b\\\u001b[2m149\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         model.\u001b[33mmodel_max_length\u001b[0m=\u001b[1;36m2048\u001b[0m  \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         parameter for trainer        \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         TrainerType.TRL_GRPO.        \u001b[2m                      \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Setting random seed to  \u001b]8;id=358934;file:///home/wizeng/repos/oumi/src/oumi/core/distributed.py\u001b\\\u001b[2mdistributed.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=692189;file:///home/wizeng/repos/oumi/src/oumi/core/distributed.py#616\u001b\\\u001b[2m616\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m42\u001b[0m on rank \u001b[1;36m0\u001b[0m.                    \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m[10/30/25 20:09:55]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Torch version:           \u001b]8;id=234053;file:///home/wizeng/repos/oumi/src/oumi/utils/torch_utils.py\u001b\\\u001b[2mtorch_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=146316;file:///home/wizeng/repos/oumi/src/oumi/utils/torch_utils.py#80\u001b\\\u001b[2m80\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m2.8\u001b[0m.\u001b[1;36m0\u001b[0m+cu128. NumPy version:       \u001b[2m                 \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m1.26\u001b[0m.\u001b[1;36m4\u001b[0m                            \u001b[2m                 \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m CUDA version: \u001b[1;36m12.8\u001b[0m       \u001b]8;id=571858;file:///home/wizeng/repos/oumi/src/oumi/utils/torch_utils.py\u001b\\\u001b[2mtorch_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=91161;file:///home/wizeng/repos/oumi/src/oumi/utils/torch_utils.py#88\u001b\\\u001b[2m88\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m CuDNN version: \u001b[1;36m90.8\u001b[0m.\u001b[1;36m0\u001b[0m    \u001b]8;id=229258;file:///home/wizeng/repos/oumi/src/oumi/utils/torch_utils.py\u001b\\\u001b[2mtorch_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=243962;file:///home/wizeng/repos/oumi/src/oumi/utils/torch_utils.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m CPU cores: \u001b[1;36m208\u001b[0m CUDA     \u001b]8;id=750800;file:///home/wizeng/repos/oumi/src/oumi/utils/torch_utils.py\u001b\\\u001b[2mtorch_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=681453;file:///home/wizeng/repos/oumi/src/oumi/utils/torch_utils.py#124\u001b\\\u001b[2m124\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         devices: \u001b[1;36m1\u001b[0m                       \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;35mdevice\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m=\u001b[32m'NVIDIA H100 80GB \u001b[0m     \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32mHBM3'\u001b[0m Capability: \u001b[1m(\u001b[0m\u001b[1;36m9\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m Memory: \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1m[\u001b[0mTotal: \u001b[1;36m79.\u001b[0m19GiB Free: \u001b[1;36m78.\u001b[0m68GiB  \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         Allocated: \u001b[1;36m0.\u001b[0m0GiB Cached:        \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m0.\u001b[0m0GiB\u001b[1m]\u001b[0m                          \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Oumi version:                 \u001b]8;id=291704;file:///home/wizeng/repos/oumi/src/oumi/train.py\u001b\\\u001b[2mtrain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=848749;file:///home/wizeng/repos/oumi/src/oumi/train.py#154\u001b\\\u001b[2m154\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m0.4\u001b[0m.\u001b[1;36m3.\u001b[0mdev7+gee26267d5.d20251030        \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Git revision hash:            \u001b]8;id=732052;file:///home/wizeng/repos/oumi/src/oumi/train.py\u001b\\\u001b[2mtrain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=443143;file:///home/wizeng/repos/oumi/src/oumi/train.py#156\u001b\\\u001b[2m156\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ee26267d53adf0e6507b602ceaddd49d2a8fcc \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         3a                                     \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Git tag: \u001b[3;35mNone\u001b[0m                 \u001b]8;id=352944;file:///home/wizeng/repos/oumi/src/oumi/train.py\u001b\\\u001b[2mtrain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=107175;file:///home/wizeng/repos/oumi/src/oumi/train.py#157\u001b\\\u001b[2m157\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m[10/30/25 20:09:55]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Resolved                      \u001b]8;id=360663;file:///home/wizeng/repos/oumi/src/oumi/train.py\u001b\\\u001b[2mtrain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=633052;file:///home/wizeng/repos/oumi/src/oumi/train.py#165\u001b\\\u001b[2m165\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32m'training.\u001b[0m\u001b[32mdataloader_num_workers\u001b[0m\u001b[32m=\u001b[0m\u001b[32mauto\u001b[0m\u001b[32m'\u001b[0m \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         to \u001b[32m'training.\u001b[0m\u001b[32mdataloader_num_workers\u001b[0m\u001b[32m=\u001b[0m\u001b[32m2\u001b[0m\u001b[32m'\u001b[0m \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Training config saved to      \u001b]8;id=562275;file:///home/wizeng/repos/oumi/src/oumi/train.py\u001b\\\u001b[2mtrain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=130889;file:///home/wizeng/repos/oumi/src/oumi/train.py#309\u001b\\\u001b[2m309\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         openenv_tutorial/echo_grpo/telemetry/t \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         raining_config.yaml                    \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m[10/30/25 20:09:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Using the model's built-in   \u001b]8;id=659176;file:///home/wizeng/repos/oumi/src/oumi/builders/models.py\u001b\\\u001b[2mmodels.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=648564;file:///home/wizeng/repos/oumi/src/oumi/builders/models.py#544\u001b\\\u001b[2m544\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         chat template for model               \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32m'Qwen/Qwen2-0.5B-Instruct'\u001b[0m.           \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m[10/30/25 20:09:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Building model using         \u001b]8;id=738797;file:///home/wizeng/repos/oumi/src/oumi/builders/models.py\u001b\\\u001b[2mmodels.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=72933;file:///home/wizeng/repos/oumi/src/oumi/builders/models.py#260\u001b\\\u001b[2m260\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         device_map: auto                      \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1m(\u001b[0m\u001b[1;35mDeviceRankInfo\u001b[0m\u001b[1m(\u001b[0m\u001b[33mworld_size\u001b[0m=\u001b[1;36m1\u001b[0m, \u001b[33mrank\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mlocal_world_size\u001b[0m=\u001b[1;36m1\u001b[0m, \u001b[33mlocal_rank\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Using model class: \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m   \u001b]8;id=83667;file:///home/wizeng/repos/oumi/src/oumi/builders/models.py\u001b\\\u001b[2mmodels.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=896865;file:///home/wizeng/repos/oumi/src/oumi/builders/models.py#336\u001b\\\u001b[2m336\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32m'transformers.models.auto.modeling_au\u001b[0m \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32mto.AutoModelForCausalLM'\u001b[0m\u001b[1m>\u001b[0m to          \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         instantiate model.                    \u001b[2m             \u001b[0m\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for \n",
      "storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory`\n",
      "in to a higher value to use more memory (at your own risk).\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m                         \u001b]8;id=475435;file:///home/wizeng/repos/oumi/src/oumi/utils/torch_utils.py\u001b\\\u001b[2mtorch_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=666563;file:///home/wizeng/repos/oumi/src/oumi/utils/torch_utils.py#288\u001b\\\u001b[2m288\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         Model Parameters Summary:        \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         üî¢ Total     parameters:         \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m494\u001b[0m,\u001b[1;36m032\u001b[0m,\u001b[1;36m768\u001b[0m                      \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         üîó Embedding parameters:         \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m136\u001b[0m,\u001b[1;36m134\u001b[0m,\u001b[1;36m656\u001b[0m                      \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         üéØ Trainable parameters:         \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m494\u001b[0m,\u001b[1;36m032\u001b[0m,\u001b[1;36m768\u001b[0m                      \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         üîí Frozen    parameters: \u001b[1;36m0\u001b[0m       \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1m(\u001b[0m\u001b[1;36m0.00\u001b[0m%\u001b[1m)\u001b[0m                          \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                                          \u001b[2m                  \u001b[0m\n",
      "INFO 10-30 20:09:57 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[2;36m[10/30/25 20:09:58]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m PROF: Torch    \u001b]8;id=702729;file:///home/wizeng/repos/oumi/src/oumi/performance/torch_profiler_utils.py\u001b\\\u001b[2mtorch_profiler_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=279946;file:///home/wizeng/repos/oumi/src/oumi/performance/torch_profiler_utils.py#164\u001b\\\u001b[2m164\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         Profiler disabled!      \u001b[2m                           \u001b[0m\n",
      "/home/wizeng/repos/oumi/src/oumi/builders/training.py:70: UserWarning: You are importing from 'rollout_func', which is an experimental feature. This API may change or be removed at any time without prior notice. Silence this warning by setting environment variable TRL_EXPERIMENTAL_SILENCE=1.\n",
      "  trainer = HuggingFaceTrainer(cls(*args, **kwargs, args=hf_args), processor)\n",
      "The model is already on multiple devices. Skipping the move to device specified \n",
      "in `args`.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/wizeng/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO:root:gcc -pthread -B /home/wizeng/miniconda3/envs/openenv/compiler_compat \n",
      "-DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem \n",
      "/home/wizeng/miniconda3/envs/openenv/include -fPIC -O2 -isystem \n",
      "/home/wizeng/miniconda3/envs/openenv/include -fPIC -c /tmp/tmph5a0c620/test.c -o\n",
      "/tmp/tmph5a0c620/test.o\n",
      "INFO:root:gcc -pthread -B /home/wizeng/miniconda3/envs/openenv/compiler_compat \n",
      "/tmp/tmph5a0c620/test.o -laio -o /tmp/tmph5a0c620/a.out\n",
      "INFO:root:gcc -pthread -B /home/wizeng/miniconda3/envs/openenv/compiler_compat \n",
      "-DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem \n",
      "/home/wizeng/miniconda3/envs/openenv/include -fPIC -O2 -isystem \n",
      "/home/wizeng/miniconda3/envs/openenv/include -fPIC -c /tmp/tmpog0w7kln/test.c -o\n",
      "/tmp/tmpog0w7kln/test.o\n",
      "INFO:root:gcc -pthread -B /home/wizeng/miniconda3/envs/openenv/compiler_compat \n",
      "/tmp/tmpog0w7kln/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o \n",
      "/tmp/tmpog0w7kln/a.out\n",
      "INFO:root:gcc -pthread -B /home/wizeng/miniconda3/envs/openenv/compiler_compat \n",
      "-DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem \n",
      "/home/wizeng/miniconda3/envs/openenv/include -fPIC -O2 -isystem \n",
      "/home/wizeng/miniconda3/envs/openenv/include -fPIC -c /tmp/tmp5wpyuo56/test.c -o\n",
      "/tmp/tmp5wpyuo56/test.o\n",
      "INFO:root:gcc -pthread -B /home/wizeng/miniconda3/envs/openenv/compiler_compat \n",
      "/tmp/tmp5wpyuo56/test.o -laio -o /tmp/tmp5wpyuo56/a.out\n",
      "INFO:trl.extras.vllm_client:Server is up!\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:59 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:09:59 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 10-30 20:09:59 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "INFO 10-30 20:09:59 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "\u001b[2;36m[10/30/25 20:10:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m GPU Metrics Before     \u001b]8;id=234053;file:///home/wizeng/repos/oumi/src/oumi/utils/device_utils.py\u001b\\\u001b[2mdevice_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=146316;file:///home/wizeng/repos/oumi/src/oumi/utils/device_utils.py#343\u001b\\\u001b[2m343\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         Training: GPU runtime info:     \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;35mNVidiaGpuRuntimeInfo\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdevice_ind\u001b[0m \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mex\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mdevice_count\u001b[0m=\u001b[1;36m2\u001b[0m,           \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mused_memory_mb\u001b[0m=\u001b[1;36m75593\u001b[0m\u001b[1;36m.0\u001b[0m,         \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mtemperature\u001b[0m=\u001b[1;36m33\u001b[0m, \u001b[33mfan_speed\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mfan_speeds\u001b[0m=\u001b[3;35mNone\u001b[0m,                \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mpower_usage_watts\u001b[0m=\u001b[1;36m124\u001b[0m\u001b[1;36m.017\u001b[0m,      \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mpower_limit_watts\u001b[0m=\u001b[1;36m700\u001b[0m\u001b[1;36m.0\u001b[0m,        \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mgpu_utilization\u001b[0m=\u001b[1;36m0\u001b[0m,              \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mmemory_utilization\u001b[0m=\u001b[1;36m0\u001b[0m,           \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mperformance_state\u001b[0m=\u001b[1;36m0\u001b[0m,            \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mclock_speed_graphics\u001b[0m=\u001b[1;36m1980\u001b[0m,      \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mclock_speed_sm\u001b[0m=\u001b[1;36m1980\u001b[0m,            \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mclock_speed_memory\u001b[0m=\u001b[1;36m2619\u001b[0m\u001b[1m)\u001b[0m.       \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Training init time: \u001b[1;36m4.\u001b[0m577s    \u001b]8;id=571858;file:///home/wizeng/repos/oumi/src/oumi/train.py\u001b\\\u001b[2mtrain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=91161;file:///home/wizeng/repos/oumi/src/oumi/train.py#549\u001b\\\u001b[2m549\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Starting training\u001b[33m...\u001b[0m          \u001b]8;id=229258;file:///home/wizeng/repos/oumi/src/oumi/train.py\u001b\\\u001b[2mtrain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=243962;file:///home/wizeng/repos/oumi/src/oumi/train.py#550\u001b\\\u001b[2m550\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1m(\u001b[0mTrainerType.TRL_GRPO, transformers:   \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m4.57\u001b[0m.\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m                                \u001b[2m            \u001b[0m\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and \n",
      "generation config. The model config and generation config were aligned \n",
      "accordingly, being updated with the tokenizer's values. Updated tokens: \n",
      "{'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:00 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1390.22it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:01<00:00, 27.97it/s, est. speed input: 1084.01 toks/s, output: 3418.09 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.3104, 'grad_norm': 5.6875, 'learning_rate': 5e-05, 'num_tokens': 5150.0, 'completions/mean_length': 122.1875, 'completions/min_length': 22.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 0.15625, 'completions/mean_terminated_length': 97.40740966796875, 'completions/min_terminated_length': 22.0, 'completions/max_terminated_length': 244.0, 'rewards/reward_from_env/mean': 60.24374771118164, 'rewards/reward_from_env/std': 46.80415344238281, 'reward': 60.24374771118164, 'reward_std': 20.179214477539062, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.10425987094640732, 'sampling/sampling_logp_difference/max': 1.4170303344726562, 'sampling/importance_sampling_ratio/min': 0.24243289232254028, 'sampling/importance_sampling_ratio/mean': 1.0240256786346436, 'sampling/importance_sampling_ratio/max': 1.5776448249816895, 'entropy': 1.3606750071048737, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.04}\n",
      "  4%|‚ñà‚ñä                                          | 1/25 [00:02<01:01,  2.55s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:03 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1469.49it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 43.54it/s, est. speed input: 3124.69 toks/s, output: 9436.59 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.1341, 'grad_norm': 4.28125, 'learning_rate': 4.8e-05, 'num_tokens': 14380.0, 'completions/mean_length': 216.6875, 'completions/min_length': 3.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 0.65625, 'completions/mean_terminated_length': 141.63636779785156, 'completions/min_terminated_length': 3.0, 'completions/max_terminated_length': 247.0, 'rewards/reward_from_env/mean': 99.38749694824219, 'rewards/reward_from_env/std': 42.04217529296875, 'reward': 99.38749694824219, 'reward_std': 17.43398094177246, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.0798419937491417, 'sampling/sampling_logp_difference/max': 1.7389039993286133, 'sampling/importance_sampling_ratio/min': 0.17571288347244263, 'sampling/importance_sampling_ratio/mean': 1.0187627077102661, 'sampling/importance_sampling_ratio/max': 1.660508632659912, 'entropy': 1.0239375233650208, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.08}\n",
      "  8%|‚ñà‚ñà‚ñà‚ñå                                        | 2/25 [00:04<00:46,  2.00s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:04 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1801.87it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 43.31it/s, est. speed input: 2057.68 toks/s, output: 11089.71 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0002, 'grad_norm': 3.765625, 'learning_rate': 4.600000000000001e-05, 'num_tokens': 24092.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 123.50312042236328, 'rewards/reward_from_env/std': 21.738571166992188, 'reward': 123.50312042236328, 'reward_std': 17.99724769592285, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.07937926799058914, 'sampling/sampling_logp_difference/max': 1.444777488708496, 'sampling/importance_sampling_ratio/min': 0.23579853773117065, 'sampling/importance_sampling_ratio/mean': 1.015897512435913, 'sampling/importance_sampling_ratio/max': 1.5411566495895386, 'entropy': 0.9619140625, 'clip_ratio/low_mean': 0.00048828125, 'clip_ratio/low_min': 0.00048828125, 'clip_ratio/high_mean': 0.0003662109375, 'clip_ratio/high_max': 0.0003662109375, 'clip_ratio/region_mean': 0.0008544921875, 'epoch': 0.12}\n",
      " 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                      | 3/25 [00:05<00:39,  1.80s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:06 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1437.39it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 42.02it/s, est. speed input: 3824.36 toks/s, output: 10758.56 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'grad_norm': 3.359375, 'learning_rate': 4.4000000000000006e-05, 'num_tokens': 35196.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 130.62811279296875, 'rewards/reward_from_env/std': 22.762493133544922, 'reward': 130.62811279296875, 'reward_std': 19.14883804321289, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.06914907693862915, 'sampling/sampling_logp_difference/max': 1.5367345809936523, 'sampling/importance_sampling_ratio/min': 0.2150823026895523, 'sampling/importance_sampling_ratio/mean': 1.0158569812774658, 'sampling/importance_sampling_ratio/max': 1.5292932987213135, 'entropy': 0.822265625, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.00146484375, 'clip_ratio/high_max': 0.00146484375, 'clip_ratio/region_mean': 0.00146484375, 'epoch': 0.16}\n",
      " 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                     | 4/25 [00:07<00:36,  1.74s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:08 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 2011.42it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 42.54it/s, est. speed input: 1616.81 toks/s, output: 10892.05 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0006, 'grad_norm': 3.40625, 'learning_rate': 4.2e-05, 'num_tokens': 44604.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 145.4812469482422, 'rewards/reward_from_env/std': 7.002968788146973, 'reward': 145.4812469482422, 'reward_std': 5.854681015014648, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.07969337701797485, 'sampling/sampling_logp_difference/max': 1.394791603088379, 'sampling/importance_sampling_ratio/min': 0.24788470566272736, 'sampling/importance_sampling_ratio/mean': 1.016239881515503, 'sampling/importance_sampling_ratio/max': 1.4886819124221802, 'entropy': 0.939453125, 'clip_ratio/low_mean': 0.000244140625, 'clip_ratio/low_min': 0.000244140625, 'clip_ratio/high_mean': 0.0008544921875, 'clip_ratio/high_max': 0.0008544921875, 'clip_ratio/region_mean': 0.0010986328125, 'epoch': 0.2}\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 5/25 [00:09<00:33,  1.69s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:09 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1760.46it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 43.12it/s, est. speed input: 2641.14 toks/s, output: 11038.80 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0008, 'grad_norm': 3.765625, 'learning_rate': 4e-05, 'num_tokens': 54756.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 142.4375, 'rewards/reward_from_env/std': 19.4844970703125, 'reward': 142.4375, 'reward_std': 15.795616149902344, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.09761585295200348, 'sampling/sampling_logp_difference/max': 1.6260900497436523, 'sampling/importance_sampling_ratio/min': 0.1966971457004547, 'sampling/importance_sampling_ratio/mean': 1.019674301147461, 'sampling/importance_sampling_ratio/max': 1.6572049856185913, 'entropy': 1.208984375, 'clip_ratio/low_mean': 0.0001220703125, 'clip_ratio/low_min': 0.0001220703125, 'clip_ratio/high_mean': 0.0003662109375, 'clip_ratio/high_max': 0.0003662109375, 'clip_ratio/region_mean': 0.00048828125, 'epoch': 0.24}\n",
      " 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 6/25 [00:10<00:31,  1.65s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:11 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1486.95it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 42.78it/s, est. speed input: 3422.94 toks/s, output: 10953.31 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0016, 'grad_norm': 4.75, 'learning_rate': 3.8e-05, 'num_tokens': 65508.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 147.90936279296875, 'rewards/reward_from_env/std': 21.37916374206543, 'reward': 147.90936279296875, 'reward_std': 17.90665054321289, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.1063748449087143, 'sampling/sampling_logp_difference/max': 1.522308349609375, 'sampling/importance_sampling_ratio/min': 0.21820761263370514, 'sampling/importance_sampling_ratio/mean': 1.0218125581741333, 'sampling/importance_sampling_ratio/max': 1.9696154594421387, 'entropy': 1.330078125, 'clip_ratio/low_mean': 0.0003662109375, 'clip_ratio/low_min': 0.0003662109375, 'clip_ratio/high_mean': 0.0013427734375, 'clip_ratio/high_max': 0.0013427734375, 'clip_ratio/region_mean': 0.001708984375, 'epoch': 0.28}\n",
      " 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 7/25 [00:12<00:29,  1.64s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:12 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1732.65it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 42.54it/s, est. speed input: 3052.72 toks/s, output: 10891.86 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0002, 'grad_norm': 6.0, 'learning_rate': 3.6e-05, 'num_tokens': 75996.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 148.5749969482422, 'rewards/reward_from_env/std': 27.652334213256836, 'reward': 148.5749969482422, 'reward_std': 26.42963981628418, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.10105139017105103, 'sampling/sampling_logp_difference/max': 1.3686447143554688, 'sampling/importance_sampling_ratio/min': 0.25445157289505005, 'sampling/importance_sampling_ratio/mean': 1.021196722984314, 'sampling/importance_sampling_ratio/max': 2.0, 'entropy': 1.2109375, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.001220703125, 'clip_ratio/high_max': 0.001220703125, 'clip_ratio/region_mean': 0.001220703125, 'epoch': 0.32}\n",
      " 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 8/25 [00:13<00:27,  1.62s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:14 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1869.12it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 35.54it/s, est. speed input: 1279.46 toks/s, output: 9098.35 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0009, 'grad_norm': 5.125, 'learning_rate': 3.4000000000000007e-05, 'num_tokens': 85340.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 150.13436889648438, 'rewards/reward_from_env/std': 25.33433723449707, 'reward': 150.13436889648438, 'reward_std': 24.00356674194336, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.09709164500236511, 'sampling/sampling_logp_difference/max': 1.6557836532592773, 'sampling/importance_sampling_ratio/min': 0.19094236195087433, 'sampling/importance_sampling_ratio/mean': 1.022766351699829, 'sampling/importance_sampling_ratio/max': 1.664004921913147, 'entropy': 1.18359375, 'clip_ratio/low_mean': 0.0001220703125, 'clip_ratio/low_min': 0.0001220703125, 'clip_ratio/high_mean': 0.0006103515625, 'clip_ratio/high_max': 0.0006103515625, 'clip_ratio/region_mean': 0.000732421875, 'epoch': 0.36}\n",
      " 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 9/25 [00:15<00:27,  1.73s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:16 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1653.91it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 42.68it/s, est. speed input: 2390.22 toks/s, output: 10926.61 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.002, 'grad_norm': 3.015625, 'learning_rate': 3.2000000000000005e-05, 'num_tokens': 95324.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 199.60000610351562, 'rewards/reward_from_env/std': 38.48003387451172, 'reward': 199.60000610351562, 'reward_std': 29.59756851196289, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.054048649966716766, 'sampling/sampling_logp_difference/max': 1.426081657409668, 'sampling/importance_sampling_ratio/min': 0.24024845659732819, 'sampling/importance_sampling_ratio/mean': 1.0123540163040161, 'sampling/importance_sampling_ratio/max': 1.4719880819320679, 'entropy': 0.5947265625, 'clip_ratio/low_mean': 0.0001220703125, 'clip_ratio/low_min': 0.0001220703125, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0001220703125, 'epoch': 0.4}\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 10/25 [00:17<00:25,  1.69s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:18 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1734.26it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 41.04it/s, est. speed input: 2555.27 toks/s, output: 10508.33 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0016, 'grad_norm': 2.1875, 'learning_rate': 3e-05, 'num_tokens': 105508.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 223.515625, 'rewards/reward_from_env/std': 35.32181167602539, 'reward': 223.515625, 'reward_std': 27.529592514038086, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.03328322619199753, 'sampling/sampling_logp_difference/max': 1.2655229568481445, 'sampling/importance_sampling_ratio/min': 0.28209173679351807, 'sampling/importance_sampling_ratio/mean': 1.0072062015533447, 'sampling/importance_sampling_ratio/max': 1.7108453512191772, 'entropy': 0.3662109375, 'clip_ratio/low_mean': 0.000244140625, 'clip_ratio/low_min': 0.000244140625, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.000244140625, 'epoch': 0.44}\n",
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 11/25 [00:18<00:23,  1.67s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:19 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1398.57it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 40.81it/s, est. speed input: 3612.23 toks/s, output: 10448.80 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0034, 'grad_norm': 2.71875, 'learning_rate': 2.8000000000000003e-05, 'num_tokens': 116532.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 236.68438720703125, 'rewards/reward_from_env/std': 45.642601013183594, 'reward': 236.68438720703125, 'reward_std': 37.60443115234375, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.03014453500509262, 'sampling/sampling_logp_difference/max': 1.6696949005126953, 'sampling/importance_sampling_ratio/min': 0.18830451369285583, 'sampling/importance_sampling_ratio/mean': 1.0063087940216064, 'sampling/importance_sampling_ratio/max': 1.4304356575012207, 'entropy': 0.33837890625, 'clip_ratio/low_mean': 0.0001220703125, 'clip_ratio/low_min': 0.0001220703125, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0001220703125, 'epoch': 0.48}\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 12/25 [00:20<00:21,  1.66s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:21 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1758.62it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 43.04it/s, est. speed input: 2130.55 toks/s, output: 11018.55 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0016, 'grad_norm': 1.8828125, 'learning_rate': 2.6000000000000002e-05, 'num_tokens': 126308.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 253.80313110351562, 'rewards/reward_from_env/std': 25.515148162841797, 'reward': 253.80313110351562, 'reward_std': 23.875682830810547, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.013019567355513573, 'sampling/sampling_logp_difference/max': 1.172348976135254, 'sampling/importance_sampling_ratio/min': 0.30963876843452454, 'sampling/importance_sampling_ratio/mean': 1.0030759572982788, 'sampling/importance_sampling_ratio/max': 1.3728663921356201, 'entropy': 0.1226806640625, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0001220703125, 'clip_ratio/high_max': 0.0001220703125, 'clip_ratio/region_mean': 0.0001220703125, 'epoch': 0.52}\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 13/25 [00:22<00:19,  1.64s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:22 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1322.81it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 41.85it/s, est. speed input: 3473.92 toks/s, output: 10714.64 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0018, 'grad_norm': 1.828125, 'learning_rate': 2.4e-05, 'num_tokens': 137156.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 268.48126220703125, 'rewards/reward_from_env/std': 23.270313262939453, 'reward': 268.48126220703125, 'reward_std': 19.19301986694336, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.019933423027396202, 'sampling/sampling_logp_difference/max': 1.4129743576049805, 'sampling/importance_sampling_ratio/min': 0.24341818690299988, 'sampling/importance_sampling_ratio/mean': 1.0036290884017944, 'sampling/importance_sampling_ratio/max': 1.4830222129821777, 'entropy': 0.1968994140625, 'clip_ratio/low_mean': 0.0001220703125, 'clip_ratio/low_min': 0.0001220703125, 'clip_ratio/high_mean': 0.00048828125, 'clip_ratio/high_max': 0.00048828125, 'clip_ratio/region_mean': 0.0006103515625, 'epoch': 0.56}\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 14/25 [00:23<00:17,  1.63s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:24 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1777.81it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 42.97it/s, est. speed input: 1385.95 toks/s, output: 11001.54 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0011, 'grad_norm': 1.53125, 'learning_rate': 2.2000000000000003e-05, 'num_tokens': 146380.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 284.0187683105469, 'rewards/reward_from_env/std': 19.812435150146484, 'reward': 284.0187683105469, 'reward_std': 12.771563529968262, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.01629074662923813, 'sampling/sampling_logp_difference/max': 1.3111591339111328, 'sampling/importance_sampling_ratio/min': 0.2695074677467346, 'sampling/importance_sampling_ratio/mean': 1.0034124851226807, 'sampling/importance_sampling_ratio/max': 1.6229891777038574, 'entropy': 0.176513671875, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.6}\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 15/25 [00:25<00:16,  1.61s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:26 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1626.01it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 42.34it/s, est. speed input: 2678.22 toks/s, output: 10839.81 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0009, 'grad_norm': 1.640625, 'learning_rate': 2e-05, 'num_tokens': 156596.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 279.65625, 'rewards/reward_from_env/std': 20.31792449951172, 'reward': 279.65625, 'reward_std': 15.107717514038086, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.016554228961467743, 'sampling/sampling_logp_difference/max': 1.2050352096557617, 'sampling/importance_sampling_ratio/min': 0.2996814548969269, 'sampling/importance_sampling_ratio/mean': 1.0037851333618164, 'sampling/importance_sampling_ratio/max': 1.4312273263931274, 'entropy': 0.18017578125, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.64}\n",
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 16/25 [00:26<00:14,  1.61s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:27 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1818.87it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 32.23it/s, est. speed input: 1498.67 toks/s, output: 8250.66 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'grad_norm': 2.21875, 'learning_rate': 1.8e-05, 'num_tokens': 166276.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 281.79376220703125, 'rewards/reward_from_env/std': 42.84409713745117, 'reward': 281.79376220703125, 'reward_std': 28.880355834960938, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.015027951449155807, 'sampling/sampling_logp_difference/max': 1.3328685760498047, 'sampling/importance_sampling_ratio/min': 0.26371967792510986, 'sampling/importance_sampling_ratio/mean': 1.0014393329620361, 'sampling/importance_sampling_ratio/max': 1.4249001741409302, 'entropy': 0.1544189453125, 'clip_ratio/low_mean': 0.0001220703125, 'clip_ratio/low_min': 0.0001220703125, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0001220703125, 'epoch': 0.68}\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 17/25 [00:28<00:13,  1.68s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:29 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1451.57it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 41.89it/s, est. speed input: 3665.56 toks/s, output: 10724.30 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0001, 'grad_norm': 1.4140625, 'learning_rate': 1.6000000000000003e-05, 'num_tokens': 177268.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 293.6875, 'rewards/reward_from_env/std': 6.768321514129639, 'reward': 293.6875, 'reward_std': 6.160709381103516, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.00917959026992321, 'sampling/sampling_logp_difference/max': 1.5185699462890625, 'sampling/importance_sampling_ratio/min': 0.2190248966217041, 'sampling/importance_sampling_ratio/mean': 1.0015687942504883, 'sampling/importance_sampling_ratio/max': 1.3430147171020508, 'entropy': 0.09228515625, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.72}\n",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 18/25 [00:30<00:11,  1.66s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:31 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1998.00it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 43.26it/s, est. speed input: 1492.64 toks/s, output: 11075.77 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0006, 'grad_norm': 1.234375, 'learning_rate': 1.4000000000000001e-05, 'num_tokens': 186564.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 296.0562744140625, 'rewards/reward_from_env/std': 3.7737557888031006, 'reward': 296.0562744140625, 'reward_std': 2.830070972442627, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.007263758685439825, 'sampling/sampling_logp_difference/max': 1.2498435974121094, 'sampling/importance_sampling_ratio/min': 0.2865495979785919, 'sampling/importance_sampling_ratio/mean': 1.0014268159866333, 'sampling/importance_sampling_ratio/max': 1.6479860544204712, 'entropy': 0.0804443359375, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.76}\n",
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 19/25 [00:31<00:09,  1.63s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:32 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1944.28it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 43.38it/s, est. speed input: 1865.48 toks/s, output: 11106.05 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 1.4453125, 'learning_rate': 1.2e-05, 'num_tokens': 196132.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 296.44061279296875, 'rewards/reward_from_env/std': 8.767380714416504, 'reward': 296.44061279296875, 'reward_std': 6.072707176208496, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.006782043259590864, 'sampling/sampling_logp_difference/max': 1.0349149703979492, 'sampling/importance_sampling_ratio/min': 0.3552566170692444, 'sampling/importance_sampling_ratio/mean': 1.001916527748108, 'sampling/importance_sampling_ratio/max': 1.400224208831787, 'entropy': 0.07220458984375, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.8}\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 20/25 [00:33<00:08,  1.62s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:34 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1492.37it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 42.57it/s, est. speed input: 3342.40 toks/s, output: 10899.73 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'grad_norm': 1.015625, 'learning_rate': 1e-05, 'num_tokens': 206836.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 298.87811279296875, 'rewards/reward_from_env/std': 2.398622512817383, 'reward': 298.87811279296875, 'reward_std': 2.237010955810547, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.005232331342995167, 'sampling/sampling_logp_difference/max': 1.1641674041748047, 'sampling/importance_sampling_ratio/min': 0.3121824562549591, 'sampling/importance_sampling_ratio/mean': 1.0014495849609375, 'sampling/importance_sampling_ratio/max': 1.3543024063110352, 'entropy': 0.05694580078125, 'clip_ratio/low_mean': 0.0001220703125, 'clip_ratio/low_min': 0.0001220703125, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0001220703125, 'epoch': 0.84}\n",
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 21/25 [00:35<00:06,  1.62s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:35 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1879.80it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 42.73it/s, est. speed input: 1912.31 toks/s, output: 10939.60 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0001, 'grad_norm': 0.875, 'learning_rate': 8.000000000000001e-06, 'num_tokens': 216460.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 299.921875, 'rewards/reward_from_env/std': 1.9435142278671265, 'reward': 299.921875, 'reward_std': 1.6189486980438232, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.0038750190287828445, 'sampling/sampling_logp_difference/max': 1.2732229232788086, 'sampling/importance_sampling_ratio/min': 0.27992796897888184, 'sampling/importance_sampling_ratio/mean': 1.000841736793518, 'sampling/importance_sampling_ratio/max': 1.2461822032928467, 'entropy': 0.0382080078125, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.88}\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 22/25 [00:36<00:04,  1.61s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:37 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1735.33it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 42.49it/s, est. speed input: 2231.19 toks/s, output: 10879.63 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0005, 'grad_norm': 1.0, 'learning_rate': 6e-06, 'num_tokens': 226332.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 299.62811279296875, 'rewards/reward_from_env/std': 2.361961603164673, 'reward': 299.62811279296875, 'reward_std': 1.885979175567627, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.004480584524571896, 'sampling/sampling_logp_difference/max': 1.2986259460449219, 'sampling/importance_sampling_ratio/min': 0.27290651202201843, 'sampling/importance_sampling_ratio/mean': 1.0008270740509033, 'sampling/importance_sampling_ratio/max': 1.284912347793579, 'entropy': 0.04254150390625, 'clip_ratio/low_mean': 0.0001220703125, 'clip_ratio/low_min': 0.0001220703125, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0001220703125, 'epoch': 0.92}\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 23/25 [00:38<00:03,  1.67s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:39 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1362.34it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 42.71it/s, est. speed input: 2178.70 toks/s, output: 10936.15 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0, 'grad_norm': 1.1328125, 'learning_rate': 4.000000000000001e-06, 'num_tokens': 236156.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 299.54376220703125, 'rewards/reward_from_env/std': 3.9939420223236084, 'reward': 299.54376220703125, 'reward_std': 2.759938955307007, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.004718138370662928, 'sampling/sampling_logp_difference/max': 1.1052160263061523, 'sampling/importance_sampling_ratio/min': 0.33113935589790344, 'sampling/importance_sampling_ratio/mean': 1.000596284866333, 'sampling/importance_sampling_ratio/max': 1.3233096599578857, 'entropy': 0.04345703125, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0001220703125, 'clip_ratio/high_max': 0.0001220703125, 'clip_ratio/region_mean': 0.0001220703125, 'epoch': 0.96}\n",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 24/25 [00:40<00:01,  1.65s/it]\u001b[1;36m(EngineCore_DP0 pid=3616607)\u001b[0;0m INFO 10-30 20:10:40 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1406.54it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 42.61it/s, est. speed input: 3100.11 toks/s, output: 10908.90 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0003, 'grad_norm': 0.87109375, 'learning_rate': 2.0000000000000003e-06, 'num_tokens': 246676.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/reward_from_env/mean': 300.546875, 'rewards/reward_from_env/std': 1.7720036506652832, 'reward': 300.546875, 'reward_std': 1.5536911487579346, 'frac_reward_zero_std': 0.0, 'sampling/sampling_logp_difference/mean': 0.003943466581404209, 'sampling/sampling_logp_difference/max': 0.8658556938171387, 'sampling/importance_sampling_ratio/min': 0.4206914007663727, 'sampling/importance_sampling_ratio/mean': 1.0006545782089233, 'sampling/importance_sampling_ratio/max': 1.237608790397644, 'entropy': 0.037139892578125, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 45.166, 'train_samples_per_second': 2.214, 'train_steps_per_second': 0.554, 'train_loss': -0.01723662968724966, 'epoch': 1.0}\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:45<00:00,  1.81s/it]\n",
      "\u001b[2;36m[10/30/25 20:10:45]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Training is Complete.         \u001b]8;id=750800;file:///home/wizeng/repos/oumi/src/oumi/train.py\u001b\\\u001b[2mtrain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=681453;file:///home/wizeng/repos/oumi/src/oumi/train.py#557\u001b\\\u001b[2m557\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m GPU Metrics After      \u001b]8;id=471029;file:///home/wizeng/repos/oumi/src/oumi/utils/device_utils.py\u001b\\\u001b[2mdevice_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=617889;file:///home/wizeng/repos/oumi/src/oumi/utils/device_utils.py#343\u001b\\\u001b[2m343\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         Training: GPU runtime info:     \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;35mNVidiaGpuRuntimeInfo\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdevice_ind\u001b[0m \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mex\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mdevice_count\u001b[0m=\u001b[1;36m2\u001b[0m,           \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mused_memory_mb\u001b[0m=\u001b[1;36m75603\u001b[0m\u001b[1;36m.0\u001b[0m,         \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mtemperature\u001b[0m=\u001b[1;36m34\u001b[0m, \u001b[33mfan_speed\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mfan_speeds\u001b[0m=\u001b[3;35mNone\u001b[0m,                \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mpower_usage_watts\u001b[0m=\u001b[1;36m124\u001b[0m\u001b[1;36m.806\u001b[0m,      \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mpower_limit_watts\u001b[0m=\u001b[1;36m700\u001b[0m\u001b[1;36m.0\u001b[0m,        \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mgpu_utilization\u001b[0m=\u001b[1;36m0\u001b[0m,              \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mmemory_utilization\u001b[0m=\u001b[1;36m0\u001b[0m,           \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mperformance_state\u001b[0m=\u001b[1;36m0\u001b[0m,            \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mclock_speed_graphics\u001b[0m=\u001b[1;36m1980\u001b[0m,      \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mclock_speed_sm\u001b[0m=\u001b[1;36m1980\u001b[0m,            \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[33mclock_speed_memory\u001b[0m=\u001b[1;36m2619\u001b[0m\u001b[1m)\u001b[0m.       \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Peak GPU memory usage:  \u001b]8;id=844962;file:///home/wizeng/repos/oumi/src/oumi/utils/torch_utils.py\u001b\\\u001b[2mtorch_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=167414;file:///home/wizeng/repos/oumi/src/oumi/utils/torch_utils.py#135\u001b\\\u001b[2m135\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m10.15\u001b[0m GB                         \u001b[2m                  \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Saving final state\u001b[33m...\u001b[0m         \u001b]8;id=225772;file:///home/wizeng/repos/oumi/src/oumi/train.py\u001b\\\u001b[2mtrain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=800581;file:///home/wizeng/repos/oumi/src/oumi/train.py#564\u001b\\\u001b[2m564\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Saving final model\u001b[33m...\u001b[0m         \u001b]8;id=376417;file:///home/wizeng/repos/oumi/src/oumi/train.py\u001b\\\u001b[2mtrain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=888662;file:///home/wizeng/repos/oumi/src/oumi/train.py#569\u001b\\\u001b[2m569\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m[10/30/25 20:10:47]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m Model has been saved at  \u001b]8;id=481741;file:///home/wizeng/repos/oumi/src/oumi/core/trainers/hf_trainer.py\u001b\\\u001b[2mhf_trainer.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=562275;file:///home/wizeng/repos/oumi/src/oumi/core/trainers/hf_trainer.py#127\u001b\\\u001b[2m127\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         openenv_tutorial/echo_grpo        \u001b[2m                 \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mrank-\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m                               \u001b]8;id=307419;file:///home/wizeng/repos/oumi/src/oumi/train.py\u001b\\\u001b[2mtrain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=869693;file:///home/wizeng/repos/oumi/src/oumi/train.py#222\u001b\\\u001b[2m222\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m                                                \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         ¬ª We're always looking for feedback.   \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         What's one thing we can improve?       \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://oumi.ai/feedback\u001b[0m               \u001b[2m            \u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 oumi train -c $tutorial_dir/grpo_train.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "openenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
