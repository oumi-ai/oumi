{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "OUMI - Train From CLI Example",
            "type": "debugpy",
            "request": "launch",
            "module": "oumi.train",
            "args": [
                "data.train.datasets.0.dataset_name=yahma/alpaca-cleaned",
                "data.train.datasets.0.preprocessing_function_name=alpaca",
                "data.train.datasets.0.target_col=prompt",
                "model.model_name=openai-community/gpt2",
                "model.trust_remote_code=true",
                "training.trainer_type=TRL_SFT",
                "training.max_steps=3",
                "training.logging_steps=3",
                "training.enable_wandb=false",
                "training.enable_tensorboard=false",
                "training.output_dir=tmp"
            ],
            "justMyCode": true
        },
        {
            "name": "OUMI - Train From Config Example",
            "type": "debugpy",
            "request": "launch",
            "module": "oumi.train",
            "args": [
                "-c",
                "configs/oumi/phi3.lora.yaml"
            ],
            "justMyCode": true
        },
        {
            "name": "OUMI - Pretrain From Config Example",
            "type": "debugpy",
            "request": "launch",
            "module": "oumi.train",
            "args": [
                "-c",
                "configs/oumi/gpt2.pt.mac.yaml"
            ],
            "justMyCode": true
        },
        {
            "name": "OUMI Distributed",
            "type": "debugpy",
            "module": "torch.distributed.run",
            "request": "launch",
            "console": "integratedTerminal",
            "args": [
                "--standalone",
                "--nproc_per_node",
                "4",
                "-m",
                "oumi.train",
                "-c",
                "configs/oumi/gpt2.pt.yaml"
            ]
        },
        {
            "name": "OUMI Distributed - Accelerate",
            "type": "debugpy",
            "module": "accelerate.commands.launch",
            "request": "launch",
            "console": "integratedTerminal",
            "args": [
                "--num_machines",
                "1",
                "--machine_rank",
                "0",
                "--num_processes",
                "8",
                "--use_fsdp",
                "--config_file",
                "configs/accelerate/llama8b.fsdp.yaml",
                "-m",
                "oumi.train",
                "-c",
                "configs/oumi/llama8b.sft.yaml",
                "training.max_steps=2",
                "training.save_steps=1",
                "training.save_final_model=true",
                "training.enable_wandb=false",
                "training.enable_tensorboard=false"
            ]
        },
        {
            "name": "OUMI - Inference From CLI Example",
            "type": "debugpy",
            "request": "launch",
            "module": "oumi.infer",
            "args": [
                "--interactive",
                "model.model_name=openai-community/gpt2",
                "model.trust_remote_code=true",
                "generation.max_new_tokens=16"
            ],
            "justMyCode": true
        },
        {
            "name": "OUMI - Evaluate with custom OUMI framework from config example",
            "type": "debugpy",
            "request": "launch",
            "module": "oumi.evaluate",
            "args": [
                "-c",
                "configs/oumi/phi3.eval.oumi.mac.yaml"
            ],
            "justMyCode": true
        },
        {
            "name": "OUMI - Evaluate with the LM Harness framework from config example",
            "type": "debugpy",
            "request": "launch",
            "module": "oumi.evaluate",
            "args": [
                "-c",
                "configs/oumi/phi3.eval.lm_harness.yaml"
            ],
            "justMyCode": true
        },
        {
            "name": "Python Debugger: Current File",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal"
        }
    ]
}
