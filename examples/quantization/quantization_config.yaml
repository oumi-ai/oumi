# Basic Model Quantization Configuration
# ðŸš§ DEVELOPMENT: This configuration demonstrates basic AWQ quantization

# Model configuration
model:
  model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  tokenizer_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  trust_remote_code: false

# Quantization settings
method: "awq_q4_0"                # AWQ 4-bit quantization (recommended)
output_path: "tinyllama-1.1b-awq4.pytorch"  # Output file path
output_format: "pytorch"          # PyTorch format (pytorch or safetensors)

# AWQ-specific settings
awq_group_size: 128              # Weight grouping size for AWQ
calibration_samples: 512         # Number of calibration samples

# Optional settings
batch_size: null                 # Auto-determine batch size
verbose: true                    # Enable detailed logging
