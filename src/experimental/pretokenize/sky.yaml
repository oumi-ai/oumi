# Sample command:
# sky launch --cloud GCP -i 10 --env LEMA_RUN_NAME=pretokenize.v01 --num-nodes 1 --no-use-spot --cluster xrdaukar-cpu-01-lema-cluster src/experimental/pretokenize/sky.yaml

name: pretokenize-example

resources:
  cpus: "32+"
  memory: "64+"
  disk_size: 200 # Disk size in GB
  region: europe-west4

  any_of:
    - use_spot: false

# Upload a working directory to remote ~/sky_workdir.
workdir: .

# Upload local files.
file_mounts:
  ~/.netrc: ~/.netrc # mount local netrc file to access private repos
  # /artifacts:
  #   name: lema-dev-private # Not available on lambda
  #   mode: MOUNT

  /dataset_parent_dir:
    source: gs://lema-dev-europe-west4/
    store: gcs
    mode: MOUNT

envs:
  LEMA_RUN_NAME: pretokenize

setup: |
  set -e
  pip install 'lema[train]'

run: |
  set -e  # Exit if any command failed.

  # Run some checks, and export "LEMA_*" env vars
  source ./configs/skypilot/sky_init.sh

  export INPUT_DATA="/dataset_parent_dir/datasets/fineweb-edu/sample-10BT"
  echo "INPUT_DATA: $INPUT_DATA. Content: $(ls -l $INPUT_DATA)"
  echo "${INPUT_DATA}: $(ls -l ${INPUT_DATA})"
  echo "${INPUT_DATA}/train: $(ls -l ${INPUT_DATA}/train)"

  export OUTPUT_DATA="/dataset_parent_dir/datasets/fineweb-edu/hf_pretokenized-sample-10BT"

  set -x # Enable command tracing.
  mkdir -p "$OUTPUT_DATA/train"
  cp -r ${INPUT_DATA}/**.json ${OUTPUT_DATA}
  python3 ./src/experimental/pretokenize/tokenize_dataset.py \
    --verbose \
    --target_col text \
    --input_dataset "${INPUT_DATA}/train/" \
    --output_dir "${OUTPUT_DATA}/train/" \
    --config ./configs/lema/llama2b.pt.yaml \

  echo "Node ${SKYPILOT_NODE_RANK} is all done!"
