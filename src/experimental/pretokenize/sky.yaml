name: pretokenize-example

resources:
  cpus: "16+"
  memory: "32+"
  disk_size: 200      # Disk size in GB
  region: europe-west4

  any_of:
    - use_spot: true
    - use_spot: false

# Upload a working directory to remote ~/sky_workdir.
workdir: .

# Upload local files.
file_mounts:
  ~/.netrc: ~/.netrc  # mount local netrc file to access private repos
  # /artifacts:
  #   name: lema-dev-private # Not available on lambda
  #   mode: MOUNT

  /dataset_parent_dir:
    source: gs://lema-dev-europe-west4/lema-dev-europe-west4/datasets/fineweb-edu/
    store: gcs
    mode: MOUNT

envs:
  LEMA_RUN_NAME: pretokenize

setup: |
  set -e
  pip install '.[train]'

run: |
  set -e  # Exit if any command failed.

  # Run some checks, and export "LEMA_*" env vars
  source ./configs/skypilot/sky_init.sh

  export INPUT_DATA="/dataset_parent_dir/sample-10BT/"
  echo "INPUT_DATA: $INPUT_DATA. Content: $(ls -l $INPUT_DATA)"
  echo "${INPUT_DATA}: $(ls -l ${INPUT_DATA})"
  echo "${INPUT_DATA}/train: $(ls -l ${INPUT_DATA}/train)"

  export OUTPUT_DATA="/dataset_parent_dir/pretokenized-sample-10BT/"
  mkdir -p "$OUTPUT_DATA/train"

  set -x # Enable command tracing.
  python3 ./src/experimental/pretokenize/tokenize_dataset.py \
    --verbose \
    --config ./configs/lema/llama2b.pt.yaml \
    --input_format parquet \
    --target_col text \
    --input_path "${INPUT_DATA}/train/data-00049-of-00099.arrow" \
    --output_dir "$OUTPUT_DATA/train" \
    --overwrite false

  echo "Node ${SKYPILOT_NODE_RANK} is all done!"
