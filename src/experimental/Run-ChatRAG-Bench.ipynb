{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from oumi.datasets import ChatRAGBenchDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_lines(filename, lines):\n",
    "    \"\"\"Write a list of lines to a file.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf8\") as f:\n",
    "        for output in lines:\n",
    "            f.write(output + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the ChatRAG-Bench dataset and evaluation scripts locally\n",
    "# !git clone git@hf.co:datasets/nvidia/ChatRAG-Bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the cloned repo\n",
    "base_chatqa_folder = \"ChatRAG-Bench\"\n",
    "\n",
    "# Add the evaluation scripts to the path\n",
    "sys.path.append(str(Path(base_chatqa_folder) / \"evaluation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models we want to test. Can be an Oumi checkpoint or a Hugging Face model\n",
    "# Checkpoints can be found on gs://\n",
    "models = [  # (display friendly name, name or path)\n",
    "    (\"phi3-original\", \"microsoft/Phi-3-mini-4k-instruct\"),\n",
    "    (\"chatqa-8b\", \"nvidia/ChatQA-1.5-8B\"),\n",
    "    (\"phi3-finetune-stage2-last-ckpt\", \"../../stage2/checkpoint-14961\"),\n",
    "    (\"phi3-finetune-stage1-last-ckpt\", \"../../stage1/checkpoint-23000\"),\n",
    "]\n",
    "\n",
    "# ChatRAGBenchmarks we want to test on\n",
    "benchmarks = [\n",
    "    \"coqa\",\n",
    "    \"inscit\",\n",
    "    \"topiocqa\",\n",
    "    \"hybridial\",\n",
    "    \"doc2dial\",\n",
    "    \"quac\",\n",
    "    \"sqa\",\n",
    "    # 'qrecc',  # there is a bug with this dataset\n",
    "    # 'doqa_cooking', 'doqa_movies', 'doqa_travel',  # these are not supported yet\n",
    "]\n",
    "\n",
    "# Outputs folder\n",
    "root_predictions_folder = \"predictions\"\n",
    "Path(root_predictions_folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id, model_name_or_path in tqdm(models, desc=\"Models\"):\n",
    "    model_predictions_folder = Path(root_predictions_folder) / model_id\n",
    "    model_predictions_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load model. Handles everything by default (kv caching, GPU)\n",
    "    # Note: only tested with a single GPU\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0, top_k=1, max_tokens=64\n",
    "    )  # Same hparams as the paper\n",
    "    llm = LLM(model=model_name_or_path)\n",
    "\n",
    "    # Manually create a tokenizer to be able to generate prompts\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "    for subset in tqdm(benchmarks, desc=\"Benchmarks\"):\n",
    "        # Load dataset and generate prompts\n",
    "        dataset = ChatRAGBenchDataset(tokenizer=tokenizer, subset=subset)\n",
    "        prompts = [dataset.prompt(idx) for idx in range(len(dataset))]\n",
    "        print(f\"Loading {len(dataset)} examples from {subset}...\")\n",
    "\n",
    "        # Generate outputs & preprocess outputs\n",
    "        outputs = llm.generate(prompts)\n",
    "        processed_outputs = [\n",
    "            output.outputs[0].text.strip().replace(\"\\n\", \" \") for output in outputs\n",
    "        ]  # same pre-processing as paper. Not sure why they strip newlines.\n",
    "\n",
    "        # Write outputs to file\n",
    "        output_filename = model_predictions_folder / f\"{subset}.txt\"\n",
    "        write_lines(output_filename, processed_outputs)\n",
    "\n",
    "    # Free up gpu memory to allow loading the next model\n",
    "    del llm\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Process Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_ground_truth_filename(benchmark):\n",
    "    base_path = Path(base_chatqa_folder) / \"data\" / benchmark\n",
    "    dev_file = base_path / \"dev.json\"\n",
    "    test_file = base_path / \"test.json\"\n",
    "\n",
    "    if dev_file.is_file():\n",
    "        return str(dev_file)\n",
    "\n",
    "    if test_file.is_file():\n",
    "        return str(test_file)\n",
    "\n",
    "    raise ValueError(f\"Could not find ground truth for {benchmark}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same script used by the ChatRAG-Bench authors\n",
    "from get_scores import evaluate_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_id, model_name_or_path in tqdm(models, desc=\"Models\"):\n",
    "    model_predictions_folder = Path(root_predictions_folder) / model_id\n",
    "\n",
    "    for benchmark in tqdm(benchmarks, desc=\"Benchmarks\"):\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        # Annoyingly, the ground truth files are named differently depending\n",
    "        # on the benchmark\n",
    "        ground_truth_file = _get_ground_truth_filename(benchmark)\n",
    "        prediction_file = model_predictions_folder / f\"{benchmark}.txt\"\n",
    "\n",
    "        precision, recall, f1 = evaluate_f1(\n",
    "            str(ground_truth_file), str(prediction_file)\n",
    "        )\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"benchmark\": benchmark,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1,\n",
    "                \"model\": model_id,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyse metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>benchmark</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>topiocqa</td>\n",
       "      <td>0.283519</td>\n",
       "      <td>0.288733</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>chatqa_eval/nvidia_chatqa_outputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>topiocqa</td>\n",
       "      <td>0.270855</td>\n",
       "      <td>0.251446</td>\n",
       "      <td>0.328955</td>\n",
       "      <td>chatqa_eval/finetuned_phi3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>topiocqa</td>\n",
       "      <td>0.240453</td>\n",
       "      <td>0.278561</td>\n",
       "      <td>0.319551</td>\n",
       "      <td>chatqa_eval/base_phi3_outputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sqa</td>\n",
       "      <td>0.732992</td>\n",
       "      <td>0.510911</td>\n",
       "      <td>0.518191</td>\n",
       "      <td>chatqa_eval/nvidia_chatqa_outputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sqa</td>\n",
       "      <td>0.583139</td>\n",
       "      <td>0.425426</td>\n",
       "      <td>0.429939</td>\n",
       "      <td>chatqa_eval/finetuned_phi3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sqa</td>\n",
       "      <td>0.354951</td>\n",
       "      <td>0.366099</td>\n",
       "      <td>0.294228</td>\n",
       "      <td>chatqa_eval/base_phi3_outputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>quac</td>\n",
       "      <td>0.311620</td>\n",
       "      <td>0.268384</td>\n",
       "      <td>0.271534</td>\n",
       "      <td>chatqa_eval/nvidia_chatqa_outputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>quac</td>\n",
       "      <td>0.199881</td>\n",
       "      <td>0.161826</td>\n",
       "      <td>0.166448</td>\n",
       "      <td>chatqa_eval/finetuned_phi3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qrecc</td>\n",
       "      <td>0.446039</td>\n",
       "      <td>0.281528</td>\n",
       "      <td>0.326638</td>\n",
       "      <td>chatqa_eval/nvidia_chatqa_outputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qrecc</td>\n",
       "      <td>0.391230</td>\n",
       "      <td>0.256864</td>\n",
       "      <td>0.293054</td>\n",
       "      <td>chatqa_eval/base_phi3_outputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>inscit</td>\n",
       "      <td>0.347971</td>\n",
       "      <td>0.126360</td>\n",
       "      <td>0.196037</td>\n",
       "      <td>chatqa_eval/nvidia_chatqa_outputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inscit</td>\n",
       "      <td>0.336776</td>\n",
       "      <td>0.116251</td>\n",
       "      <td>0.187411</td>\n",
       "      <td>chatqa_eval/base_phi3_outputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>inscit</td>\n",
       "      <td>0.325252</td>\n",
       "      <td>0.106394</td>\n",
       "      <td>0.169698</td>\n",
       "      <td>chatqa_eval/finetuned_phi3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hybridial</td>\n",
       "      <td>0.200245</td>\n",
       "      <td>0.436182</td>\n",
       "      <td>0.493175</td>\n",
       "      <td>chatqa_eval/nvidia_chatqa_outputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hybridial</td>\n",
       "      <td>0.164882</td>\n",
       "      <td>0.311281</td>\n",
       "      <td>0.383197</td>\n",
       "      <td>chatqa_eval/finetuned_phi3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hybridial</td>\n",
       "      <td>0.156439</td>\n",
       "      <td>0.343203</td>\n",
       "      <td>0.377632</td>\n",
       "      <td>chatqa_eval/base_phi3_outputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>doc2dial</td>\n",
       "      <td>0.331686</td>\n",
       "      <td>0.248204</td>\n",
       "      <td>0.260882</td>\n",
       "      <td>chatqa_eval/nvidia_chatqa_outputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc2dial</td>\n",
       "      <td>0.281861</td>\n",
       "      <td>0.208183</td>\n",
       "      <td>0.219401</td>\n",
       "      <td>chatqa_eval/base_phi3_outputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>doc2dial</td>\n",
       "      <td>0.283747</td>\n",
       "      <td>0.204626</td>\n",
       "      <td>0.217881</td>\n",
       "      <td>chatqa_eval/finetuned_phi3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>coqa</td>\n",
       "      <td>0.626215</td>\n",
       "      <td>0.669038</td>\n",
       "      <td>0.713862</td>\n",
       "      <td>chatqa_eval/finetuned_phi3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coqa</td>\n",
       "      <td>0.601433</td>\n",
       "      <td>0.738063</td>\n",
       "      <td>0.713218</td>\n",
       "      <td>chatqa_eval/base_phi3_outputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coqa</td>\n",
       "      <td>0.500611</td>\n",
       "      <td>0.603324</td>\n",
       "      <td>0.590848</td>\n",
       "      <td>chatqa_eval/nvidia_chatqa_outputs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    benchmark  precision    recall        f1  \\\n",
       "9    topiocqa   0.283519  0.288733  0.353500   \n",
       "17   topiocqa   0.270855  0.251446  0.328955   \n",
       "2    topiocqa   0.240453  0.278561  0.319551   \n",
       "14        sqa   0.732992  0.510911  0.518191   \n",
       "21        sqa   0.583139  0.425426  0.429939   \n",
       "6         sqa   0.354951  0.366099  0.294228   \n",
       "12       quac   0.311620  0.268384  0.271534   \n",
       "20       quac   0.199881  0.161826  0.166448   \n",
       "13      qrecc   0.446039  0.281528  0.326638   \n",
       "5       qrecc   0.391230  0.256864  0.293054   \n",
       "8      inscit   0.347971  0.126360  0.196037   \n",
       "1      inscit   0.336776  0.116251  0.187411   \n",
       "16     inscit   0.325252  0.106394  0.169698   \n",
       "10  hybridial   0.200245  0.436182  0.493175   \n",
       "18  hybridial   0.164882  0.311281  0.383197   \n",
       "3   hybridial   0.156439  0.343203  0.377632   \n",
       "11   doc2dial   0.331686  0.248204  0.260882   \n",
       "4    doc2dial   0.281861  0.208183  0.219401   \n",
       "19   doc2dial   0.283747  0.204626  0.217881   \n",
       "15       coqa   0.626215  0.669038  0.713862   \n",
       "0        coqa   0.601433  0.738063  0.713218   \n",
       "7        coqa   0.500611  0.603324  0.590848   \n",
       "\n",
       "                                model  \n",
       "9   chatqa_eval/nvidia_chatqa_outputs  \n",
       "17         chatqa_eval/finetuned_phi3  \n",
       "2       chatqa_eval/base_phi3_outputs  \n",
       "14  chatqa_eval/nvidia_chatqa_outputs  \n",
       "21         chatqa_eval/finetuned_phi3  \n",
       "6       chatqa_eval/base_phi3_outputs  \n",
       "12  chatqa_eval/nvidia_chatqa_outputs  \n",
       "20         chatqa_eval/finetuned_phi3  \n",
       "13  chatqa_eval/nvidia_chatqa_outputs  \n",
       "5       chatqa_eval/base_phi3_outputs  \n",
       "8   chatqa_eval/nvidia_chatqa_outputs  \n",
       "1       chatqa_eval/base_phi3_outputs  \n",
       "16         chatqa_eval/finetuned_phi3  \n",
       "10  chatqa_eval/nvidia_chatqa_outputs  \n",
       "18         chatqa_eval/finetuned_phi3  \n",
       "3       chatqa_eval/base_phi3_outputs  \n",
       "11  chatqa_eval/nvidia_chatqa_outputs  \n",
       "4       chatqa_eval/base_phi3_outputs  \n",
       "19         chatqa_eval/finetuned_phi3  \n",
       "15         chatqa_eval/finetuned_phi3  \n",
       "0       chatqa_eval/base_phi3_outputs  \n",
       "7   chatqa_eval/nvidia_chatqa_outputs  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values([\"benchmark\", \"f1\"], ascending=False)  # .style.highlight_max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Add missing dataset splits\n",
    "- Add Nvidia's template to evaluate their base model more accurately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
