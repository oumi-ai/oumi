# Copyright 2025 - Oumi
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import Any, Optional, Union

import pandas as pd

from oumi.utils.logging import logger


@dataclass
class DataFrameWithSchema:
    """A DataFrame paired with its schema for analysis.

    Attributes:
        dataframe: The pandas DataFrame to analyze
        schema: Column schema defining types and content types
        name: Optional name for tracking/debugging purposes
    """

    dataframe: pd.DataFrame
    schema: dict
    name: Optional[str] = None


@dataclass
class AnalysisResult:
    """Result of DataFrame analysis containing processed DataFrames.

    Attributes:
        dataframes: Dictionary mapping names to processed DataFrames
        merged_df: DataFrame with merged analysis results from all input DataFrames
        merged_schema: Schema for the merged DataFrame, combining schemas from
            all inputs
    """

    dataframes: dict[str, pd.DataFrame]
    merged_df: pd.DataFrame
    merged_schema: dict

    @property
    def conversations_df(self) -> pd.DataFrame:
        """Get the 'conversations' DataFrame."""
        return self.dataframes.get("conversations", pd.DataFrame())

    @property
    def messages_df(self) -> pd.DataFrame:
        """Get the 'messages' DataFrame."""
        return self.dataframes.get("messages", pd.DataFrame())


class DataFrameAnalyzer:
    """Core DataFrame analysis engine.

    This class encapsulates the essence of analysis: applying sample analyzers
    to pandas DataFrames with column configuration.
    """

    def __init__(
        self,
        sample_analyzers: dict[str, Any],
    ):
        """Initialize the DataFrame analyzer.

        Args:
            sample_analyzers: Dictionary of sample analyzers to apply
        """
        self.sample_analyzers = sample_analyzers

    @staticmethod
    def _update_schema_with_generated_columns(
        generated_schema: dict, schema: dict
    ) -> dict:
        """Update schema with columns generated by analyzers.

        Args:
            generated_schema: Dictionary mapping column names to schema configs
                from analyzer.analyze_sample() return value
            schema: Existing schema dictionary

        Returns:
            Updated schema dictionary with generated columns added
        """
        updated_schema = schema.copy()
        updated_schema.update(generated_schema)
        return updated_schema

    @staticmethod
    def _merge_schema_list(schemas: list[dict]) -> dict:
        """Merge a list of schemas into a single schema.

        When multiple schemas define the same column, the first definition takes
        precedence. This is appropriate for merging schemas from DataFrames that
        are being merged.

        Args:
            schemas: List of schema dictionaries to merge

        Returns:
            Merged schema dictionary containing all columns from all schemas
        """
        merged_schema = {}
        for schema in schemas:
            # Update merged schema, but don't overwrite existing keys
            # This means the first schema's definition for a column takes precedence
            for col, config in schema.items():
                if col not in merged_schema:
                    merged_schema[col] = config
        return merged_schema

    def analyze_dataframe(
        self,
        input_data: DataFrameWithSchema,
    ) -> DataFrameWithSchema:
        """Apply analyzers to a DataFrame.

        Args:
            input_data: DataFrameWithSchema containing DataFrame and its schema
        Returns:
            DataFrameWithSchema with analysis results added and updated schema
        """
        if input_data.dataframe.empty:
            return DataFrameWithSchema(
                dataframe=input_data.dataframe.copy(),
                schema=input_data.schema.copy(),
                name=input_data.name,
            )

        result_df = input_data.dataframe.copy()
        all_generated_schema = {}

        for analyzer_id, analyzer in self.sample_analyzers.items():
            try:
                # Run the analyzer - it returns both the DataFrame and generated schema
                result_df, generated_schema = analyzer.analyze_sample(
                    result_df,
                    schema=input_data.schema,
                )
                all_generated_schema.update(generated_schema)
            except Exception as e:
                logger.warning(f"Analyzer {analyzer_id} failed: {e}")

        # Update schema with columns generated by analyzers
        updated_schema = self._update_schema_with_generated_columns(
            all_generated_schema, input_data.schema
        )

        return DataFrameWithSchema(
            dataframe=result_df,
            schema=updated_schema,
            name=input_data.name,
        )

    def analyze_dataframe_list(
        self,
        input_data_list: list[DataFrameWithSchema],
        merge_on: Union[str, list[str]],
        use_parallel: bool = True,
        chunk_size: Optional[int] = None,
        max_workers: Optional[int] = None,
    ) -> AnalysisResult:
        """Apply analyzers to a list of DataFrames with their schemas and merge results.

        This is a general method that can handle any number of DataFrames,
        each with its own schema, analyze each one, and then merge them sequentially.
        Each DataFrame is processed with optional parallel chunk-based processing.

        Args:
            input_data_list: List of DataFrameWithSchema objects to analyze and merge
            merge_on: Column(s) to merge on - can be a string or list of strings
            use_parallel: Whether to use parallel processing for each DataFrame.
                Defaults to True.
            chunk_size: Number of rows per chunk for parallel processing.
                If None, auto-calculates. Only used if use_parallel=True.
            max_workers: Maximum number of parallel workers per DataFrame.
                If None, uses a reasonable default. Only used if use_parallel=True.

        Returns:
            AnalysisResult with processed DataFrames and final merged result
        """
        if not input_data_list:
            return AnalysisResult(
                dataframes={},
                merged_df=pd.DataFrame(),
                merged_schema={},
            )

        # Apply analyzers to all DataFrames using their respective schemas
        # Use parallel processing for better performance on large DataFrames
        processed_results = []
        dataframes_dict = {}

        for input_data in input_data_list:
            if use_parallel:
                processed_result = self.analyze_dataframe_parallel(
                    input_data,
                    chunk_size=chunk_size,
                    max_workers=max_workers,
                    use_parallel=True,
                )
            else:
                processed_result = self.analyze_dataframe(input_data)
            processed_results.append(processed_result)

            # Store in dictionary with name if provided
            if input_data.name:
                dataframes_dict[input_data.name] = processed_result.dataframe

        processed_dataframes = [result.dataframe for result in processed_results]

        # Merge all DataFrames sequentially
        merged_df = self._merge_dataframe_list(processed_dataframes, merge_on)

        # Merge schemas from all processed results
        schemas = [result.schema for result in processed_results]
        merged_schema = self._merge_schema_list(schemas)

        # Filter merged schema to only include columns that actually exist in merged_df
        merged_schema = {
            col: config
            for col, config in merged_schema.items()
            if col in merged_df.columns
        }

        return AnalysisResult(
            dataframes=dataframes_dict,
            merged_df=merged_df,
            merged_schema=merged_schema,
        )

    def _merge_dataframe_list(
        self,
        dataframes: list[pd.DataFrame],
        merge_on: Union[str, list[str]],
    ) -> pd.DataFrame:
        """Merge a list of DataFrames sequentially.

        Args:
            dataframes: List of DataFrames to merge
            merge_on: Column(s) to merge on - can be a string or list of strings
        Returns:
            Final merged DataFrame
        """
        if not dataframes:
            return pd.DataFrame()

        # Filter out empty DataFrames
        non_empty_dfs = [df for df in dataframes if not df.empty]

        if not non_empty_dfs:
            return pd.DataFrame()

        if len(non_empty_dfs) == 1:
            return non_empty_dfs[0].copy()

        # Start with the first DataFrame and merge the rest sequentially
        result_df = non_empty_dfs[0].copy()

        for df in non_empty_dfs[1:]:
            # Normalize merge_on to a list for consistent handling
            merge_columns = [merge_on] if isinstance(merge_on, str) else merge_on

            # Check if all merge columns exist in both DataFrames
            merge_cols_in_result = all(
                col in result_df.columns for col in merge_columns
            )
            merge_cols_in_df = all(col in df.columns for col in merge_columns)

            if merge_cols_in_result and merge_cols_in_df:
                # Merge on the specified column(s)
                result_df = result_df.merge(df, on=merge_columns, how="left")
            else:
                # If merge columns don't exist, just concatenate
                # This handles cases where DataFrames have different structures
                result_df = pd.concat([result_df, df], ignore_index=True)

        return result_df

    @staticmethod
    def combine_analysis_results(
        results: list[DataFrameWithSchema],
        merge_on: Optional[Union[str, list[str]]] = None,
    ) -> DataFrameWithSchema:
        """Combine analysis results from multiple batches/chunks.

        This is useful for parallel processing where different batches are analyzed
        separately and need to be combined.

        Args:
            results: List of DataFrameWithSchema objects from different batches
            merge_on: Optional column(s) to merge on. If None, concatenates instead.

        Returns:
            DataFrameWithSchema with combined DataFrame and merged schema
        """
        if not results:
            return DataFrameWithSchema(
                dataframe=pd.DataFrame(),
                schema={},
            )

        # Filter out empty results
        non_empty_results = [r for r in results if not r.dataframe.empty]

        if not non_empty_results:
            # Return first result's schema even if empty
            return DataFrameWithSchema(
                dataframe=pd.DataFrame(),
                schema=results[0].schema.copy() if results else {},
            )

        if len(non_empty_results) == 1:
            return DataFrameWithSchema(
                dataframe=non_empty_results[0].dataframe.copy(),
                schema=non_empty_results[0].schema.copy(),
                name=non_empty_results[0].name,
            )

        # Combine DataFrames
        if merge_on is not None:
            # Merge on specified columns
            merge_columns = [merge_on] if isinstance(merge_on, str) else merge_on
            combined_df = non_empty_results[0].dataframe.copy()

            for result in non_empty_results[1:]:
                result_df = result.dataframe
                if all(col in combined_df.columns for col in merge_columns) and all(
                    col in result_df.columns for col in merge_columns
                ):
                    combined_df = combined_df.merge(
                        result_df, on=merge_columns, how="outer"
                    )
                else:
                    # Fallback to concatenation if merge columns missing
                    combined_df = pd.concat([combined_df, result_df], ignore_index=True)
        else:
            # Simple concatenation
            combined_df = pd.concat(
                [r.dataframe for r in non_empty_results], ignore_index=True
            )

        # Merge schemas from all results
        schemas = [r.schema for r in non_empty_results]
        merged_schema = DataFrameAnalyzer._merge_schema_list(schemas)

        # Filter schema to only include columns that exist in combined_df
        merged_schema = {
            col: config
            for col, config in merged_schema.items()
            if col in combined_df.columns
        }

        return DataFrameWithSchema(
            dataframe=combined_df,
            schema=merged_schema,
            name=non_empty_results[0].name,  # Use first name if available
        )

    def analyze_dataframe_parallel(
        self,
        input_data: DataFrameWithSchema,
        chunk_size: Optional[int] = None,
        max_workers: Optional[int] = None,
        use_parallel: bool = True,
    ) -> DataFrameWithSchema:
        """Apply analyzers to a DataFrame with optional parallel chunk-based processing.

        This method can process the DataFrame in parallel chunks for better performance
        on large datasets. If the DataFrame is small or parallel processing is disabled,
        it falls back to the standard sequential processing.

        Args:
            input_data: DataFrameWithSchema containing DataFrame and its schema
            chunk_size: Number of rows per chunk. If None, auto-calculates based on
                DataFrame size and number of workers. If DataFrame is small, uses
                sequential processing.
            max_workers: Maximum number of parallel workers. If None, uses a reasonable
                default based on system capabilities.
            use_parallel: Whether to use parallel processing. Set to False to force
                sequential processing.

        Returns:
            DataFrameWithSchema with analysis results added and updated schema
        """
        if input_data.dataframe.empty:
            return DataFrameWithSchema(
                dataframe=input_data.dataframe.copy(),
                schema=input_data.schema.copy(),
                name=input_data.name,
            )

        input_df = input_data.dataframe
        n_rows = len(input_df)

        # Determine if parallel processing is beneficial
        # Use parallel if: enabled, DataFrame is large enough,
        # and we have multiple analyzers
        should_parallelize = (
            use_parallel
            and n_rows > 100  # Minimum rows to benefit from parallelization
            and len(self.sample_analyzers) > 0
        )

        if not should_parallelize:
            # Fall back to standard sequential processing
            return self.analyze_dataframe(input_data)

        # Calculate chunk size if not provided
        if chunk_size is None:
            # Aim for 4-8 chunks per worker, with minimum chunk size of 100
            estimated_workers = max_workers or min(8, len(self.sample_analyzers) * 2)
            chunk_size = max(100, n_rows // (estimated_workers * 4))

        # Calculate number of chunks
        n_chunks = (n_rows + chunk_size - 1) // chunk_size

        # If only one chunk, use sequential processing
        if n_chunks == 1:
            return self.analyze_dataframe(input_data)

        # Split into chunks
        chunks = []
        for i in range(0, n_rows, chunk_size):
            chunk_df = input_df.iloc[i : i + chunk_size].copy()
            chunk_name = (
                f"{input_data.name}_chunk_{i // chunk_size}"
                if input_data.name
                else f"chunk_{i // chunk_size}"
            )
            chunks.append(
                DataFrameWithSchema(
                    dataframe=chunk_df,
                    schema=input_data.schema.copy(),
                    name=chunk_name,
                )
            )

        # Process chunks in parallel
        if max_workers is None:
            max_workers = min(len(chunks), 8)

        results = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(self.analyze_dataframe, chunk): chunk
                for chunk in chunks
            }

            for future in as_completed(futures):
                chunk = futures[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    logger.warning(f"Chunk {chunk.name} failed: {e}")
                    # Add original chunk as fallback (no analysis applied)
                    results.append(
                        DataFrameWithSchema(
                            dataframe=chunk.dataframe.copy(),
                            schema=chunk.schema.copy(),
                            name=chunk.name,
                        )
                    )

        # Combine results using the helper function
        return self.combine_analysis_results(results)
