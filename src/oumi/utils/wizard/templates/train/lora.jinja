{# LoRA fine-tuning template #}
{% include 'base/model.jinja' %}
  
{% include 'base/data.jinja' %}
    
{% include 'base/training.jinja' %}
  
peft:
  enable_peft: true
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]