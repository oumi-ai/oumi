<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Analysis Report: yahma/alpaca-cleaned</title>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Instrument+Serif:ital@0;1&family=JetBrains+Mono:wght@400;500;600&family=Source+Sans+3:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --color-bg: #0f0f0f;
            --color-bg-elevated: #1a1a1a;
            --color-bg-card: #141414;
            --color-bg-hover: #1f1f1f;
            --color-surface: #242424;
            --color-text: #e8e6e3;
            --color-text-secondary: #a39e93;
            --color-text-muted: #6b6660;
            --color-border: #2a2825;
            --color-border-subtle: #1f1e1c;
            --color-accent: #d4a574;
            --color-accent-dim: #a67c52;
            --color-accent-glow: rgba(212, 165, 116, 0.15);
            --color-success: #7cb97c;
            --color-success-dim: rgba(124, 185, 124, 0.12);
            --color-warning: #e0b854;
            --color-warning-dim: rgba(224, 184, 84, 0.12);
            --color-danger: #d66a6a;
            --color-danger-dim: rgba(214, 106, 106, 0.12);
            --color-info: #6b9bd1;
            --color-info-dim: rgba(107, 155, 209, 0.12);
            --font-display: 'Instrument Serif', Georgia, serif;
            --font-body: 'Source Sans 3', -apple-system, sans-serif;
            --font-mono: 'JetBrains Mono', 'SF Mono', monospace;
            --ease-out-expo: cubic-bezier(0.16, 1, 0.3, 1);
            --ease-out-back: cubic-bezier(0.34, 1.56, 0.64, 1);
        }

        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: var(--font-body);
            font-size: 15px;
            line-height: 1.65;
            color: var(--color-text);
            background-color: var(--color-bg);
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        /* Noise texture overlay */
        body::before {
            content: '';
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none;
            opacity: 0.025;
            background-image: url("data:image/svg+xml,%3Csvg viewBox='0 0 256 256' xmlns='http://www.w3.org/2000/svg'%3E%3Cfilter id='noise'%3E%3CfeTurbulence type='fractalNoise' baseFrequency='0.9' numOctaves='4' stitchTiles='stitch'/%3E%3C/filter%3E%3Crect width='100%25' height='100%25' filter='url(%23noise)'/%3E%3C/svg%3E");
            z-index: 9999;
        }

        /* Animated gradient accent line */
        .accent-line {
            height: 2px;
            background: linear-gradient(90deg,
                transparent 0%,
                var(--color-accent) 20%,
                var(--color-accent) 80%,
                transparent 100%);
            opacity: 0.6;
        }

        /* Container */
        .container {
            max-width: 1280px;
            margin: 0 auto;
            padding: 0 32px;
        }

        /* Header */
        header {
            padding: 80px 0 60px;
            position: relative;
            overflow: hidden;
        }

        header::before {
            content: '';
            position: absolute;
            top: -50%;
            left: -20%;
            width: 140%;
            height: 200%;
            background: radial-gradient(ellipse at 30% 20%, var(--color-accent-glow) 0%, transparent 50%);
            pointer-events: none;
            animation: headerGlow 8s ease-in-out infinite alternate;
        }

        @keyframes headerGlow {
            0% { opacity: 0.5; transform: translate(0, 0); }
            100% { opacity: 0.8; transform: translate(5%, 5%); }
        }

        .header-content {
            position: relative;
            z-index: 1;
        }

        .header-badge {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            font-family: var(--font-mono);
            font-size: 11px;
            font-weight: 500;
            letter-spacing: 0.1em;
            text-transform: uppercase;
            color: var(--color-accent);
            background: var(--color-accent-glow);
            border: 1px solid var(--color-accent-dim);
            padding: 6px 14px;
            border-radius: 100px;
            margin-bottom: 24px;
            animation: fadeInUp 0.8s var(--ease-out-expo) both;
        }

        .header-badge::before {
            content: '';
            width: 6px;
            height: 6px;
            background: var(--color-accent);
            border-radius: 50%;
            animation: pulse 2s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% { opacity: 1; transform: scale(1); }
            50% { opacity: 0.5; transform: scale(1.2); }
        }

        header h1 {
            font-family: var(--font-display);
            font-size: clamp(2.5rem, 5vw, 4rem);
            font-weight: 400;
            line-height: 1.1;
            letter-spacing: -0.02em;
            margin-bottom: 16px;
            animation: fadeInUp 0.8s var(--ease-out-expo) 0.1s both;
        }

        header h1 em {
            font-style: italic;
            color: var(--color-accent);
        }

        .header-meta {
            display: flex;
            align-items: center;
            gap: 24px;
            font-family: var(--font-mono);
            font-size: 12px;
            color: var(--color-text-muted);
            animation: fadeInUp 0.8s var(--ease-out-expo) 0.2s both;
        }

        .header-meta span {
            display: flex;
            align-items: center;
            gap: 8px;
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* Health Score Hero */
        

        /* Main Content */
        main {
            padding: 40px 0 80px;
        }

        /* Section */
        .section {
            margin-bottom: 64px;
            animation: fadeInUp 0.8s var(--ease-out-expo) both;
        }

        .section:nth-child(1) { animation-delay: 0.1s; }
        .section:nth-child(2) { animation-delay: 0.2s; }
        .section:nth-child(3) { animation-delay: 0.3s; }
        .section:nth-child(4) { animation-delay: 0.4s; }

        .section-header {
            display: flex;
            align-items: baseline;
            gap: 16px;
            margin-bottom: 32px;
            padding-bottom: 16px;
            border-bottom: 1px solid var(--color-border);
        }

        .section-header h2 {
            font-family: var(--font-display);
            font-size: 1.75rem;
            font-weight: 400;
            letter-spacing: -0.01em;
        }

        .section-header .count {
            font-family: var(--font-mono);
            font-size: 12px;
            color: var(--color-text-muted);
            background: var(--color-surface);
            padding: 4px 10px;
            border-radius: 100px;
        }

        /* Overview Stats */
        .overview-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
            gap: 16px;
        }

        .stat-card {
            background: var(--color-bg-card);
            border: 1px solid var(--color-border-subtle);
            border-radius: 12px;
            padding: 24px;
            position: relative;
            overflow: hidden;
            transition: all 0.4s var(--ease-out-expo);
        }

        .stat-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 2px;
            background: linear-gradient(90deg, var(--color-accent-dim), var(--color-accent));
            opacity: 0;
            transition: opacity 0.3s ease;
        }

        .stat-card:hover {
            border-color: var(--color-border);
            transform: translateY(-2px);
        }

        .stat-card:hover::before {
            opacity: 1;
        }

        .stat-card .label {
            font-size: 12px;
            font-weight: 500;
            text-transform: uppercase;
            letter-spacing: 0.08em;
            color: var(--color-text-muted);
            margin-bottom: 8px;
        }

        .stat-card .value {
            font-family: var(--font-mono);
            font-size: 28px;
            font-weight: 600;
            letter-spacing: -0.02em;
            color: var(--color-text);
        }

        .stat-card .value.small {
            font-size: 14px;
            font-weight: 500;
            color: var(--color-text-secondary);
        }

        .stat-card .value .unit {
            font-size: 16px;
            font-weight: 400;
            color: var(--color-text-muted);
        }

        /* Recommendations */
        .recommendations-list {
            display: flex;
            flex-direction: column;
            gap: 12px;
        }

        .recommendation {
            display: grid;
            grid-template-columns: 4px 1fr;
            gap: 0;
            background: var(--color-bg-card);
            border: 1px solid var(--color-border-subtle);
            border-radius: 12px;
            overflow: hidden;
            transition: all 0.3s var(--ease-out-expo);
        }

        .recommendation:hover {
            border-color: var(--color-border);
            transform: translateX(4px);
        }

        .recommendation .severity-bar {
            border-radius: 0;
        }

        .recommendation.high .severity-bar { background: var(--color-danger); }
        .recommendation.medium .severity-bar { background: var(--color-warning); }
        .recommendation.low .severity-bar { background: var(--color-info); }

        .recommendation-content {
            padding: 20px 24px;
        }

        .recommendation-header {
            display: flex;
            align-items: center;
            gap: 12px;
            margin-bottom: 8px;
        }

        .recommendation .badge {
            font-family: var(--font-mono);
            font-size: 10px;
            font-weight: 600;
            letter-spacing: 0.05em;
            text-transform: uppercase;
            padding: 4px 10px;
            border-radius: 4px;
        }

        .recommendation.high .badge {
            background: var(--color-danger-dim);
            color: var(--color-danger);
        }

        .recommendation.medium .badge {
            background: var(--color-warning-dim);
            color: var(--color-warning);
        }

        .recommendation.low .badge {
            background: var(--color-info-dim);
            color: var(--color-info);
        }

        .recommendation .title {
            font-weight: 600;
            font-size: 15px;
        }

        .recommendation .description {
            color: var(--color-text-secondary);
            font-size: 14px;
            line-height: 1.6;
            margin-bottom: 12px;
        }

        .recommendation .meta {
            display: flex;
            gap: 20px;
            font-family: var(--font-mono);
            font-size: 11px;
            color: var(--color-text-muted);
        }

        .recommendation .meta span {
            display: flex;
            align-items: center;
            gap: 6px;
        }

        /* Empty State */
        .empty-state {
            text-align: center;
            padding: 80px 40px;
            background: var(--color-bg-card);
            border: 1px dashed var(--color-border);
            border-radius: 16px;
        }

        .empty-state .icon {
            width: 64px;
            height: 64px;
            margin: 0 auto 20px;
            background: var(--color-success-dim);
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .empty-state .icon svg {
            width: 32px;
            height: 32px;
            stroke: var(--color-success);
        }

        .empty-state h3 {
            font-family: var(--font-display);
            font-size: 1.5rem;
            margin-bottom: 8px;
        }

        .empty-state p {
            color: var(--color-text-muted);
        }

        /* Charts */
        .chart-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(480px, 1fr));
            gap: 24px;
        }

        .chart-card {
            background: var(--color-bg-card);
            border: 1px solid var(--color-border-subtle);
            border-radius: 12px;
            padding: 24px;
            transition: border-color 0.3s ease;
        }

        .chart-card:hover {
            border-color: var(--color-border);
        }

        .chart-card h3 {
            font-family: var(--font-body);
            font-size: 13px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.06em;
            color: var(--color-text-muted);
            margin-bottom: 16px;
        }

        .chart-wrapper {
            border-radius: 8px;
            overflow: hidden;
        }

        /* Anomaly Chart Card */
        .anomaly-card {
            position: relative;
        }

        .anomaly-card .anomaly-badge {
            position: absolute;
            top: 20px;
            right: 20px;
            font-family: var(--font-mono);
            font-size: 11px;
            font-weight: 500;
            background: var(--color-danger-dim);
            color: var(--color-danger);
            padding: 4px 10px;
            border-radius: 100px;
        }

        /* Tables */
        .table-wrapper {
            background: var(--color-bg-card);
            border: 1px solid var(--color-border-subtle);
            border-radius: 12px;
            overflow: hidden;
        }

        table {
            width: 100%;
            border-collapse: collapse;
        }

        th, td {
            padding: 14px 20px;
            text-align: left;
        }

        th {
            font-family: var(--font-mono);
            font-size: 11px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.08em;
            color: var(--color-text-muted);
            background: var(--color-surface);
            border-bottom: 1px solid var(--color-border);
        }

        td {
            font-family: var(--font-mono);
            font-size: 13px;
            border-bottom: 1px solid var(--color-border-subtle);
            transition: background 0.2s ease;
        }

        tr:last-child td {
            border-bottom: none;
        }

        tbody tr:hover td {
            background: var(--color-bg-hover);
        }

        td:first-child {
            font-family: var(--font-body);
            font-weight: 500;
            color: var(--color-text-secondary);
        }

        td:not(:first-child) {
            color: var(--color-text);
        }

        /* Footer */
        footer {
            padding: 40px 0;
            border-top: 1px solid var(--color-border);
        }

        .footer-content {
            display: flex;
            align-items: center;
            justify-content: space-between;
        }

        .footer-brand {
            display: flex;
            align-items: center;
            gap: 12px;
        }

        .footer-brand svg {
            width: 28px;
            height: 28px;
            fill: var(--color-accent);
        }

        .footer-brand span {
            font-family: var(--font-display);
            font-size: 18px;
            color: var(--color-text-secondary);
        }

        .footer-meta {
            font-family: var(--font-mono);
            font-size: 11px;
            color: var(--color-text-muted);
        }

        /* Responsive */
        @media (max-width: 900px) {
            .container {
                padding: 0 20px;
            }

            header {
                padding: 60px 0 40px;
            }

            

            .chart-grid {
                grid-template-columns: 1fr;
            }

            .header-meta {
                flex-direction: column;
                align-items: flex-start;
                gap: 8px;
            }

            .footer-content {
                flex-direction: column;
                gap: 16px;
                text-align: center;
            }
        }

        /* View Samples Button */
        .view-samples-btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            font-family: var(--font-mono);
            font-size: 11px;
            font-weight: 500;
            color: var(--color-accent);
            background: var(--color-accent-glow);
            border: 1px solid var(--color-accent-dim);
            padding: 4px 12px;
            border-radius: 100px;
            cursor: pointer;
            transition: all 0.2s ease;
        }

        .view-samples-btn:hover {
            background: var(--color-accent-dim);
            color: var(--color-text);
        }

        /* Sample Modal */
        .sample-modal-overlay {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: rgba(0, 0, 0, 0.8);
            backdrop-filter: blur(4px);
            z-index: 10000;
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s var(--ease-out-expo);
        }

        .sample-modal-overlay.active {
            opacity: 1;
            visibility: visible;
        }

        .sample-modal {
            position: fixed;
            top: 0;
            right: 0;
            bottom: 0;
            width: 600px;
            max-width: 90vw;
            background: var(--color-bg-elevated);
            border-left: 1px solid var(--color-border);
            z-index: 10001;
            transform: translateX(100%);
            transition: transform 0.4s var(--ease-out-expo);
            display: flex;
            flex-direction: column;
        }

        .sample-modal-overlay.active .sample-modal {
            transform: translateX(0);
        }

        .sample-modal-header {
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 20px 24px;
            border-bottom: 1px solid var(--color-border);
            flex-shrink: 0;
        }

        .sample-modal-header h3 {
            font-family: var(--font-display);
            font-size: 1.25rem;
            font-weight: 400;
        }

        .sample-modal-close {
            width: 36px;
            height: 36px;
            display: flex;
            align-items: center;
            justify-content: center;
            background: var(--color-surface);
            border: 1px solid var(--color-border-subtle);
            border-radius: 8px;
            color: var(--color-text-muted);
            cursor: pointer;
            transition: all 0.2s ease;
        }

        .sample-modal-close:hover {
            background: var(--color-bg-hover);
            color: var(--color-text);
        }

        .sample-modal-body {
            flex: 1;
            overflow-y: auto;
            padding: 0;
        }

        /* Conversation List */
        .conversation-list {
            display: flex;
            flex-direction: column;
        }

        .conversation-item {
            border-bottom: 1px solid var(--color-border);
        }

        .conversation-item:last-child {
            border-bottom: none;
        }

        .conversation-header {
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 16px 24px;
            background: var(--color-surface);
            cursor: pointer;
            transition: background 0.2s ease;
        }

        .conversation-header:hover {
            background: var(--color-bg-hover);
        }

        .conversation-header-left {
            display: flex;
            align-items: center;
            gap: 12px;
        }

        .conversation-id {
            font-family: var(--font-mono);
            font-size: 12px;
            font-weight: 600;
            color: var(--color-text);
        }

        .conversation-stats {
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .conversation-stat {
            font-family: var(--font-mono);
            font-size: 11px;
            padding: 3px 8px;
            border-radius: 4px;
            background: var(--color-bg-card);
            color: var(--color-text-muted);
        }

        .conversation-stat.flagged {
            background: var(--color-danger-dim);
            color: var(--color-danger);
        }

        .conversation-toggle {
            width: 24px;
            height: 24px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: var(--color-text-muted);
            transition: transform 0.2s ease;
        }

        .conversation-item.expanded .conversation-toggle {
            transform: rotate(180deg);
        }

        .conversation-turns {
            display: none;
            padding: 0 24px 20px;
        }

        .conversation-item.expanded .conversation-turns {
            display: block;
        }

        /* Conversation Turn (Message) */
        .turn {
            display: flex;
            gap: 12px;
            padding: 12px 0;
            border-bottom: 1px solid var(--color-border-subtle);
        }

        .turn:last-child {
            border-bottom: none;
        }

        .turn.flagged {
            background: var(--color-danger-dim);
            margin: 0 -16px;
            padding: 12px 16px;
            border-radius: 8px;
            border: 1px solid rgba(214, 106, 106, 0.3);
        }

        .turn-role {
            flex-shrink: 0;
            width: 80px;
        }

        .turn-role-badge {
            font-family: var(--font-mono);
            font-size: 10px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            padding: 4px 8px;
            border-radius: 4px;
            display: inline-block;
        }

        .turn-role-badge.user {
            background: var(--color-info-dim);
            color: var(--color-info);
        }

        .turn-role-badge.assistant {
            background: var(--color-success-dim);
            color: var(--color-success);
        }

        .turn-role-badge.system {
            background: var(--color-warning-dim);
            color: var(--color-warning);
        }

        .turn-content-wrapper {
            flex: 1;
            min-width: 0;
        }

        .turn-content {
            font-size: 13px;
            line-height: 1.6;
            color: var(--color-text-secondary);
            white-space: pre-wrap;
            word-break: break-word;
            max-height: 150px;
            overflow-y: auto;
        }

        .turn.flagged .turn-content {
            color: var(--color-text);
        }

        .turn-content.expanded {
            max-height: none;
        }

        .turn-meta {
            display: flex;
            align-items: center;
            gap: 12px;
            margin-top: 8px;
        }

        .turn-metric {
            font-family: var(--font-mono);
            font-size: 11px;
            color: var(--color-danger);
            background: var(--color-bg-card);
            padding: 2px 8px;
            border-radius: 4px;
        }

        .turn-expand-btn {
            display: inline-flex;
            align-items: center;
            gap: 4px;
            font-family: var(--font-mono);
            font-size: 11px;
            color: var(--color-text-muted);
            background: none;
            border: none;
            cursor: pointer;
            padding: 0;
            transition: color 0.2s ease;
        }

        .turn-expand-btn:hover {
            color: var(--color-accent);
        }

        .flagged-indicator {
            font-family: var(--font-mono);
            font-size: 10px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--color-danger);
            display: flex;
            align-items: center;
            gap: 4px;
        }

        .flagged-indicator svg {
            width: 12px;
            height: 12px;
        }

        .sample-modal-footer {
            padding: 16px 24px;
            border-top: 1px solid var(--color-border);
            background: var(--color-bg-card);
            flex-shrink: 0;
        }

        .sample-modal-footer p {
            font-size: 12px;
            color: var(--color-text-muted);
            text-align: center;
        }

        /* Print styles */
        @media print {
            body::before {
                display: none;
            }

            header::before {
                display: none;
            }

            .recommendation:hover,
            .stat-card:hover,
            .chart-card:hover {
                transform: none;
            }

            .view-samples-btn,
            .sample-modal-overlay {
                display: none !important;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <div class="header-content">
                <div class="header-badge">Dataset Analysis</div>
                <h1>yahma/alpaca-cleaned</h1>
                <div class="header-meta">
                    <span>
                        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <circle cx="12" cy="12" r="10"></circle>
                            <polyline points="12,6 12,12 16,14"></polyline>
                        </svg>
                        2025-12-07 10:16:53
                    </span>
                    <span>
                        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                        </svg>
                        10000 conversations
                    </span>
                    <span>
                        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <line x1="4" y1="9" x2="20" y2="9"></line>
                            <line x1="4" y1="15" x2="20" y2="15"></line>
                            <line x1="10" y1="3" x2="8" y2="21"></line>
                            <line x1="16" y1="3" x2="14" y2="21"></line>
                        </svg>
                        3000 messages
                    </span>
                </div>
            </div>
        </div>
    </header>

    <div class="accent-line"></div>

    

    <main>
        <div class="container">
            <!-- Dataset Overview -->
            <section class="section">
                <div class="section-header">
                    <h2>Overview</h2>
                </div>
                <div class="overview-grid">
                    <div class="stat-card">
                        <div class="label">Dataset</div>
                        <div class="value small">yahma/alpaca-cleaned</div>
                    </div>
                    <div class="stat-card">
                        <div class="label">Conversations</div>
                        <div class="value">10000</div>
                    </div>
                    <div class="stat-card">
                        <div class="label">Analyzed</div>
                        <div class="value">1000</div>
                    </div>
                    <div class="stat-card">
                        <div class="label">Coverage</div>
                        <div class="value">10.0<span class="unit">%</span></div>
                    </div>
                    <div class="stat-card">
                        <div class="label">Messages</div>
                        <div class="value">3000</div>
                    </div>
                    <div class="stat-card">
                        <div class="label">Analyzers</div>
                        <div class="value small">length, diversity, format, quality, training_quality, cost</div>
                    </div>
                </div>
            </section>

            <!-- Recommendations -->
            <section class="section">
                <div class="section-header">
                    <h2>Recommendations</h2>
                    
                    <span class="count">16 issues</span>
                    
                </div>
                
                <div class="recommendations-list">
                    
                    <div class="recommendation medium">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">medium</span>
                                <span class="title">Outliers detected in length word count</span>
                            </div>
                            <div class="description">Found 96 samples (3.2%) with values outside 3.0 standard deviations from the mean. High outliers: 96, Low outliers: 0. Consider reviewing these samples for potential data quality issues.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    96 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content_length_word_count
                                </span>
                                
                                
                                <button class="view-samples-btn" onclick="openSampleModal('rec_0')">
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                                    </svg>
                                    View 20 of 96 conversations
                                </button>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="recommendation medium">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">medium</span>
                                <span class="title">Outliers detected in length token count</span>
                            </div>
                            <div class="description">Found 97 samples (3.2%) with values outside 3.0 standard deviations from the mean. High outliers: 97, Low outliers: 0. Consider reviewing these samples for potential data quality issues.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    97 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content_length_token_count
                                </span>
                                
                                
                                <button class="view-samples-btn" onclick="openSampleModal('rec_1')">
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                                    </svg>
                                    View 20 of 97 conversations
                                </button>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="recommendation medium">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">medium</span>
                                <span class="title">Outliers detected in diversity vocabulary richness</span>
                            </div>
                            <div class="description">Found 39 samples (1.3%) with values outside 3.0 standard deviations from the mean. High outliers: 39, Low outliers: 0. Consider reviewing these samples for potential data quality issues.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    39 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content_diversity_vocabulary_richness
                                </span>
                                
                                
                                <button class="view-samples-btn" onclick="openSampleModal('rec_2')">
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                                    </svg>
                                    View 20 of 39 conversations
                                </button>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="recommendation medium">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">medium</span>
                                <span class="title">Outliers detected in training quality instruction word count</span>
                            </div>
                            <div class="description">Found 17 samples (1.7%) with values outside 3.0 standard deviations from the mean. High outliers: 17, Low outliers: 0. Consider reviewing these samples for potential data quality issues.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    17 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content_training_quality_instruction_word_count
                                </span>
                                
                                
                                <button class="view-samples-btn" onclick="openSampleModal('rec_3')">
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                                    </svg>
                                    View 17 conversations
                                </button>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="recommendation medium">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">medium</span>
                                <span class="title">Inconsistent instruction formatting detected</span>
                            </div>
                            <div class="description">Found multiple instruction format patterns in the dataset: alpaca: 410, vicuna: 3. Mixing formats may confuse the model and reduce training effectiveness. Consider standardizing to a single format.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    413 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content
                                </span>
                                
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="recommendation medium">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">medium</span>
                                <span class="title">Truncated responses detected</span>
                            </div>
                            <div class="description">Found 84 assistant responses (8.4%) that appear to be truncated (ending mid-sentence or with incomplete punctuation). Training on truncated responses may cause the model to generate incomplete outputs. Consider completing or removing these samples.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    84 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content_training_quality_has_proper_ending
                                </span>
                                
                                
                                <button class="view-samples-btn" onclick="openSampleModal('rec_5')">
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                                    </svg>
                                    View 20 of 84 conversations
                                </button>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="recommendation low">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">low</span>
                                <span class="title">Outliers detected in diversity unique words ratio</span>
                            </div>
                            <div class="description">Found 12 samples (0.4%) with values outside 3.0 standard deviations from the mean. High outliers: 0, Low outliers: 12. Consider reviewing these samples for potential data quality issues.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    12 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content_diversity_unique_words_ratio
                                </span>
                                
                                
                                <button class="view-samples-btn" onclick="openSampleModal('rec_6')">
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                                    </svg>
                                    View 12 conversations
                                </button>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="recommendation low">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">low</span>
                                <span class="title">Outliers detected in diversity type token ratio</span>
                            </div>
                            <div class="description">Found 12 samples (0.4%) with values outside 3.0 standard deviations from the mean. High outliers: 0, Low outliers: 12. Consider reviewing these samples for potential data quality issues.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    12 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content_diversity_type_token_ratio
                                </span>
                                
                                
                                <button class="view-samples-btn" onclick="openSampleModal('rec_7')">
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                                    </svg>
                                    View 12 conversations
                                </button>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="recommendation low">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">low</span>
                                <span class="title">Outliers detected in format code block count</span>
                            </div>
                            <div class="description">Found 22 samples (0.7%) with values outside 3.0 standard deviations from the mean. High outliers: 22, Low outliers: 0. Consider reviewing these samples for potential data quality issues.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    22 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content_format_code_block_count
                                </span>
                                
                                
                                <button class="view-samples-btn" onclick="openSampleModal('rec_8')">
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                                    </svg>
                                    View 20 of 22 conversations
                                </button>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="recommendation low">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">low</span>
                                <span class="title">Outliers detected in format format complexity score</span>
                            </div>
                            <div class="description">Found 30 samples (1.0%) with values outside 3.0 standard deviations from the mean. High outliers: 30, Low outliers: 0. Consider reviewing these samples for potential data quality issues.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    30 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content_format_format_complexity_score
                                </span>
                                
                                
                                <button class="view-samples-btn" onclick="openSampleModal('rec_9')">
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                                    </svg>
                                    View 17 of 30 conversations
                                </button>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="recommendation low">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">low</span>
                                <span class="title">Outliers detected in quality pii count</span>
                            </div>
                            <div class="description">Found 1 samples (0.0%) with values outside 3.0 standard deviations from the mean. High outliers: 1, Low outliers: 0. Consider reviewing these samples for potential data quality issues.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    1 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content_quality_pii_count
                                </span>
                                
                                
                                <button class="view-samples-btn" onclick="openSampleModal('rec_10')">
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                                    </svg>
                                    View 1 conversation
                                </button>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="recommendation low">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">low</span>
                                <span class="title">Outliers detected in training quality response word count</span>
                            </div>
                            <div class="description">Found 3 samples (0.3%) with values outside 3.0 standard deviations from the mean. High outliers: 3, Low outliers: 0. Consider reviewing these samples for potential data quality issues.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    3 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content_training_quality_response_word_count
                                </span>
                                
                                
                                <button class="view-samples-btn" onclick="openSampleModal('rec_11')">
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                                    </svg>
                                    View 3 conversations
                                </button>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="recommendation low">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">low</span>
                                <span class="title">Empty or near-empty messages detected</span>
                            </div>
                            <div class="description">Found 6 messages (0.2%) with 5 or fewer characters. These may indicate data quality issues or placeholder content that should be reviewed.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    6 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content
                                </span>
                                
                                
                                <button class="view-samples-btn" onclick="openSampleModal('rec_12')">
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                                    </svg>
                                    View 6 conversations
                                </button>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="recommendation low">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">low</span>
                                <span class="title">Many short messages detected</span>
                            </div>
                            <div class="description">Found 471 messages (15.7%) with fewer than 10 words. This may be intentional (e.g., short responses) or indicate low-quality samples worth reviewing.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    471 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content_length_word_count
                                </span>
                                
                                
                                <button class="view-samples-btn" onclick="openSampleModal('rec_13')">
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                                    </svg>
                                    View 19 of 471 conversations
                                </button>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="recommendation low">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">low</span>
                                <span class="title">Highly repetitive content detected</span>
                            </div>
                            <div class="description">Found 2 messages (0.1%) with high repetition ratios. Repetitive content may cause the model to learn repetitive patterns. Consider reviewing or filtering these samples.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    2 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content_quality_has_high_repetition
                                </span>
                                
                                
                                <button class="view-samples-btn" onclick="openSampleModal('rec_14')">
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                                    </svg>
                                    View 2 conversations
                                </button>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="recommendation low">
                        <div class="severity-bar"></div>
                        <div class="recommendation-content">
                            <div class="recommendation-header">
                                <span class="badge">low</span>
                                <span class="title">Incomplete responses detected</span>
                            </div>
                            <div class="description">Found 29 assistant responses (2.9%) with low completeness scores (below 0.5). Average completeness score: 0.95. Incomplete responses may teach the model to generate truncated or minimal outputs. Consider expanding short responses or removing low-quality samples.</div>
                            <div class="meta">
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
                                        <circle cx="9" cy="7" r="4"></circle>
                                        <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
                                        <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
                                    </svg>
                                    29 samples
                                </span>
                                
                                <span>
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <polyline points="22,12 18,12 15,21 9,3 6,12 2,12"></polyline>
                                    </svg>
                                    text_content_training_quality_response_completeness_score
                                </span>
                                
                                
                                <button class="view-samples-btn" onclick="openSampleModal('rec_15')">
                                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                                    </svg>
                                    View 20 of 29 conversations
                                </button>
                                
                            </div>
                        </div>
                    </div>
                    
                </div>
                
            </section>

            <!-- Charts Section -->
            
            <section class="section">
                <div class="section-header">
                    <h2>Distributions</h2>
                    <span class="count">10 charts</span>
                </div>
                <div class="chart-grid">
                    
                    <div class="chart-card">
                        <h3>Length Word Count</h3>
                        <div class="chart-wrapper" id="chart_0"></div>
                    </div>
                    
                    <div class="chart-card">
                        <h3>Length Token Count</h3>
                        <div class="chart-wrapper" id="chart_1"></div>
                    </div>
                    
                    <div class="chart-card">
                        <h3>Diversity Unique Words Ratio</h3>
                        <div class="chart-wrapper" id="chart_2"></div>
                    </div>
                    
                    <div class="chart-card">
                        <h3>Diversity Type Token Ratio</h3>
                        <div class="chart-wrapper" id="chart_3"></div>
                    </div>
                    
                    <div class="chart-card">
                        <h3>Diversity Vocabulary Richness</h3>
                        <div class="chart-wrapper" id="chart_4"></div>
                    </div>
                    
                    <div class="chart-card">
                        <h3>Format Code Block Count</h3>
                        <div class="chart-wrapper" id="chart_5"></div>
                    </div>
                    
                    <div class="chart-card">
                        <h3>Format Format Complexity Score</h3>
                        <div class="chart-wrapper" id="chart_6"></div>
                    </div>
                    
                    <div class="chart-card">
                        <h3>Quality Pii Count</h3>
                        <div class="chart-wrapper" id="chart_7"></div>
                    </div>
                    
                    <div class="chart-card">
                        <h3>Quality Repetition Ratio</h3>
                        <div class="chart-wrapper" id="chart_8"></div>
                    </div>
                    
                    <div class="chart-card">
                        <h3>Quality Quality Score</h3>
                        <div class="chart-wrapper" id="chart_9"></div>
                    </div>
                    
                </div>
            </section>
            

            <!-- Anomaly Charts Section -->
            
            <section class="section">
                <div class="section-header">
                    <h2>Anomaly Detection</h2>
                    <span class="count">5 visualizations</span>
                </div>
                <div class="chart-grid">
                    
                    <div class="chart-card anomaly-card">
                        <h3>Outliers in Length Word Count</h3>
                        <span class="anomaly-badge">96 outliers</span>
                        <div class="chart-wrapper" id="anomaly_chart_0"></div>
                    </div>
                    
                    <div class="chart-card anomaly-card">
                        <h3>Outliers in Length Token Count</h3>
                        <span class="anomaly-badge">97 outliers</span>
                        <div class="chart-wrapper" id="anomaly_chart_1"></div>
                    </div>
                    
                    <div class="chart-card anomaly-card">
                        <h3>Outliers in Diversity Unique Words Ratio</h3>
                        <span class="anomaly-badge">12 outliers</span>
                        <div class="chart-wrapper" id="anomaly_chart_2"></div>
                    </div>
                    
                    <div class="chart-card anomaly-card">
                        <h3>Outliers in Diversity Type Token Ratio</h3>
                        <span class="anomaly-badge">12 outliers</span>
                        <div class="chart-wrapper" id="anomaly_chart_3"></div>
                    </div>
                    
                    <div class="chart-card anomaly-card">
                        <h3>Outliers in Diversity Vocabulary Richness</h3>
                        <span class="anomaly-badge">39 outliers</span>
                        <div class="chart-wrapper" id="anomaly_chart_4"></div>
                    </div>
                    
                </div>
            </section>
            

            <!-- Message-Level Statistics -->
            
            <section class="section">
                <div class="section-header">
                    <h2>Message Statistics</h2>
                </div>
                <div class="table-wrapper">
                    <table>
                        <thead>
                            <tr>
                                <th>Metric</th>
                                <th>Mean</th>
                                <th>Std</th>
                                <th>Min</th>
                                <th>Max</th>
                                <th>Median</th>
                            </tr>
                        </thead>
                        <tbody>
                            
                            
                            <tr>
                                <td>text_content_word_count</td>
                                <td>46.27</td>
                                <td>71.62</td>
                                <td>1.0</td>
                                <td>421.0</td>
                                <td>16.0</td>
                            </tr>
                            
                            <tr>
                                <td>text_content_token_count</td>
                                <td>56.89</td>
                                <td>88.14</td>
                                <td>1.0</td>
                                <td>503.0</td>
                                <td>20.0</td>
                            </tr>
                            
                            
                            
                            <tr>
                                <td>text_content_words_ratio</td>
                                <td>0.85</td>
                                <td>0.13</td>
                                <td>0.35</td>
                                <td>1.0</td>
                                <td>0.88</td>
                            </tr>
                            
                            
                            
                            <tr>
                                <td>text_content_token_ratio</td>
                                <td>0.85</td>
                                <td>0.13</td>
                                <td>0.35</td>
                                <td>1.0</td>
                                <td>0.88</td>
                            </tr>
                            
                            
                            
                            <tr>
                                <td>text_content_vocabulary_richness</td>
                                <td>4.36</td>
                                <td>1.92</td>
                                <td>1.0</td>
                                <td>11.61</td>
                                <td>3.64</td>
                            </tr>
                            
                            
                            
                            <tr>
                                <td>text_content_block_count</td>
                                <td>0.01</td>
                                <td>0.1</td>
                                <td>0.0</td>
                                <td>2.0</td>
                                <td>0.0</td>
                            </tr>
                            
                            
                            
                            <tr>
                                <td>text_content_complexity_score</td>
                                <td>0.05</td>
                                <td>0.09</td>
                                <td>0.0</td>
                                <td>0.7</td>
                                <td>0.0</td>
                            </tr>
                            
                            
                            
                            <tr>
                                <td>text_content_pii_count</td>
                                <td>0.0</td>
                                <td>0.04</td>
                                <td>0.0</td>
                                <td>2.0</td>
                                <td>0.0</td>
                            </tr>
                            
                            <tr>
                                <td>text_content_repetition_ratio</td>
                                <td>0.01</td>
                                <td>0.03</td>
                                <td>0.0</td>
                                <td>0.33</td>
                                <td>0.0</td>
                            </tr>
                            
                            <tr>
                                <td>text_content_quality_score</td>
                                <td>1.0</td>
                                <td>0.01</td>
                                <td>0.7</td>
                                <td>1.0</td>
                                <td>1.0</td>
                            </tr>
                            
                            
                            
                            <tr>
                                <td>text_content_clarity_score</td>
                                <td>0.96</td>
                                <td>0.07</td>
                                <td>0.5</td>
                                <td>1.0</td>
                                <td>1.0</td>
                            </tr>
                            
                            
                            
                            <tr>
                                <td>text_content_quality_instruction_word_count</td>
                                <td>18.53</td>
                                <td>27.09</td>
                                <td>4.0</td>
                                <td>323.0</td>
                                <td>12.0</td>
                            </tr>
                            
                            <tr>
                                <td>text_content_quality_response_word_count</td>
                                <td>101.01</td>
                                <td>100.74</td>
                                <td>1.0</td>
                                <td>421.0</td>
                                <td>64.5</td>
                            </tr>
                            
                            
                            
                            <tr>
                                <td>text_content_completeness_score</td>
                                <td>0.95</td>
                                <td>0.18</td>
                                <td>0.0</td>
                                <td>1.0</td>
                                <td>1.0</td>
                            </tr>
                            
                            
                            
                            <tr>
                                <td>text_content_quality_score</td>
                                <td>1.0</td>
                                <td>0.0</td>
                                <td>0.8</td>
                                <td>1.0</td>
                                <td>1.0</td>
                            </tr>
                            
                            
                        </tbody>
                    </table>
                </div>
            </section>
            

            <!-- Conversation-Level Statistics -->
            

            <!-- Conversation Turns -->
            
            <section class="section">
                <div class="section-header">
                    <h2>Conversation Turns</h2>
                </div>
                <div class="table-wrapper">
                    <table>
                        <thead>
                            <tr>
                                <th>Statistic</th>
                                <th>Value</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>Count</td><td>1000</td></tr>
                            <tr><td>Mean</td><td>3.0</td></tr>
                            <tr><td>Std</td><td>0.0</td></tr>
                            <tr><td>Min</td><td>3</td></tr>
                            <tr><td>Max</td><td>3</td></tr>
                            <tr><td>Median</td><td>3.0</td></tr>
                        </tbody>
                    </table>
                </div>
            </section>
            
        </div>
    </main>

    <!-- Conversation Modal -->
    
    <div id="sampleModal" class="sample-modal-overlay" onclick="closeSampleModal(event)">
        <div class="sample-modal" onclick="event.stopPropagation()">
            <div class="sample-modal-header">
                <h3 id="sampleModalTitle">Affected Conversations</h3>
                <button class="sample-modal-close" onclick="closeSampleModal()">
                    <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <line x1="18" y1="6" x2="6" y2="18"></line>
                        <line x1="6" y1="6" x2="18" y2="18"></line>
                    </svg>
                </button>
            </div>
            <div class="sample-modal-body" id="sampleModalBody">
                <!-- Conversations will be inserted here -->
            </div>
            <div class="sample-modal-footer">
                <p>Showing up to 20 conversations with flagged messages highlighted.</p>
            </div>
        </div>
    </div>
    

    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <svg viewBox="0 0 24 24">
                        <path d="M12 2L2 7l10 5 10-5-10-5zM2 17l10 5 10-5M2 12l10 5 10-5"/>
                    </svg>
                    <span>Oumi Dataset Analyzer</span>
                </div>
                <div class="footer-meta">
                    Generated 2025-12-07 10:16:53
                </div>
            </div>
        </div>
    </footer>

    <!-- Render Plotly Charts -->
    
    <script>
        const plotlyConfig = {
            responsive: true,
            displayModeBar: false
        };

        const darkLayout = {
            paper_bgcolor: 'rgba(0,0,0,0)',
            plot_bgcolor: 'rgba(0,0,0,0)',
            font: {
                family: 'Source Sans 3, sans-serif',
                color: '#a39e93'
            },
            xaxis: {
                gridcolor: 'rgba(255,255,255,0.05)',
                linecolor: 'rgba(255,255,255,0.1)',
                tickfont: { family: 'JetBrains Mono, monospace', size: 11 }
            },
            yaxis: {
                gridcolor: 'rgba(255,255,255,0.05)',
                linecolor: 'rgba(255,255,255,0.1)',
                tickfont: { family: 'JetBrains Mono, monospace', size: 11 }
            },
            margin: { l: 50, r: 20, t: 20, b: 50 }
        };

        
        (function() {
            const data = [{"marker": {"color": "#d4a574"}, "nbinsx": 30, "opacity": 0.85, "x": [16, 6, 121, 16, 6, 53, 16, 6, 209, 16, 6, 216, 16, 21, 133, 16, 125, 67, 24, 12, 124, 16, 19, 267, 24, 18, 21, 16, 5, 115, 16, 6, 7, 16, 14, 124, 16, 7, 287, 24, 15, 8, 16, 10, 169, 16, 7, 172, 24, 23, 66, 16, 10, 208, 16, 10, 11, 16, 5, 26, 16, 11, 365, 16, 6, 28, 24, 32, 16, 16, 7, 80, 16, 6, 132, 16, 7, 17, 16, 9, 304, 24, 14, 58, 16, 11, 25, 16, 8, 253, 24, 17, 6, 16, 11, 30, 16, 5, 291, 16, 10, 297, 24, 20, 5, 24, 26, 37, 16, 27, 42, 24, 60, 21, 16, 126, 21, 16, 6, 121, 16, 12, 39, 16, 7, 304, 24, 17, 102, 16, 15, 108, 24, 16, 106, 16, 9, 113, 16, 7, 261, 24, 64, 48, 24, 43, 39, 24, 31, 14, 16, 6, 78, 16, 126, 7, 16, 34, 24, 16, 15, 76, 16, 14, 73, 16, 7, 84, 24, 17, 13, 16, 6, 63, 16, 8, 8, 16, 8, 79, 16, 61, 89, 16, 6, 13, 24, 17, 9, 24, 14, 6, 24, 15, 223, 24, 13, 17, 24, 55, 32, 24, 17, 17, 24, 20, 259, 24, 14, 4, 24, 14, 11, 24, 13, 42, 16, 12, 94, 24, 20, 5, 24, 16, 47, 24, 13, 3, 16, 11, 242, 24, 23, 13, 16, 24, 125, 16, 12, 295, 24, 27, 1, 24, 15, 328, 16, 9, 257, 16, 7, 221, 16, 11, 189, 16, 9, 203, 16, 126, 52, 24, 15, 35, 24, 14, 5, 16, 12, 208, 24, 22, 9, 24, 18, 118, 16, 275, 79, 16, 10, 100, 24, 11, 3, 16, 5, 158, 16, 12, 53, 16, 5, 6, 24, 20, 1, 16, 8, 167, 16, 274, 17, 24, 19, 15, 16, 6, 34, 16, 11, 66, 16, 12, 227, 24, 40, 26, 16, 7, 40, 16, 10, 19, 24, 31, 13, 24, 21, 11, 24, 21, 16, 24, 17, 12, 24, 17, 5, 24, 13, 5, 24, 44, 27, 24, 15, 38, 24, 16, 11, 24, 14, 63, 24, 14, 20, 24, 13, 11, 24, 74, 44, 16, 4, 46, 24, 12, 11, 16, 8, 28, 24, 9, 43, 24, 12, 62, 24, 13, 7, 16, 8, 66, 24, 23, 6, 24, 16, 96, 16, 30, 186, 24, 17, 11, 24, 23, 3, 16, 8, 5, 16, 323, 30, 24, 18, 4, 16, 12, 10, 24, 15, 37, 16, 15, 138, 24, 19, 6, 24, 18, 40, 16, 8, 49, 24, 22, 64, 24, 27, 11, 16, 11, 108, 16, 17, 70, 24, 12, 105, 16, 15, 111, 16, 6, 162, 24, 19, 94, 16, 19, 177, 24, 20, 101, 24, 21, 9, 24, 23, 169, 16, 25, 227, 16, 240, 50, 16, 22, 142, 16, 12, 350, 16, 9, 304, 24, 12, 8, 16, 221, 53, 24, 15, 323, 24, 18, 76, 16, 23, 17, 24, 13, 298, 24, 13, 14, 16, 8, 19, 16, 7, 20, 16, 8, 134, 24, 36, 142, 16, 30, 17, 24, 11, 5, 24, 14, 4, 24, 23, 281, 24, 16, 7, 24, 11, 208, 24, 11, 105, 24, 17, 5, 24, 24, 18, 24, 20, 8, 24, 20, 10, 24, 13, 49, 24, 13, 104, 24, 11, 11, 24, 32, 1, 24, 57, 24, 24, 22, 9, 24, 16, 6, 16, 10, 42, 24, 15, 5, 24, 16, 7, 24, 20, 15, 24, 12, 33, 24, 16, 8, 24, 21, 30, 16, 12, 118, 24, 16, 30, 16, 9, 187, 16, 9, 14, 16, 9, 151, 16, 8, 102, 24, 20, 5, 16, 14, 297, 16, 9, 166, 16, 6, 243, 16, 7, 7, 16, 8, 90, 24, 13, 10, 24, 31, 8, 24, 10, 158, 24, 112, 16, 24, 23, 219, 24, 28, 39, 16, 10, 86, 16, 14, 9, 16, 8, 180, 24, 18, 111, 24, 31, 84, 16, 4, 11, 24, 71, 10, 16, 4, 218, 24, 20, 1, 16, 9, 13, 24, 14, 5, 16, 11, 62, 16, 7, 98, 16, 8, 201, 16, 6, 141, 24, 15, 44, 16, 4, 13, 16, 25, 40, 16, 5, 131, 16, 5, 42, 16, 10, 21, 24, 17, 39, 16, 8, 154, 16, 11, 104, 24, 19, 7, 16, 7, 43, 16, 8, 21, 16, 13, 56, 16, 10, 30, 16, 8, 10, 16, 8, 22, 16, 10, 103, 16, 5, 66, 16, 9, 43, 16, 14, 7, 16, 7, 95, 16, 21, 118, 16, 9, 72, 16, 12, 262, 16, 12, 248, 16, 10, 209, 16, 8, 254, 16, 9, 28, 16, 12, 20, 16, 11, 220, 16, 7, 184, 16, 9, 81, 16, 10, 227, 16, 11, 218, 16, 9, 291, 16, 5, 13, 16, 14, 24, 16, 10, 280, 24, 21, 6, 24, 18, 9, 16, 23, 109, 16, 7, 251, 16, 10, 202, 16, 8, 72, 16, 8, 22, 24, 16, 85, 24, 60, 22, 16, 12, 37, 24, 18, 4, 24, 32, 1, 24, 19, 8, 24, 90, 31, 16, 8, 148, 16, 10, 122, 24, 14, 9, 24, 25, 9, 24, 66, 33, 16, 48, 122, 16, 14, 76, 24, 16, 6, 24, 23, 69, 16, 12, 22, 24, 12, 74, 24, 16, 18, 24, 14, 31, 24, 13, 94, 24, 11, 221, 24, 27, 18, 24, 20, 214, 16, 10, 118, 16, 15, 10, 16, 9, 207, 24, 12, 299, 24, 17, 63, 24, 13, 3, 24, 15, 13, 24, 12, 5, 24, 10, 17, 24, 14, 165, 24, 21, 293, 16, 9, 354, 16, 5, 79, 16, 8, 139, 16, 4, 186, 16, 5, 89, 16, 5, 94, 24, 16, 18, 16, 5, 113, 16, 10, 215, 16, 6, 131, 16, 5, 211, 16, 7, 136, 16, 8, 226, 24, 23, 306, 24, 88, 93, 16, 20, 19, 16, 13, 340, 16, 13, 31, 24, 23, 63, 16, 6, 343, 24, 17, 1, 24, 52, 42, 16, 10, 241, 24, 17, 7, 24, 75, 45, 16, 8, 11, 16, 7, 205, 24, 13, 30, 16, 7, 221, 24, 79, 54, 16, 7, 136, 24, 14, 6, 16, 5, 18, 16, 12, 385, 16, 11, 204, 16, 6, 277, 24, 13, 235, 16, 10, 86, 16, 8, 309, 16, 10, 86, 16, 6, 122, 24, 33, 1, 24, 61, 43, 24, 21, 18, 24, 15, 94, 24, 40, 12, 24, 13, 84, 24, 23, 111, 24, 20, 20, 24, 13, 25, 16, 12, 93, 24, 18, 6, 24, 28, 70, 24, 40, 31, 24, 22, 15, 16, 10, 3, 16, 9, 146, 24, 19, 7, 24, 16, 8, 24, 15, 6, 16, 8, 188, 16, 7, 6, 24, 14, 5, 16, 10, 197, 16, 9, 217, 24, 18, 99, 16, 10, 112, 16, 4, 6, 16, 7, 112, 16, 12, 111, 16, 8, 20, 16, 6, 207, 16, 11, 33, 16, 10, 212, 16, 18, 78, 16, 7, 90, 16, 9, 179, 16, 8, 123, 16, 9, 206, 16, 6, 171, 16, 4, 69, 24, 43, 12, 16, 8, 9, 24, 10, 11, 24, 36, 23, 16, 6, 19, 24, 65, 33, 24, 12, 11, 16, 7, 8, 16, 30, 59, 16, 10, 8, 16, 5, 306, 16, 7, 20, 16, 31, 169, 16, 14, 6, 16, 6, 285, 16, 11, 176, 16, 12, 1, 16, 5, 6, 16, 11, 6, 24, 14, 25, 16, 5, 15, 16, 10, 19, 16, 7, 62, 16, 11, 19, 16, 4, 10, 16, 4, 14, 16, 9, 274, 16, 8, 184, 16, 5, 209, 16, 12, 118, 16, 10, 205, 16, 7, 244, 16, 7, 194, 16, 8, 268, 16, 9, 83, 24, 14, 8, 16, 8, 177, 16, 8, 47, 24, 10, 31, 24, 15, 3, 24, 20, 11, 24, 16, 6, 24, 17, 1, 24, 18, 33, 24, 15, 8, 24, 14, 94, 24, 13, 325, 24, 11, 1, 16, 8, 212, 16, 7, 173, 24, 31, 28, 16, 6, 270, 16, 4, 206, 16, 7, 95, 24, 23, 8, 16, 15, 52, 16, 9, 298, 16, 7, 237, 16, 7, 31, 16, 7, 258, 16, 4, 144, 24, 23, 17, 16, 12, 13, 24, 21, 40, 16, 6, 185, 16, 8, 224, 16, 11, 56, 16, 9, 92, 16, 9, 124, 16, 13, 201, 16, 15, 47, 16, 6, 123, 16, 11, 288, 16, 11, 9, 16, 6, 200, 24, 12, 66, 16, 11, 264, 16, 11, 96, 16, 9, 309, 24, 44, 64, 16, 7, 342, 16, 8, 310, 16, 11, 277, 16, 11, 420, 24, 22, 17, 16, 11, 10, 16, 14, 147, 24, 59, 28, 24, 11, 133, 24, 21, 33, 24, 16, 11, 16, 6, 159, 24, 18, 69, 16, 8, 94, 16, 15, 121, 16, 14, 89, 24, 23, 11, 16, 8, 34, 16, 12, 52, 16, 7, 107, 16, 18, 7, 16, 6, 32, 16, 7, 41, 16, 6, 8, 24, 14, 4, 24, 12, 12, 24, 16, 7, 16, 8, 22, 24, 22, 12, 16, 9, 111, 24, 25, 13, 16, 15, 51, 16, 7, 155, 24, 16, 5, 24, 13, 38, 16, 9, 217, 16, 10, 136, 24, 12, 1, 16, 7, 184, 16, 8, 167, 24, 28, 28, 24, 31, 242, 16, 17, 88, 16, 13, 176, 16, 9, 388, 16, 6, 10, 24, 41, 35, 24, 105, 68, 24, 24, 10, 16, 6, 37, 16, 10, 11, 24, 10, 32, 24, 74, 6, 16, 11, 354, 16, 7, 204, 24, 14, 34, 16, 5, 29, 16, 24, 54, 16, 10, 368, 16, 13, 131, 16, 7, 185, 16, 8, 8, 16, 8, 201, 16, 7, 94, 16, 5, 250, 16, 8, 117, 16, 17, 14, 16, 13, 245, 24, 13, 43, 16, 4, 10, 16, 9, 371, 16, 13, 359, 24, 12, 7, 16, 7, 90, 16, 295, 13, 24, 12, 38, 16, 11, 272, 16, 8, 158, 16, 7, 115, 16, 4, 10, 16, 11, 10, 16, 6, 141, 16, 9, 268, 16, 14, 47, 16, 10, 185, 16, 10, 251, 16, 6, 128, 16, 13, 268, 16, 16, 9, 16, 7, 234, 16, 7, 108, 16, 19, 11, 16, 15, 28, 16, 11, 142, 16, 11, 329, 24, 19, 109, 24, 22, 87, 16, 9, 245, 16, 13, 38, 24, 11, 145, 24, 22, 28, 16, 13, 81, 16, 17, 29, 16, 10, 143, 16, 13, 34, 24, 17, 12, 24, 12, 20, 24, 22, 96, 24, 17, 20, 24, 17, 9, 24, 15, 6, 24, 12, 192, 24, 21, 106, 24, 31, 11, 24, 51, 3, 24, 19, 10, 24, 23, 83, 16, 12, 9, 24, 42, 44, 24, 23, 9, 16, 8, 40, 24, 26, 16, 24, 23, 11, 16, 23, 262, 16, 7, 170, 16, 11, 101, 16, 37, 31, 16, 47, 65, 16, 21, 184, 16, 11, 113, 16, 7, 210, 16, 5, 237, 16, 10, 46, 24, 16, 21, 16, 9, 207, 24, 16, 1, 24, 16, 8, 24, 26, 11, 24, 19, 2, 24, 21, 11, 16, 11, 190, 24, 59, 339, 24, 19, 17, 16, 6, 25, 24, 61, 43, 24, 19, 30, 16, 18, 94, 24, 22, 1, 24, 54, 25, 16, 10, 40, 24, 34, 39, 24, 17, 12, 24, 27, 218, 24, 19, 72, 24, 17, 6, 24, 34, 166, 16, 9, 421, 16, 7, 29, 16, 5, 119, 24, 17, 47, 24, 14, 1, 16, 11, 16, 16, 10, 393, 24, 15, 10, 16, 7, 18, 16, 4, 155, 24, 7, 7, 24, 12, 3, 16, 5, 160, 16, 9, 207, 16, 9, 253, 24, 14, 6, 16, 9, 12, 16, 9, 45, 16, 6, 258, 24, 20, 48, 16, 10, 402, 24, 23, 17, 16, 8, 185, 24, 28, 14, 16, 8, 113, 24, 28, 16, 24, 14, 50, 24, 23, 152, 24, 18, 124, 16, 6, 331, 24, 13, 255, 24, 10, 32, 24, 18, 1, 16, 7, 238, 16, 8, 289, 16, 10, 60, 16, 8, 282, 16, 11, 360, 16, 9, 338, 16, 9, 208, 16, 7, 186, 16, 10, 180, 16, 11, 410, 16, 12, 290, 16, 9, 91, 16, 9, 219, 24, 80, 42, 16, 8, 148, 24, 21, 13, 16, 8, 255, 16, 8, 22, 24, 60, 30, 24, 25, 58, 16, 7, 249, 16, 8, 78, 24, 15, 138, 24, 31, 132, 24, 43, 29, 16, 9, 10, 24, 15, 83, 24, 16, 7, 16, 30, 106, 24, 13, 10, 24, 13, 17, 16, 10, 84, 16, 11, 82, 16, 6, 26, 16, 8, 14, 16, 11, 157, 16, 9, 79, 16, 11, 111, 24, 59, 37, 16, 12, 155, 16, 11, 7, 16, 11, 20, 16, 8, 268, 16, 18, 66, 16, 8, 114, 16, 14, 115, 16, 13, 199, 16, 12, 44, 16, 8, 185, 16, 8, 198, 16, 6, 314, 16, 10, 193, 16, 13, 101, 16, 10, 210, 16, 5, 8, 16, 226, 23, 16, 8, 232, 16, 4, 1, 24, 22, 267, 24, 32, 28, 24, 42, 239, 24, 27, 105, 24, 13, 33, 24, 21, 62, 24, 15, 65, 24, 18, 3, 24, 12, 77, 24, 18, 9, 16, 7, 137, 16, 250, 119, 24, 17, 13, 16, 8, 1, 24, 67, 57, 24, 17, 162, 16, 8, 12, 16, 11, 47, 24, 16, 42, 16, 6, 139, 16, 8, 66, 24, 81, 25, 16, 10, 263, 16, 17, 9, 16, 4, 10, 24, 139, 41, 16, 11, 140, 16, 11, 111, 16, 12, 51, 16, 10, 44, 16, 7, 29, 16, 9, 13, 16, 9, 18, 16, 11, 80, 16, 6, 261, 16, 10, 233, 24, 17, 5, 24, 15, 1, 16, 7, 83, 16, 8, 385, 16, 7, 78, 24, 17, 19, 16, 13, 12, 16, 19, 8, 16, 7, 228, 24, 16, 33, 16, 11, 157, 16, 10, 17, 24, 22, 227, 24, 15, 368, 16, 9, 25, 24, 22, 14, 16, 16, 90, 24, 14, 3, 16, 7, 64, 24, 13, 57, 24, 14, 22, 24, 11, 36, 24, 11, 4, 24, 15, 170, 24, 74, 27, 24, 20, 6, 24, 24, 14, 16, 10, 32, 24, 11, 250, 16, 7, 243, 16, 14, 225, 16, 13, 298, 16, 7, 190, 16, 10, 299, 24, 13, 8, 24, 13, 106, 24, 106, 51, 24, 20, 32, 24, 34, 12, 24, 15, 3, 16, 7, 11, 16, 6, 219, 24, 18, 5, 16, 7, 149, 16, 8, 8, 16, 15, 328, 16, 9, 261, 16, 9, 119, 16, 9, 130, 16, 9, 267, 16, 12, 183, 16, 10, 63, 16, 15, 55, 24, 37, 160, 24, 15, 234, 16, 11, 165, 24, 34, 45, 16, 14, 10, 24, 20, 5, 16, 7, 11, 16, 9, 112, 16, 12, 62, 16, 10, 257, 16, 8, 113, 24, 20, 60, 24, 21, 67, 16, 6, 69, 16, 5, 209, 16, 14, 122, 16, 26, 76, 16, 6, 236, 24, 23, 122, 16, 10, 345, 24, 20, 2, 24, 33, 11, 24, 28, 15, 24, 32, 7, 24, 46, 92, 24, 31, 42, 16, 13, 159, 16, 4, 108, 16, 9, 160, 24, 15, 56, 24, 20, 8, 16, 13, 11, 16, 11, 253, 24, 19, 30, 16, 4, 281, 16, 16, 166, 16, 11, 12, 16, 8, 8, 16, 11, 211, 16, 9, 106, 16, 9, 396, 16, 9, 203, 24, 63, 101, 16, 14, 198, 16, 10, 82, 16, 8, 259, 16, 15, 352, 24, 21, 11, 16, 6, 294, 24, 31, 103, 16, 20, 247, 16, 9, 264, 16, 8, 238, 16, 7, 269, 16, 9, 403, 16, 8, 215, 24, 17, 11, 16, 8, 111, 24, 13, 4, 16, 8, 84, 24, 13, 84, 16, 10, 78, 24, 24, 1, 16, 7, 72, 16, 13, 139, 16, 7, 142, 16, 8, 1, 16, 9, 51, 16, 14, 307, 16, 23, 12, 24, 25, 5, 16, 11, 40, 16, 11, 30, 16, 29, 11, 16, 6, 19, 16, 24, 87, 24, 18, 71, 16, 21, 9, 16, 11, 21, 16, 18, 338, 16, 14, 11, 16, 16, 66, 24, 45, 57, 24, 16, 4, 24, 26, 13, 16, 8, 71, 16, 10, 20, 16, 5, 345, 16, 19, 114, 16, 6, 244, 16, 9, 246, 24, 29, 49, 16, 9, 35, 16, 7, 186, 24, 29, 318, 24, 28, 19, 24, 40, 31, 16, 16, 35, 24, 30, 1, 24, 50, 1, 24, 31, 8, 24, 23, 1, 24, 20, 6, 16, 13, 178, 24, 43, 31, 16, 11, 94, 24, 21, 20, 24, 11, 15, 24, 28, 1, 16, 9, 94, 24, 14, 9, 16, 14, 71, 16, 5, 109, 24, 20, 17, 24, 35, 243, 16, 8, 230, 24, 25, 70, 16, 9, 146, 16, 26, 13, 16, 19, 75, 16, 10, 165, 16, 30, 128, 16, 6, 83, 24, 44, 98, 24, 18, 1, 24, 20, 30, 16, 10, 38, 24, 18, 15, 24, 28, 16, 24, 15, 21, 24, 16, 186, 24, 23, 148, 16, 10, 232, 24, 27, 11, 16, 9, 122, 16, 9, 326, 24, 20, 12, 24, 18, 5, 24, 13, 121, 24, 37, 28, 24, 14, 9, 16, 14, 195, 24, 16, 5, 24, 30, 11, 24, 12, 189, 16, 6, 105, 24, 32, 11, 24, 22, 23, 16, 10, 244, 24, 15, 166, 16, 9, 271, 16, 13, 257, 24, 17, 206, 24, 135, 58, 16, 9, 120, 16, 8, 102, 16, 16, 121, 16, 11, 367, 16, 16, 250, 16, 11, 187, 24, 13, 221, 16, 9, 314, 16, 8, 296, 24, 12, 78, 16, 13, 337, 24, 14, 12, 16, 6, 250, 24, 31, 1, 24, 19, 35, 16, 13, 100, 16, 8, 149, 16, 6, 4, 16, 13, 52, 16, 10, 180, 24, 27, 15, 16, 11, 52, 16, 10, 205, 16, 11, 26, 16, 9, 287, 16, 19, 7, 16, 9, 192, 16, 9, 25, 16, 6, 182, 16, 15, 6, 16, 17, 10, 16, 11, 218, 24, 9, 69, 16, 8, 20, 24, 31, 19, 24, 17, 61, 24, 12, 124, 24, 13, 102, 24, 12, 156, 16, 10, 274, 16, 28, 154], "type": "histogram"}];
            const layout = Object.assign({}, {"template": {"data": {"histogram2dcontour": [{"type": "histogram2dcontour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "choropleth": [{"type": "choropleth", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "histogram2d": [{"type": "histogram2d", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "heatmap": [{"type": "heatmap", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "contourcarpet": [{"type": "contourcarpet", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "contour": [{"type": "contour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "surface": [{"type": "surface", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "mesh3d": [{"type": "mesh3d", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "scatter": [{"fillpattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}, "type": "scatter"}], "parcoords": [{"type": "parcoords", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolargl": [{"type": "scatterpolargl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "bar"}], "scattergeo": [{"type": "scattergeo", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolar": [{"type": "scatterpolar", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "histogram": [{"marker": {"pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "histogram"}], "scattergl": [{"type": "scattergl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatter3d": [{"type": "scatter3d", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermap": [{"type": "scattermap", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermapbox": [{"type": "scattermapbox", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterternary": [{"type": "scatterternary", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattercarpet": [{"type": "scattercarpet", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "barpolar"}], "pie": [{"automargin": true, "type": "pie"}]}, "layout": {"autotypenumbers": "strict", "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "hovermode": "closest", "hoverlabel": {"align": "left"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"bgcolor": "#E5ECF6", "angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "ternary": {"bgcolor": "#E5ECF6", "aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]]}, "xaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "yaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "geo": {"bgcolor": "white", "landcolor": "#E5ECF6", "subunitcolor": "white", "showland": true, "showlakes": true, "lakecolor": "white"}, "title": {"x": 0.05}, "mapbox": {"style": "light"}}}, "margin": {"l": 50, "r": 30, "t": 30, "b": 50}, "font": {"color": "#a39e93"}, "xaxis": {"title": {"text": "Length Word Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "yaxis": {"title": {"text": "Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "title": {}, "height": 400, "paper_bgcolor": "rgba(0,0,0,0)", "plot_bgcolor": "rgba(0,0,0,0)"}, darkLayout);
            Plotly.newPlot('chart_0', data, layout, plotlyConfig);
        })();
        
        (function() {
            const data = [{"marker": {"color": "#d4a574"}, "nbinsx": 30, "opacity": 0.85, "x": [18, 7, 147, 18, 7, 63, 18, 7, 247, 18, 7, 262, 18, 24, 147, 18, 190, 103, 27, 19, 175, 18, 20, 309, 27, 22, 25, 18, 6, 148, 18, 7, 8, 18, 14, 151, 18, 7, 352, 27, 22, 14, 18, 10, 203, 18, 7, 199, 27, 40, 103, 18, 11, 235, 18, 12, 18, 18, 6, 36, 18, 12, 423, 18, 7, 68, 27, 36, 17, 18, 8, 95, 18, 8, 155, 18, 8, 20, 18, 10, 388, 27, 17, 83, 18, 12, 37, 18, 9, 318, 27, 20, 7, 18, 12, 79, 18, 7, 361, 18, 11, 359, 27, 24, 6, 27, 31, 52, 18, 35, 61, 27, 68, 25, 18, 152, 23, 18, 7, 153, 18, 21, 91, 18, 9, 354, 27, 20, 122, 18, 22, 149, 27, 18, 129, 18, 10, 133, 18, 8, 308, 27, 74, 52, 27, 49, 49, 27, 34, 15, 18, 8, 94, 18, 152, 10, 18, 38, 35, 18, 17, 102, 18, 16, 103, 18, 8, 98, 27, 31, 25, 18, 7, 80, 18, 11, 12, 18, 10, 113, 18, 77, 124, 18, 8, 16, 27, 22, 11, 27, 16, 7, 27, 19, 289, 27, 16, 24, 27, 61, 33, 27, 19, 20, 27, 22, 303, 27, 16, 5, 27, 16, 16, 27, 15, 51, 18, 16, 133, 27, 23, 6, 27, 19, 58, 27, 15, 6, 18, 12, 325, 27, 25, 15, 18, 28, 137, 18, 15, 334, 27, 34, 1, 27, 18, 376, 18, 10, 299, 18, 10, 252, 18, 12, 206, 18, 10, 245, 18, 154, 63, 27, 19, 55, 27, 21, 8, 18, 13, 252, 27, 24, 11, 27, 23, 141, 18, 348, 102, 18, 11, 180, 27, 13, 5, 18, 6, 173, 18, 17, 64, 18, 6, 7, 27, 24, 1, 18, 9, 227, 18, 348, 21, 27, 25, 28, 18, 8, 44, 18, 12, 73, 18, 13, 276, 27, 48, 27, 18, 8, 84, 18, 13, 22, 27, 34, 14, 27, 24, 13, 27, 23, 18, 27, 22, 20, 27, 20, 6, 27, 16, 6, 27, 53, 29, 27, 19, 46, 27, 18, 12, 27, 18, 83, 27, 18, 22, 27, 16, 14, 27, 97, 55, 18, 7, 91, 27, 17, 13, 18, 12, 65, 27, 12, 54, 27, 21, 89, 27, 16, 8, 18, 9, 84, 27, 28, 7, 27, 24, 155, 18, 47, 296, 27, 20, 14, 27, 48, 5, 18, 9, 6, 18, 423, 48, 27, 21, 5, 18, 14, 19, 27, 18, 47, 18, 18, 164, 27, 21, 7, 27, 21, 50, 18, 11, 59, 27, 27, 71, 27, 33, 12, 18, 12, 162, 18, 19, 101, 27, 30, 153, 18, 16, 181, 18, 8, 237, 27, 34, 159, 18, 20, 255, 27, 33, 139, 27, 38, 11, 27, 43, 263, 18, 29, 307, 18, 327, 67, 18, 25, 210, 18, 13, 399, 18, 9, 394, 27, 16, 9, 18, 258, 60, 27, 17, 368, 27, 20, 121, 18, 31, 24, 27, 15, 349, 27, 17, 21, 18, 8, 24, 18, 8, 42, 18, 10, 151, 27, 42, 161, 18, 32, 20, 27, 13, 11, 27, 19, 6, 27, 27, 340, 27, 19, 8, 27, 13, 240, 27, 14, 134, 27, 21, 6, 27, 34, 22, 27, 25, 14, 27, 23, 11, 27, 17, 58, 27, 15, 125, 27, 13, 14, 27, 37, 1, 27, 64, 25, 27, 25, 10, 27, 20, 7, 18, 11, 59, 27, 21, 8, 27, 19, 8, 27, 23, 20, 27, 14, 35, 27, 19, 9, 27, 35, 44, 18, 15, 147, 27, 21, 36, 18, 10, 215, 18, 12, 18, 18, 10, 174, 18, 10, 134, 27, 23, 6, 18, 15, 343, 18, 10, 194, 18, 7, 276, 18, 9, 12, 18, 9, 132, 27, 16, 15, 27, 36, 9, 27, 13, 213, 27, 135, 18, 27, 32, 261, 27, 33, 44, 18, 11, 94, 18, 19, 10, 18, 9, 200, 27, 28, 158, 27, 37, 98, 18, 6, 18, 27, 89, 12, 18, 5, 254, 27, 28, 2, 18, 10, 19, 27, 18, 6, 18, 16, 75, 18, 8, 118, 18, 9, 255, 18, 9, 177, 27, 20, 60, 18, 5, 16, 18, 30, 51, 18, 6, 145, 18, 6, 50, 18, 11, 24, 27, 20, 42, 18, 10, 204, 18, 13, 129, 27, 21, 8, 18, 8, 53, 18, 10, 23, 18, 14, 69, 18, 11, 36, 18, 12, 17, 18, 10, 30, 18, 13, 143, 18, 8, 81, 18, 10, 48, 18, 17, 8, 18, 9, 115, 18, 23, 127, 18, 10, 82, 18, 14, 336, 18, 13, 293, 18, 13, 264, 18, 11, 300, 18, 10, 32, 18, 13, 27, 18, 19, 281, 18, 9, 218, 18, 13, 92, 18, 12, 291, 18, 13, 278, 18, 10, 339, 18, 6, 21, 18, 28, 42, 18, 11, 355, 27, 25, 9, 27, 22, 13, 18, 34, 176, 18, 7, 306, 18, 10, 233, 18, 9, 87, 18, 8, 24, 27, 22, 104, 27, 78, 25, 18, 18, 50, 27, 22, 9, 27, 44, 2, 27, 23, 9, 27, 102, 37, 18, 11, 229, 18, 11, 146, 27, 24, 18, 27, 28, 10, 27, 81, 39, 18, 74, 172, 18, 16, 90, 27, 19, 11, 27, 34, 84, 18, 19, 54, 27, 16, 82, 27, 19, 25, 27, 22, 56, 27, 17, 110, 27, 14, 276, 27, 32, 20, 27, 23, 244, 18, 11, 147, 18, 17, 18, 18, 11, 240, 27, 17, 355, 27, 25, 84, 27, 17, 4, 27, 18, 23, 27, 14, 6, 27, 13, 44, 27, 17, 186, 27, 25, 346, 18, 10, 419, 18, 8, 101, 18, 9, 179, 18, 6, 266, 18, 6, 104, 18, 8, 109, 27, 20, 26, 18, 6, 139, 18, 11, 251, 18, 7, 163, 18, 9, 253, 18, 8, 170, 18, 9, 280, 27, 26, 354, 27, 104, 108, 18, 37, 37, 18, 14, 396, 18, 15, 34, 27, 28, 79, 18, 7, 405, 27, 21, 1, 27, 60, 49, 18, 11, 272, 27, 20, 8, 27, 89, 50, 18, 10, 13, 18, 8, 243, 27, 17, 39, 18, 7, 292, 27, 97, 64, 18, 8, 173, 27, 17, 7, 18, 27, 69, 18, 13, 436, 18, 12, 239, 18, 6, 331, 27, 17, 282, 18, 11, 124, 18, 9, 372, 18, 12, 167, 18, 6, 137, 27, 39, 1, 27, 82, 56, 27, 41, 24, 27, 17, 105, 27, 44, 13, 27, 15, 107, 27, 26, 129, 27, 26, 28, 27, 17, 28, 18, 13, 110, 27, 24, 8, 27, 34, 88, 27, 44, 38, 27, 27, 18, 18, 11, 6, 18, 10, 171, 27, 22, 8, 27, 19, 9, 27, 18, 7, 18, 9, 221, 18, 8, 10, 27, 18, 6, 18, 13, 219, 18, 10, 251, 27, 23, 121, 18, 11, 135, 18, 5, 12, 18, 8, 144, 18, 16, 126, 18, 13, 46, 18, 7, 237, 18, 12, 41, 18, 11, 254, 18, 21, 101, 18, 10, 112, 18, 10, 209, 18, 11, 138, 18, 10, 233, 18, 7, 211, 18, 5, 86, 27, 50, 13, 18, 9, 12, 27, 13, 18, 27, 42, 25, 18, 7, 27, 27, 79, 51, 27, 14, 14, 18, 8, 9, 18, 36, 84, 18, 11, 12, 18, 6, 369, 18, 8, 23, 18, 40, 281, 18, 16, 7, 18, 7, 355, 18, 12, 203, 18, 13, 2, 18, 7, 10, 18, 15, 14, 27, 19, 35, 18, 6, 19, 18, 12, 24, 18, 8, 72, 18, 13, 22, 18, 5, 21, 18, 7, 16, 18, 12, 341, 18, 10, 218, 18, 6, 243, 18, 13, 185, 18, 11, 225, 18, 10, 293, 18, 8, 229, 18, 12, 328, 18, 10, 95, 27, 18, 11, 18, 9, 221, 18, 9, 110, 27, 16, 40, 27, 18, 5, 27, 24, 12, 27, 19, 8, 27, 23, 2, 27, 21, 38, 27, 20, 9, 27, 19, 109, 27, 17, 428, 27, 15, 2, 18, 10, 316, 18, 8, 197, 27, 49, 36, 18, 7, 303, 18, 5, 255, 18, 9, 118, 27, 33, 14, 18, 17, 66, 18, 10, 344, 18, 8, 284, 18, 11, 43, 18, 11, 340, 18, 7, 172, 27, 28, 18, 18, 14, 17, 27, 25, 48, 18, 7, 219, 18, 10, 276, 18, 14, 68, 18, 11, 151, 18, 11, 154, 18, 14, 227, 18, 16, 54, 18, 7, 248, 18, 13, 333, 18, 12, 15, 18, 7, 230, 27, 16, 73, 18, 13, 315, 18, 12, 109, 18, 10, 359, 27, 53, 70, 18, 8, 496, 18, 9, 390, 18, 12, 314, 18, 12, 499, 27, 30, 20, 18, 11, 20, 18, 14, 204, 27, 72, 32, 27, 14, 151, 27, 25, 57, 27, 18, 15, 18, 7, 171, 27, 23, 111, 18, 9, 113, 18, 22, 139, 18, 15, 146, 27, 26, 12, 18, 9, 44, 18, 13, 61, 18, 8, 141, 18, 25, 26, 18, 9, 42, 18, 10, 48, 18, 8, 16, 27, 18, 6, 27, 17, 21, 27, 18, 8, 18, 9, 27, 27, 25, 14, 18, 10, 140, 27, 28, 16, 18, 16, 67, 18, 8, 186, 27, 20, 6, 27, 21, 55, 18, 10, 250, 18, 11, 155, 27, 14, 2, 18, 8, 219, 18, 9, 197, 27, 40, 36, 27, 44, 288, 18, 19, 125, 18, 15, 208, 18, 10, 469, 18, 8, 13, 27, 48, 42, 27, 124, 79, 27, 36, 13, 18, 7, 50, 18, 13, 16, 27, 14, 39, 27, 90, 9, 18, 12, 407, 18, 8, 234, 27, 17, 42, 18, 14, 62, 18, 28, 68, 18, 12, 473, 18, 20, 225, 18, 8, 224, 18, 9, 15, 18, 9, 263, 18, 14, 192, 18, 7, 297, 18, 9, 131, 18, 19, 18, 18, 15, 277, 27, 18, 51, 18, 5, 12, 18, 10, 421, 18, 15, 427, 27, 17, 11, 18, 10, 104, 18, 359, 15, 27, 15, 67, 18, 14, 331, 18, 9, 191, 18, 9, 141, 18, 5, 20, 18, 13, 11, 18, 7, 183, 18, 10, 333, 18, 16, 59, 18, 11, 206, 18, 11, 281, 18, 7, 164, 18, 13, 320, 18, 17, 10, 18, 8, 283, 18, 7, 119, 18, 20, 12, 18, 17, 37, 18, 12, 165, 18, 12, 376, 27, 30, 124, 27, 26, 97, 18, 10, 289, 18, 14, 41, 27, 15, 188, 27, 24, 33, 18, 14, 90, 18, 18, 31, 18, 11, 169, 18, 14, 44, 27, 21, 13, 27, 19, 27, 27, 25, 111, 27, 20, 23, 27, 21, 10, 27, 20, 11, 27, 14, 222, 27, 26, 119, 27, 33, 14, 27, 58, 4, 27, 22, 11, 27, 26, 92, 18, 13, 21, 27, 46, 50, 27, 25, 10, 18, 10, 53, 27, 29, 20, 27, 28, 15, 18, 26, 291, 18, 8, 200, 18, 12, 122, 18, 44, 35, 18, 57, 74, 18, 24, 200, 18, 13, 124, 18, 9, 230, 18, 6, 287, 18, 11, 50, 27, 18, 25, 18, 10, 246, 27, 20, 3, 27, 20, 9, 27, 30, 16, 27, 24, 9, 27, 24, 14, 18, 11, 272, 27, 67, 379, 27, 23, 19, 18, 6, 35, 27, 71, 51, 27, 22, 34, 18, 21, 138, 27, 26, 6, 27, 67, 36, 18, 13, 48, 27, 43, 64, 27, 28, 16, 27, 34, 277, 27, 22, 101, 27, 21, 7, 27, 39, 192, 18, 9, 496, 18, 10, 56, 18, 5, 148, 27, 20, 72, 27, 19, 4, 18, 11, 32, 18, 12, 458, 27, 37, 21, 18, 8, 22, 18, 8, 179, 27, 15, 12, 27, 14, 4, 18, 6, 205, 18, 10, 256, 18, 10, 307, 27, 22, 15, 18, 10, 18, 18, 10, 51, 18, 7, 299, 27, 27, 65, 18, 11, 457, 27, 34, 30, 18, 9, 211, 27, 35, 18, 18, 12, 131, 27, 32, 19, 27, 17, 81, 27, 28, 192, 27, 21, 136, 18, 8, 395, 27, 15, 293, 27, 13, 42, 27, 24, 3, 18, 8, 293, 18, 11, 356, 18, 11, 66, 18, 10, 339, 18, 12, 407, 18, 11, 405, 18, 10, 230, 18, 7, 213, 18, 11, 208, 18, 12, 500, 18, 13, 346, 18, 11, 120, 18, 12, 245, 27, 89, 47, 18, 10, 289, 27, 30, 14, 18, 8, 286, 18, 11, 26, 27, 67, 33, 27, 31, 71, 18, 8, 277, 18, 8, 149, 27, 29, 240, 27, 52, 203, 27, 52, 32, 18, 10, 13, 27, 19, 118, 27, 19, 8, 18, 60, 220, 27, 17, 13, 27, 17, 25, 18, 11, 145, 18, 12, 132, 18, 8, 55, 18, 11, 20, 18, 14, 190, 18, 12, 101, 18, 12, 152, 27, 71, 41, 18, 14, 179, 18, 13, 8, 18, 13, 47, 18, 10, 331, 18, 41, 108, 18, 9, 133, 18, 15, 130, 18, 14, 261, 18, 13, 62, 18, 10, 244, 18, 9, 223, 18, 7, 379, 18, 12, 251, 18, 29, 252, 18, 12, 268, 18, 7, 12, 18, 292, 36, 18, 10, 321, 18, 8, 9, 27, 25, 355, 27, 36, 34, 27, 47, 279, 27, 34, 119, 27, 20, 45, 27, 25, 81, 27, 18, 74, 27, 21, 4, 27, 15, 85, 27, 21, 11, 18, 9, 189, 18, 318, 162, 27, 20, 18, 18, 10, 3, 27, 86, 71, 27, 21, 240, 18, 10, 16, 18, 14, 78, 27, 21, 45, 18, 7, 161, 18, 11, 102, 27, 101, 28, 18, 11, 307, 18, 19, 10, 18, 5, 13, 27, 178, 57, 18, 13, 161, 18, 13, 127, 18, 13, 70, 18, 11, 57, 18, 8, 37, 18, 10, 16, 18, 10, 19, 18, 12, 100, 18, 8, 323, 18, 12, 296, 27, 20, 6, 27, 18, 1, 18, 10, 114, 18, 9, 486, 18, 8, 98, 27, 20, 24, 18, 16, 15, 18, 22, 9, 18, 9, 274, 27, 20, 40, 18, 14, 206, 18, 14, 26, 27, 27, 275, 27, 27, 499, 18, 10, 28, 27, 26, 15, 18, 18, 104, 27, 17, 4, 18, 8, 82, 27, 16, 86, 27, 20, 42, 27, 14, 38, 27, 17, 6, 27, 18, 503, 27, 81, 32, 27, 23, 7, 27, 27, 21, 18, 11, 41, 27, 13, 357, 18, 8, 283, 18, 15, 259, 18, 14, 354, 18, 8, 254, 18, 11, 331, 27, 20, 11, 27, 22, 130, 27, 122, 57, 27, 28, 37, 27, 38, 14, 27, 20, 4, 18, 8, 21, 18, 7, 242, 27, 21, 6, 18, 8, 181, 18, 9, 16, 18, 18, 495, 18, 10, 330, 18, 11, 137, 18, 10, 177, 18, 11, 320, 18, 21, 261, 18, 11, 70, 18, 17, 70, 27, 54, 209, 27, 17, 274, 18, 14, 189, 27, 48, 53, 18, 15, 15, 27, 23, 6, 18, 9, 17, 18, 10, 126, 18, 14, 77, 18, 11, 303, 18, 9, 136, 27, 33, 86, 27, 30, 101, 18, 7, 81, 18, 6, 242, 18, 20, 168, 18, 29, 83, 18, 7, 271, 27, 26, 153, 18, 10, 397, 27, 22, 3, 27, 40, 17, 27, 35, 22, 27, 39, 9, 27, 59, 109, 27, 39, 51, 18, 14, 178, 18, 8, 129, 18, 11, 192, 27, 18, 73, 27, 23, 9, 18, 14, 15, 18, 16, 306, 27, 27, 50, 18, 8, 350, 18, 19, 230, 18, 20, 22, 18, 9, 13, 18, 13, 257, 18, 10, 121, 18, 11, 465, 18, 10, 240, 27, 75, 112, 18, 15, 328, 18, 11, 96, 18, 9, 312, 18, 17, 402, 27, 27, 15, 18, 7, 344, 27, 39, 127, 18, 22, 293, 18, 10, 329, 18, 9, 292, 18, 8, 312, 18, 10, 473, 18, 15, 260, 27, 23, 14, 18, 10, 143, 27, 18, 5, 18, 10, 92, 27, 15, 96, 18, 11, 88, 27, 31, 2, 18, 8, 85, 18, 16, 164, 18, 9, 166, 18, 9, 6, 18, 11, 69, 18, 16, 353, 18, 28, 13, 27, 30, 6, 18, 14, 55, 18, 13, 40, 18, 33, 13, 18, 6, 28, 18, 29, 115, 27, 25, 81, 18, 27, 14, 18, 12, 33, 18, 18, 470, 18, 15, 19, 18, 18, 74, 27, 50, 71, 27, 21, 8, 27, 34, 16, 18, 12, 99, 18, 12, 51, 18, 7, 431, 18, 59, 274, 18, 7, 291, 18, 10, 290, 27, 37, 56, 18, 14, 52, 18, 8, 229, 27, 33, 367, 27, 34, 21, 27, 51, 35, 18, 19, 40, 27, 40, 1, 27, 61, 3, 27, 37, 9, 27, 28, 2, 27, 25, 7, 18, 14, 200, 27, 48, 33, 18, 14, 111, 27, 24, 24, 27, 13, 16, 27, 42, 3, 18, 11, 104, 27, 19, 11, 18, 17, 83, 18, 5, 129, 27, 26, 19, 27, 45, 285, 18, 9, 292, 27, 31, 79, 18, 10, 190, 18, 30, 15, 18, 20, 83, 18, 11, 200, 18, 36, 181, 18, 7, 98, 27, 52, 135, 27, 21, 3, 27, 27, 41, 18, 14, 55, 27, 21, 17, 27, 35, 17, 27, 18, 43, 27, 20, 217, 27, 26, 160, 18, 13, 267, 27, 35, 12, 18, 10, 136, 18, 11, 392, 27, 24, 17, 27, 25, 10, 27, 17, 144, 27, 43, 31, 27, 18, 14, 18, 16, 238, 27, 19, 7, 27, 34, 14, 27, 14, 210, 18, 7, 120, 27, 37, 19, 27, 25, 25, 18, 11, 322, 27, 25, 276, 18, 10, 326, 18, 14, 307, 27, 19, 242, 27, 158, 71, 18, 10, 144, 18, 9, 157, 18, 17, 144, 18, 12, 429, 18, 17, 281, 18, 12, 220, 27, 15, 250, 18, 10, 365, 18, 9, 371, 27, 14, 109, 18, 14, 424, 27, 18, 18, 18, 7, 371, 27, 45, 3, 27, 25, 47, 18, 22, 183, 18, 9, 176, 18, 8, 6, 18, 14, 75, 18, 15, 269, 27, 32, 17, 18, 12, 64, 18, 13, 234, 18, 13, 29, 18, 10, 350, 18, 23, 9, 18, 10, 236, 18, 12, 57, 18, 8, 211, 18, 17, 7, 18, 21, 14, 18, 12, 246, 27, 13, 80, 18, 10, 35, 27, 45, 26, 27, 20, 74, 27, 14, 142, 27, 15, 141, 27, 15, 201, 18, 10, 323, 18, 35, 174], "type": "histogram"}];
            const layout = Object.assign({}, {"template": {"data": {"histogram2dcontour": [{"type": "histogram2dcontour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "choropleth": [{"type": "choropleth", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "histogram2d": [{"type": "histogram2d", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "heatmap": [{"type": "heatmap", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "contourcarpet": [{"type": "contourcarpet", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "contour": [{"type": "contour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "surface": [{"type": "surface", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "mesh3d": [{"type": "mesh3d", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "scatter": [{"fillpattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}, "type": "scatter"}], "parcoords": [{"type": "parcoords", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolargl": [{"type": "scatterpolargl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "bar"}], "scattergeo": [{"type": "scattergeo", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolar": [{"type": "scatterpolar", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "histogram": [{"marker": {"pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "histogram"}], "scattergl": [{"type": "scattergl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatter3d": [{"type": "scatter3d", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermap": [{"type": "scattermap", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermapbox": [{"type": "scattermapbox", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterternary": [{"type": "scatterternary", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattercarpet": [{"type": "scattercarpet", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "barpolar"}], "pie": [{"automargin": true, "type": "pie"}]}, "layout": {"autotypenumbers": "strict", "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "hovermode": "closest", "hoverlabel": {"align": "left"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"bgcolor": "#E5ECF6", "angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "ternary": {"bgcolor": "#E5ECF6", "aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]]}, "xaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "yaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "geo": {"bgcolor": "white", "landcolor": "#E5ECF6", "subunitcolor": "white", "showland": true, "showlakes": true, "lakecolor": "white"}, "title": {"x": 0.05}, "mapbox": {"style": "light"}}}, "margin": {"l": 50, "r": 30, "t": 30, "b": 50}, "font": {"color": "#a39e93"}, "xaxis": {"title": {"text": "Length Token Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "yaxis": {"title": {"text": "Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "title": {}, "height": 400, "paper_bgcolor": "rgba(0,0,0,0)", "plot_bgcolor": "rgba(0,0,0,0)"}, darkLayout);
            Plotly.newPlot('chart_1', data, layout, plotlyConfig);
        })();
        
        (function() {
            const data = [{"marker": {"color": "#d4a574"}, "nbinsx": 30, "opacity": 0.85, "x": [0.875, 1.0, 0.743801652892562, 0.875, 1.0, 0.6792452830188679, 0.875, 1.0, 0.45933014354066987, 0.875, 1.0, 0.6620370370370371, 0.875, 0.8095238095238095, 0.5714285714285714, 0.875, 0.768, 0.8507462686567164, 0.8333333333333334, 1.0, 0.5161290322580645, 0.875, 0.9473684210526315, 0.5280898876404494, 0.8333333333333334, 0.9444444444444444, 0.9047619047619048, 0.875, 1.0, 0.6869565217391305, 0.875, 1.0, 1.0, 0.875, 0.8571428571428571, 0.6935483870967742, 0.875, 0.8571428571428571, 0.578397212543554, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.6449704142011834, 0.875, 1.0, 0.622093023255814, 0.8333333333333334, 0.8695652173913043, 0.5909090909090909, 0.875, 1.0, 0.6153846153846154, 0.875, 1.0, 0.8181818181818182, 0.875, 1.0, 0.9230769230769231, 0.875, 0.8181818181818182, 0.4849315068493151, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9375, 0.9375, 0.875, 1.0, 0.75, 0.875, 1.0, 0.5454545454545454, 0.875, 1.0, 0.9411764705882353, 0.875, 1.0, 0.5592105263157895, 0.8333333333333334, 1.0, 0.5172413793103449, 0.875, 1.0, 0.88, 0.875, 1.0, 0.45454545454545453, 0.8333333333333334, 0.8823529411764706, 0.8333333333333334, 0.875, 0.9090909090909091, 1.0, 0.875, 1.0, 0.5979381443298969, 0.875, 0.9, 0.5993265993265994, 0.8333333333333334, 0.9, 1.0, 0.8333333333333334, 0.8846153846153846, 0.7027027027027027, 0.875, 0.5555555555555556, 0.7857142857142857, 0.8333333333333334, 0.8166666666666667, 0.9523809523809523, 0.875, 0.7619047619047619, 0.9523809523809523, 0.875, 1.0, 0.6776859504132231, 0.875, 0.9166666666666666, 0.6923076923076923, 0.875, 1.0, 0.5197368421052632, 0.8333333333333334, 0.9411764705882353, 0.7058823529411765, 0.875, 1.0, 0.5092592592592593, 0.8333333333333334, 0.9375, 0.6886792452830188, 0.875, 1.0, 0.7522123893805309, 0.875, 1.0, 0.5593869731800766, 0.8333333333333334, 0.828125, 0.8541666666666666, 0.8333333333333334, 0.8604651162790697, 0.8717948717948718, 0.8333333333333334, 0.7741935483870968, 0.7857142857142857, 0.875, 1.0, 0.6923076923076923, 0.875, 0.7619047619047619, 1.0, 0.875, 0.8823529411764706, 0.9166666666666666, 0.875, 0.8666666666666667, 0.5657894736842105, 0.875, 0.9285714285714286, 0.5342465753424658, 0.875, 1.0, 0.6071428571428571, 0.8333333333333334, 1.0, 0.9230769230769231, 0.875, 1.0, 0.7619047619047619, 0.875, 1.0, 1.0, 0.875, 1.0, 0.7341772151898734, 0.875, 0.7540983606557377, 0.5393258426966292, 0.875, 1.0, 0.9230769230769231, 0.8333333333333334, 0.9411764705882353, 0.8888888888888888, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.9333333333333333, 0.6233183856502242, 0.8333333333333334, 0.9230769230769231, 0.8823529411764706, 0.8333333333333334, 0.8181818181818182, 0.8125, 0.8333333333333334, 0.9411764705882353, 0.9411764705882353, 0.8333333333333334, 0.9, 0.5907335907335908, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9230769230769231, 0.7619047619047619, 0.875, 1.0, 0.6063829787234043, 0.8333333333333334, 0.95, 1.0, 0.8333333333333334, 0.9375, 0.7021276595744681, 0.8333333333333334, 0.9230769230769231, 1.0, 0.875, 0.9090909090909091, 0.6363636363636364, 0.8333333333333334, 0.9130434782608695, 0.8461538461538461, 0.875, 0.875, 0.72, 0.875, 0.9166666666666666, 0.48135593220338985, 0.8333333333333334, 0.8888888888888888, 1.0, 0.8333333333333334, 0.9333333333333333, 0.5, 0.875, 1.0, 0.5525291828793775, 0.875, 1.0, 0.5972850678733032, 0.875, 1.0, 0.4603174603174603, 0.875, 1.0, 0.6650246305418719, 0.875, 0.7698412698412699, 0.8269230769230769, 0.8333333333333334, 1.0, 0.9142857142857143, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 0.9166666666666666, 0.6009615384615384, 0.8333333333333334, 0.8636363636363636, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.7203389830508474, 0.875, 0.6509090909090909, 0.7721518987341772, 0.875, 1.0, 0.92, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.6265822784810127, 0.875, 1.0, 0.6226415094339622, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8, 1.0, 0.875, 1.0, 0.6167664670658682, 0.875, 0.656934306569343, 1.0, 0.8333333333333334, 1.0, 0.7333333333333333, 0.875, 1.0, 0.8823529411764706, 0.875, 1.0, 0.7272727272727273, 0.875, 1.0, 0.5550660792951542, 0.8333333333333334, 0.825, 0.8076923076923077, 0.875, 1.0, 1.0, 0.875, 1.0, 0.9473684210526315, 0.8333333333333334, 0.8709677419354839, 1.0, 0.8333333333333334, 0.9047619047619048, 1.0, 0.8333333333333334, 0.8095238095238095, 0.875, 0.8333333333333334, 0.8823529411764706, 1.0, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.7727272727272727, 0.8148148148148148, 0.8333333333333334, 0.9333333333333333, 0.8421052631578947, 0.8333333333333334, 0.9375, 0.9090909090909091, 0.8333333333333334, 0.9285714285714286, 0.6825396825396826, 0.8333333333333334, 0.8571428571428571, 0.95, 0.8333333333333334, 0.9230769230769231, 0.9090909090909091, 0.8333333333333334, 0.7162162162162162, 0.8409090909090909, 0.875, 1.0, 0.6521739130434783, 0.8333333333333334, 1.0, 0.9090909090909091, 0.875, 0.875, 1.0, 0.8333333333333334, 1.0, 0.9302325581395349, 0.8333333333333334, 0.9166666666666666, 0.6290322580645161, 0.8333333333333334, 0.9230769230769231, 1.0, 0.875, 0.875, 0.7878787878787878, 0.8333333333333334, 0.8695652173913043, 1.0, 0.8333333333333334, 0.9375, 0.5833333333333334, 0.875, 0.7333333333333333, 0.45698924731182794, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.782608695652174, 1.0, 0.875, 1.0, 1.0, 0.875, 0.6346749226006192, 0.9333333333333333, 0.8333333333333334, 0.8333333333333334, 1.0, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 0.6756756756756757, 0.875, 0.9333333333333333, 0.7898550724637681, 0.8333333333333334, 0.8421052631578947, 1.0, 0.8333333333333334, 0.9444444444444444, 0.75, 0.875, 1.0, 0.8571428571428571, 0.8333333333333334, 0.8636363636363636, 0.734375, 0.8333333333333334, 0.9259259259259259, 0.9090909090909091, 0.875, 0.9090909090909091, 0.6388888888888888, 0.875, 0.8235294117647058, 0.6714285714285714, 0.8333333333333334, 1.0, 0.7047619047619048, 0.875, 0.9333333333333333, 0.6756756756756757, 0.875, 1.0, 0.5185185185185185, 0.8333333333333334, 1.0, 0.7446808510638298, 0.875, 1.0, 0.480225988700565, 0.8333333333333334, 0.95, 0.5346534653465347, 0.8333333333333334, 0.9523809523809523, 0.8888888888888888, 0.8333333333333334, 0.9565217391304348, 0.48520710059171596, 0.875, 0.84, 0.4933920704845815, 0.875, 0.6708333333333333, 0.74, 0.875, 0.9090909090909091, 0.5774647887323944, 0.875, 0.8333333333333334, 0.44285714285714284, 0.875, 0.8888888888888888, 0.6118421052631579, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 0.6470588235294118, 0.8490566037735849, 0.8333333333333334, 0.9333333333333333, 0.5510835913312694, 0.8333333333333334, 0.9444444444444444, 0.8552631578947368, 0.875, 0.9130434782608695, 1.0, 0.8333333333333334, 1.0, 0.5503355704697986, 0.8333333333333334, 0.9230769230769231, 0.9285714285714286, 0.875, 1.0, 0.9473684210526315, 0.875, 1.0, 1.0, 0.875, 0.875, 0.6343283582089553, 0.8333333333333334, 0.8611111111111112, 0.6267605633802817, 0.875, 0.9, 0.9411764705882353, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9130434782608695, 0.5871886120996441, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 0.5721153846153846, 0.8333333333333334, 0.9090909090909091, 0.638095238095238, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 0.9583333333333334, 0.8888888888888888, 0.8333333333333334, 0.85, 1.0, 0.8333333333333334, 0.9, 0.9, 0.8333333333333334, 0.9230769230769231, 0.8163265306122449, 0.8333333333333334, 0.9230769230769231, 0.6634615384615384, 0.8333333333333334, 0.9090909090909091, 1.0, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.7543859649122807, 0.8333333333333334, 0.8333333333333334, 0.9545454545454546, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.9047619047619048, 0.8333333333333334, 0.9333333333333333, 1.0, 0.8333333333333334, 0.875, 1.0, 0.8333333333333334, 0.75, 0.8666666666666667, 0.8333333333333334, 1.0, 0.9090909090909091, 0.8333333333333334, 0.875, 0.875, 0.8333333333333334, 0.9523809523809523, 0.7333333333333333, 0.875, 0.9166666666666666, 0.711864406779661, 0.8333333333333334, 1.0, 0.8, 0.875, 0.7777777777777778, 0.6737967914438503, 0.875, 0.8888888888888888, 0.9285714285714286, 0.875, 0.8888888888888888, 0.7019867549668874, 0.875, 1.0, 0.7254901960784313, 0.8333333333333334, 0.9, 1.0, 0.875, 0.8571428571428571, 0.5791245791245792, 0.875, 0.8888888888888888, 0.6325301204819277, 0.875, 0.8333333333333334, 0.5390946502057613, 0.875, 1.0, 1.0, 0.875, 0.875, 0.8333333333333334, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9032258064516129, 0.875, 0.8333333333333334, 1.0, 0.44936708860759494, 0.8333333333333334, 0.7053571428571429, 1.0, 0.8333333333333334, 0.8260869565217391, 0.589041095890411, 0.8333333333333334, 1.0, 0.8461538461538461, 0.875, 1.0, 0.7209302325581395, 0.875, 1.0, 1.0, 0.875, 0.75, 0.45, 0.8333333333333334, 0.8888888888888888, 0.4774774774774775, 0.8333333333333334, 0.8387096774193549, 0.7023809523809523, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8591549295774648, 1.0, 0.875, 1.0, 0.5688073394495413, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.9230769230769231, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.8548387096774194, 0.875, 1.0, 0.7142857142857143, 0.875, 0.875, 0.5920398009950248, 0.875, 1.0, 0.7304964539007093, 0.8333333333333334, 0.9333333333333333, 0.75, 0.875, 1.0, 0.9230769230769231, 0.875, 0.96, 0.8, 0.875, 1.0, 0.6106870229007634, 0.875, 1.0, 0.8095238095238095, 0.875, 0.9, 0.7619047619047619, 0.8333333333333334, 0.9411764705882353, 0.8205128205128205, 0.875, 0.875, 0.512987012987013, 0.875, 1.0, 0.7019230769230769, 0.8333333333333334, 0.9473684210526315, 1.0, 0.875, 1.0, 0.7674418604651163, 0.875, 1.0, 1.0, 0.875, 0.8461538461538461, 0.8035714285714286, 0.875, 0.9, 0.9333333333333333, 0.875, 0.875, 0.9, 0.875, 1.0, 0.5909090909090909, 0.875, 1.0, 0.4854368932038835, 0.875, 1.0, 0.8181818181818182, 0.875, 1.0, 0.8837209302325582, 0.875, 0.9285714285714286, 1.0, 0.875, 1.0, 0.6105263157894737, 0.875, 0.8571428571428571, 0.5169491525423728, 0.875, 0.7777777777777778, 0.7777777777777778, 0.875, 0.8333333333333334, 0.6030534351145038, 0.875, 1.0, 0.5604838709677419, 0.875, 1.0, 0.5789473684210527, 0.875, 0.875, 0.5511811023622047, 0.875, 0.8888888888888888, 0.9285714285714286, 0.875, 1.0, 0.95, 0.875, 1.0, 0.6272727272727273, 0.875, 1.0, 0.6032608695652174, 0.875, 0.8888888888888888, 0.6666666666666666, 0.875, 0.9, 0.5418502202643172, 0.875, 0.9090909090909091, 0.5825688073394495, 0.875, 1.0, 0.5429553264604811, 0.875, 1.0, 1.0, 0.875, 0.8571428571428571, 0.7916666666666666, 0.875, 1.0, 0.6035714285714285, 0.8333333333333334, 0.7142857142857143, 1.0, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.875, 0.8260869565217391, 0.5137614678899083, 0.875, 1.0, 0.5936254980079682, 0.875, 0.9, 0.5693069306930693, 0.875, 1.0, 0.8333333333333334, 0.875, 1.0, 0.8636363636363636, 0.8333333333333334, 0.9375, 0.8235294117647058, 0.8333333333333334, 0.8833333333333333, 1.0, 0.875, 0.9166666666666666, 0.918918918918919, 0.8333333333333334, 0.9444444444444444, 1.0, 0.8333333333333334, 0.96875, 1.0, 0.8333333333333334, 0.9473684210526315, 1.0, 0.8333333333333334, 0.6666666666666666, 0.8709677419354839, 0.875, 1.0, 0.6418918918918919, 0.875, 0.9, 0.6967213114754098, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.88, 1.0, 0.8333333333333334, 0.696969696969697, 0.8181818181818182, 0.875, 0.8125, 0.5819672131147541, 0.875, 0.9285714285714286, 0.8157894736842105, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.8260869565217391, 0.6521739130434783, 0.875, 1.0, 0.8636363636363636, 0.8333333333333334, 0.9166666666666666, 0.7702702702702703, 0.8333333333333334, 0.9375, 0.8888888888888888, 0.8333333333333334, 1.0, 0.7741935483870968, 0.8333333333333334, 0.7692307692307693, 0.6702127659574468, 0.8333333333333334, 0.9090909090909091, 0.6063348416289592, 0.8333333333333334, 0.9259259259259259, 0.9444444444444444, 0.8333333333333334, 0.95, 0.5233644859813084, 0.875, 1.0, 0.6610169491525424, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5362318840579711, 0.8333333333333334, 1.0, 0.5217391304347826, 0.8333333333333334, 0.8823529411764706, 0.6349206349206349, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9, 1.0, 0.8333333333333334, 1.0, 0.5757575757575758, 0.8333333333333334, 0.8571428571428571, 0.5938566552901023, 0.875, 1.0, 0.6016949152542372, 0.875, 1.0, 0.7721518987341772, 0.875, 1.0, 0.6618705035971223, 0.875, 1.0, 0.6612903225806451, 0.875, 1.0, 0.8089887640449438, 0.875, 1.0, 0.7021276595744681, 0.8333333333333334, 0.875, 0.7222222222222222, 0.875, 1.0, 0.672566371681416, 0.875, 1.0, 0.5767441860465117, 0.875, 1.0, 0.6259541984732825, 0.875, 1.0, 0.6635071090047393, 0.875, 1.0, 0.7058823529411765, 0.875, 1.0, 0.6061946902654868, 0.8333333333333334, 0.782608695652174, 0.49019607843137253, 0.8333333333333334, 0.7840909090909091, 0.7956989247311828, 0.875, 0.95, 1.0, 0.875, 1.0, 0.5264705882352941, 0.875, 0.9230769230769231, 0.9032258064516129, 0.8333333333333334, 0.782608695652174, 0.6031746031746031, 0.875, 1.0, 0.5685131195335277, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.7692307692307693, 0.7857142857142857, 0.875, 1.0, 0.5892116182572614, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 0.7066666666666667, 0.8444444444444444, 0.875, 1.0, 1.0, 0.875, 1.0, 0.551219512195122, 0.8333333333333334, 0.8461538461538461, 0.9, 0.875, 1.0, 0.5475113122171946, 0.8333333333333334, 0.759493670886076, 0.7962962962962963, 0.875, 1.0, 0.625, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.9444444444444444, 0.875, 1.0, 0.4831168831168831, 0.875, 0.9090909090909091, 0.6225490196078431, 0.875, 1.0, 0.5523465703971119, 0.8333333333333334, 0.8461538461538461, 0.625531914893617, 0.875, 0.9, 0.6976744186046512, 0.875, 1.0, 0.6601941747572816, 0.875, 1.0, 0.6511627906976745, 0.875, 1.0, 0.7049180327868853, 0.8333333333333334, 0.9090909090909091, 1.0, 0.8333333333333334, 0.8852459016393442, 0.8837209302325582, 0.8333333333333334, 0.8571428571428571, 0.9444444444444444, 0.8333333333333334, 0.9333333333333333, 0.7340425531914894, 0.8333333333333334, 0.825, 0.9166666666666666, 0.8333333333333334, 1.0, 0.7142857142857143, 0.8333333333333334, 0.9565217391304348, 0.7477477477477478, 0.8333333333333334, 0.95, 0.9, 0.8333333333333334, 1.0, 0.88, 0.875, 1.0, 0.7634408602150538, 0.8333333333333334, 0.9444444444444444, 1.0, 0.8333333333333334, 0.7857142857142857, 0.6714285714285714, 0.8333333333333334, 0.9, 0.8387096774193549, 0.8333333333333334, 0.9090909090909091, 0.9333333333333333, 0.875, 0.9, 1.0, 0.875, 1.0, 0.678082191780822, 0.8333333333333334, 0.8421052631578947, 0.8571428571428571, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.5957446808510638, 0.875, 0.8571428571428571, 1.0, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.6548223350253807, 0.875, 1.0, 0.5668202764976958, 0.8333333333333334, 1.0, 0.6161616161616161, 0.875, 1.0, 0.6785714285714286, 0.875, 1.0, 1.0, 0.875, 1.0, 0.7589285714285714, 0.875, 1.0, 0.7387387387387387, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5072463768115942, 0.875, 0.9090909090909091, 0.7878787878787878, 0.875, 1.0, 0.5141509433962265, 0.875, 0.8888888888888888, 0.47435897435897434, 0.875, 0.8571428571428571, 0.7444444444444445, 0.875, 1.0, 0.48044692737430167, 0.875, 1.0, 0.6178861788617886, 0.875, 1.0, 0.5194174757281553, 0.875, 1.0, 0.6608187134502924, 0.875, 1.0, 0.7246376811594203, 0.8333333333333334, 0.7441860465116279, 0.9166666666666666, 0.875, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8611111111111112, 0.8260869565217391, 0.875, 1.0, 0.9473684210526315, 0.8333333333333334, 0.7692307692307693, 0.8181818181818182, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 1.0, 0.875, 0.8666666666666667, 0.6271186440677966, 0.875, 0.9, 1.0, 0.875, 1.0, 0.5261437908496732, 0.875, 1.0, 0.75, 0.875, 0.5161290322580645, 0.40828402366863903, 0.875, 0.8571428571428571, 1.0, 0.875, 1.0, 0.6, 0.875, 1.0, 0.7159090909090909, 0.875, 0.8333333333333334, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9285714285714286, 0.84, 0.875, 1.0, 0.8, 0.875, 0.9, 0.8421052631578947, 0.875, 1.0, 0.8064516129032258, 0.875, 0.9090909090909091, 0.9473684210526315, 0.875, 1.0, 1.0, 0.875, 1.0, 0.9285714285714286, 0.875, 1.0, 0.6715328467153284, 0.875, 1.0, 0.5760869565217391, 0.875, 1.0, 0.5741626794258373, 0.875, 1.0, 0.788135593220339, 0.875, 1.0, 0.5024390243902439, 0.875, 0.8571428571428571, 0.5819672131147541, 0.875, 1.0, 0.6030927835051546, 0.875, 1.0, 0.5522388059701493, 0.875, 1.0, 0.7469879518072289, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.5536723163841808, 0.875, 0.875, 0.9787234042553191, 0.8333333333333334, 1.0, 0.9354838709677419, 0.8333333333333334, 0.9333333333333333, 1.0, 0.8333333333333334, 0.9, 1.0, 0.8333333333333334, 1.0, 0.8333333333333334, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 0.9444444444444444, 0.7272727272727273, 0.8333333333333334, 0.8666666666666667, 0.875, 0.8333333333333334, 0.9285714285714286, 0.7127659574468085, 0.8333333333333334, 1.0, 0.49538461538461537, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.4481132075471698, 0.875, 1.0, 0.6589595375722543, 0.8333333333333334, 0.8064516129032258, 0.8214285714285714, 0.875, 1.0, 0.5407407407407407, 0.875, 1.0, 0.6359223300970874, 0.875, 1.0, 0.7052631578947368, 0.8333333333333334, 0.8260869565217391, 1.0, 0.875, 0.8, 0.6730769230769231, 0.875, 1.0, 0.5335570469798657, 0.875, 1.0, 0.569620253164557, 0.875, 0.8571428571428571, 0.8709677419354839, 0.875, 0.8571428571428571, 0.5310077519379846, 0.875, 1.0, 0.6875, 0.8333333333333334, 0.8260869565217391, 0.9411764705882353, 0.875, 0.9166666666666666, 0.9230769230769231, 0.8333333333333334, 0.9523809523809523, 0.825, 0.875, 1.0, 0.6270270270270271, 0.875, 1.0, 0.6830357142857143, 0.875, 0.8181818181818182, 0.5357142857142857, 0.875, 0.8888888888888888, 0.8804347826086957, 0.875, 1.0, 0.6370967741935484, 0.875, 1.0, 0.572139303482587, 0.875, 0.8666666666666667, 0.7872340425531915, 0.875, 1.0, 0.5203252032520326, 0.875, 1.0, 0.5486111111111112, 0.875, 0.9090909090909091, 1.0, 0.875, 1.0, 0.605, 0.8333333333333334, 1.0, 0.7272727272727273, 0.875, 0.9090909090909091, 0.625, 0.875, 0.9090909090909091, 0.6875, 0.875, 1.0, 0.517799352750809, 0.8333333333333334, 0.7954545454545454, 0.75, 0.875, 1.0, 0.49707602339181284, 0.875, 1.0, 0.5290322580645161, 0.875, 1.0, 0.5415162454873647, 0.875, 1.0, 0.4642857142857143, 0.8333333333333334, 0.9090909090909091, 0.9411764705882353, 0.875, 0.8181818181818182, 1.0, 0.875, 0.7857142857142857, 0.5782312925170068, 0.8333333333333334, 0.8135593220338984, 0.8928571428571429, 0.8333333333333334, 0.9090909090909091, 0.6766917293233082, 0.8333333333333334, 0.9523809523809523, 0.9090909090909091, 0.8333333333333334, 0.875, 0.9090909090909091, 0.875, 1.0, 0.6163522012578616, 0.8333333333333334, 0.9444444444444444, 0.6956521739130435, 0.875, 1.0, 0.7340425531914894, 0.875, 0.9333333333333333, 0.7107438016528925, 0.875, 1.0, 0.6404494382022472, 0.8333333333333334, 0.782608695652174, 0.8181818181818182, 0.875, 1.0, 0.8529411764705882, 0.875, 1.0, 0.8653846153846154, 0.875, 0.8571428571428571, 0.6355140186915887, 0.875, 0.9444444444444444, 0.7142857142857143, 0.875, 0.8333333333333334, 0.96875, 0.875, 1.0, 0.7804878048780488, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 1.0, 0.6666666666666666, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.5454545454545454, 0.8333333333333334, 0.9545454545454546, 0.8333333333333334, 0.875, 0.8888888888888888, 0.7297297297297297, 0.8333333333333334, 0.64, 0.8461538461538461, 0.875, 0.9333333333333333, 0.9019607843137255, 0.875, 1.0, 0.6580645161290323, 0.8333333333333334, 0.9375, 0.8, 0.8333333333333334, 1.0, 0.7894736842105263, 0.875, 0.8888888888888888, 0.5161290322580645, 0.875, 1.0, 0.6691176470588235, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.6902173913043478, 0.875, 1.0, 0.6047904191616766, 0.8333333333333334, 0.9642857142857143, 0.8571428571428571, 0.8333333333333334, 0.967741935483871, 0.6198347107438017, 0.875, 0.8823529411764706, 0.6136363636363636, 0.875, 1.0, 0.6704545454545454, 0.875, 1.0, 0.5128865979381443, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8780487804878049, 0.8857142857142857, 0.8333333333333334, 0.7904761904761904, 0.8676470588235294, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 0.918918918918919, 0.875, 0.9, 0.9090909090909091, 0.8333333333333334, 1.0, 0.65625, 0.8333333333333334, 0.8108108108108109, 1.0, 0.875, 0.9090909090909091, 0.5169491525423728, 0.875, 1.0, 0.5882352941176471, 0.8333333333333334, 1.0, 0.7941176470588235, 0.875, 1.0, 0.7931034482758621, 0.875, 0.9166666666666666, 0.8518518518518519, 0.875, 0.8, 0.5978260869565217, 0.875, 0.9230769230769231, 0.5114503816793893, 0.875, 1.0, 0.654054054054054, 0.875, 0.875, 1.0, 0.875, 1.0, 0.6716417910447762, 0.875, 1.0, 0.5531914893617021, 0.875, 1.0, 0.484, 0.875, 0.875, 0.6666666666666666, 0.875, 0.8823529411764706, 1.0, 0.875, 0.9230769230769231, 0.49795918367346936, 0.8333333333333334, 0.9230769230769231, 0.8372093023255814, 0.875, 1.0, 1.0, 0.875, 0.8888888888888888, 0.5256064690026954, 0.875, 0.9230769230769231, 0.5626740947075209, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 0.6555555555555556, 0.875, 0.6, 0.8461538461538461, 0.8333333333333334, 0.9166666666666666, 0.7368421052631579, 0.875, 0.9090909090909091, 0.5772058823529411, 0.875, 1.0, 0.7151898734177216, 0.875, 1.0, 0.6347826086956522, 0.875, 1.0, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 0.6666666666666666, 0.875, 0.8888888888888888, 0.6044776119402985, 0.875, 1.0, 0.8723404255319149, 0.875, 0.9, 0.6054054054054054, 0.875, 1.0, 0.5179282868525896, 0.875, 1.0, 0.796875, 0.875, 0.7692307692307693, 0.5522388059701493, 0.875, 0.9375, 1.0, 0.875, 1.0, 0.5897435897435898, 0.875, 0.8571428571428571, 0.7037037037037037, 0.875, 0.7894736842105263, 0.8181818181818182, 0.875, 0.9333333333333333, 0.9285714285714286, 0.875, 1.0, 0.7112676056338029, 0.875, 0.9090909090909091, 0.5562310030395137, 0.8333333333333334, 0.9473684210526315, 0.6605504587155964, 0.8333333333333334, 0.8636363636363636, 0.6781609195402298, 0.875, 1.0, 0.6244897959183674, 0.875, 0.9230769230769231, 0.7631578947368421, 0.8333333333333334, 0.9090909090909091, 0.6896551724137931, 0.8333333333333334, 0.9545454545454546, 0.9642857142857143, 0.875, 1.0, 0.7530864197530864, 0.875, 0.9411764705882353, 0.896551724137931, 0.875, 1.0, 0.6923076923076923, 0.875, 0.9230769230769231, 0.9117647058823529, 0.8333333333333334, 0.8823529411764706, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9545454545454546, 0.65625, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9166666666666666, 0.578125, 0.8333333333333334, 0.9047619047619048, 0.6226415094339622, 0.8333333333333334, 0.8387096774193549, 1.0, 0.8333333333333334, 0.7647058823529411, 1.0, 0.8333333333333334, 0.8947368421052632, 0.9, 0.8333333333333334, 0.9565217391304348, 0.7590361445783133, 0.875, 1.0, 0.7777777777777778, 0.8333333333333334, 0.8095238095238095, 0.9090909090909091, 0.8333333333333334, 0.8695652173913043, 0.8888888888888888, 0.875, 1.0, 0.775, 0.8333333333333334, 0.8846153846153846, 0.9375, 0.8333333333333334, 0.9130434782608695, 1.0, 0.875, 0.9130434782608695, 0.5267175572519084, 0.875, 1.0, 0.6882352941176471, 0.875, 1.0, 0.7425742574257426, 0.875, 0.7297297297297297, 0.8064516129032258, 0.875, 0.8085106382978723, 0.7846153846153846, 0.875, 0.9047619047619048, 0.6086956521739131, 0.875, 0.9090909090909091, 0.6991150442477876, 0.875, 1.0, 0.580952380952381, 0.875, 1.0, 0.5822784810126582, 0.875, 0.9, 0.8478260869565217, 0.8333333333333334, 0.9375, 0.8571428571428571, 0.875, 1.0, 0.6570048309178744, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8846153846153846, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9047619047619048, 1.0, 0.875, 0.9090909090909091, 0.4631578947368421, 0.8333333333333334, 0.847457627118644, 0.5014749262536873, 0.8333333333333334, 0.9473684210526315, 1.0, 0.875, 1.0, 0.96, 0.8333333333333334, 0.8524590163934426, 0.8604651162790697, 0.8333333333333334, 0.9473684210526315, 0.9, 0.875, 0.7777777777777778, 0.6276595744680851, 0.8333333333333334, 0.9090909090909091, 1.0, 0.8333333333333334, 0.7962962962962963, 0.88, 0.875, 0.8, 0.8, 0.8333333333333334, 0.8235294117647058, 0.48717948717948717, 0.8333333333333334, 1.0, 0.9166666666666666, 0.8333333333333334, 0.7037037037037037, 0.6192660550458715, 0.8333333333333334, 0.9473684210526315, 0.8472222222222222, 0.8333333333333334, 0.8823529411764706, 1.0, 0.8333333333333334, 0.8529411764705882, 0.6265060240963856, 0.875, 0.8888888888888888, 0.5296912114014252, 0.875, 0.8571428571428571, 0.8275862068965517, 0.875, 1.0, 0.7815126050420168, 0.8333333333333334, 1.0, 0.6595744680851063, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5292620865139949, 0.8333333333333334, 0.6666666666666666, 1.0, 0.875, 1.0, 0.9444444444444444, 0.875, 1.0, 0.5806451612903226, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.675, 0.875, 0.8888888888888888, 0.5942028985507246, 0.875, 1.0, 0.5138339920948617, 0.8333333333333334, 1.0, 1.0, 0.875, 0.8888888888888888, 1.0, 0.875, 1.0, 0.8666666666666667, 0.875, 1.0, 0.5658914728682171, 0.8333333333333334, 0.9, 0.6666666666666666, 0.875, 1.0, 0.43532338308457713, 0.8333333333333334, 0.9130434782608695, 0.8235294117647058, 0.875, 1.0, 0.5513513513513514, 0.8333333333333334, 0.8928571428571429, 0.9285714285714286, 0.875, 1.0, 0.6991150442477876, 0.8333333333333334, 0.8928571428571429, 0.875, 0.8333333333333334, 1.0, 0.56, 0.8333333333333334, 0.9130434782608695, 0.6447368421052632, 0.8333333333333334, 0.9444444444444444, 0.5725806451612904, 0.875, 0.8333333333333334, 0.525679758308157, 0.8333333333333334, 0.9230769230769231, 0.6078431372549019, 0.8333333333333334, 1.0, 0.9375, 0.8333333333333334, 0.9444444444444444, 1.0, 0.875, 1.0, 0.5504201680672269, 0.875, 1.0, 0.6262975778546713, 0.875, 1.0, 0.7666666666666667, 0.875, 0.875, 0.5957446808510638, 0.875, 1.0, 0.4638888888888889, 0.875, 1.0, 0.5621301775147929, 0.875, 0.8888888888888888, 0.46634615384615385, 0.875, 1.0, 0.5913978494623656, 0.875, 1.0, 0.6388888888888888, 0.875, 0.9090909090909091, 0.5317073170731708, 0.875, 0.9166666666666666, 0.5448275862068965, 0.875, 1.0, 0.6593406593406593, 0.875, 1.0, 0.6027397260273972, 0.8333333333333334, 0.7125, 0.7857142857142857, 0.875, 1.0, 0.9527027027027027, 0.8333333333333334, 1.0, 0.9230769230769231, 0.875, 1.0, 0.5372549019607843, 0.875, 1.0, 0.8181818181818182, 0.8333333333333334, 0.8, 0.9333333333333333, 0.8333333333333334, 0.96, 0.8275862068965517, 0.875, 1.0, 0.4578313253012048, 0.875, 0.875, 0.6794871794871795, 0.8333333333333334, 0.9333333333333333, 0.4855072463768116, 0.8333333333333334, 0.7741935483870968, 0.4621212121212121, 0.8333333333333334, 0.6976744186046512, 0.7586206896551724, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 0.4578313253012048, 0.8333333333333334, 0.9375, 1.0, 0.875, 0.7, 0.4339622641509434, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.8690476190476191, 0.875, 0.9090909090909091, 0.6829268292682927, 0.875, 1.0, 0.9615384615384616, 0.875, 1.0, 0.9285714285714286, 0.875, 0.9090909090909091, 0.5796178343949044, 0.875, 0.8888888888888888, 0.759493670886076, 0.875, 0.9090909090909091, 0.5495495495495496, 0.8333333333333334, 0.8135593220338984, 0.7567567567567568, 0.875, 1.0, 0.6967741935483871, 0.875, 0.9090909090909091, 1.0, 0.875, 1.0, 1.0, 0.875, 0.75, 0.5373134328358209, 0.875, 0.9444444444444444, 0.7575757575757576, 0.875, 1.0, 0.6929824561403509, 0.875, 0.8571428571428571, 0.7478260869565218, 0.875, 0.9230769230769231, 0.6984924623115578, 0.875, 1.0, 0.6363636363636364, 0.875, 0.875, 0.6324324324324324, 0.875, 0.875, 0.5505050505050505, 0.875, 1.0, 0.5636942675159236, 0.875, 0.9, 0.48704663212435234, 0.875, 0.9230769230769231, 0.6831683168316832, 0.875, 0.9, 0.5428571428571428, 0.875, 1.0, 1.0, 0.875, 0.6769911504424779, 0.8260869565217391, 0.875, 0.875, 0.6206896551724138, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9090909090909091, 0.5430711610486891, 0.8333333333333334, 0.875, 0.9285714285714286, 0.8333333333333334, 0.8333333333333334, 0.5104602510460251, 0.8333333333333334, 0.9259259259259259, 0.6952380952380952, 0.8333333333333334, 1.0, 0.6060606060606061, 0.8333333333333334, 1.0, 0.6129032258064516, 0.8333333333333334, 0.9333333333333333, 0.8307692307692308, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 0.7662337662337663, 0.8333333333333334, 0.9444444444444444, 1.0, 0.875, 1.0, 0.6058394160583942, 0.875, 0.64, 0.5966386554621849, 0.8333333333333334, 0.7647058823529411, 0.8461538461538461, 0.875, 1.0, 1.0, 0.8333333333333334, 0.7761194029850746, 0.7368421052631579, 0.8333333333333334, 1.0, 0.5370370370370371, 0.875, 1.0, 1.0, 0.875, 1.0, 0.8297872340425532, 0.8333333333333334, 0.875, 0.8809523809523809, 0.875, 1.0, 0.6906474820143885, 0.875, 1.0, 0.696969696969697, 0.8333333333333334, 0.7901234567901234, 0.88, 0.875, 0.8, 0.6007604562737643, 0.875, 1.0, 1.0, 0.875, 1.0, 1.0, 0.8333333333333334, 0.762589928057554, 0.7073170731707317, 0.875, 1.0, 0.7, 0.875, 1.0, 0.6576576576576577, 0.875, 1.0, 0.8235294117647058, 0.875, 1.0, 0.6590909090909091, 0.875, 1.0, 0.9310344827586207, 0.875, 0.8888888888888888, 1.0, 0.875, 1.0, 0.8888888888888888, 0.875, 1.0, 0.625, 0.875, 1.0, 0.5402298850574713, 0.875, 1.0, 0.6781115879828327, 0.8333333333333334, 0.8823529411764706, 0.8, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.7710843373493976, 0.875, 1.0, 0.5012987012987012, 0.875, 1.0, 0.7692307692307693, 0.8333333333333334, 0.8235294117647058, 0.6842105263157895, 0.875, 1.0, 0.9166666666666666, 0.875, 0.8421052631578947, 0.875, 0.875, 1.0, 0.6096491228070176, 0.8333333333333334, 1.0, 0.8787878787878788, 0.875, 1.0, 0.6305732484076433, 0.875, 1.0, 0.9411764705882353, 0.8333333333333334, 1.0, 0.5374449339207048, 0.8333333333333334, 1.0, 0.3532608695652174, 0.875, 1.0, 0.96, 0.8333333333333334, 0.9090909090909091, 0.8571428571428571, 0.875, 0.9375, 0.4444444444444444, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.78125, 0.8333333333333334, 0.9230769230769231, 0.7192982456140351, 0.8333333333333334, 1.0, 0.8636363636363636, 0.8333333333333334, 1.0, 0.7777777777777778, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 0.7, 0.8333333333333334, 0.7837837837837838, 0.9629629629629629, 0.8333333333333334, 0.95, 1.0, 0.8333333333333334, 0.875, 0.9285714285714286, 0.875, 1.0, 0.90625, 0.8333333333333334, 1.0, 0.58, 0.875, 1.0, 0.5349794238683128, 0.875, 0.9285714285714286, 0.64, 0.875, 0.9230769230769231, 0.6342281879194631, 0.875, 0.8571428571428571, 0.5578947368421052, 0.875, 1.0, 0.5284280936454849, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 0.6792452830188679, 0.8333333333333334, 0.6509433962264151, 0.8627450980392157, 0.8333333333333334, 0.8, 0.875, 0.8333333333333334, 0.8235294117647058, 0.8333333333333334, 0.8333333333333334, 0.9333333333333333, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 0.547945205479452, 0.8333333333333334, 0.8888888888888888, 0.8, 0.875, 1.0, 0.7114093959731543, 0.875, 1.0, 1.0, 0.875, 0.9333333333333333, 0.5853658536585366, 0.875, 1.0, 0.5325670498084292, 0.875, 0.8888888888888888, 0.5798319327731093, 0.875, 1.0, 0.5923076923076923, 0.875, 1.0, 0.5805243445692884, 0.875, 1.0, 0.4918032786885246, 0.875, 1.0, 0.7301587301587301, 0.875, 0.8, 0.8, 0.8333333333333334, 0.7027027027027027, 0.59375, 0.8333333333333334, 1.0, 0.6025641025641025, 0.875, 0.9090909090909091, 0.6424242424242425, 0.8333333333333334, 0.9411764705882353, 0.7111111111111111, 0.875, 1.0, 1.0, 0.8333333333333334, 0.85, 1.0, 0.875, 0.8571428571428571, 1.0, 0.875, 0.8888888888888888, 0.6071428571428571, 0.875, 1.0, 0.5483870967741935, 0.875, 0.8, 0.5603112840466926, 0.875, 1.0, 0.6194690265486725, 0.8333333333333334, 0.9, 0.6333333333333333, 0.8333333333333334, 0.9047619047619048, 0.6567164179104478, 0.875, 1.0, 0.7101449275362319, 0.875, 1.0, 0.49760765550239233, 0.875, 1.0, 0.47540983606557374, 0.875, 0.9230769230769231, 0.7894736842105263, 0.875, 0.8333333333333334, 0.5127118644067796, 0.8333333333333334, 0.782608695652174, 0.6639344262295082, 0.875, 0.9, 0.5072463768115942, 0.8333333333333334, 0.95, 1.0, 0.8333333333333334, 0.9393939393939394, 1.0, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.8695652173913043, 0.717391304347826, 0.8333333333333334, 0.9354838709677419, 0.7619047619047619, 0.875, 0.9230769230769231, 0.6918238993710691, 0.875, 1.0, 0.6481481481481481, 0.875, 1.0, 0.675, 0.8333333333333334, 1.0, 0.5714285714285714, 0.8333333333333334, 0.95, 1.0, 0.875, 0.9230769230769231, 1.0, 0.875, 1.0, 0.5533596837944664, 0.8333333333333334, 0.8421052631578947, 0.7333333333333333, 0.875, 1.0, 0.5231316725978647, 0.875, 1.0, 0.6927710843373494, 0.875, 1.0, 1.0, 0.875, 0.875, 1.0, 0.875, 0.9090909090909091, 0.5829383886255924, 0.875, 0.8888888888888888, 0.7264150943396226, 0.875, 0.8888888888888888, 0.5075757575757576, 0.875, 0.8888888888888888, 0.6206896551724138, 0.8333333333333334, 0.8571428571428571, 0.6633663366336634, 0.875, 0.9285714285714286, 0.35858585858585856, 0.875, 0.7, 0.7804878048780488, 0.875, 1.0, 0.6525096525096525, 0.875, 0.8, 0.5568181818181818, 0.8333333333333334, 0.9047619047619048, 0.9090909090909091, 0.875, 1.0, 0.6598639455782312, 0.8333333333333334, 0.6774193548387096, 0.4563106796116505, 0.875, 0.95, 0.5101214574898786, 0.875, 1.0, 0.6401515151515151, 0.875, 1.0, 0.542016806722689, 0.875, 1.0, 0.5687732342007435, 0.875, 0.8888888888888888, 0.456575682382134, 0.875, 1.0, 0.6232558139534884, 0.8333333333333334, 0.9411764705882353, 1.0, 0.875, 1.0, 0.6846846846846847, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.7738095238095238, 0.8333333333333334, 1.0, 0.7023809523809523, 0.875, 1.0, 0.7435897435897436, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 0.7638888888888888, 0.875, 1.0, 0.6330935251798561, 0.875, 1.0, 0.6830985915492958, 0.875, 0.875, 1.0, 0.875, 1.0, 0.7647058823529411, 0.875, 0.8571428571428571, 0.5570032573289903, 0.875, 0.8695652173913043, 0.9166666666666666, 0.8333333333333334, 0.96, 1.0, 0.875, 1.0, 0.825, 0.875, 1.0, 0.9, 0.875, 0.8620689655172413, 1.0, 0.875, 1.0, 0.9473684210526315, 0.875, 0.8333333333333334, 0.5057471264367817, 0.8333333333333334, 0.9444444444444444, 0.7183098591549296, 0.875, 0.8571428571428571, 1.0, 0.875, 1.0, 0.8095238095238095, 0.875, 0.8888888888888888, 0.4940828402366864, 0.875, 1.0, 1.0, 0.875, 0.9375, 0.7727272727272727, 0.8333333333333334, 0.8222222222222222, 0.8070175438596491, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8461538461538461, 1.0, 0.875, 1.0, 0.647887323943662, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5565217391304348, 0.875, 0.8947368421052632, 0.41228070175438597, 0.875, 1.0, 0.6434426229508197, 0.875, 1.0, 0.6300813008130082, 0.8333333333333334, 0.7586206896551724, 0.4897959183673469, 0.875, 1.0, 0.7428571428571429, 0.875, 1.0, 0.6505376344086021, 0.8333333333333334, 0.8620689655172413, 0.5754716981132075, 0.8333333333333334, 0.9642857142857143, 0.9473684210526315, 0.8333333333333334, 0.95, 0.8709677419354839, 0.875, 0.9375, 0.9428571428571428, 0.8333333333333334, 0.8666666666666667, 1.0, 0.8333333333333334, 0.84, 1.0, 0.8333333333333334, 0.8709677419354839, 1.0, 0.8333333333333334, 0.9130434782608695, 1.0, 0.8333333333333334, 0.9, 0.8333333333333334, 0.875, 0.9230769230769231, 0.601123595505618, 0.8333333333333334, 0.7906976744186046, 0.7741935483870968, 0.875, 1.0, 0.7659574468085106, 0.8333333333333334, 0.9523809523809523, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8214285714285714, 1.0, 0.875, 1.0, 0.7446808510638298, 0.8333333333333334, 1.0, 0.6666666666666666, 0.875, 0.7857142857142857, 0.7323943661971831, 0.875, 1.0, 0.6330275229357798, 0.8333333333333334, 0.95, 0.8235294117647058, 0.8333333333333334, 0.8857142857142857, 0.5061728395061729, 0.875, 0.875, 0.6652173913043479, 0.8333333333333334, 0.92, 0.7428571428571429, 0.875, 1.0, 0.7123287671232876, 0.875, 0.9615384615384616, 1.0, 0.875, 0.8947368421052632, 0.7333333333333333, 0.875, 1.0, 0.6363636363636364, 0.875, 0.9, 0.6796875, 0.875, 1.0, 0.7710843373493976, 0.8333333333333334, 0.9772727272727273, 0.7755102040816326, 0.8333333333333334, 0.8888888888888888, 1.0, 0.8333333333333334, 0.95, 0.9333333333333333, 0.875, 1.0, 0.6842105263157895, 0.8333333333333334, 0.8333333333333334, 0.9333333333333333, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.9333333333333333, 0.8571428571428571, 0.8333333333333334, 1.0, 0.5698924731182796, 0.8333333333333334, 0.8260869565217391, 0.5878378378378378, 0.875, 1.0, 0.6077586206896551, 0.8333333333333334, 0.8148148148148148, 1.0, 0.875, 1.0, 0.7049180327868853, 0.875, 0.8888888888888888, 0.5276073619631901, 0.8333333333333334, 0.85, 0.9166666666666666, 0.8333333333333334, 0.8333333333333334, 1.0, 0.8333333333333334, 0.9230769230769231, 0.743801652892562, 0.8333333333333334, 0.8918918918918919, 0.9642857142857143, 0.8333333333333334, 1.0, 1.0, 0.875, 0.9285714285714286, 0.5846153846153846, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.9, 0.9090909090909091, 0.8333333333333334, 1.0, 0.6137566137566137, 0.875, 1.0, 0.6666666666666666, 0.8333333333333334, 0.875, 1.0, 0.8333333333333334, 0.9090909090909091, 0.8695652173913043, 0.875, 0.9, 0.5655737704918032, 0.8333333333333334, 1.0, 0.6204819277108434, 0.875, 1.0, 0.5867158671586716, 0.875, 1.0, 0.6303501945525292, 0.8333333333333334, 0.8823529411764706, 0.5631067961165048, 0.8333333333333334, 0.7333333333333333, 0.8448275862068966, 0.875, 1.0, 0.75, 0.875, 1.0, 0.696078431372549, 0.875, 1.0, 0.45454545454545453, 0.875, 1.0, 0.5068119891008175, 0.875, 0.875, 0.54, 0.875, 1.0, 0.45454545454545453, 0.8333333333333334, 0.9230769230769231, 0.5701357466063348, 0.875, 1.0, 0.5031847133757962, 0.875, 1.0, 0.5945945945945946, 0.8333333333333334, 1.0, 0.7307692307692307, 0.875, 0.8461538461538461, 0.5786350148367952, 0.8333333333333334, 0.9285714285714286, 0.9166666666666666, 0.875, 1.0, 0.356, 0.8333333333333334, 0.9032258064516129, 1.0, 0.8333333333333334, 0.7894736842105263, 0.7428571428571429, 0.875, 1.0, 0.61, 0.875, 1.0, 0.6510067114093959, 0.875, 1.0, 1.0, 0.875, 0.8461538461538461, 0.6153846153846154, 0.875, 0.9, 0.5722222222222222, 0.8333333333333334, 0.9259259259259259, 1.0, 0.875, 1.0, 0.6346153846153846, 0.875, 1.0, 0.5756097560975609, 0.875, 0.9090909090909091, 0.8846153846153846, 0.875, 1.0, 0.627177700348432, 0.875, 0.9473684210526315, 1.0, 0.875, 0.8888888888888888, 0.6145833333333334, 0.875, 1.0, 1.0, 0.875, 1.0, 0.6373626373626373, 0.875, 0.9333333333333333, 1.0, 0.875, 0.8823529411764706, 0.8, 0.875, 1.0, 0.5963302752293578, 0.8333333333333334, 1.0, 0.7536231884057971, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8709677419354839, 0.8947368421052632, 0.8333333333333334, 0.9411764705882353, 0.5901639344262295, 0.8333333333333334, 0.9166666666666666, 0.6048387096774194, 0.8333333333333334, 0.8461538461538461, 0.803921568627451, 0.8333333333333334, 1.0, 0.6602564102564102, 0.875, 0.9, 0.48905109489051096, 0.875, 0.8571428571428571, 0.6233766233766234], "type": "histogram"}];
            const layout = Object.assign({}, {"template": {"data": {"histogram2dcontour": [{"type": "histogram2dcontour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "choropleth": [{"type": "choropleth", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "histogram2d": [{"type": "histogram2d", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "heatmap": [{"type": "heatmap", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "contourcarpet": [{"type": "contourcarpet", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "contour": [{"type": "contour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "surface": [{"type": "surface", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "mesh3d": [{"type": "mesh3d", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "scatter": [{"fillpattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}, "type": "scatter"}], "parcoords": [{"type": "parcoords", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolargl": [{"type": "scatterpolargl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "bar"}], "scattergeo": [{"type": "scattergeo", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolar": [{"type": "scatterpolar", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "histogram": [{"marker": {"pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "histogram"}], "scattergl": [{"type": "scattergl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatter3d": [{"type": "scatter3d", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermap": [{"type": "scattermap", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermapbox": [{"type": "scattermapbox", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterternary": [{"type": "scatterternary", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattercarpet": [{"type": "scattercarpet", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "barpolar"}], "pie": [{"automargin": true, "type": "pie"}]}, "layout": {"autotypenumbers": "strict", "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "hovermode": "closest", "hoverlabel": {"align": "left"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"bgcolor": "#E5ECF6", "angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "ternary": {"bgcolor": "#E5ECF6", "aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]]}, "xaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "yaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "geo": {"bgcolor": "white", "landcolor": "#E5ECF6", "subunitcolor": "white", "showland": true, "showlakes": true, "lakecolor": "white"}, "title": {"x": 0.05}, "mapbox": {"style": "light"}}}, "margin": {"l": 50, "r": 30, "t": 30, "b": 50}, "font": {"color": "#a39e93"}, "xaxis": {"title": {"text": "Diversity Unique Words Ratio"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "yaxis": {"title": {"text": "Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "title": {}, "height": 400, "paper_bgcolor": "rgba(0,0,0,0)", "plot_bgcolor": "rgba(0,0,0,0)"}, darkLayout);
            Plotly.newPlot('chart_2', data, layout, plotlyConfig);
        })();
        
        (function() {
            const data = [{"marker": {"color": "#d4a574"}, "nbinsx": 30, "opacity": 0.85, "x": [0.875, 1.0, 0.743801652892562, 0.875, 1.0, 0.6792452830188679, 0.875, 1.0, 0.45933014354066987, 0.875, 1.0, 0.6620370370370371, 0.875, 0.8095238095238095, 0.5714285714285714, 0.875, 0.768, 0.8507462686567164, 0.8333333333333334, 1.0, 0.5161290322580645, 0.875, 0.9473684210526315, 0.5280898876404494, 0.8333333333333334, 0.9444444444444444, 0.9047619047619048, 0.875, 1.0, 0.6869565217391305, 0.875, 1.0, 1.0, 0.875, 0.8571428571428571, 0.6935483870967742, 0.875, 0.8571428571428571, 0.578397212543554, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.6449704142011834, 0.875, 1.0, 0.622093023255814, 0.8333333333333334, 0.8695652173913043, 0.5909090909090909, 0.875, 1.0, 0.6153846153846154, 0.875, 1.0, 0.8181818181818182, 0.875, 1.0, 0.9230769230769231, 0.875, 0.8181818181818182, 0.4849315068493151, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9375, 0.9375, 0.875, 1.0, 0.75, 0.875, 1.0, 0.5454545454545454, 0.875, 1.0, 0.9411764705882353, 0.875, 1.0, 0.5592105263157895, 0.8333333333333334, 1.0, 0.5172413793103449, 0.875, 1.0, 0.88, 0.875, 1.0, 0.45454545454545453, 0.8333333333333334, 0.8823529411764706, 0.8333333333333334, 0.875, 0.9090909090909091, 1.0, 0.875, 1.0, 0.5979381443298969, 0.875, 0.9, 0.5993265993265994, 0.8333333333333334, 0.9, 1.0, 0.8333333333333334, 0.8846153846153846, 0.7027027027027027, 0.875, 0.5555555555555556, 0.7857142857142857, 0.8333333333333334, 0.8166666666666667, 0.9523809523809523, 0.875, 0.7619047619047619, 0.9523809523809523, 0.875, 1.0, 0.6776859504132231, 0.875, 0.9166666666666666, 0.6923076923076923, 0.875, 1.0, 0.5197368421052632, 0.8333333333333334, 0.9411764705882353, 0.7058823529411765, 0.875, 1.0, 0.5092592592592593, 0.8333333333333334, 0.9375, 0.6886792452830188, 0.875, 1.0, 0.7522123893805309, 0.875, 1.0, 0.5593869731800766, 0.8333333333333334, 0.828125, 0.8541666666666666, 0.8333333333333334, 0.8604651162790697, 0.8717948717948718, 0.8333333333333334, 0.7741935483870968, 0.7857142857142857, 0.875, 1.0, 0.6923076923076923, 0.875, 0.7619047619047619, 1.0, 0.875, 0.8823529411764706, 0.9166666666666666, 0.875, 0.8666666666666667, 0.5657894736842105, 0.875, 0.9285714285714286, 0.5342465753424658, 0.875, 1.0, 0.6071428571428571, 0.8333333333333334, 1.0, 0.9230769230769231, 0.875, 1.0, 0.7619047619047619, 0.875, 1.0, 1.0, 0.875, 1.0, 0.7341772151898734, 0.875, 0.7540983606557377, 0.5393258426966292, 0.875, 1.0, 0.9230769230769231, 0.8333333333333334, 0.9411764705882353, 0.8888888888888888, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.9333333333333333, 0.6233183856502242, 0.8333333333333334, 0.9230769230769231, 0.8823529411764706, 0.8333333333333334, 0.8181818181818182, 0.8125, 0.8333333333333334, 0.9411764705882353, 0.9411764705882353, 0.8333333333333334, 0.9, 0.5907335907335908, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9230769230769231, 0.7619047619047619, 0.875, 1.0, 0.6063829787234043, 0.8333333333333334, 0.95, 1.0, 0.8333333333333334, 0.9375, 0.7021276595744681, 0.8333333333333334, 0.9230769230769231, 1.0, 0.875, 0.9090909090909091, 0.6363636363636364, 0.8333333333333334, 0.9130434782608695, 0.8461538461538461, 0.875, 0.875, 0.72, 0.875, 0.9166666666666666, 0.48135593220338985, 0.8333333333333334, 0.8888888888888888, 1.0, 0.8333333333333334, 0.9333333333333333, 0.5, 0.875, 1.0, 0.5525291828793775, 0.875, 1.0, 0.5972850678733032, 0.875, 1.0, 0.4603174603174603, 0.875, 1.0, 0.6650246305418719, 0.875, 0.7698412698412699, 0.8269230769230769, 0.8333333333333334, 1.0, 0.9142857142857143, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 0.9166666666666666, 0.6009615384615384, 0.8333333333333334, 0.8636363636363636, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.7203389830508474, 0.875, 0.6509090909090909, 0.7721518987341772, 0.875, 1.0, 0.92, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.6265822784810127, 0.875, 1.0, 0.6226415094339622, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8, 1.0, 0.875, 1.0, 0.6167664670658682, 0.875, 0.656934306569343, 1.0, 0.8333333333333334, 1.0, 0.7333333333333333, 0.875, 1.0, 0.8823529411764706, 0.875, 1.0, 0.7272727272727273, 0.875, 1.0, 0.5550660792951542, 0.8333333333333334, 0.825, 0.8076923076923077, 0.875, 1.0, 1.0, 0.875, 1.0, 0.9473684210526315, 0.8333333333333334, 0.8709677419354839, 1.0, 0.8333333333333334, 0.9047619047619048, 1.0, 0.8333333333333334, 0.8095238095238095, 0.875, 0.8333333333333334, 0.8823529411764706, 1.0, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.7727272727272727, 0.8148148148148148, 0.8333333333333334, 0.9333333333333333, 0.8421052631578947, 0.8333333333333334, 0.9375, 0.9090909090909091, 0.8333333333333334, 0.9285714285714286, 0.6825396825396826, 0.8333333333333334, 0.8571428571428571, 0.95, 0.8333333333333334, 0.9230769230769231, 0.9090909090909091, 0.8333333333333334, 0.7162162162162162, 0.8409090909090909, 0.875, 1.0, 0.6521739130434783, 0.8333333333333334, 1.0, 0.9090909090909091, 0.875, 0.875, 1.0, 0.8333333333333334, 1.0, 0.9302325581395349, 0.8333333333333334, 0.9166666666666666, 0.6290322580645161, 0.8333333333333334, 0.9230769230769231, 1.0, 0.875, 0.875, 0.7878787878787878, 0.8333333333333334, 0.8695652173913043, 1.0, 0.8333333333333334, 0.9375, 0.5833333333333334, 0.875, 0.7333333333333333, 0.45698924731182794, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.782608695652174, 1.0, 0.875, 1.0, 1.0, 0.875, 0.6346749226006192, 0.9333333333333333, 0.8333333333333334, 0.8333333333333334, 1.0, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 0.6756756756756757, 0.875, 0.9333333333333333, 0.7898550724637681, 0.8333333333333334, 0.8421052631578947, 1.0, 0.8333333333333334, 0.9444444444444444, 0.75, 0.875, 1.0, 0.8571428571428571, 0.8333333333333334, 0.8636363636363636, 0.734375, 0.8333333333333334, 0.9259259259259259, 0.9090909090909091, 0.875, 0.9090909090909091, 0.6388888888888888, 0.875, 0.8235294117647058, 0.6714285714285714, 0.8333333333333334, 1.0, 0.7047619047619048, 0.875, 0.9333333333333333, 0.6756756756756757, 0.875, 1.0, 0.5185185185185185, 0.8333333333333334, 1.0, 0.7446808510638298, 0.875, 1.0, 0.480225988700565, 0.8333333333333334, 0.95, 0.5346534653465347, 0.8333333333333334, 0.9523809523809523, 0.8888888888888888, 0.8333333333333334, 0.9565217391304348, 0.48520710059171596, 0.875, 0.84, 0.4933920704845815, 0.875, 0.6708333333333333, 0.74, 0.875, 0.9090909090909091, 0.5774647887323944, 0.875, 0.8333333333333334, 0.44285714285714284, 0.875, 0.8888888888888888, 0.6118421052631579, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 0.6470588235294118, 0.8490566037735849, 0.8333333333333334, 0.9333333333333333, 0.5510835913312694, 0.8333333333333334, 0.9444444444444444, 0.8552631578947368, 0.875, 0.9130434782608695, 1.0, 0.8333333333333334, 1.0, 0.5503355704697986, 0.8333333333333334, 0.9230769230769231, 0.9285714285714286, 0.875, 1.0, 0.9473684210526315, 0.875, 1.0, 1.0, 0.875, 0.875, 0.6343283582089553, 0.8333333333333334, 0.8611111111111112, 0.6267605633802817, 0.875, 0.9, 0.9411764705882353, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9130434782608695, 0.5871886120996441, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 0.5721153846153846, 0.8333333333333334, 0.9090909090909091, 0.638095238095238, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 0.9583333333333334, 0.8888888888888888, 0.8333333333333334, 0.85, 1.0, 0.8333333333333334, 0.9, 0.9, 0.8333333333333334, 0.9230769230769231, 0.8163265306122449, 0.8333333333333334, 0.9230769230769231, 0.6634615384615384, 0.8333333333333334, 0.9090909090909091, 1.0, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.7543859649122807, 0.8333333333333334, 0.8333333333333334, 0.9545454545454546, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.9047619047619048, 0.8333333333333334, 0.9333333333333333, 1.0, 0.8333333333333334, 0.875, 1.0, 0.8333333333333334, 0.75, 0.8666666666666667, 0.8333333333333334, 1.0, 0.9090909090909091, 0.8333333333333334, 0.875, 0.875, 0.8333333333333334, 0.9523809523809523, 0.7333333333333333, 0.875, 0.9166666666666666, 0.711864406779661, 0.8333333333333334, 1.0, 0.8, 0.875, 0.7777777777777778, 0.6737967914438503, 0.875, 0.8888888888888888, 0.9285714285714286, 0.875, 0.8888888888888888, 0.7019867549668874, 0.875, 1.0, 0.7254901960784313, 0.8333333333333334, 0.9, 1.0, 0.875, 0.8571428571428571, 0.5791245791245792, 0.875, 0.8888888888888888, 0.6325301204819277, 0.875, 0.8333333333333334, 0.5390946502057613, 0.875, 1.0, 1.0, 0.875, 0.875, 0.8333333333333334, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9032258064516129, 0.875, 0.8333333333333334, 1.0, 0.44936708860759494, 0.8333333333333334, 0.7053571428571429, 1.0, 0.8333333333333334, 0.8260869565217391, 0.589041095890411, 0.8333333333333334, 1.0, 0.8461538461538461, 0.875, 1.0, 0.7209302325581395, 0.875, 1.0, 1.0, 0.875, 0.75, 0.45, 0.8333333333333334, 0.8888888888888888, 0.4774774774774775, 0.8333333333333334, 0.8387096774193549, 0.7023809523809523, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8591549295774648, 1.0, 0.875, 1.0, 0.5688073394495413, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.9230769230769231, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.8548387096774194, 0.875, 1.0, 0.7142857142857143, 0.875, 0.875, 0.5920398009950248, 0.875, 1.0, 0.7304964539007093, 0.8333333333333334, 0.9333333333333333, 0.75, 0.875, 1.0, 0.9230769230769231, 0.875, 0.96, 0.8, 0.875, 1.0, 0.6106870229007634, 0.875, 1.0, 0.8095238095238095, 0.875, 0.9, 0.7619047619047619, 0.8333333333333334, 0.9411764705882353, 0.8205128205128205, 0.875, 0.875, 0.512987012987013, 0.875, 1.0, 0.7019230769230769, 0.8333333333333334, 0.9473684210526315, 1.0, 0.875, 1.0, 0.7674418604651163, 0.875, 1.0, 1.0, 0.875, 0.8461538461538461, 0.8035714285714286, 0.875, 0.9, 0.9333333333333333, 0.875, 0.875, 0.9, 0.875, 1.0, 0.5909090909090909, 0.875, 1.0, 0.4854368932038835, 0.875, 1.0, 0.8181818181818182, 0.875, 1.0, 0.8837209302325582, 0.875, 0.9285714285714286, 1.0, 0.875, 1.0, 0.6105263157894737, 0.875, 0.8571428571428571, 0.5169491525423728, 0.875, 0.7777777777777778, 0.7777777777777778, 0.875, 0.8333333333333334, 0.6030534351145038, 0.875, 1.0, 0.5604838709677419, 0.875, 1.0, 0.5789473684210527, 0.875, 0.875, 0.5511811023622047, 0.875, 0.8888888888888888, 0.9285714285714286, 0.875, 1.0, 0.95, 0.875, 1.0, 0.6272727272727273, 0.875, 1.0, 0.6032608695652174, 0.875, 0.8888888888888888, 0.6666666666666666, 0.875, 0.9, 0.5418502202643172, 0.875, 0.9090909090909091, 0.5825688073394495, 0.875, 1.0, 0.5429553264604811, 0.875, 1.0, 1.0, 0.875, 0.8571428571428571, 0.7916666666666666, 0.875, 1.0, 0.6035714285714285, 0.8333333333333334, 0.7142857142857143, 1.0, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.875, 0.8260869565217391, 0.5137614678899083, 0.875, 1.0, 0.5936254980079682, 0.875, 0.9, 0.5693069306930693, 0.875, 1.0, 0.8333333333333334, 0.875, 1.0, 0.8636363636363636, 0.8333333333333334, 0.9375, 0.8235294117647058, 0.8333333333333334, 0.8833333333333333, 1.0, 0.875, 0.9166666666666666, 0.918918918918919, 0.8333333333333334, 0.9444444444444444, 1.0, 0.8333333333333334, 0.96875, 1.0, 0.8333333333333334, 0.9473684210526315, 1.0, 0.8333333333333334, 0.6666666666666666, 0.8709677419354839, 0.875, 1.0, 0.6418918918918919, 0.875, 0.9, 0.6967213114754098, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.88, 1.0, 0.8333333333333334, 0.696969696969697, 0.8181818181818182, 0.875, 0.8125, 0.5819672131147541, 0.875, 0.9285714285714286, 0.8157894736842105, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.8260869565217391, 0.6521739130434783, 0.875, 1.0, 0.8636363636363636, 0.8333333333333334, 0.9166666666666666, 0.7702702702702703, 0.8333333333333334, 0.9375, 0.8888888888888888, 0.8333333333333334, 1.0, 0.7741935483870968, 0.8333333333333334, 0.7692307692307693, 0.6702127659574468, 0.8333333333333334, 0.9090909090909091, 0.6063348416289592, 0.8333333333333334, 0.9259259259259259, 0.9444444444444444, 0.8333333333333334, 0.95, 0.5233644859813084, 0.875, 1.0, 0.6610169491525424, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5362318840579711, 0.8333333333333334, 1.0, 0.5217391304347826, 0.8333333333333334, 0.8823529411764706, 0.6349206349206349, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9, 1.0, 0.8333333333333334, 1.0, 0.5757575757575758, 0.8333333333333334, 0.8571428571428571, 0.5938566552901023, 0.875, 1.0, 0.6016949152542372, 0.875, 1.0, 0.7721518987341772, 0.875, 1.0, 0.6618705035971223, 0.875, 1.0, 0.6612903225806451, 0.875, 1.0, 0.8089887640449438, 0.875, 1.0, 0.7021276595744681, 0.8333333333333334, 0.875, 0.7222222222222222, 0.875, 1.0, 0.672566371681416, 0.875, 1.0, 0.5767441860465117, 0.875, 1.0, 0.6259541984732825, 0.875, 1.0, 0.6635071090047393, 0.875, 1.0, 0.7058823529411765, 0.875, 1.0, 0.6061946902654868, 0.8333333333333334, 0.782608695652174, 0.49019607843137253, 0.8333333333333334, 0.7840909090909091, 0.7956989247311828, 0.875, 0.95, 1.0, 0.875, 1.0, 0.5264705882352941, 0.875, 0.9230769230769231, 0.9032258064516129, 0.8333333333333334, 0.782608695652174, 0.6031746031746031, 0.875, 1.0, 0.5685131195335277, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.7692307692307693, 0.7857142857142857, 0.875, 1.0, 0.5892116182572614, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 0.7066666666666667, 0.8444444444444444, 0.875, 1.0, 1.0, 0.875, 1.0, 0.551219512195122, 0.8333333333333334, 0.8461538461538461, 0.9, 0.875, 1.0, 0.5475113122171946, 0.8333333333333334, 0.759493670886076, 0.7962962962962963, 0.875, 1.0, 0.625, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.9444444444444444, 0.875, 1.0, 0.4831168831168831, 0.875, 0.9090909090909091, 0.6225490196078431, 0.875, 1.0, 0.5523465703971119, 0.8333333333333334, 0.8461538461538461, 0.625531914893617, 0.875, 0.9, 0.6976744186046512, 0.875, 1.0, 0.6601941747572816, 0.875, 1.0, 0.6511627906976745, 0.875, 1.0, 0.7049180327868853, 0.8333333333333334, 0.9090909090909091, 1.0, 0.8333333333333334, 0.8852459016393442, 0.8837209302325582, 0.8333333333333334, 0.8571428571428571, 0.9444444444444444, 0.8333333333333334, 0.9333333333333333, 0.7340425531914894, 0.8333333333333334, 0.825, 0.9166666666666666, 0.8333333333333334, 1.0, 0.7142857142857143, 0.8333333333333334, 0.9565217391304348, 0.7477477477477478, 0.8333333333333334, 0.95, 0.9, 0.8333333333333334, 1.0, 0.88, 0.875, 1.0, 0.7634408602150538, 0.8333333333333334, 0.9444444444444444, 1.0, 0.8333333333333334, 0.7857142857142857, 0.6714285714285714, 0.8333333333333334, 0.9, 0.8387096774193549, 0.8333333333333334, 0.9090909090909091, 0.9333333333333333, 0.875, 0.9, 1.0, 0.875, 1.0, 0.678082191780822, 0.8333333333333334, 0.8421052631578947, 0.8571428571428571, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.5957446808510638, 0.875, 0.8571428571428571, 1.0, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.6548223350253807, 0.875, 1.0, 0.5668202764976958, 0.8333333333333334, 1.0, 0.6161616161616161, 0.875, 1.0, 0.6785714285714286, 0.875, 1.0, 1.0, 0.875, 1.0, 0.7589285714285714, 0.875, 1.0, 0.7387387387387387, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5072463768115942, 0.875, 0.9090909090909091, 0.7878787878787878, 0.875, 1.0, 0.5141509433962265, 0.875, 0.8888888888888888, 0.47435897435897434, 0.875, 0.8571428571428571, 0.7444444444444445, 0.875, 1.0, 0.48044692737430167, 0.875, 1.0, 0.6178861788617886, 0.875, 1.0, 0.5194174757281553, 0.875, 1.0, 0.6608187134502924, 0.875, 1.0, 0.7246376811594203, 0.8333333333333334, 0.7441860465116279, 0.9166666666666666, 0.875, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8611111111111112, 0.8260869565217391, 0.875, 1.0, 0.9473684210526315, 0.8333333333333334, 0.7692307692307693, 0.8181818181818182, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 1.0, 0.875, 0.8666666666666667, 0.6271186440677966, 0.875, 0.9, 1.0, 0.875, 1.0, 0.5261437908496732, 0.875, 1.0, 0.75, 0.875, 0.5161290322580645, 0.40828402366863903, 0.875, 0.8571428571428571, 1.0, 0.875, 1.0, 0.6, 0.875, 1.0, 0.7159090909090909, 0.875, 0.8333333333333334, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9285714285714286, 0.84, 0.875, 1.0, 0.8, 0.875, 0.9, 0.8421052631578947, 0.875, 1.0, 0.8064516129032258, 0.875, 0.9090909090909091, 0.9473684210526315, 0.875, 1.0, 1.0, 0.875, 1.0, 0.9285714285714286, 0.875, 1.0, 0.6715328467153284, 0.875, 1.0, 0.5760869565217391, 0.875, 1.0, 0.5741626794258373, 0.875, 1.0, 0.788135593220339, 0.875, 1.0, 0.5024390243902439, 0.875, 0.8571428571428571, 0.5819672131147541, 0.875, 1.0, 0.6030927835051546, 0.875, 1.0, 0.5522388059701493, 0.875, 1.0, 0.7469879518072289, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.5536723163841808, 0.875, 0.875, 0.9787234042553191, 0.8333333333333334, 1.0, 0.9354838709677419, 0.8333333333333334, 0.9333333333333333, 1.0, 0.8333333333333334, 0.9, 1.0, 0.8333333333333334, 1.0, 0.8333333333333334, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 0.9444444444444444, 0.7272727272727273, 0.8333333333333334, 0.8666666666666667, 0.875, 0.8333333333333334, 0.9285714285714286, 0.7127659574468085, 0.8333333333333334, 1.0, 0.49538461538461537, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.4481132075471698, 0.875, 1.0, 0.6589595375722543, 0.8333333333333334, 0.8064516129032258, 0.8214285714285714, 0.875, 1.0, 0.5407407407407407, 0.875, 1.0, 0.6359223300970874, 0.875, 1.0, 0.7052631578947368, 0.8333333333333334, 0.8260869565217391, 1.0, 0.875, 0.8, 0.6730769230769231, 0.875, 1.0, 0.5335570469798657, 0.875, 1.0, 0.569620253164557, 0.875, 0.8571428571428571, 0.8709677419354839, 0.875, 0.8571428571428571, 0.5310077519379846, 0.875, 1.0, 0.6875, 0.8333333333333334, 0.8260869565217391, 0.9411764705882353, 0.875, 0.9166666666666666, 0.9230769230769231, 0.8333333333333334, 0.9523809523809523, 0.825, 0.875, 1.0, 0.6270270270270271, 0.875, 1.0, 0.6830357142857143, 0.875, 0.8181818181818182, 0.5357142857142857, 0.875, 0.8888888888888888, 0.8804347826086957, 0.875, 1.0, 0.6370967741935484, 0.875, 1.0, 0.572139303482587, 0.875, 0.8666666666666667, 0.7872340425531915, 0.875, 1.0, 0.5203252032520326, 0.875, 1.0, 0.5486111111111112, 0.875, 0.9090909090909091, 1.0, 0.875, 1.0, 0.605, 0.8333333333333334, 1.0, 0.7272727272727273, 0.875, 0.9090909090909091, 0.625, 0.875, 0.9090909090909091, 0.6875, 0.875, 1.0, 0.517799352750809, 0.8333333333333334, 0.7954545454545454, 0.75, 0.875, 1.0, 0.49707602339181284, 0.875, 1.0, 0.5290322580645161, 0.875, 1.0, 0.5415162454873647, 0.875, 1.0, 0.4642857142857143, 0.8333333333333334, 0.9090909090909091, 0.9411764705882353, 0.875, 0.8181818181818182, 1.0, 0.875, 0.7857142857142857, 0.5782312925170068, 0.8333333333333334, 0.8135593220338984, 0.8928571428571429, 0.8333333333333334, 0.9090909090909091, 0.6766917293233082, 0.8333333333333334, 0.9523809523809523, 0.9090909090909091, 0.8333333333333334, 0.875, 0.9090909090909091, 0.875, 1.0, 0.6163522012578616, 0.8333333333333334, 0.9444444444444444, 0.6956521739130435, 0.875, 1.0, 0.7340425531914894, 0.875, 0.9333333333333333, 0.7107438016528925, 0.875, 1.0, 0.6404494382022472, 0.8333333333333334, 0.782608695652174, 0.8181818181818182, 0.875, 1.0, 0.8529411764705882, 0.875, 1.0, 0.8653846153846154, 0.875, 0.8571428571428571, 0.6355140186915887, 0.875, 0.9444444444444444, 0.7142857142857143, 0.875, 0.8333333333333334, 0.96875, 0.875, 1.0, 0.7804878048780488, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 1.0, 0.6666666666666666, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.5454545454545454, 0.8333333333333334, 0.9545454545454546, 0.8333333333333334, 0.875, 0.8888888888888888, 0.7297297297297297, 0.8333333333333334, 0.64, 0.8461538461538461, 0.875, 0.9333333333333333, 0.9019607843137255, 0.875, 1.0, 0.6580645161290323, 0.8333333333333334, 0.9375, 0.8, 0.8333333333333334, 1.0, 0.7894736842105263, 0.875, 0.8888888888888888, 0.5161290322580645, 0.875, 1.0, 0.6691176470588235, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.6902173913043478, 0.875, 1.0, 0.6047904191616766, 0.8333333333333334, 0.9642857142857143, 0.8571428571428571, 0.8333333333333334, 0.967741935483871, 0.6198347107438017, 0.875, 0.8823529411764706, 0.6136363636363636, 0.875, 1.0, 0.6704545454545454, 0.875, 1.0, 0.5128865979381443, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8780487804878049, 0.8857142857142857, 0.8333333333333334, 0.7904761904761904, 0.8676470588235294, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 0.918918918918919, 0.875, 0.9, 0.9090909090909091, 0.8333333333333334, 1.0, 0.65625, 0.8333333333333334, 0.8108108108108109, 1.0, 0.875, 0.9090909090909091, 0.5169491525423728, 0.875, 1.0, 0.5882352941176471, 0.8333333333333334, 1.0, 0.7941176470588235, 0.875, 1.0, 0.7931034482758621, 0.875, 0.9166666666666666, 0.8518518518518519, 0.875, 0.8, 0.5978260869565217, 0.875, 0.9230769230769231, 0.5114503816793893, 0.875, 1.0, 0.654054054054054, 0.875, 0.875, 1.0, 0.875, 1.0, 0.6716417910447762, 0.875, 1.0, 0.5531914893617021, 0.875, 1.0, 0.484, 0.875, 0.875, 0.6666666666666666, 0.875, 0.8823529411764706, 1.0, 0.875, 0.9230769230769231, 0.49795918367346936, 0.8333333333333334, 0.9230769230769231, 0.8372093023255814, 0.875, 1.0, 1.0, 0.875, 0.8888888888888888, 0.5256064690026954, 0.875, 0.9230769230769231, 0.5626740947075209, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 0.6555555555555556, 0.875, 0.6, 0.8461538461538461, 0.8333333333333334, 0.9166666666666666, 0.7368421052631579, 0.875, 0.9090909090909091, 0.5772058823529411, 0.875, 1.0, 0.7151898734177216, 0.875, 1.0, 0.6347826086956522, 0.875, 1.0, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 0.6666666666666666, 0.875, 0.8888888888888888, 0.6044776119402985, 0.875, 1.0, 0.8723404255319149, 0.875, 0.9, 0.6054054054054054, 0.875, 1.0, 0.5179282868525896, 0.875, 1.0, 0.796875, 0.875, 0.7692307692307693, 0.5522388059701493, 0.875, 0.9375, 1.0, 0.875, 1.0, 0.5897435897435898, 0.875, 0.8571428571428571, 0.7037037037037037, 0.875, 0.7894736842105263, 0.8181818181818182, 0.875, 0.9333333333333333, 0.9285714285714286, 0.875, 1.0, 0.7112676056338029, 0.875, 0.9090909090909091, 0.5562310030395137, 0.8333333333333334, 0.9473684210526315, 0.6605504587155964, 0.8333333333333334, 0.8636363636363636, 0.6781609195402298, 0.875, 1.0, 0.6244897959183674, 0.875, 0.9230769230769231, 0.7631578947368421, 0.8333333333333334, 0.9090909090909091, 0.6896551724137931, 0.8333333333333334, 0.9545454545454546, 0.9642857142857143, 0.875, 1.0, 0.7530864197530864, 0.875, 0.9411764705882353, 0.896551724137931, 0.875, 1.0, 0.6923076923076923, 0.875, 0.9230769230769231, 0.9117647058823529, 0.8333333333333334, 0.8823529411764706, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9545454545454546, 0.65625, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9166666666666666, 0.578125, 0.8333333333333334, 0.9047619047619048, 0.6226415094339622, 0.8333333333333334, 0.8387096774193549, 1.0, 0.8333333333333334, 0.7647058823529411, 1.0, 0.8333333333333334, 0.8947368421052632, 0.9, 0.8333333333333334, 0.9565217391304348, 0.7590361445783133, 0.875, 1.0, 0.7777777777777778, 0.8333333333333334, 0.8095238095238095, 0.9090909090909091, 0.8333333333333334, 0.8695652173913043, 0.8888888888888888, 0.875, 1.0, 0.775, 0.8333333333333334, 0.8846153846153846, 0.9375, 0.8333333333333334, 0.9130434782608695, 1.0, 0.875, 0.9130434782608695, 0.5267175572519084, 0.875, 1.0, 0.6882352941176471, 0.875, 1.0, 0.7425742574257426, 0.875, 0.7297297297297297, 0.8064516129032258, 0.875, 0.8085106382978723, 0.7846153846153846, 0.875, 0.9047619047619048, 0.6086956521739131, 0.875, 0.9090909090909091, 0.6991150442477876, 0.875, 1.0, 0.580952380952381, 0.875, 1.0, 0.5822784810126582, 0.875, 0.9, 0.8478260869565217, 0.8333333333333334, 0.9375, 0.8571428571428571, 0.875, 1.0, 0.6570048309178744, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8846153846153846, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9047619047619048, 1.0, 0.875, 0.9090909090909091, 0.4631578947368421, 0.8333333333333334, 0.847457627118644, 0.5014749262536873, 0.8333333333333334, 0.9473684210526315, 1.0, 0.875, 1.0, 0.96, 0.8333333333333334, 0.8524590163934426, 0.8604651162790697, 0.8333333333333334, 0.9473684210526315, 0.9, 0.875, 0.7777777777777778, 0.6276595744680851, 0.8333333333333334, 0.9090909090909091, 1.0, 0.8333333333333334, 0.7962962962962963, 0.88, 0.875, 0.8, 0.8, 0.8333333333333334, 0.8235294117647058, 0.48717948717948717, 0.8333333333333334, 1.0, 0.9166666666666666, 0.8333333333333334, 0.7037037037037037, 0.6192660550458715, 0.8333333333333334, 0.9473684210526315, 0.8472222222222222, 0.8333333333333334, 0.8823529411764706, 1.0, 0.8333333333333334, 0.8529411764705882, 0.6265060240963856, 0.875, 0.8888888888888888, 0.5296912114014252, 0.875, 0.8571428571428571, 0.8275862068965517, 0.875, 1.0, 0.7815126050420168, 0.8333333333333334, 1.0, 0.6595744680851063, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5292620865139949, 0.8333333333333334, 0.6666666666666666, 1.0, 0.875, 1.0, 0.9444444444444444, 0.875, 1.0, 0.5806451612903226, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.675, 0.875, 0.8888888888888888, 0.5942028985507246, 0.875, 1.0, 0.5138339920948617, 0.8333333333333334, 1.0, 1.0, 0.875, 0.8888888888888888, 1.0, 0.875, 1.0, 0.8666666666666667, 0.875, 1.0, 0.5658914728682171, 0.8333333333333334, 0.9, 0.6666666666666666, 0.875, 1.0, 0.43532338308457713, 0.8333333333333334, 0.9130434782608695, 0.8235294117647058, 0.875, 1.0, 0.5513513513513514, 0.8333333333333334, 0.8928571428571429, 0.9285714285714286, 0.875, 1.0, 0.6991150442477876, 0.8333333333333334, 0.8928571428571429, 0.875, 0.8333333333333334, 1.0, 0.56, 0.8333333333333334, 0.9130434782608695, 0.6447368421052632, 0.8333333333333334, 0.9444444444444444, 0.5725806451612904, 0.875, 0.8333333333333334, 0.525679758308157, 0.8333333333333334, 0.9230769230769231, 0.6078431372549019, 0.8333333333333334, 1.0, 0.9375, 0.8333333333333334, 0.9444444444444444, 1.0, 0.875, 1.0, 0.5504201680672269, 0.875, 1.0, 0.6262975778546713, 0.875, 1.0, 0.7666666666666667, 0.875, 0.875, 0.5957446808510638, 0.875, 1.0, 0.4638888888888889, 0.875, 1.0, 0.5621301775147929, 0.875, 0.8888888888888888, 0.46634615384615385, 0.875, 1.0, 0.5913978494623656, 0.875, 1.0, 0.6388888888888888, 0.875, 0.9090909090909091, 0.5317073170731708, 0.875, 0.9166666666666666, 0.5448275862068965, 0.875, 1.0, 0.6593406593406593, 0.875, 1.0, 0.6027397260273972, 0.8333333333333334, 0.7125, 0.7857142857142857, 0.875, 1.0, 0.9527027027027027, 0.8333333333333334, 1.0, 0.9230769230769231, 0.875, 1.0, 0.5372549019607843, 0.875, 1.0, 0.8181818181818182, 0.8333333333333334, 0.8, 0.9333333333333333, 0.8333333333333334, 0.96, 0.8275862068965517, 0.875, 1.0, 0.4578313253012048, 0.875, 0.875, 0.6794871794871795, 0.8333333333333334, 0.9333333333333333, 0.4855072463768116, 0.8333333333333334, 0.7741935483870968, 0.4621212121212121, 0.8333333333333334, 0.6976744186046512, 0.7586206896551724, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 0.4578313253012048, 0.8333333333333334, 0.9375, 1.0, 0.875, 0.7, 0.4339622641509434, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.8690476190476191, 0.875, 0.9090909090909091, 0.6829268292682927, 0.875, 1.0, 0.9615384615384616, 0.875, 1.0, 0.9285714285714286, 0.875, 0.9090909090909091, 0.5796178343949044, 0.875, 0.8888888888888888, 0.759493670886076, 0.875, 0.9090909090909091, 0.5495495495495496, 0.8333333333333334, 0.8135593220338984, 0.7567567567567568, 0.875, 1.0, 0.6967741935483871, 0.875, 0.9090909090909091, 1.0, 0.875, 1.0, 1.0, 0.875, 0.75, 0.5373134328358209, 0.875, 0.9444444444444444, 0.7575757575757576, 0.875, 1.0, 0.6929824561403509, 0.875, 0.8571428571428571, 0.7478260869565218, 0.875, 0.9230769230769231, 0.6984924623115578, 0.875, 1.0, 0.6363636363636364, 0.875, 0.875, 0.6324324324324324, 0.875, 0.875, 0.5505050505050505, 0.875, 1.0, 0.5636942675159236, 0.875, 0.9, 0.48704663212435234, 0.875, 0.9230769230769231, 0.6831683168316832, 0.875, 0.9, 0.5428571428571428, 0.875, 1.0, 1.0, 0.875, 0.6769911504424779, 0.8260869565217391, 0.875, 0.875, 0.6206896551724138, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9090909090909091, 0.5430711610486891, 0.8333333333333334, 0.875, 0.9285714285714286, 0.8333333333333334, 0.8333333333333334, 0.5104602510460251, 0.8333333333333334, 0.9259259259259259, 0.6952380952380952, 0.8333333333333334, 1.0, 0.6060606060606061, 0.8333333333333334, 1.0, 0.6129032258064516, 0.8333333333333334, 0.9333333333333333, 0.8307692307692308, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 0.7662337662337663, 0.8333333333333334, 0.9444444444444444, 1.0, 0.875, 1.0, 0.6058394160583942, 0.875, 0.64, 0.5966386554621849, 0.8333333333333334, 0.7647058823529411, 0.8461538461538461, 0.875, 1.0, 1.0, 0.8333333333333334, 0.7761194029850746, 0.7368421052631579, 0.8333333333333334, 1.0, 0.5370370370370371, 0.875, 1.0, 1.0, 0.875, 1.0, 0.8297872340425532, 0.8333333333333334, 0.875, 0.8809523809523809, 0.875, 1.0, 0.6906474820143885, 0.875, 1.0, 0.696969696969697, 0.8333333333333334, 0.7901234567901234, 0.88, 0.875, 0.8, 0.6007604562737643, 0.875, 1.0, 1.0, 0.875, 1.0, 1.0, 0.8333333333333334, 0.762589928057554, 0.7073170731707317, 0.875, 1.0, 0.7, 0.875, 1.0, 0.6576576576576577, 0.875, 1.0, 0.8235294117647058, 0.875, 1.0, 0.6590909090909091, 0.875, 1.0, 0.9310344827586207, 0.875, 0.8888888888888888, 1.0, 0.875, 1.0, 0.8888888888888888, 0.875, 1.0, 0.625, 0.875, 1.0, 0.5402298850574713, 0.875, 1.0, 0.6781115879828327, 0.8333333333333334, 0.8823529411764706, 0.8, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.7710843373493976, 0.875, 1.0, 0.5012987012987012, 0.875, 1.0, 0.7692307692307693, 0.8333333333333334, 0.8235294117647058, 0.6842105263157895, 0.875, 1.0, 0.9166666666666666, 0.875, 0.8421052631578947, 0.875, 0.875, 1.0, 0.6096491228070176, 0.8333333333333334, 1.0, 0.8787878787878788, 0.875, 1.0, 0.6305732484076433, 0.875, 1.0, 0.9411764705882353, 0.8333333333333334, 1.0, 0.5374449339207048, 0.8333333333333334, 1.0, 0.3532608695652174, 0.875, 1.0, 0.96, 0.8333333333333334, 0.9090909090909091, 0.8571428571428571, 0.875, 0.9375, 0.4444444444444444, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.78125, 0.8333333333333334, 0.9230769230769231, 0.7192982456140351, 0.8333333333333334, 1.0, 0.8636363636363636, 0.8333333333333334, 1.0, 0.7777777777777778, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 0.7, 0.8333333333333334, 0.7837837837837838, 0.9629629629629629, 0.8333333333333334, 0.95, 1.0, 0.8333333333333334, 0.875, 0.9285714285714286, 0.875, 1.0, 0.90625, 0.8333333333333334, 1.0, 0.58, 0.875, 1.0, 0.5349794238683128, 0.875, 0.9285714285714286, 0.64, 0.875, 0.9230769230769231, 0.6342281879194631, 0.875, 0.8571428571428571, 0.5578947368421052, 0.875, 1.0, 0.5284280936454849, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 0.6792452830188679, 0.8333333333333334, 0.6509433962264151, 0.8627450980392157, 0.8333333333333334, 0.8, 0.875, 0.8333333333333334, 0.8235294117647058, 0.8333333333333334, 0.8333333333333334, 0.9333333333333333, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 0.547945205479452, 0.8333333333333334, 0.8888888888888888, 0.8, 0.875, 1.0, 0.7114093959731543, 0.875, 1.0, 1.0, 0.875, 0.9333333333333333, 0.5853658536585366, 0.875, 1.0, 0.5325670498084292, 0.875, 0.8888888888888888, 0.5798319327731093, 0.875, 1.0, 0.5923076923076923, 0.875, 1.0, 0.5805243445692884, 0.875, 1.0, 0.4918032786885246, 0.875, 1.0, 0.7301587301587301, 0.875, 0.8, 0.8, 0.8333333333333334, 0.7027027027027027, 0.59375, 0.8333333333333334, 1.0, 0.6025641025641025, 0.875, 0.9090909090909091, 0.6424242424242425, 0.8333333333333334, 0.9411764705882353, 0.7111111111111111, 0.875, 1.0, 1.0, 0.8333333333333334, 0.85, 1.0, 0.875, 0.8571428571428571, 1.0, 0.875, 0.8888888888888888, 0.6071428571428571, 0.875, 1.0, 0.5483870967741935, 0.875, 0.8, 0.5603112840466926, 0.875, 1.0, 0.6194690265486725, 0.8333333333333334, 0.9, 0.6333333333333333, 0.8333333333333334, 0.9047619047619048, 0.6567164179104478, 0.875, 1.0, 0.7101449275362319, 0.875, 1.0, 0.49760765550239233, 0.875, 1.0, 0.47540983606557374, 0.875, 0.9230769230769231, 0.7894736842105263, 0.875, 0.8333333333333334, 0.5127118644067796, 0.8333333333333334, 0.782608695652174, 0.6639344262295082, 0.875, 0.9, 0.5072463768115942, 0.8333333333333334, 0.95, 1.0, 0.8333333333333334, 0.9393939393939394, 1.0, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.8695652173913043, 0.717391304347826, 0.8333333333333334, 0.9354838709677419, 0.7619047619047619, 0.875, 0.9230769230769231, 0.6918238993710691, 0.875, 1.0, 0.6481481481481481, 0.875, 1.0, 0.675, 0.8333333333333334, 1.0, 0.5714285714285714, 0.8333333333333334, 0.95, 1.0, 0.875, 0.9230769230769231, 1.0, 0.875, 1.0, 0.5533596837944664, 0.8333333333333334, 0.8421052631578947, 0.7333333333333333, 0.875, 1.0, 0.5231316725978647, 0.875, 1.0, 0.6927710843373494, 0.875, 1.0, 1.0, 0.875, 0.875, 1.0, 0.875, 0.9090909090909091, 0.5829383886255924, 0.875, 0.8888888888888888, 0.7264150943396226, 0.875, 0.8888888888888888, 0.5075757575757576, 0.875, 0.8888888888888888, 0.6206896551724138, 0.8333333333333334, 0.8571428571428571, 0.6633663366336634, 0.875, 0.9285714285714286, 0.35858585858585856, 0.875, 0.7, 0.7804878048780488, 0.875, 1.0, 0.6525096525096525, 0.875, 0.8, 0.5568181818181818, 0.8333333333333334, 0.9047619047619048, 0.9090909090909091, 0.875, 1.0, 0.6598639455782312, 0.8333333333333334, 0.6774193548387096, 0.4563106796116505, 0.875, 0.95, 0.5101214574898786, 0.875, 1.0, 0.6401515151515151, 0.875, 1.0, 0.542016806722689, 0.875, 1.0, 0.5687732342007435, 0.875, 0.8888888888888888, 0.456575682382134, 0.875, 1.0, 0.6232558139534884, 0.8333333333333334, 0.9411764705882353, 1.0, 0.875, 1.0, 0.6846846846846847, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.7738095238095238, 0.8333333333333334, 1.0, 0.7023809523809523, 0.875, 1.0, 0.7435897435897436, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 0.7638888888888888, 0.875, 1.0, 0.6330935251798561, 0.875, 1.0, 0.6830985915492958, 0.875, 0.875, 1.0, 0.875, 1.0, 0.7647058823529411, 0.875, 0.8571428571428571, 0.5570032573289903, 0.875, 0.8695652173913043, 0.9166666666666666, 0.8333333333333334, 0.96, 1.0, 0.875, 1.0, 0.825, 0.875, 1.0, 0.9, 0.875, 0.8620689655172413, 1.0, 0.875, 1.0, 0.9473684210526315, 0.875, 0.8333333333333334, 0.5057471264367817, 0.8333333333333334, 0.9444444444444444, 0.7183098591549296, 0.875, 0.8571428571428571, 1.0, 0.875, 1.0, 0.8095238095238095, 0.875, 0.8888888888888888, 0.4940828402366864, 0.875, 1.0, 1.0, 0.875, 0.9375, 0.7727272727272727, 0.8333333333333334, 0.8222222222222222, 0.8070175438596491, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8461538461538461, 1.0, 0.875, 1.0, 0.647887323943662, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5565217391304348, 0.875, 0.8947368421052632, 0.41228070175438597, 0.875, 1.0, 0.6434426229508197, 0.875, 1.0, 0.6300813008130082, 0.8333333333333334, 0.7586206896551724, 0.4897959183673469, 0.875, 1.0, 0.7428571428571429, 0.875, 1.0, 0.6505376344086021, 0.8333333333333334, 0.8620689655172413, 0.5754716981132075, 0.8333333333333334, 0.9642857142857143, 0.9473684210526315, 0.8333333333333334, 0.95, 0.8709677419354839, 0.875, 0.9375, 0.9428571428571428, 0.8333333333333334, 0.8666666666666667, 1.0, 0.8333333333333334, 0.84, 1.0, 0.8333333333333334, 0.8709677419354839, 1.0, 0.8333333333333334, 0.9130434782608695, 1.0, 0.8333333333333334, 0.9, 0.8333333333333334, 0.875, 0.9230769230769231, 0.601123595505618, 0.8333333333333334, 0.7906976744186046, 0.7741935483870968, 0.875, 1.0, 0.7659574468085106, 0.8333333333333334, 0.9523809523809523, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8214285714285714, 1.0, 0.875, 1.0, 0.7446808510638298, 0.8333333333333334, 1.0, 0.6666666666666666, 0.875, 0.7857142857142857, 0.7323943661971831, 0.875, 1.0, 0.6330275229357798, 0.8333333333333334, 0.95, 0.8235294117647058, 0.8333333333333334, 0.8857142857142857, 0.5061728395061729, 0.875, 0.875, 0.6652173913043479, 0.8333333333333334, 0.92, 0.7428571428571429, 0.875, 1.0, 0.7123287671232876, 0.875, 0.9615384615384616, 1.0, 0.875, 0.8947368421052632, 0.7333333333333333, 0.875, 1.0, 0.6363636363636364, 0.875, 0.9, 0.6796875, 0.875, 1.0, 0.7710843373493976, 0.8333333333333334, 0.9772727272727273, 0.7755102040816326, 0.8333333333333334, 0.8888888888888888, 1.0, 0.8333333333333334, 0.95, 0.9333333333333333, 0.875, 1.0, 0.6842105263157895, 0.8333333333333334, 0.8333333333333334, 0.9333333333333333, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.9333333333333333, 0.8571428571428571, 0.8333333333333334, 1.0, 0.5698924731182796, 0.8333333333333334, 0.8260869565217391, 0.5878378378378378, 0.875, 1.0, 0.6077586206896551, 0.8333333333333334, 0.8148148148148148, 1.0, 0.875, 1.0, 0.7049180327868853, 0.875, 0.8888888888888888, 0.5276073619631901, 0.8333333333333334, 0.85, 0.9166666666666666, 0.8333333333333334, 0.8333333333333334, 1.0, 0.8333333333333334, 0.9230769230769231, 0.743801652892562, 0.8333333333333334, 0.8918918918918919, 0.9642857142857143, 0.8333333333333334, 1.0, 1.0, 0.875, 0.9285714285714286, 0.5846153846153846, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.9, 0.9090909090909091, 0.8333333333333334, 1.0, 0.6137566137566137, 0.875, 1.0, 0.6666666666666666, 0.8333333333333334, 0.875, 1.0, 0.8333333333333334, 0.9090909090909091, 0.8695652173913043, 0.875, 0.9, 0.5655737704918032, 0.8333333333333334, 1.0, 0.6204819277108434, 0.875, 1.0, 0.5867158671586716, 0.875, 1.0, 0.6303501945525292, 0.8333333333333334, 0.8823529411764706, 0.5631067961165048, 0.8333333333333334, 0.7333333333333333, 0.8448275862068966, 0.875, 1.0, 0.75, 0.875, 1.0, 0.696078431372549, 0.875, 1.0, 0.45454545454545453, 0.875, 1.0, 0.5068119891008175, 0.875, 0.875, 0.54, 0.875, 1.0, 0.45454545454545453, 0.8333333333333334, 0.9230769230769231, 0.5701357466063348, 0.875, 1.0, 0.5031847133757962, 0.875, 1.0, 0.5945945945945946, 0.8333333333333334, 1.0, 0.7307692307692307, 0.875, 0.8461538461538461, 0.5786350148367952, 0.8333333333333334, 0.9285714285714286, 0.9166666666666666, 0.875, 1.0, 0.356, 0.8333333333333334, 0.9032258064516129, 1.0, 0.8333333333333334, 0.7894736842105263, 0.7428571428571429, 0.875, 1.0, 0.61, 0.875, 1.0, 0.6510067114093959, 0.875, 1.0, 1.0, 0.875, 0.8461538461538461, 0.6153846153846154, 0.875, 0.9, 0.5722222222222222, 0.8333333333333334, 0.9259259259259259, 1.0, 0.875, 1.0, 0.6346153846153846, 0.875, 1.0, 0.5756097560975609, 0.875, 0.9090909090909091, 0.8846153846153846, 0.875, 1.0, 0.627177700348432, 0.875, 0.9473684210526315, 1.0, 0.875, 0.8888888888888888, 0.6145833333333334, 0.875, 1.0, 1.0, 0.875, 1.0, 0.6373626373626373, 0.875, 0.9333333333333333, 1.0, 0.875, 0.8823529411764706, 0.8, 0.875, 1.0, 0.5963302752293578, 0.8333333333333334, 1.0, 0.7536231884057971, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8709677419354839, 0.8947368421052632, 0.8333333333333334, 0.9411764705882353, 0.5901639344262295, 0.8333333333333334, 0.9166666666666666, 0.6048387096774194, 0.8333333333333334, 0.8461538461538461, 0.803921568627451, 0.8333333333333334, 1.0, 0.6602564102564102, 0.875, 0.9, 0.48905109489051096, 0.875, 0.8571428571428571, 0.6233766233766234], "type": "histogram"}];
            const layout = Object.assign({}, {"template": {"data": {"histogram2dcontour": [{"type": "histogram2dcontour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "choropleth": [{"type": "choropleth", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "histogram2d": [{"type": "histogram2d", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "heatmap": [{"type": "heatmap", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "contourcarpet": [{"type": "contourcarpet", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "contour": [{"type": "contour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "surface": [{"type": "surface", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "mesh3d": [{"type": "mesh3d", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "scatter": [{"fillpattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}, "type": "scatter"}], "parcoords": [{"type": "parcoords", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolargl": [{"type": "scatterpolargl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "bar"}], "scattergeo": [{"type": "scattergeo", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolar": [{"type": "scatterpolar", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "histogram": [{"marker": {"pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "histogram"}], "scattergl": [{"type": "scattergl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatter3d": [{"type": "scatter3d", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermap": [{"type": "scattermap", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermapbox": [{"type": "scattermapbox", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterternary": [{"type": "scatterternary", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattercarpet": [{"type": "scattercarpet", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "barpolar"}], "pie": [{"automargin": true, "type": "pie"}]}, "layout": {"autotypenumbers": "strict", "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "hovermode": "closest", "hoverlabel": {"align": "left"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"bgcolor": "#E5ECF6", "angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "ternary": {"bgcolor": "#E5ECF6", "aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]]}, "xaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "yaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "geo": {"bgcolor": "white", "landcolor": "#E5ECF6", "subunitcolor": "white", "showland": true, "showlakes": true, "lakecolor": "white"}, "title": {"x": 0.05}, "mapbox": {"style": "light"}}}, "margin": {"l": 50, "r": 30, "t": 30, "b": 50}, "font": {"color": "#a39e93"}, "xaxis": {"title": {"text": "Diversity Type Token Ratio"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "yaxis": {"title": {"text": "Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "title": {}, "height": 400, "paper_bgcolor": "rgba(0,0,0,0)", "plot_bgcolor": "rgba(0,0,0,0)"}, darkLayout);
            Plotly.newPlot('chart_3', data, layout, plotlyConfig);
        })();
        
        (function() {
            const data = [{"marker": {"color": "#d4a574"}, "nbinsx": 30, "opacity": 0.85, "x": [3.5, 2.4494897427831783, 8.181818181818182, 3.5, 2.4494897427831783, 4.944980302152805, 3.5, 2.4494897427831783, 6.640458853114317, 3.5, 2.4494897427831783, 9.729917589388736, 3.5, 3.709704134011871, 6.590035768383312, 3.5, 8.586501033599191, 6.963658328309397, 4.08248290463863, 3.464101615137755, 5.747369664856797, 3.5, 4.129483209670111, 8.629059865205674, 4.08248290463863, 4.00693842672377, 4.1461399144838555, 3.5, 2.23606797749979, 7.366787985098479, 3.5, 2.4494897427831783, 2.6457513110645903, 3.5, 3.2071349029490928, 7.723027987151322, 3.5, 2.2677868380553634, 9.798670179275858, 4.08248290463863, 3.8729833462074166, 2.82842712474619, 3.5, 3.162277660168379, 8.384615384615385, 3.5, 2.6457513110645903, 8.15867351279435, 4.08248290463863, 4.170288281141495, 4.800568148193976, 3.5, 3.162277660168379, 8.875203139603666, 3.5, 3.162277660168379, 2.7136021011998728, 3.5, 2.23606797749979, 4.706787243316417, 3.5, 2.7136021011998728, 9.264603429846783, 3.5, 2.4494897427831783, 5.2915026221291805, 4.08248290463863, 5.303300858899106, 3.75, 3.5, 2.6457513110645903, 6.7082039324993685, 3.5, 2.4494897427831783, 6.2667956144051224, 3.5, 2.6457513110645903, 3.8805700005813275, 3.5, 3.0, 9.750168689498874, 4.08248290463863, 3.7416573867739413, 3.9391929857916765, 3.5, 3.3166247903554, 4.4, 3.5, 2.82842712474619, 7.229988054812212, 4.08248290463863, 3.6380343755449944, 2.041241452319315, 3.5, 3.015113445777636, 5.477225575051661, 3.5, 2.23606797749979, 10.200060642633556, 3.5, 2.846049894151541, 10.328607586704738, 4.08248290463863, 4.024922359499621, 2.23606797749979, 4.08248290463863, 4.510671108178233, 4.27437366993929, 3.5, 2.8867513459481287, 5.092010548749033, 4.08248290463863, 6.325872798805447, 4.364357804719848, 3.5, 8.55235974119758, 4.364357804719848, 3.5, 2.4494897427831783, 7.454545454545454, 3.5, 3.1754264805429417, 4.323460152737352, 3.5, 2.6457513110645903, 9.061921487887188, 4.08248290463863, 3.8805700005813275, 7.129062309432055, 3.5, 3.8729833462074166, 5.292377467571569, 4.08248290463863, 3.75, 7.090386795208028, 3.5, 3.0, 7.996127381260576, 3.5, 2.6457513110645903, 9.037173124616524, 4.08248290463863, 6.625, 5.917840259193665, 4.08248290463863, 5.642447102306373, 5.4443572293729625, 4.08248290463863, 4.310527248642598, 2.9398736610366685, 3.5, 2.4494897427831783, 6.114295984380816, 3.5, 8.55235974119758, 2.6457513110645903, 3.5, 5.144957554275265, 4.4907311951024935, 3.5, 3.3565855667130946, 4.932438278217077, 3.5, 3.474396144861517, 4.564604740649092, 3.5, 2.6457513110645903, 5.564556201017806, 4.08248290463863, 4.123105625617661, 3.328201177351375, 3.5, 2.4494897427831783, 6.047431568147635, 3.5, 2.82842712474619, 2.82842712474619, 3.5, 2.82842712474619, 6.525509825370939, 3.5, 5.889696476913215, 5.087989824030529, 3.5, 2.4494897427831783, 3.328201177351375, 4.08248290463863, 3.8805700005813275, 2.6666666666666665, 4.08248290463863, 3.474396144861517, 2.4494897427831783, 4.08248290463863, 3.6147844564602556, 9.308128469535708, 4.08248290463863, 3.328201177351375, 3.6380343755449944, 4.08248290463863, 6.067798762169179, 4.596194077712559, 4.08248290463863, 3.8805700005813275, 3.8805700005813275, 4.08248290463863, 4.024922359499621, 9.506957419818361, 4.08248290463863, 3.474396144861517, 2.0, 4.08248290463863, 3.7416573867739413, 3.3166247903554, 4.08248290463863, 3.328201177351375, 4.937707198786941, 3.5, 3.464101615137755, 5.879101103675122, 4.08248290463863, 4.2485291572496005, 2.23606797749979, 4.08248290463863, 3.75, 4.813544719430521, 4.08248290463863, 3.328201177351375, 1.7320508075688774, 3.5, 3.015113445777636, 9.899494936611665, 4.08248290463863, 4.37880269519857, 3.05085107923876, 3.5, 4.286607049870562, 8.049844718999243, 3.5, 3.1754264805429417, 8.267559638302064, 4.08248290463863, 4.618802153517006, 1.0, 4.08248290463863, 3.6147844564602556, 9.055385138137416, 3.5, 3.0, 8.857716634035635, 3.5, 2.6457513110645903, 8.879280880751324, 3.5, 3.3166247903554, 6.328318816843779, 3.5, 3.0, 9.475142485638552, 3.5, 8.641446821835055, 5.963027109421214, 4.08248290463863, 3.8729833462074166, 5.408987230262506, 4.08248290463863, 3.474396144861517, 2.23606797749979, 3.5, 3.1754264805429417, 8.667190566019205, 4.08248290463863, 4.050813610756598, 2.6666666666666665, 4.08248290463863, 4.00693842672377, 7.824884252135749, 3.5, 10.794106135883936, 6.863036195648746, 3.5, 3.162277660168379, 9.2, 4.08248290463863, 3.3166247903554, 1.7320508075688774, 3.5, 2.23606797749979, 7.876017113339727, 3.5, 3.464101615137755, 4.532898610306738, 3.5, 2.23606797749979, 2.4494897427831783, 4.08248290463863, 3.5777087639996634, 1.0, 3.5, 2.82842712474619, 7.970379295101609, 3.5, 10.874197679943185, 4.123105625617661, 4.08248290463863, 4.358898943540673, 2.840187787218772, 3.5, 2.4494897427831783, 5.144957554275265, 3.5, 3.3166247903554, 5.908391567007971, 3.5, 3.464101615137755, 8.362913726159647, 4.08248290463863, 5.217758139277826, 4.118438837901865, 3.5, 2.6457513110645903, 6.324555320336758, 3.5, 3.162277660168379, 4.129483209670111, 4.08248290463863, 4.849343154722923, 3.6055512754639896, 4.08248290463863, 4.1461399144838555, 3.3166247903554, 4.08248290463863, 3.709704134011871, 3.5, 4.08248290463863, 3.6380343755449944, 3.464101615137755, 4.08248290463863, 3.8805700005813275, 2.23606797749979, 4.08248290463863, 3.6055512754639896, 2.23606797749979, 4.08248290463863, 5.125692857821981, 4.233901974057256, 4.08248290463863, 3.6147844564602556, 5.191085476184401, 4.08248290463863, 3.75, 3.015113445777636, 4.08248290463863, 3.474396144861517, 5.417490779798923, 4.08248290463863, 3.2071349029490928, 4.2485291572496005, 4.08248290463863, 3.328201177351375, 3.015113445777636, 4.08248290463863, 6.161124853422422, 5.577959874688627, 3.5, 2.0, 4.423258684646914, 4.08248290463863, 3.464101615137755, 3.015113445777636, 3.5, 2.4748737341529163, 5.2915026221291805, 4.08248290463863, 3.0, 6.099942813304187, 4.08248290463863, 3.1754264805429417, 4.953004953007429, 4.08248290463863, 3.328201177351375, 2.6457513110645903, 3.5, 2.4748737341529163, 6.4007575309253015, 4.08248290463863, 4.170288281141495, 2.4494897427831783, 4.08248290463863, 3.75, 5.715476066494083, 3.5, 4.016632088371218, 6.232502388407515, 4.08248290463863, 4.123105625617661, 3.3166247903554, 4.08248290463863, 3.753259453027346, 1.7320508075688774, 3.5, 2.82842712474619, 2.23606797749979, 3.5, 11.406505123530474, 5.11207720338155, 4.08248290463863, 3.5355339059327378, 2.0, 3.5, 3.464101615137755, 3.162277660168379, 4.08248290463863, 3.6147844564602556, 4.109974682633933, 3.5, 3.6147844564602556, 9.27869618527036, 4.08248290463863, 3.670651741928988, 2.4494897427831783, 4.08248290463863, 4.00693842672377, 4.743416490252569, 3.5, 2.82842712474619, 6.0, 4.08248290463863, 4.050813610756598, 5.875, 4.08248290463863, 4.811252243246881, 3.015113445777636, 3.5, 3.015113445777636, 6.639528095680696, 3.5, 3.395498750508662, 5.61757446387165, 4.08248290463863, 3.464101615137755, 7.221660539819146, 3.5, 3.6147844564602556, 7.118684968143743, 3.5, 2.4494897427831783, 6.599663291074443, 4.08248290463863, 4.358898943540673, 7.219948723811553, 3.5, 4.358898943540673, 6.388991238024046, 4.08248290463863, 4.2485291572496005, 5.3732008271339415, 4.08248290463863, 4.364357804719848, 2.6666666666666665, 4.08248290463863, 4.587317109255645, 6.3076923076923075, 3.5, 4.2, 7.433701089919686, 3.5, 10.392505312323236, 5.232590180780451, 3.5, 4.264014327112209, 6.88128713803285, 3.5, 2.886751345948129, 8.285098499285155, 3.5, 2.6666666666666665, 10.66783162498112, 4.08248290463863, 3.1754264805429417, 2.82842712474619, 3.5, 9.619220954147268, 6.181225377691006, 4.08248290463863, 3.6147844564602556, 9.904184936528898, 4.08248290463863, 4.00693842672377, 7.456011350793257, 3.5, 4.37880269519857, 4.123105625617661, 4.08248290463863, 3.6055512754639896, 9.500264920361273, 4.08248290463863, 3.328201177351375, 3.474396144861517, 3.5, 2.82842712474619, 4.129483209670111, 3.5, 2.6457513110645903, 4.47213595499958, 3.5, 2.4748737341529163, 7.342881617441561, 4.08248290463863, 5.166666666666667, 7.4687140888405334, 3.5, 4.929503017546495, 3.8805700005813275, 4.08248290463863, 3.3166247903554, 2.23606797749979, 4.08248290463863, 3.7416573867739413, 2.0, 4.08248290463863, 4.37880269519857, 9.843074773486244, 4.08248290463863, 4.0, 2.6457513110645903, 4.08248290463863, 3.3166247903554, 8.251165418850283, 4.08248290463863, 3.015113445777636, 6.538530488755173, 4.08248290463863, 3.8805700005813275, 2.23606797749979, 4.08248290463863, 4.694855340334425, 3.771236166328254, 4.08248290463863, 3.801315561749642, 2.82842712474619, 4.08248290463863, 4.024922359499621, 2.846049894151541, 4.08248290463863, 3.328201177351375, 5.714285714285714, 4.08248290463863, 3.328201177351375, 6.766006662267349, 4.08248290463863, 3.015113445777636, 3.3166247903554, 4.08248290463863, 5.303300858899106, 1.0, 4.08248290463863, 5.695489135379688, 4.08248290463863, 4.08248290463863, 4.477215043467819, 3.0, 4.08248290463863, 4.0, 2.4494897427831783, 3.5, 3.162277660168379, 5.863527298559492, 4.08248290463863, 3.6147844564602556, 2.23606797749979, 4.08248290463863, 3.5, 2.6457513110645903, 4.08248290463863, 3.3541019662496843, 3.3565855667130946, 4.08248290463863, 3.464101615137755, 5.222329678670935, 4.08248290463863, 3.5, 2.4748737341529163, 4.08248290463863, 4.364357804719848, 4.016632088371218, 3.5, 3.1754264805429417, 7.732826790345916, 4.08248290463863, 4.0, 4.381780460041329, 3.5, 2.3333333333333335, 9.214032544001846, 3.5, 2.6666666666666665, 3.474396144861517, 3.5, 2.6666666666666665, 8.62615766297429, 3.5, 2.82842712474619, 7.32709181802739, 4.08248290463863, 4.024922359499621, 2.23606797749979, 3.5, 3.2071349029490928, 9.980452274793343, 3.5, 2.6666666666666665, 8.149580519916494, 3.5, 2.041241452319315, 8.403653918204553, 3.5, 2.6457513110645903, 2.6457513110645903, 3.5, 2.4748737341529163, 7.905694150420948, 4.08248290463863, 3.6055512754639896, 3.162277660168379, 4.08248290463863, 5.028948456749697, 2.4748737341529163, 4.08248290463863, 3.162277660168379, 5.648456717647683, 4.08248290463863, 7.464798341932237, 4.0, 4.08248290463863, 3.9617738670844207, 8.717012181353368, 4.08248290463863, 5.2915026221291805, 5.284229075567875, 3.5, 3.162277660168379, 6.685631938613182, 3.5, 3.7416573867739413, 3.0, 3.5, 2.1213203435596424, 6.037383539249432, 4.08248290463863, 3.771236166328254, 5.030537377488245, 4.08248290463863, 4.669737852696148, 6.437427761961776, 3.5, 2.0, 3.3166247903554, 4.08248290463863, 7.239368114982505, 3.162277660168379, 3.5, 2.0, 8.398339722334596, 4.08248290463863, 4.47213595499958, 1.0, 3.5, 3.0, 3.328201177351375, 4.08248290463863, 3.7416573867739413, 2.23606797749979, 3.5, 3.3166247903554, 6.731006731010097, 3.5, 2.6457513110645903, 7.0710678118654755, 3.5, 2.4748737341529163, 8.39361282871732, 3.5, 2.4494897427831783, 8.674164786985145, 4.08248290463863, 3.6147844564602556, 4.9749371855331, 3.5, 2.0, 3.328201177351375, 3.5, 4.8, 5.059644256269407, 3.5, 2.23606797749979, 6.989632453288303, 3.5, 2.23606797749979, 5.246313898711125, 3.5, 2.846049894151541, 3.4914862437758782, 4.08248290463863, 3.8805700005813275, 5.124100921762788, 3.5, 2.4748737341529163, 6.366001415800504, 3.5, 3.3166247903554, 7.158238932543718, 4.08248290463863, 4.129483209670111, 2.6457513110645903, 3.5, 2.6457513110645903, 5.032452820975954, 3.5, 2.82842712474619, 4.58257569495584, 3.5, 3.05085107923876, 6.013377943029549, 3.5, 2.846049894151541, 5.11207720338155, 3.5, 2.4748737341529163, 2.846049894151541, 3.5, 2.82842712474619, 2.7716093126229358, 3.5, 3.162277660168379, 4.926646390821466, 3.5, 2.23606797749979, 6.646940512883967, 3.5, 3.0, 5.794945672638978, 3.5, 3.474396144861517, 2.6457513110645903, 3.5, 2.6457513110645903, 5.950674442093894, 3.5, 3.927922024247863, 5.615505169179773, 3.5, 2.3333333333333335, 6.599663291074444, 3.5, 2.886751345948129, 9.761272598800403, 3.5, 3.464101615137755, 8.82650882651324, 3.5, 3.162277660168379, 8.369745012779504, 3.5, 2.4748737341529163, 8.784381271934219, 3.5, 2.6666666666666665, 4.913538149119954, 3.5, 3.464101615137755, 4.2485291572496005, 3.5, 3.3166247903554, 9.303958101992741, 3.5, 2.6457513110645903, 8.183028566596791, 3.5, 2.6666666666666665, 6.0, 3.5, 2.846049894151541, 8.163796732679655, 3.5, 3.015113445777636, 8.601525360778174, 3.5, 3.0, 9.262124031816677, 3.5, 2.23606797749979, 3.6055512754639896, 3.5, 3.2071349029490928, 3.878358759406699, 3.5, 3.162277660168379, 10.099681748875627, 4.08248290463863, 3.2732683535398857, 2.4494897427831783, 4.08248290463863, 4.00693842672377, 2.6666666666666665, 3.5, 3.9617738670844207, 5.3638271972384475, 3.5, 2.6457513110645903, 9.404796606157268, 3.5, 2.846049894151541, 8.091371764398357, 3.5, 2.82842712474619, 7.0710678118654755, 3.5, 2.82842712474619, 4.050813610756598, 4.08248290463863, 3.75, 7.592566023652966, 4.08248290463863, 6.84227057829977, 4.69041575982343, 3.5, 3.1754264805429417, 5.589565568382148, 4.08248290463863, 4.00693842672377, 2.0, 4.08248290463863, 5.480077554195743, 1.0, 4.08248290463863, 4.129483209670111, 2.82842712474619, 4.08248290463863, 6.324555320336758, 4.849343154722923, 3.5, 2.82842712474619, 7.808951897004472, 3.5, 2.846049894151541, 7.6955384136140745, 4.08248290463863, 3.474396144861517, 3.0, 4.08248290463863, 4.4, 3.0, 4.08248290463863, 5.662208585049306, 4.700096710803842, 3.5, 5.629165124598852, 6.428037969018815, 3.5, 3.474396144861517, 7.111887749987414, 4.08248290463863, 3.75, 2.4494897427831783, 4.08248290463863, 3.9617738670844207, 5.417363388859614, 3.5, 3.464101615137755, 4.050813610756598, 4.08248290463863, 3.1754264805429417, 6.626115408397699, 4.08248290463863, 3.75, 3.771236166328254, 4.08248290463863, 3.7416573867739413, 4.310527248642598, 4.08248290463863, 2.773500981126146, 6.497953851430398, 4.08248290463863, 3.015113445777636, 9.013815439550587, 4.08248290463863, 4.811252243246881, 4.00693842672377, 4.08248290463863, 4.2485291572496005, 7.656162382676229, 3.5, 3.162277660168379, 7.180482019606923, 3.5, 3.8729833462074166, 3.162277660168379, 3.5, 3.0, 7.715033320111766, 4.08248290463863, 3.464101615137755, 9.021712938673348, 4.08248290463863, 3.6380343755449944, 5.039526306789696, 4.08248290463863, 3.6055512754639896, 1.7320508075688774, 4.08248290463863, 3.6147844564602556, 3.6055512754639896, 4.08248290463863, 3.464101615137755, 2.23606797749979, 4.08248290463863, 2.846049894151541, 4.123105625617661, 4.08248290463863, 3.7416573867739413, 7.395739969534468, 4.08248290463863, 3.927922024247863, 10.165188538363557, 3.5, 3.0, 11.320822273543232, 3.5, 2.23606797749979, 6.863036195648746, 3.5, 2.82842712474619, 7.803338153055733, 3.5, 2.0, 9.018797573813227, 3.5, 2.23606797749979, 7.631984736045792, 3.5, 2.23606797749979, 6.807380225308036, 4.08248290463863, 3.5, 3.0641293851417064, 3.5, 2.23606797749979, 7.149478599715339, 3.5, 3.162277660168379, 8.456729809433872, 3.5, 2.4494897427831783, 7.164373264620511, 3.5, 2.23606797749979, 9.6379974715012, 3.5, 2.6457513110645903, 8.231932086840423, 3.5, 2.82842712474619, 9.113104441757029, 4.08248290463863, 3.753259453027346, 8.574929257125442, 4.08248290463863, 7.35542471426856, 7.673442541005147, 3.5, 4.2485291572496005, 4.358898943540673, 3.5, 3.6055512754639896, 9.707637987384864, 3.5, 3.328201177351375, 5.028948456749697, 4.08248290463863, 3.753259453027346, 4.787549991450211, 3.5, 2.4494897427831783, 10.529010319542758, 4.08248290463863, 4.123105625617661, 1.0, 4.08248290463863, 5.547001962252292, 5.092010548749033, 3.5, 3.162277660168379, 9.147024094891798, 4.08248290463863, 3.8805700005813275, 2.6457513110645903, 4.08248290463863, 6.119912853410033, 5.664705542999467, 3.5, 2.82842712474619, 3.3166247903554, 3.5, 2.6457513110645903, 7.892262342196234, 4.08248290463863, 3.05085107923876, 4.929503017546495, 3.5, 2.6457513110645903, 8.139340807355381, 4.08248290463863, 6.750527405556143, 5.851558829982037, 3.5, 2.6457513110645903, 7.2886898685566255, 4.08248290463863, 3.474396144861517, 2.4494897427831783, 3.5, 2.23606797749979, 4.00693842672377, 3.5, 3.464101615137755, 9.479437760739836, 3.5, 3.015113445777636, 8.891778533577861, 3.5, 2.4494897427831783, 9.192879052329477, 4.08248290463863, 3.05085107923876, 9.589222673885601, 3.5, 2.846049894151541, 6.469966392206305, 3.5, 2.82842712474619, 11.605154529366915, 3.5, 3.162277660168379, 6.038635299392551, 3.5, 2.4494897427831783, 7.786074159656593, 4.08248290463863, 5.222329678670935, 1.0, 4.08248290463863, 6.913991516376383, 5.794945672638978, 4.08248290463863, 3.927922024247863, 4.00693842672377, 4.08248290463863, 3.6147844564602556, 7.116806599185674, 4.08248290463863, 5.217758139277826, 3.1754264805429417, 4.08248290463863, 3.6055512754639896, 6.546536707079771, 4.08248290463863, 4.587317109255645, 7.878011364745742, 4.08248290463863, 4.2485291572496005, 4.024922359499621, 4.08248290463863, 3.6055512754639896, 4.4, 3.5, 3.464101615137755, 7.3623570325860195, 4.08248290463863, 4.00693842672377, 2.4494897427831783, 4.08248290463863, 4.157609203101499, 5.61757446387165, 4.08248290463863, 5.692099788303082, 4.669737852696148, 4.08248290463863, 4.264014327112209, 3.6147844564602556, 3.5, 2.846049894151541, 1.7320508075688774, 3.5, 3.0, 8.193298297163443, 4.08248290463863, 3.670651741928988, 2.2677868380553634, 4.08248290463863, 3.75, 2.82842712474619, 4.08248290463863, 3.8729833462074166, 2.4494897427831783, 3.5, 2.82842712474619, 8.168439523882096, 3.5, 2.2677868380553634, 2.4494897427831783, 4.08248290463863, 3.474396144861517, 2.23606797749979, 3.5, 3.162277660168379, 9.190869448440344, 3.5, 3.0, 8.349784069616208, 4.08248290463863, 4.242640687119286, 6.130730673081194, 3.5, 3.162277660168379, 7.181324987175317, 3.5, 2.0, 2.4494897427831783, 3.5, 2.6457513110645903, 8.031745051446078, 3.5, 3.464101615137755, 7.783095565170492, 3.5, 2.82842712474619, 4.47213595499958, 3.5, 2.4494897427831783, 7.298004491997617, 3.5, 3.015113445777636, 4.526019054848144, 3.5, 3.162277660168379, 7.486150735203552, 3.5, 3.771236166328254, 4.1894250263350035, 3.5, 2.2677868380553634, 7.0624201077093804, 3.5, 3.0, 6.427941797666649, 3.5, 2.82842712474619, 6.852689223472485, 3.5, 3.0, 7.4550432529203094, 3.5, 2.4494897427831783, 8.64132597579116, 3.5, 2.0, 6.01929265428846, 4.08248290463863, 4.879954250643349, 3.1754264805429417, 3.5, 2.82842712474619, 3.0, 4.08248290463863, 3.162277660168379, 3.3166247903554, 4.08248290463863, 5.166666666666667, 3.9617738670844207, 3.5, 2.4494897427831783, 4.129483209670111, 4.08248290463863, 6.201736729460423, 4.700096710803842, 4.08248290463863, 3.1754264805429417, 3.3166247903554, 3.5, 2.6457513110645903, 2.82842712474619, 3.5, 4.74692883171144, 4.816989706290483, 3.5, 2.846049894151541, 2.82842712474619, 3.5, 2.23606797749979, 9.203757402647975, 3.5, 2.6457513110645903, 3.3541019662496843, 3.5, 2.8736848324283986, 5.3076923076923075, 3.5, 3.2071349029490928, 2.4494897427831783, 3.5, 2.4494897427831783, 10.129165809680478, 3.5, 3.3166247903554, 9.497607354199554, 3.5, 2.886751345948129, 1.0, 3.5, 2.23606797749979, 2.4494897427831783, 3.5, 3.3166247903554, 2.4494897427831783, 4.08248290463863, 3.474396144861517, 4.2, 3.5, 2.23606797749979, 3.0983866769659336, 3.5, 2.846049894151541, 3.670651741928988, 3.5, 2.6457513110645903, 6.350006350009525, 3.5, 3.015113445777636, 4.129483209670111, 3.5, 2.0, 3.162277660168379, 3.5, 2.0, 3.474396144861517, 3.5, 3.0, 11.115846517275257, 3.5, 2.82842712474619, 7.8144236762095485, 3.5, 2.23606797749979, 8.300573566392895, 3.5, 3.464101615137755, 8.561343946454407, 3.5, 3.162277660168379, 7.193832046426655, 3.5, 2.2677868380553634, 9.090618475235615, 3.5, 2.6457513110645903, 8.400110455827535, 3.5, 2.82842712474619, 9.040538882366587, 3.5, 3.0, 6.805384119360801, 4.08248290463863, 3.474396144861517, 2.82842712474619, 3.5, 2.82842712474619, 7.366131074427723, 3.5, 2.4748737341529163, 6.70978960890315, 4.08248290463863, 3.162277660168379, 5.208553758776472, 4.08248290463863, 3.6147844564602556, 1.7320508075688774, 4.08248290463863, 4.024922359499621, 3.3166247903554, 4.08248290463863, 4.0, 2.041241452319315, 4.08248290463863, 3.8805700005813275, 1.0, 4.08248290463863, 4.00693842672377, 4.177863742936748, 4.08248290463863, 3.3565855667130946, 2.4748737341529163, 4.08248290463863, 3.474396144861517, 6.910522349933915, 4.08248290463863, 3.6055512754639896, 8.930673159226188, 4.08248290463863, 3.3166247903554, 1.0, 3.5, 2.82842712474619, 6.524626787562728, 3.5, 2.6457513110645903, 8.667259502474643, 4.08248290463863, 4.490132550669373, 4.3465914396061125, 3.5, 2.4494897427831783, 8.885277043972694, 3.5, 2.0, 9.127202487220192, 3.5, 2.6457513110645903, 6.874054958970533, 4.08248290463863, 3.9617738670844207, 2.82842712474619, 3.5, 3.0983866769659336, 4.853626716970755, 3.5, 3.0, 9.210622697179527, 3.5, 2.6457513110645903, 8.769192333232017, 3.5, 2.2677868380553634, 4.849343154722923, 3.5, 2.2677868380553634, 8.529247447196258, 3.5, 2.0, 8.25, 4.08248290463863, 3.9617738670844207, 3.8805700005813275, 3.5, 3.1754264805429417, 3.328201177351375, 4.08248290463863, 4.364357804719848, 5.217758139277826, 3.5, 2.4494897427831783, 8.52848961628817, 3.5, 2.82842712474619, 10.222742503150233, 3.5, 2.7136021011998728, 4.008918628686366, 3.5, 2.6666666666666665, 8.44483376931153, 3.5, 3.0, 7.094409430057609, 3.5, 3.6055512754639896, 8.11147458237388, 3.5, 3.3565855667130946, 5.3970046854220985, 3.5, 2.4494897427831783, 5.770685661871567, 3.5, 3.3166247903554, 9.310239285622876, 3.5, 3.015113445777636, 3.0, 3.5, 2.4494897427831783, 8.555992052357224, 4.08248290463863, 3.464101615137755, 5.908391567007971, 3.5, 3.015113445777636, 10.15504800579495, 3.5, 3.015113445777636, 6.736096792653741, 3.5, 3.0, 9.102081983817188, 4.08248290463863, 5.276448530110864, 6.0, 3.5, 2.6457513110645903, 9.192547197409878, 3.5, 2.82842712474619, 9.314574081651863, 3.5, 3.3166247903554, 9.012626521891645, 3.5, 3.3166247903554, 9.5150257112482, 4.08248290463863, 4.264014327112209, 3.8805700005813275, 3.5, 2.7136021011998728, 3.162277660168379, 3.5, 2.9398736610366685, 7.010681840159742, 4.08248290463863, 6.249067727079546, 4.72455591261534, 4.08248290463863, 3.015113445777636, 7.80398972571708, 4.08248290463863, 4.364357804719848, 5.222329678670935, 4.08248290463863, 3.5, 3.015113445777636, 3.5, 2.4494897427831783, 7.771905540037812, 4.08248290463863, 4.00693842672377, 5.778520948116921, 3.5, 2.82842712474619, 7.116806599185674, 3.5, 3.6147844564602556, 7.818181818181818, 3.5, 3.7416573867739413, 6.041987916036252, 4.08248290463863, 3.753259453027346, 2.7136021011998728, 3.5, 2.82842712474619, 4.973458969132756, 3.5, 3.464101615137755, 6.240377207533828, 3.5, 2.2677868380553634, 6.573808125510512, 3.5, 4.00693842672377, 1.889822365046136, 3.5, 2.041241452319315, 5.480077554195743, 3.5, 2.6457513110645903, 4.997560380435394, 3.5, 2.4494897427831783, 2.82842712474619, 4.08248290463863, 3.474396144861517, 2.0, 4.08248290463863, 3.464101615137755, 2.3094010767585034, 4.08248290463863, 4.0, 2.6457513110645903, 3.5, 2.82842712474619, 2.5584085962673253, 4.08248290463863, 4.477215043467819, 2.886751345948129, 3.5, 2.6666666666666665, 7.688179765595242, 4.08248290463863, 3.2, 3.05085107923876, 3.5, 3.6147844564602556, 6.441288386528845, 3.5, 2.6457513110645903, 8.192837154805488, 4.08248290463863, 3.75, 1.7888543819998317, 4.08248290463863, 3.6055512754639896, 4.8666426339228765, 3.5, 2.6666666666666665, 7.6030554129838634, 3.5, 3.162277660168379, 7.8031856239841515, 4.08248290463863, 3.464101615137755, 1.0, 3.5, 2.6457513110645903, 9.362564215835969, 3.5, 2.82842712474619, 7.815614648594782, 4.08248290463863, 5.102520385624567, 4.535573676110727, 4.08248290463863, 5.388159060803248, 9.642365197998375, 3.5, 3.6380343755449944, 5.7564193416014815, 3.5, 3.6055512754639896, 8.894584665044027, 3.5, 3.0, 10.102694343079511, 3.5, 2.4494897427831783, 3.162277660168379, 4.08248290463863, 5.622255427989819, 5.239956379316803, 4.08248290463863, 8.099970605472826, 7.154800938571823, 4.08248290463863, 4.4907311951024935, 3.162277660168379, 3.5, 2.4494897427831783, 5.589565568382148, 3.5, 2.846049894151541, 3.015113445777636, 4.08248290463863, 3.162277660168379, 3.7123106012293743, 4.08248290463863, 6.974858324629157, 2.4494897427831783, 3.5, 3.015113445777636, 9.72634026318503, 3.5, 2.6457513110645903, 8.40168050416806, 4.08248290463863, 3.7416573867739413, 4.630461798847739, 3.5, 2.23606797749979, 4.270992778072193, 3.5, 4.4907311951024935, 6.2598071204459, 3.5, 2.5298221281347035, 11.468292773139114, 3.5, 3.328201177351375, 5.853817179628954, 3.5, 2.6457513110645903, 8.896096927335073, 3.5, 2.4748737341529163, 2.82842712474619, 3.5, 2.82842712474619, 9.522165814091077, 3.5, 2.6457513110645903, 5.363390480545726, 3.5, 2.23606797749979, 7.652711937607478, 3.5, 2.4748737341529163, 7.211102550927978, 3.5, 3.6380343755449944, 3.7416573867739413, 3.5, 3.328201177351375, 7.794294092999267, 4.08248290463863, 3.328201177351375, 5.489948531973768, 3.5, 2.0, 3.162277660168379, 3.5, 2.6666666666666665, 10.123895567197717, 3.5, 3.328201177351375, 10.661152242179043, 4.08248290463863, 3.1754264805429417, 2.6457513110645903, 3.5, 2.6457513110645903, 6.219146064997813, 3.5, 10.305338422390601, 3.05085107923876, 4.08248290463863, 3.1754264805429417, 4.542199791661352, 3.5, 3.015113445777636, 9.519523282676069, 3.5, 2.82842712474619, 8.989797311185749, 3.5, 2.6457513110645903, 6.8072851001542904, 3.5, 2.0, 3.162277660168379, 3.5, 3.3166247903554, 3.162277660168379, 3.5, 2.4494897427831783, 7.916228058025278, 3.5, 2.6666666666666665, 9.895724992860723, 3.5, 3.7416573867739413, 5.980464651413677, 3.5, 2.846049894151541, 8.234403767450646, 3.5, 3.162277660168379, 8.205527240271442, 3.5, 2.4494897427831783, 9.015611460128481, 3.5, 2.773500981126146, 9.040538882366587, 3.5, 3.75, 3.0, 3.5, 2.6457513110645903, 9.021342216356466, 3.5, 2.2677868380553634, 7.31310340973526, 3.5, 3.4412360080584263, 2.7136021011998728, 3.5, 3.6147844564602556, 4.913538149119954, 3.5, 3.3166247903554, 8.47573171879656, 3.5, 3.015113445777636, 10.089116589485474, 4.08248290463863, 4.129483209670111, 6.89634925359229, 4.08248290463863, 4.050813610756598, 6.325463955542989, 3.5, 3.0, 9.77481144449908, 3.5, 3.328201177351375, 4.704421212792114, 4.08248290463863, 3.015113445777636, 8.304547985373997, 4.08248290463863, 4.477215043467819, 5.102520385624567, 3.5, 3.6055512754639896, 6.777777777777778, 3.5, 3.8805700005813275, 4.828078792603349, 3.5, 3.162277660168379, 8.278795899070198, 3.5, 3.328201177351375, 5.316456139417774, 4.08248290463863, 3.6380343755449944, 3.464101615137755, 4.08248290463863, 3.464101615137755, 4.47213595499958, 4.08248290463863, 4.477215043467819, 6.429910574805843, 4.08248290463863, 4.123105625617661, 4.47213595499958, 4.08248290463863, 4.123105625617661, 3.0, 4.08248290463863, 3.8729833462074166, 2.4494897427831783, 4.08248290463863, 3.1754264805429417, 8.010734985006058, 4.08248290463863, 4.1461399144838555, 6.410486691557943, 4.08248290463863, 4.669737852696148, 3.3166247903554, 4.08248290463863, 5.461092327709238, 1.7320508075688774, 4.08248290463863, 3.9000674757995495, 2.846049894151541, 4.08248290463863, 4.587317109255645, 6.915148379350492, 3.5, 3.464101615137755, 2.3333333333333335, 4.08248290463863, 5.246313898711125, 6.030226891555272, 4.08248290463863, 4.170288281141495, 2.6666666666666665, 3.5, 2.82842712474619, 4.901530373260988, 4.08248290463863, 4.510671108178233, 3.75, 4.08248290463863, 4.37880269519857, 3.3166247903554, 3.5, 4.37880269519857, 8.525668472369972, 3.5, 2.6457513110645903, 8.973490369514234, 3.5, 3.3166247903554, 7.462778926574919, 3.5, 4.438772657244647, 4.490132550669373, 3.5, 5.542869676919993, 6.325771464049632, 3.5, 4.1461399144838555, 8.25674954467424, 3.5, 3.015113445777636, 7.431694860230418, 3.5, 2.6457513110645903, 8.418799823976721, 3.5, 2.23606797749979, 8.964063273970506, 3.5, 2.846049894151541, 5.750236290040988, 4.08248290463863, 3.75, 3.927922024247863, 3.5, 3.0, 9.452653437254055, 4.08248290463863, 4.0, 1.0, 4.08248290463863, 4.0, 2.82842712474619, 4.08248290463863, 4.510671108178233, 3.3166247903554, 4.08248290463863, 4.358898943540673, 1.414213562373095, 4.08248290463863, 4.1461399144838555, 3.3166247903554, 3.5, 3.015113445777636, 6.384191000968102, 4.08248290463863, 6.509445549041193, 9.233132592090664, 4.08248290463863, 4.129483209670111, 4.123105625617661, 3.5, 2.4494897427831783, 4.8, 4.08248290463863, 6.657917756510591, 5.642447102306373, 4.08248290463863, 4.129483209670111, 4.929503017546495, 3.5, 3.299831645537222, 6.085385352926881, 4.08248290463863, 4.264014327112209, 1.0, 4.08248290463863, 5.851558829982037, 4.4, 3.5, 2.5298221281347035, 5.059644256269407, 4.08248290463863, 4.801960383990247, 3.0424349222966556, 4.08248290463863, 4.123105625617661, 3.1754264805429417, 4.08248290463863, 3.65655170486763, 9.143353729961051, 4.08248290463863, 4.129483209670111, 7.188918942063234, 4.08248290463863, 3.6380343755449944, 2.4494897427831783, 4.08248290463863, 4.973458969132756, 8.071965467345862, 3.5, 2.6666666666666665, 10.86835498787732, 3.5, 2.2677868380553634, 4.456688116249245, 3.5, 2.23606797749979, 8.525296022362365, 4.08248290463863, 4.123105625617661, 4.521814736434731, 4.08248290463863, 3.7416573867739413, 1.0, 3.5, 3.3166247903554, 4.0, 3.5, 3.162277660168379, 10.492212063950621, 4.08248290463863, 2.581988897471611, 3.162277660168379, 3.5, 2.6457513110645903, 4.00693842672377, 3.5, 2.0, 7.22897396012249, 4.08248290463863, 2.6457513110645903, 2.6457513110645903, 4.08248290463863, 3.464101615137755, 1.7320508075688774, 3.5, 2.23606797749979, 8.538149682454623, 3.5, 2.6666666666666665, 8.549090976340064, 3.5, 3.0, 8.17302997500511, 4.08248290463863, 3.7416573867739413, 2.4494897427831783, 3.5, 2.6666666666666665, 3.464101615137755, 3.5, 3.0, 5.813776741499453, 3.5, 2.4494897427831783, 9.08956297292448, 4.08248290463863, 4.024922359499621, 4.618802153517007, 3.5, 3.162277660168379, 8.728206690941787, 4.08248290463863, 4.37880269519857, 3.395498750508662, 3.5, 2.82842712474619, 7.499189145356839, 4.08248290463863, 4.72455591261534, 3.474396144861517, 3.5, 2.82842712474619, 7.431694860230418, 4.08248290463863, 4.72455591261534, 3.5, 4.08248290463863, 3.7416573867739413, 3.959797974644666, 4.08248290463863, 4.37880269519857, 7.948849635407365, 4.08248290463863, 4.00693842672377, 6.375988221950509, 3.5, 2.041241452319315, 9.563904952770041, 4.08248290463863, 3.328201177351375, 9.706476511819817, 4.08248290463863, 3.162277660168379, 5.303300858899106, 4.08248290463863, 4.00693842672377, 1.0, 3.5, 2.6457513110645903, 8.491468778533354, 3.5, 2.82842712474619, 10.647058823529411, 3.5, 3.162277660168379, 5.938574464184706, 3.5, 2.4748737341529163, 10.00425441414695, 3.5, 3.3166247903554, 8.80167282080199, 3.5, 3.0, 10.334637571188003, 3.5, 2.6666666666666665, 6.725739879230903, 3.5, 2.6457513110645903, 8.065591326174431, 3.5, 3.162277660168379, 8.571593913749194, 3.5, 3.015113445777636, 10.766252603480527, 3.5, 3.1754264805429417, 9.278079468332315, 3.5, 3.0, 6.28970902033151, 3.5, 3.0, 8.919733394873214, 4.08248290463863, 6.3727937358744, 5.092010548749033, 3.5, 2.82842712474619, 11.59012860502769, 4.08248290463863, 4.58257569495584, 3.328201177351375, 3.5, 2.82842712474619, 8.579272787866548, 3.5, 2.82842712474619, 3.8376128944009875, 4.08248290463863, 6.196773353931867, 5.11207720338155, 4.08248290463863, 4.8, 6.302708777266682, 3.5, 2.6457513110645903, 7.224456455979048, 3.5, 2.4748737341529163, 6.001068280966357, 4.08248290463863, 3.6147844564602556, 5.703418756083616, 4.08248290463863, 4.310527248642598, 5.309368506648784, 4.08248290463863, 4.57495710997814, 4.085297439895141, 3.5, 3.0, 3.162277660168379, 4.08248290463863, 3.6147844564602556, 4.171041879608233, 4.08248290463863, 3.75, 2.6457513110645903, 3.5, 3.8340579025361627, 4.467914966843415, 4.08248290463863, 3.6055512754639896, 3.162277660168379, 4.08248290463863, 3.6055512754639896, 4.123105625617661, 3.5, 3.162277660168379, 7.964952993613722, 3.5, 3.015113445777636, 6.184165460191406, 3.5, 2.4494897427831783, 4.902903378454601, 3.5, 2.82842712474619, 3.474396144861517, 3.5, 3.015113445777636, 7.262590648655362, 3.5, 2.6666666666666665, 6.750527405556143, 3.5, 3.015113445777636, 5.789863774090244, 4.08248290463863, 6.249067727079546, 4.603171644550004, 3.5, 3.464101615137755, 8.674768752146987, 3.5, 3.015113445777636, 2.6457513110645903, 3.5, 3.3166247903554, 4.47213595499958, 3.5, 2.1213203435596424, 8.796199993653977, 3.5, 4.00693842672377, 6.154574548966636, 3.5, 2.82842712474619, 7.3990279114953825, 3.5, 3.2071349029490928, 8.0195413508667, 3.5, 3.328201177351375, 9.853448749615868, 3.5, 3.464101615137755, 4.221158824088691, 3.5, 2.4748737341529163, 8.60201107849755, 3.5, 2.4748737341529163, 7.746292694253846, 3.5, 2.4494897427831783, 9.988687869300877, 3.5, 2.846049894151541, 6.7662680570377285, 3.5, 3.328201177351375, 6.8657566124489255, 3.5, 2.846049894151541, 7.866747376502838, 3.5, 2.23606797749979, 2.82842712474619, 3.5, 10.17740861013741, 3.9617738670844207, 3.5, 2.4748737341529163, 9.454063165900024, 3.5, 2.0, 1.0, 4.08248290463863, 4.264014327112209, 8.873855889750516, 4.08248290463863, 4.949747468305833, 4.913538149119954, 4.08248290463863, 5.400617248673217, 7.89152397370844, 4.08248290463863, 4.811252243246881, 7.124070532524293, 4.08248290463863, 3.6055512754639896, 3.481553119113957, 4.08248290463863, 4.58257569495584, 4.826004826007239, 4.08248290463863, 3.6147844564602556, 6.697875667817257, 4.08248290463863, 4.242640687119286, 1.7320508075688774, 4.08248290463863, 3.464101615137755, 6.723674011118638, 4.08248290463863, 4.00693842672377, 3.0, 3.5, 2.6457513110645903, 7.091168559049116, 3.5, 10.119288512538814, 6.508559328900301, 4.08248290463863, 3.1529631254723287, 3.05085107923876, 3.5, 2.82842712474619, 1.0, 4.08248290463863, 6.352811106527871, 5.563035899673184, 4.08248290463863, 4.123105625617661, 6.83536555146996, 3.5, 2.82842712474619, 3.464101615137755, 3.5, 3.3166247903554, 5.688734668417887, 4.08248290463863, 3.5, 5.7092239485974, 3.5, 2.4494897427831783, 8.142613724927722, 3.5, 2.82842712474619, 5.662208585049306, 4.08248290463863, 7.111111111111111, 4.4, 3.5, 2.5298221281347035, 9.742697372455677, 3.5, 4.123105625617661, 3.0, 3.5, 2.0, 3.162277660168379, 4.08248290463863, 8.990802654607693, 4.529039094769576, 3.5, 3.3166247903554, 8.282511696339462, 3.5, 3.3166247903554, 6.928853368993243, 3.5, 3.464101615137755, 5.881176352917641, 3.5, 3.162277660168379, 4.371914496377572, 3.5, 2.6457513110645903, 5.0137741307804005, 3.5, 2.6666666666666665, 3.6055512754639896, 3.5, 3.0, 3.771236166328254, 3.5, 3.3166247903554, 5.590169943749474, 3.5, 2.4494897427831783, 8.727680894321438, 3.5, 3.162277660168379, 10.350924156870612, 4.08248290463863, 3.6380343755449944, 1.7888543819998317, 4.08248290463863, 3.8729833462074166, 1.0, 3.5, 2.6457513110645903, 7.024912639340182, 3.5, 2.82842712474619, 9.836190794746173, 3.5, 2.6457513110645903, 6.793662204867574, 4.08248290463863, 3.395498750508662, 2.9824045403173027, 3.5, 3.6055512754639896, 3.1754264805429417, 3.5, 3.670651741928988, 2.4748737341529163, 3.5, 2.6457513110645903, 9.205499881602055, 4.08248290463863, 4.0, 5.048252022715237, 3.5, 3.3166247903554, 7.90106015622946, 3.5, 3.162277660168379, 3.8805700005813275, 4.08248290463863, 4.69041575982343, 8.097424401519659, 4.08248290463863, 3.8729833462074166, 6.776718456854931, 3.5, 3.0, 4.8, 4.08248290463863, 4.264014327112209, 3.2071349029490928, 3.5, 3.75, 4.216370213557839, 4.08248290463863, 3.474396144861517, 1.7320508075688774, 3.5, 2.6457513110645903, 6.25, 4.08248290463863, 3.328201177351375, 5.430582663966679, 4.08248290463863, 3.7416573867739413, 4.050813610756598, 4.08248290463863, 3.3166247903554, 4.666666666666667, 4.08248290463863, 3.3166247903554, 2.0, 4.08248290463863, 3.6147844564602556, 9.126883367283707, 4.08248290463863, 6.742363047141518, 5.003702332976757, 4.08248290463863, 4.2485291572496005, 2.4494897427831783, 4.08248290463863, 4.286607049870562, 3.474396144861517, 3.5, 3.162277660168379, 5.126524163602469, 4.08248290463863, 3.3166247903554, 9.170605214488301, 3.5, 2.6457513110645903, 8.339503888294594, 3.5, 3.474396144861517, 9.6, 3.5, 3.328201177351375, 10.948476036270003, 3.5, 2.2677868380553634, 7.6900482511661234, 3.5, 3.162277660168379, 9.137375925066596, 4.08248290463863, 3.6055512754639896, 2.82842712474619, 4.08248290463863, 3.6055512754639896, 6.993258208972302, 4.08248290463863, 6.701872450265123, 6.161232369723243, 4.08248290463863, 3.5777087639996634, 4.949747468305833, 4.08248290463863, 4.801960383990247, 2.886751345948129, 4.08248290463863, 3.6147844564602556, 1.7320508075688774, 3.5, 2.6457513110645903, 3.3166247903554, 3.5, 2.4494897427831783, 8.108848540793831, 4.08248290463863, 3.771236166328254, 1.7888543819998317, 3.5, 2.6457513110645903, 8.68385835750183, 3.5, 2.82842712474619, 2.82842712474619, 3.5, 3.6147844564602556, 10.601426503185268, 3.5, 3.0, 8.603884002203404, 3.5, 2.6666666666666665, 6.325219629494658, 3.5, 3.0, 6.753346748664125, 3.5, 3.0, 9.485845951112621, 3.5, 3.464101615137755, 6.6529914385911555, 3.5, 3.162277660168379, 5.795455252808151, 3.5, 3.0983866769659336, 5.932958789676531, 4.08248290463863, 4.27437366993929, 7.510409442899901, 4.08248290463863, 3.8729833462074166, 9.217458351494649, 3.5, 3.015113445777636, 8.252088808112143, 4.08248290463863, 5.487954724560282, 4.7702783519995515, 3.5, 3.7416573867739413, 3.162277660168379, 4.08248290463863, 3.801315561749642, 2.23606797749979, 3.5, 2.2677868380553634, 3.3166247903554, 3.5, 2.6666666666666665, 6.425396041156863, 3.5, 3.464101615137755, 4.318004318006477, 3.5, 2.5298221281347035, 8.982473206345997, 3.5, 2.82842712474619, 6.58504607868518, 4.08248290463863, 4.024922359499621, 4.905778905196061, 4.08248290463863, 4.1461399144838555, 5.37545555167743, 3.5, 2.4494897427831783, 5.898906801202691, 3.5, 2.23606797749979, 7.193830424207176, 3.5, 3.7416573867739413, 5.251073270466074, 3.5, 4.706787243316417, 6.882472016116853, 3.5, 2.041241452319315, 7.876429114339844, 4.08248290463863, 3.753259453027346, 7.333395429444001, 3.5, 2.846049894151541, 9.421683286017897, 4.08248290463863, 4.2485291572496005, 1.414213562373095, 4.08248290463863, 5.396407334626633, 3.3166247903554, 4.08248290463863, 4.913538149119954, 3.8729833462074166, 4.08248290463863, 5.303300858899106, 2.6457513110645903, 4.08248290463863, 5.897678246195885, 6.880975663883468, 4.08248290463863, 5.208553758776472, 4.937707198786941, 3.5, 3.328201177351375, 8.723567442899585, 3.5, 2.0, 6.735753140545634, 3.5, 3.0, 8.538149682454623, 4.08248290463863, 3.8729833462074166, 4.27617987059879, 4.08248290463863, 4.2485291572496005, 2.82842712474619, 3.5, 3.328201177351375, 3.3166247903554, 3.5, 3.3166247903554, 8.80172458846704, 4.08248290463863, 3.670651741928988, 4.016632088371218, 3.5, 2.0, 8.769284798196836, 3.5, 4.0, 8.925731045622827, 3.5, 3.3166247903554, 3.464101615137755, 3.5, 2.4748737341529163, 2.82842712474619, 3.5, 3.015113445777636, 8.467669207104626, 3.5, 2.6666666666666665, 7.478901140150934, 3.5, 2.6666666666666665, 10.100630043355082, 3.5, 2.6666666666666665, 8.843466319929314, 4.08248290463863, 6.80336051416609, 6.666749174406927, 3.5, 3.474396144861517, 5.045750287082781, 3.5, 2.2135943621178655, 7.067617668790178, 3.5, 2.82842712474619, 10.50114904542028, 3.5, 3.0983866769659336, 10.446835101424911, 4.08248290463863, 4.1461399144838555, 3.015113445777636, 3.5, 2.4494897427831783, 11.314309764284204, 4.08248290463863, 3.771711342562273, 4.631047607372178, 3.5, 4.2485291572496005, 8.017188013494799, 3.5, 3.0, 10.401230987753616, 3.5, 2.82842712474619, 8.36182803382292, 3.5, 2.6457513110645903, 9.328574641000293, 3.5, 2.6666666666666665, 9.165692857515609, 3.5, 2.82842712474619, 9.138724148904345, 4.08248290463863, 3.8805700005813275, 3.3166247903554, 3.5, 2.82842712474619, 7.213600767718993, 4.08248290463863, 3.6055512754639896, 2.0, 3.5, 2.82842712474619, 7.092081432669753, 4.08248290463863, 3.6055512754639896, 6.437427761961776, 3.5, 3.162277660168379, 6.567206798038654, 4.08248290463863, 4.4907311951024935, 1.0, 3.5, 2.6457513110645903, 6.481812160876686, 3.5, 3.6055512754639896, 7.464062581183745, 3.5, 2.6457513110645903, 8.140059175477884, 3.5, 2.4748737341529163, 1.0, 3.5, 3.0, 5.461092327709238, 3.5, 3.2071349029490928, 9.759485488654478, 3.5, 4.170288281141495, 3.1754264805429417, 4.08248290463863, 4.8, 2.23606797749979, 3.5, 3.3166247903554, 5.217758139277826, 3.5, 3.3166247903554, 4.929503017546495, 3.5, 4.642383454426297, 3.3166247903554, 3.5, 2.4494897427831783, 4.129483209670111, 3.5, 4.08248290463863, 4.717295153286297, 4.08248290463863, 4.00693842672377, 6.052586456788652, 3.5, 3.927922024247863, 3.0, 3.5, 3.3166247903554, 3.709704134011871, 3.5, 3.771236166328254, 9.083602496781033, 3.5, 3.7416573867739413, 3.3166247903554, 3.5, 3.75, 6.277666039945969, 4.08248290463863, 5.515634344499481, 6.092848842499201, 4.08248290463863, 4.0, 2.0, 4.08248290463863, 4.314554973040049, 3.6055512754639896, 3.5, 2.82842712474619, 5.459195627691725, 3.5, 3.162277660168379, 4.47213595499958, 3.5, 2.23606797749979, 10.336932519516777, 3.5, 3.9000674757995495, 4.401953314433961, 3.5, 2.4494897427831783, 10.050895074732335, 3.5, 3.0, 9.882439052481743, 4.08248290463863, 4.085297439895141, 3.4285714285714284, 3.5, 3.0, 4.394802124588286, 3.5, 2.6457513110645903, 8.872150458791875, 4.08248290463863, 4.642383454426297, 10.26213041988441, 4.08248290463863, 5.102520385624567, 4.129483209670111, 4.08248290463863, 6.00832755431992, 4.849343154722923, 3.5, 3.75, 5.578018081208209, 4.08248290463863, 4.74692883171144, 1.0, 4.08248290463863, 5.939696961966999, 1.0, 4.08248290463863, 4.849343154722923, 2.82842712474619, 4.08248290463863, 4.37880269519857, 1.0, 4.08248290463863, 4.024922359499621, 2.041241452319315, 3.5, 3.328201177351375, 8.019989072255717, 4.08248290463863, 5.184951391308559, 4.310527248642598, 3.5, 3.3166247903554, 7.426232973063312, 4.08248290463863, 4.364357804719848, 4.47213595499958, 4.08248290463863, 3.3166247903554, 3.8729833462074166, 4.08248290463863, 4.3465914396061125, 1.0, 3.5, 3.0, 7.219948723811553, 4.08248290463863, 3.7416573867739413, 2.0, 3.5, 2.9398736610366685, 6.171264622608037, 3.5, 2.23606797749979, 6.609001368025944, 4.08248290463863, 4.2485291572496005, 3.395498750508662, 4.08248290463863, 5.239956379316803, 7.890453678924885, 3.5, 2.4748737341529163, 10.08852124295554, 4.08248290463863, 4.6, 6.215188768538847, 3.5, 3.0, 8.607101241464626, 3.5, 4.902903378454601, 3.6055512754639896, 3.5, 3.9000674757995495, 6.350852961085883, 3.5, 3.162277660168379, 8.17423891369599, 3.5, 4.929503017546495, 7.689786245403703, 3.5, 2.4494897427831783, 7.024912639340182, 4.08248290463863, 6.482493908421918, 7.677159338596802, 4.08248290463863, 3.771236166328254, 1.0, 4.08248290463863, 4.2485291572496005, 5.11207720338155, 3.5, 3.162277660168379, 4.217756949399826, 4.08248290463863, 3.5355339059327378, 3.6147844564602556, 4.08248290463863, 4.913538149119954, 4.0, 4.08248290463863, 3.6147844564602556, 3.927922024247863, 4.08248290463863, 4.0, 7.772297096131725, 4.08248290463863, 3.9617738670844207, 7.151355947783043, 3.5, 3.162277660168379, 9.25710351661044, 4.08248290463863, 4.233901974057256, 3.3166247903554, 3.5, 3.0, 7.786074159656593, 3.5, 2.6666666666666665, 9.526198940693433, 4.08248290463863, 3.801315561749642, 3.1754264805429417, 4.08248290463863, 3.5355339059327378, 2.23606797749979, 4.08248290463863, 3.328201177351375, 8.181818181818182, 4.08248290463863, 5.425166581076791, 5.102520385624567, 4.08248290463863, 3.7416573867739413, 3.0, 3.5, 3.474396144861517, 8.163709564049535, 4.08248290463863, 3.75, 2.23606797749979, 4.08248290463863, 4.929503017546495, 3.015113445777636, 4.08248290463863, 3.464101615137755, 8.437758422458371, 3.5, 2.4494897427831783, 6.831300510639733, 4.08248290463863, 4.949747468305833, 3.3166247903554, 4.08248290463863, 4.264014327112209, 4.170288281141495, 3.5, 2.846049894151541, 8.834544715369823, 4.08248290463863, 3.8729833462074166, 7.994350414775228, 3.5, 3.0, 9.658562153769513, 3.5, 3.6055512754639896, 10.105282357139245, 4.08248290463863, 3.6380343755449944, 8.082102965782765, 4.08248290463863, 8.520563361656318, 6.434015210126405, 3.5, 3.0, 8.215838362577491, 3.5, 2.82842712474619, 7.030047555134388, 3.5, 4.0, 5.0, 3.5, 3.3166247903554, 9.709120968076979, 3.5, 3.5, 8.538149682454625, 3.5, 3.3166247903554, 6.215815605080611, 4.08248290463863, 3.328201177351375, 8.475677204353536, 3.5, 3.0, 8.916455838132986, 3.5, 2.82842712474619, 10.229792209456097, 4.08248290463863, 3.464101615137755, 6.453979094624195, 3.5, 3.05085107923876, 10.622326858705444, 4.08248290463863, 3.474396144861517, 3.1754264805429417, 3.5, 2.4494897427831783, 5.628854235099715, 4.08248290463863, 5.028948456749697, 1.0, 4.08248290463863, 3.4412360080584263, 4.394802124588286, 3.5, 3.6055512754639896, 6.1, 3.5, 2.82842712474619, 7.946549629034693, 3.5, 2.4494897427831783, 2.0, 3.5, 3.05085107923876, 4.437601569801833, 3.5, 2.846049894151541, 7.677166722749278, 4.08248290463863, 4.811252243246881, 3.8729833462074166, 3.5, 3.3166247903554, 4.576276618858141, 3.5, 3.162277660168379, 8.241477490081023, 3.5, 3.015113445777636, 4.510671108178233, 3.5, 3.0, 10.625064049817196, 3.5, 4.129483209670111, 2.6457513110645903, 3.5, 2.6666666666666665, 8.51591647054698, 3.5, 3.0, 5.0, 3.5, 2.4494897427831783, 8.598492073268774, 3.5, 3.6147844564602556, 2.4494897427831783, 3.5, 3.6380343755449944, 2.5298221281347035, 3.5, 3.3166247903554, 8.804710999221752, 4.08248290463863, 3.0, 6.260064360459999, 3.5, 2.82842712474619, 4.47213595499958, 4.08248290463863, 4.849343154722923, 3.9000674757995495, 4.08248290463863, 3.8805700005813275, 4.609327677584255, 4.08248290463863, 3.1754264805429417, 6.735198826004059, 4.08248290463863, 3.05085107923876, 8.11920985240873, 4.08248290463863, 3.464101615137755, 8.246599920961987, 3.5, 2.846049894151541, 8.095236050624372, 3.5, 4.535573676110727, 7.735900454643651], "type": "histogram"}];
            const layout = Object.assign({}, {"template": {"data": {"histogram2dcontour": [{"type": "histogram2dcontour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "choropleth": [{"type": "choropleth", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "histogram2d": [{"type": "histogram2d", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "heatmap": [{"type": "heatmap", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "contourcarpet": [{"type": "contourcarpet", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "contour": [{"type": "contour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "surface": [{"type": "surface", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "mesh3d": [{"type": "mesh3d", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "scatter": [{"fillpattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}, "type": "scatter"}], "parcoords": [{"type": "parcoords", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolargl": [{"type": "scatterpolargl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "bar"}], "scattergeo": [{"type": "scattergeo", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolar": [{"type": "scatterpolar", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "histogram": [{"marker": {"pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "histogram"}], "scattergl": [{"type": "scattergl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatter3d": [{"type": "scatter3d", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermap": [{"type": "scattermap", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermapbox": [{"type": "scattermapbox", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterternary": [{"type": "scatterternary", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattercarpet": [{"type": "scattercarpet", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "barpolar"}], "pie": [{"automargin": true, "type": "pie"}]}, "layout": {"autotypenumbers": "strict", "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "hovermode": "closest", "hoverlabel": {"align": "left"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"bgcolor": "#E5ECF6", "angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "ternary": {"bgcolor": "#E5ECF6", "aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]]}, "xaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "yaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "geo": {"bgcolor": "white", "landcolor": "#E5ECF6", "subunitcolor": "white", "showland": true, "showlakes": true, "lakecolor": "white"}, "title": {"x": 0.05}, "mapbox": {"style": "light"}}}, "margin": {"l": 50, "r": 30, "t": 30, "b": 50}, "font": {"color": "#a39e93"}, "xaxis": {"title": {"text": "Diversity Vocabulary Richness"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "yaxis": {"title": {"text": "Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "title": {}, "height": 400, "paper_bgcolor": "rgba(0,0,0,0)", "plot_bgcolor": "rgba(0,0,0,0)"}, darkLayout);
            Plotly.newPlot('chart_4', data, layout, plotlyConfig);
        })();
        
        (function() {
            const data = [{"marker": {"color": "#d4a574"}, "nbinsx": 30, "opacity": 0.85, "x": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "type": "histogram"}];
            const layout = Object.assign({}, {"template": {"data": {"histogram2dcontour": [{"type": "histogram2dcontour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "choropleth": [{"type": "choropleth", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "histogram2d": [{"type": "histogram2d", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "heatmap": [{"type": "heatmap", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "contourcarpet": [{"type": "contourcarpet", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "contour": [{"type": "contour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "surface": [{"type": "surface", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "mesh3d": [{"type": "mesh3d", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "scatter": [{"fillpattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}, "type": "scatter"}], "parcoords": [{"type": "parcoords", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolargl": [{"type": "scatterpolargl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "bar"}], "scattergeo": [{"type": "scattergeo", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolar": [{"type": "scatterpolar", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "histogram": [{"marker": {"pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "histogram"}], "scattergl": [{"type": "scattergl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatter3d": [{"type": "scatter3d", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermap": [{"type": "scattermap", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermapbox": [{"type": "scattermapbox", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterternary": [{"type": "scatterternary", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattercarpet": [{"type": "scattercarpet", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "barpolar"}], "pie": [{"automargin": true, "type": "pie"}]}, "layout": {"autotypenumbers": "strict", "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "hovermode": "closest", "hoverlabel": {"align": "left"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"bgcolor": "#E5ECF6", "angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "ternary": {"bgcolor": "#E5ECF6", "aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]]}, "xaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "yaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "geo": {"bgcolor": "white", "landcolor": "#E5ECF6", "subunitcolor": "white", "showland": true, "showlakes": true, "lakecolor": "white"}, "title": {"x": 0.05}, "mapbox": {"style": "light"}}}, "margin": {"l": 50, "r": 30, "t": 30, "b": 50}, "font": {"color": "#a39e93"}, "xaxis": {"title": {"text": "Format Code Block Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "yaxis": {"title": {"text": "Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "title": {}, "height": 400, "paper_bgcolor": "rgba(0,0,0,0)", "plot_bgcolor": "rgba(0,0,0,0)"}, darkLayout);
            Plotly.newPlot('chart_5', data, layout, plotlyConfig);
        })();
        
        (function() {
            const data = [{"marker": {"color": "#d4a574"}, "nbinsx": 30, "opacity": 0.85, "x": [0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.7, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.4, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.3, 0.0, 0.4, 0.5, 0.0, 0.0, 0.5, 0.0, 0.2, 0.0, 0.0, 0.4, 0.0, 0.0, 0.4, 0.4, 0.0, 0.0, 0.5, 0.0, 0.7, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.4, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.4, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.3, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.55, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.55, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.7, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.4, 0.4, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.4, 0.7, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "type": "histogram"}];
            const layout = Object.assign({}, {"template": {"data": {"histogram2dcontour": [{"type": "histogram2dcontour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "choropleth": [{"type": "choropleth", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "histogram2d": [{"type": "histogram2d", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "heatmap": [{"type": "heatmap", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "contourcarpet": [{"type": "contourcarpet", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "contour": [{"type": "contour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "surface": [{"type": "surface", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "mesh3d": [{"type": "mesh3d", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "scatter": [{"fillpattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}, "type": "scatter"}], "parcoords": [{"type": "parcoords", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolargl": [{"type": "scatterpolargl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "bar"}], "scattergeo": [{"type": "scattergeo", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolar": [{"type": "scatterpolar", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "histogram": [{"marker": {"pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "histogram"}], "scattergl": [{"type": "scattergl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatter3d": [{"type": "scatter3d", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermap": [{"type": "scattermap", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermapbox": [{"type": "scattermapbox", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterternary": [{"type": "scatterternary", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattercarpet": [{"type": "scattercarpet", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "barpolar"}], "pie": [{"automargin": true, "type": "pie"}]}, "layout": {"autotypenumbers": "strict", "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "hovermode": "closest", "hoverlabel": {"align": "left"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"bgcolor": "#E5ECF6", "angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "ternary": {"bgcolor": "#E5ECF6", "aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]]}, "xaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "yaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "geo": {"bgcolor": "white", "landcolor": "#E5ECF6", "subunitcolor": "white", "showland": true, "showlakes": true, "lakecolor": "white"}, "title": {"x": 0.05}, "mapbox": {"style": "light"}}}, "margin": {"l": 50, "r": 30, "t": 30, "b": 50}, "font": {"color": "#a39e93"}, "xaxis": {"title": {"text": "Format Format Complexity Score"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "yaxis": {"title": {"text": "Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "title": {}, "height": 400, "paper_bgcolor": "rgba(0,0,0,0)", "plot_bgcolor": "rgba(0,0,0,0)"}, darkLayout);
            Plotly.newPlot('chart_6', data, layout, plotlyConfig);
        })();
        
        (function() {
            const data = [{"marker": {"color": "#d4a574"}, "nbinsx": 30, "opacity": 0.85, "x": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "type": "histogram"}];
            const layout = Object.assign({}, {"template": {"data": {"histogram2dcontour": [{"type": "histogram2dcontour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "choropleth": [{"type": "choropleth", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "histogram2d": [{"type": "histogram2d", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "heatmap": [{"type": "heatmap", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "contourcarpet": [{"type": "contourcarpet", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "contour": [{"type": "contour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "surface": [{"type": "surface", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "mesh3d": [{"type": "mesh3d", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "scatter": [{"fillpattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}, "type": "scatter"}], "parcoords": [{"type": "parcoords", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolargl": [{"type": "scatterpolargl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "bar"}], "scattergeo": [{"type": "scattergeo", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolar": [{"type": "scatterpolar", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "histogram": [{"marker": {"pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "histogram"}], "scattergl": [{"type": "scattergl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatter3d": [{"type": "scatter3d", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermap": [{"type": "scattermap", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermapbox": [{"type": "scattermapbox", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterternary": [{"type": "scatterternary", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattercarpet": [{"type": "scattercarpet", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "barpolar"}], "pie": [{"automargin": true, "type": "pie"}]}, "layout": {"autotypenumbers": "strict", "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "hovermode": "closest", "hoverlabel": {"align": "left"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"bgcolor": "#E5ECF6", "angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "ternary": {"bgcolor": "#E5ECF6", "aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]]}, "xaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "yaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "geo": {"bgcolor": "white", "landcolor": "#E5ECF6", "subunitcolor": "white", "showland": true, "showlakes": true, "lakecolor": "white"}, "title": {"x": 0.05}, "mapbox": {"style": "light"}}}, "margin": {"l": 50, "r": 30, "t": 30, "b": 50}, "font": {"color": "#a39e93"}, "xaxis": {"title": {"text": "Quality Pii Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "yaxis": {"title": {"text": "Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "title": {}, "height": 400, "paper_bgcolor": "rgba(0,0,0,0)", "plot_bgcolor": "rgba(0,0,0,0)"}, darkLayout);
            Plotly.newPlot('chart_7', data, layout, plotlyConfig);
        })();
        
        (function() {
            const data = [{"marker": {"color": "#d4a574"}, "nbinsx": 30, "opacity": 0.85, "x": [0.0, 0.0, 0.008, 0.0, 0.0, 0.039, 0.0, 0.0, 0.092, 0.0, 0.0, 0.014, 0.0, 0.0, 0.053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.066, 0.0, 0.0, 0.034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025, 0.0, 0.0, 0.028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024, 0.0, 0.0, 0.006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.096, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.071, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.063, 0.0, 0.0, 0.01, 0.0, 0.0, 0.094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035, 0.0, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031, 0.0, 0.0, 0.0, 0.081, 0.0, 0.0, 0.127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025, 0.0, 0.0, 0.065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.0, 0.0, 0.055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.067, 0.0, 0.0, 0.035, 0.0, 0.0, 0.05, 0.0, 0.0, 0.102, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006, 0.0, 0.0, 0.039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0, 0.141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.009, 0.0, 0.0, 0.031, 0.0, 0.0, 0.022, 0.0, 0.0, 0.069, 0.0, 0.0, 0.121, 0.0, 0.0, 0.0, 0.0, 0.0, 0.114, 0.0, 0.0, 0.071, 0.0, 0.013, 0.167, 0.0, 0.0, 0.05, 0.0, 0.0, 0.069, 0.0, 0.0, 0.026, 0.0, 0.0, 0.0, 0.0, 0.014, 0.0, 0.0, 0.0, 0.034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.0, 0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019, 0.0, 0.0, 0.058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.115, 0.0, 0.027, 0.0, 0.0, 0.0, 0.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.185, 0.0, 0.0, 0.156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.119, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011, 0.0, 0.0, 0.043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023, 0.0, 0.0, 0.037, 0.0, 0.0, 0.019, 0.0, 0.0, 0.032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014, 0.0, 0.0, 0.011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.058, 0.0, 0.0, 0.009, 0.0, 0.0, 0.035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018, 0.0, 0.211, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.084, 0.0, 0.0, 0.04, 0.0, 0.0, 0.065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011, 0.0, 0.0, 0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0, 0.0, 0.033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022, 0.0, 0.0, 0.023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.066, 0.0, 0.0, 0.026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.063, 0.0, 0.0, 0.013, 0.0, 0.0, 0.082, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025, 0.0, 0.0, 0.048, 0.0, 0.0, 0.011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036, 0.0, 0.0, 0.076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023, 0.0, 0.0, 0.016, 0.0, 0.0, 0.033, 0.0, 0.0, 0.037, 0.0, 0.0, 0.027, 0.0, 0.0, 0.086, 0.0, 0.0, 0.011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.065, 0.0, 0.0, 0.0, 0.0, 0.048, 0.164, 0.0, 0.0, 0.038, 0.0, 0.0, 0.0, 0.0, 0.02, 0.025, 0.0, 0.0, 0.038, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.047, 0.0, 0.0, 0.02, 0.0, 0.0, 0.051, 0.0, 0.0, 0.009, 0.0, 0.0, 0.024, 0.0, 0.0, 0.003, 0.0, 0.0, 0.024, 0.0, 0.0, 0.008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.009, 0.0, 0.0, 0.01, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052, 0.0, 0.0, 0.184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.079, 0.0, 0.0, 0.008, 0.0, 0.0, 0.025, 0.0, 0.0, 0.006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033, 0.0, 0.0, 0.167, 0.0, 0.103, 0.156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.067, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.055, 0.0, 0.0, 0.019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.099, 0.0, 0.0, 0.041, 0.0, 0.0, 0.016, 0.0, 0.0, 0.038, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041, 0.0, 0.0, 0.01, 0.0, 0.0, 0.043, 0.0, 0.0, 0.0, 0.0, 0.077, 0.04, 0.0, 0.0, 0.051, 0.0, 0.0, 0.034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.082, 0.0, 0.0, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011, 0.0, 0.0, 0.014, 0.0, 0.0, 0.037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041, 0.0, 0.0, 0.063, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.016, 0.0, 0.0, 0.038, 0.0, 0.0, 0.011, 0.0, 0.0, 0.121, 0.0, 0.0, 0.0, 0.0, 0.0, 0.088, 0.0, 0.0, 0.071, 0.0, 0.0, 0.105, 0.0, 0.0, 0.057, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006, 0.0, 0.0, 0.06, 0.0, 0.0, 0.011, 0.0, 0.0, 0.017, 0.0, 0.0, 0.011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009, 0.0, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005, 0.0, 0.0, 0.048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025, 0.0, 0.0, 0.058, 0.0, 0.0, 0.017, 0.0, 0.0, 0.039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034, 0.0, 0.0, 0.025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.093, 0.0, 0.0, 0.033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005, 0.0, 0.0, 0.087, 0.0, 0.0, 0.024, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046, 0.0, 0.0, 0.011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.056, 0.0, 0.0, 0.019, 0.0, 0.0, 0.013, 0.0, 0.0, 0.027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036, 0.0, 0.0, 0.045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.054, 0.0, 0.0, 0.006, 0.0, 0.0, 0.0, 0.0, 0.057, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029, 0.0, 0.0, 0.077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.144, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012, 0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026, 0.0, 0.231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0, 0.01, 0.0, 0.0, 0.092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.038, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.0, 0.0, 0.049, 0.0, 0.0, 0.043, 0.0, 0.0, 0.004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.0, 0.0, 0.12, 0.0, 0.0, 0.033, 0.0, 0.0, 0.112, 0.0, 0.0, 0.033, 0.0, 0.0, 0.017, 0.0, 0.0, 0.029, 0.0, 0.0, 0.056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.063, 0.0, 0.0, 0.0, 0.0, 0.017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.077, 0.0, 0.0, 0.026, 0.0, 0.0, 0.103, 0.0, 0.069, 0.108, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.123, 0.0, 0.0, 0.0, 0.0, 0.036, 0.106, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052, 0.0, 0.0, 0.026, 0.0, 0.0, 0.064, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.048, 0.0, 0.0, 0.022, 0.0, 0.0, 0.041, 0.0, 0.0, 0.016, 0.0, 0.0, 0.131, 0.0, 0.0, 0.04, 0.0, 0.0, 0.053, 0.0, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.038, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.194, 0.0, 0.0, 0.017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.059, 0.0, 0.008, 0.103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036, 0.0, 0.0, 0.069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0, 0.047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.026, 0.0, 0.0, 0.007, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.038, 0.0, 0.0, 0.012, 0.0, 0.0, 0.004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.049, 0.0, 0.0, 0.224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.239, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.083, 0.0, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044, 0.0, 0.0, 0.046, 0.0, 0.0, 0.018, 0.0, 0.0, 0.007, 0.0, 0.0, 0.043, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.042, 0.0, 0.0, 0.026, 0.0, 0.0, 0.055, 0.0, 0.0, 0.034, 0.0, 0.0, 0.083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025, 0.0, 0.0, 0.009, 0.0, 0.0, 0.012, 0.0, 0.0, 0.047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027, 0.0, 0.0, 0.05, 0.0, 0.0, 0.035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.01, 0.0, 0.0, 0.083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0, 0.0, 0.042, 0.0, 0.0, 0.017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022, 0.0, 0.0, 0.025, 0.0, 0.0, 0.006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006, 0.0, 0.0, 0.056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044, 0.0, 0.0, 0.036, 0.0, 0.0, 0.032, 0.0, 0.0, 0.006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046, 0.0, 0.0, 0.025, 0.0, 0.0, 0.01, 0.0, 0.0, 0.311, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0, 0.011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.0, 0.0, 0.129, 0.0, 0.0, 0.061, 0.0, 0.0, 0.011, 0.0, 0.0, 0.051, 0.0, 0.0, 0.067, 0.0, 0.0, 0.075, 0.0, 0.0, 0.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012, 0.0, 0.0, 0.223, 0.0, 0.0, 0.021, 0.0, 0.0, 0.016, 0.0, 0.0, 0.191, 0.0, 0.0, 0.03, 0.0, 0.0, 0.005, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014, 0.0, 0.0, 0.037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033, 0.0, 0.0, 0.004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014, 0.0, 0.0, 0.055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.054, 0.0, 0.0, 0.014, 0.0, 0.0, 0.043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017, 0.0, 0.0, 0.037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.0, 0.0, 0.018, 0.0, 0.0, 0.022, 0.0, 0.0, 0.051, 0.0, 0.0, 0.083, 0.0, 0.03, 0.0, 0.0, 0.0, 0.008, 0.0, 0.0, 0.04, 0.0, 0.0, 0.176, 0.0, 0.0, 0.047, 0.0, 0.0, 0.048, 0.0, 0.0, 0.178, 0.0, 0.0, 0.064, 0.0, 0.0, 0.058, 0.0, 0.0, 0.058, 0.0, 0.0, 0.039, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.331, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.071, 0.0, 0.0, 0.034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0, 0.067, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.068, 0.0, 0.0, 0.025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006, 0.0, 0.0, 0.114, 0.0, 0.0, 0.02], "type": "histogram"}];
            const layout = Object.assign({}, {"template": {"data": {"histogram2dcontour": [{"type": "histogram2dcontour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "choropleth": [{"type": "choropleth", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "histogram2d": [{"type": "histogram2d", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "heatmap": [{"type": "heatmap", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "contourcarpet": [{"type": "contourcarpet", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "contour": [{"type": "contour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "surface": [{"type": "surface", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "mesh3d": [{"type": "mesh3d", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "scatter": [{"fillpattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}, "type": "scatter"}], "parcoords": [{"type": "parcoords", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolargl": [{"type": "scatterpolargl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "bar"}], "scattergeo": [{"type": "scattergeo", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolar": [{"type": "scatterpolar", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "histogram": [{"marker": {"pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "histogram"}], "scattergl": [{"type": "scattergl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatter3d": [{"type": "scatter3d", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermap": [{"type": "scattermap", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermapbox": [{"type": "scattermapbox", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterternary": [{"type": "scatterternary", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattercarpet": [{"type": "scattercarpet", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "barpolar"}], "pie": [{"automargin": true, "type": "pie"}]}, "layout": {"autotypenumbers": "strict", "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "hovermode": "closest", "hoverlabel": {"align": "left"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"bgcolor": "#E5ECF6", "angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "ternary": {"bgcolor": "#E5ECF6", "aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]]}, "xaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "yaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "geo": {"bgcolor": "white", "landcolor": "#E5ECF6", "subunitcolor": "white", "showland": true, "showlakes": true, "lakecolor": "white"}, "title": {"x": 0.05}, "mapbox": {"style": "light"}}}, "margin": {"l": 50, "r": 30, "t": 30, "b": 50}, "font": {"color": "#a39e93"}, "xaxis": {"title": {"text": "Quality Repetition Ratio"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "yaxis": {"title": {"text": "Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "title": {}, "height": 400, "paper_bgcolor": "rgba(0,0,0,0)", "plot_bgcolor": "rgba(0,0,0,0)"}, darkLayout);
            Plotly.newPlot('chart_8', data, layout, plotlyConfig);
        })();
        
        (function() {
            const data = [{"marker": {"color": "#d4a574"}, "nbinsx": 30, "opacity": 0.85, "x": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.845, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.835, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "type": "histogram"}];
            const layout = Object.assign({}, {"template": {"data": {"histogram2dcontour": [{"type": "histogram2dcontour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "choropleth": [{"type": "choropleth", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "histogram2d": [{"type": "histogram2d", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "heatmap": [{"type": "heatmap", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "contourcarpet": [{"type": "contourcarpet", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "contour": [{"type": "contour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "surface": [{"type": "surface", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "mesh3d": [{"type": "mesh3d", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "scatter": [{"fillpattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}, "type": "scatter"}], "parcoords": [{"type": "parcoords", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolargl": [{"type": "scatterpolargl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "bar"}], "scattergeo": [{"type": "scattergeo", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolar": [{"type": "scatterpolar", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "histogram": [{"marker": {"pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "histogram"}], "scattergl": [{"type": "scattergl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatter3d": [{"type": "scatter3d", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermap": [{"type": "scattermap", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermapbox": [{"type": "scattermapbox", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterternary": [{"type": "scatterternary", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattercarpet": [{"type": "scattercarpet", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "barpolar"}], "pie": [{"automargin": true, "type": "pie"}]}, "layout": {"autotypenumbers": "strict", "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "hovermode": "closest", "hoverlabel": {"align": "left"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"bgcolor": "#E5ECF6", "angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "ternary": {"bgcolor": "#E5ECF6", "aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]]}, "xaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "yaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "geo": {"bgcolor": "white", "landcolor": "#E5ECF6", "subunitcolor": "white", "showland": true, "showlakes": true, "lakecolor": "white"}, "title": {"x": 0.05}, "mapbox": {"style": "light"}}}, "margin": {"l": 50, "r": 30, "t": 30, "b": 50}, "font": {"color": "#a39e93"}, "xaxis": {"title": {"text": "Quality Quality Score"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "yaxis": {"title": {"text": "Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "title": {}, "height": 400, "paper_bgcolor": "rgba(0,0,0,0)", "plot_bgcolor": "rgba(0,0,0,0)"}, darkLayout);
            Plotly.newPlot('chart_9', data, layout, plotlyConfig);
        })();
        

        
        (function() {
            const data = [{"hovertemplate": "Index: %{x}<br>Value: %{y:.2f}<extra></extra>", "marker": {"color": "#6b9bd1", "opacity": 0.6, "size": 6}, "mode": "markers", "name": "Normal", "x": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903], "y": [16, 6, 121, 16, 6, 53, 16, 6, 209, 16, 6, 216, 16, 21, 133, 16, 125, 67, 24, 12, 124, 16, 19, 24, 18, 21, 16, 5, 115, 16, 6, 7, 16, 14, 124, 16, 7, 24, 15, 8, 16, 10, 169, 16, 7, 172, 24, 23, 66, 16, 10, 208, 16, 10, 11, 16, 5, 26, 16, 11, 16, 6, 28, 24, 32, 16, 16, 7, 80, 16, 6, 132, 16, 7, 17, 16, 9, 24, 14, 58, 16, 11, 25, 16, 8, 253, 24, 17, 6, 16, 11, 30, 16, 5, 16, 10, 24, 20, 5, 24, 26, 37, 16, 27, 42, 24, 60, 21, 16, 126, 21, 16, 6, 121, 16, 12, 39, 16, 7, 24, 17, 102, 16, 15, 108, 24, 16, 106, 16, 9, 113, 16, 7, 261, 24, 64, 48, 24, 43, 39, 24, 31, 14, 16, 6, 78, 16, 126, 7, 16, 34, 24, 16, 15, 76, 16, 14, 73, 16, 7, 84, 24, 17, 13, 16, 6, 63, 16, 8, 8, 16, 8, 79, 16, 61, 89, 16, 6, 13, 24, 17, 9, 24, 14, 6, 24, 15, 223, 24, 13, 17, 24, 55, 32, 24, 17, 17, 24, 20, 259, 24, 14, 4, 24, 14, 11, 24, 13, 42, 16, 12, 94, 24, 20, 5, 24, 16, 47, 24, 13, 3, 16, 11, 242, 24, 23, 13, 16, 24, 125, 16, 12, 24, 27, 1, 24, 15, 16, 9, 257, 16, 7, 221, 16, 11, 189, 16, 9, 203, 16, 126, 52, 24, 15, 35, 24, 14, 5, 16, 12, 208, 24, 22, 9, 24, 18, 118, 16, 79, 16, 10, 100, 24, 11, 3, 16, 5, 158, 16, 12, 53, 16, 5, 6, 24, 20, 1, 16, 8, 167, 16, 17, 24, 19, 15, 16, 6, 34, 16, 11, 66, 16, 12, 227, 24, 40, 26, 16, 7, 40, 16, 10, 19, 24, 31, 13, 24, 21, 11, 24, 21, 16, 24, 17, 12, 24, 17, 5, 24, 13, 5, 24, 44, 27, 24, 15, 38, 24, 16, 11, 24, 14, 63, 24, 14, 20, 24, 13, 11, 24, 74, 44, 16, 4, 46, 24, 12, 11, 16, 8, 28, 24, 9, 43, 24, 12, 62, 24, 13, 7, 16, 8, 66, 24, 23, 6, 24, 16, 96, 16, 30, 186, 24, 17, 11, 24, 23, 3, 16, 8, 5, 16, 30, 24, 18, 4, 16, 12, 10, 24, 15, 37, 16, 15, 138, 24, 19, 6, 24, 18, 40, 16, 8, 49, 24, 22, 64, 24, 27, 11, 16, 11, 108, 16, 17, 70, 24, 12, 105, 16, 15, 111, 16, 6, 162, 24, 19, 94, 16, 19, 177, 24, 20, 101, 24, 21, 9, 24, 23, 169, 16, 25, 227, 16, 240, 50, 16, 22, 142, 16, 12, 16, 9, 24, 12, 8, 16, 221, 53, 24, 15, 24, 18, 76, 16, 23, 17, 24, 13, 24, 13, 14, 16, 8, 19, 16, 7, 20, 16, 8, 134, 24, 36, 142, 16, 30, 17, 24, 11, 5, 24, 14, 4, 24, 23, 24, 16, 7, 24, 11, 208, 24, 11, 105, 24, 17, 5, 24, 24, 18, 24, 20, 8, 24, 20, 10, 24, 13, 49, 24, 13, 104, 24, 11, 11, 24, 32, 1, 24, 57, 24, 24, 22, 9, 24, 16, 6, 16, 10, 42, 24, 15, 5, 24, 16, 7, 24, 20, 15, 24, 12, 33, 24, 16, 8, 24, 21, 30, 16, 12, 118, 24, 16, 30, 16, 9, 187, 16, 9, 14, 16, 9, 151, 16, 8, 102, 24, 20, 5, 16, 14, 16, 9, 166, 16, 6, 243, 16, 7, 7, 16, 8, 90, 24, 13, 10, 24, 31, 8, 24, 10, 158, 24, 112, 16, 24, 23, 219, 24, 28, 39, 16, 10, 86, 16, 14, 9, 16, 8, 180, 24, 18, 111, 24, 31, 84, 16, 4, 11, 24, 71, 10, 16, 4, 218, 24, 20, 1, 16, 9, 13, 24, 14, 5, 16, 11, 62, 16, 7, 98, 16, 8, 201, 16, 6, 141, 24, 15, 44, 16, 4, 13, 16, 25, 40, 16, 5, 131, 16, 5, 42, 16, 10, 21, 24, 17, 39, 16, 8, 154, 16, 11, 104, 24, 19, 7, 16, 7, 43, 16, 8, 21, 16, 13, 56, 16, 10, 30, 16, 8, 10, 16, 8, 22, 16, 10, 103, 16, 5, 66, 16, 9, 43, 16, 14, 7, 16, 7, 95, 16, 21, 118, 16, 9, 72, 16, 12, 16, 12, 248, 16, 10, 209, 16, 8, 254, 16, 9, 28, 16, 12, 20, 16, 11, 220, 16, 7, 184, 16, 9, 81, 16, 10, 227, 16, 11, 218, 16, 9, 16, 5, 13, 16, 14, 24, 16, 10, 24, 21, 6, 24, 18, 9, 16, 23, 109, 16, 7, 251, 16, 10, 202, 16, 8, 72, 16, 8, 22, 24, 16, 85, 24, 60, 22, 16, 12, 37, 24, 18, 4, 24, 32, 1, 24, 19, 8, 24, 90, 31, 16, 8, 148, 16, 10, 122, 24, 14, 9, 24, 25, 9, 24, 66, 33, 16, 48, 122, 16, 14, 76, 24, 16, 6, 24, 23, 69, 16, 12, 22, 24, 12, 74, 24, 16, 18, 24, 14, 31, 24, 13, 94, 24, 11, 221, 24, 27, 18, 24, 20, 214, 16, 10, 118, 16, 15, 10, 16, 9, 207, 24, 12, 24, 17, 63, 24, 13, 3, 24, 15, 13, 24, 12, 5, 24, 10, 17, 24, 14, 165, 24, 21, 16, 9, 16, 5, 79, 16, 8, 139, 16, 4, 186, 16, 5, 89, 16, 5, 94, 24, 16, 18, 16, 5, 113, 16, 10, 215, 16, 6, 131, 16, 5, 211, 16, 7, 136, 16, 8, 226, 24, 23, 24, 88, 93, 16, 20, 19, 16, 13, 16, 13, 31, 24, 23, 63, 16, 6, 24, 17, 1, 24, 52, 42, 16, 10, 241, 24, 17, 7, 24, 75, 45, 16, 8, 11, 16, 7, 205, 24, 13, 30, 16, 7, 221, 24, 79, 54, 16, 7, 136, 24, 14, 6, 16, 5, 18, 16, 12, 16, 11, 204, 16, 6, 24, 13, 235, 16, 10, 86, 16, 8, 16, 10, 86, 16, 6, 122, 24, 33, 1, 24, 61, 43, 24, 21, 18, 24, 15, 94, 24, 40, 12, 24, 13, 84, 24, 23, 111, 24, 20, 20, 24, 13, 25, 16, 12, 93, 24, 18, 6, 24, 28, 70, 24, 40, 31, 24, 22, 15, 16, 10, 3, 16, 9, 146, 24, 19, 7, 24, 16, 8, 24, 15, 6, 16, 8, 188, 16, 7, 6, 24, 14, 5, 16, 10, 197, 16, 9, 217, 24, 18, 99, 16, 10, 112, 16, 4, 6, 16, 7, 112, 16, 12, 111, 16, 8, 20, 16, 6, 207, 16, 11, 33, 16, 10, 212, 16, 18, 78, 16, 7, 90, 16, 9, 179, 16, 8, 123, 16, 9, 206, 16, 6, 171, 16, 4, 69, 24, 43, 12, 16, 8, 9, 24, 10, 11, 24, 36, 23, 16, 6, 19, 24, 65, 33, 24, 12, 11, 16, 7, 8, 16, 30, 59, 16, 10, 8, 16, 5, 16, 7, 20, 16, 31, 169, 16, 14, 6, 16, 6, 16, 11, 176, 16, 12, 1, 16, 5, 6, 16, 11, 6, 24, 14, 25, 16, 5, 15, 16, 10, 19, 16, 7, 62, 16, 11, 19, 16, 4, 10, 16, 4, 14, 16, 9, 16, 8, 184, 16, 5, 209, 16, 12, 118, 16, 10, 205, 16, 7, 244, 16, 7, 194, 16, 8, 16, 9, 83, 24, 14, 8, 16, 8, 177, 16, 8, 47, 24, 10, 31, 24, 15, 3, 24, 20, 11, 24, 16, 6, 24, 17, 1, 24, 18, 33, 24, 15, 8, 24, 14, 94, 24, 13, 24, 11, 1, 16, 8, 212, 16, 7, 173, 24, 31, 28, 16, 6, 16, 4, 206, 16, 7, 95, 24, 23, 8, 16, 15, 52, 16, 9, 16, 7, 237, 16, 7, 31, 16, 7, 258, 16, 4, 144, 24, 23, 17, 16, 12, 13, 24, 21, 40, 16, 6, 185, 16, 8, 224, 16, 11, 56, 16, 9, 92, 16, 9, 124, 16, 13, 201, 16, 15, 47, 16, 6, 123, 16, 11, 16, 11, 9, 16, 6, 200, 24, 12, 66, 16, 11, 16, 11, 96, 16, 9, 24, 44, 64, 16, 7, 16, 8, 16, 11, 16, 11, 24, 22, 17, 16, 11, 10, 16, 14, 147, 24, 59, 28, 24, 11, 133, 24, 21, 33, 24, 16, 11, 16, 6, 159, 24, 18, 69, 16, 8, 94, 16, 15, 121, 16, 14, 89, 24, 23, 11, 16, 8, 34, 16, 12, 52, 16, 7, 107, 16, 18, 7, 16, 6, 32, 16, 7, 41, 16, 6, 8, 24, 14, 4, 24, 12, 12, 24, 16, 7, 16, 8, 22, 24, 22, 12, 16, 9, 111, 24, 25, 13, 16, 15, 51, 16, 7, 155, 24, 16, 5, 24, 13, 38, 16, 9, 217, 16, 10, 136, 24, 12, 1, 16, 7, 184, 16, 8, 167, 24, 28, 28, 24, 31, 242, 16, 17, 88, 16, 13, 176, 16, 9, 16, 6, 10, 24, 41, 35, 24, 105, 68, 24, 24, 10, 16, 6, 37, 16, 10, 11, 24, 10, 32, 24, 74, 6, 16, 11, 16, 7, 204, 24, 14, 34, 16, 5, 29, 16, 24, 54, 16, 10, 16, 13, 131, 16, 7, 185, 16, 8, 8, 16, 8, 201, 16, 7, 94, 16, 5, 250, 16, 8, 117, 16, 17, 14, 16, 13, 245, 24, 13, 43, 16, 4, 10, 16, 9, 16, 13, 24, 12, 7, 16, 7, 90, 16, 13, 24, 12, 38, 16, 11, 16, 8, 158, 16, 7, 115, 16, 4, 10, 16, 11, 10, 16, 6, 141, 16, 9, 16, 14, 47, 16, 10, 185, 16, 10, 251, 16, 6, 128, 16, 13, 16, 16, 9, 16, 7, 234, 16, 7, 108, 16, 19, 11, 16, 15, 28, 16, 11, 142, 16, 11, 24, 19, 109, 24, 22, 87, 16, 9, 245, 16, 13, 38, 24, 11, 145, 24, 22, 28, 16, 13, 81, 16, 17, 29, 16, 10, 143, 16, 13, 34, 24, 17, 12, 24, 12, 20, 24, 22, 96, 24, 17, 20, 24, 17, 9, 24, 15, 6, 24, 12, 192, 24, 21, 106, 24, 31, 11, 24, 51, 3, 24, 19, 10, 24, 23, 83, 16, 12, 9, 24, 42, 44, 24, 23, 9, 16, 8, 40, 24, 26, 16, 24, 23, 11, 16, 23, 16, 7, 170, 16, 11, 101, 16, 37, 31, 16, 47, 65, 16, 21, 184, 16, 11, 113, 16, 7, 210, 16, 5, 237, 16, 10, 46, 24, 16, 21, 16, 9, 207, 24, 16, 1, 24, 16, 8, 24, 26, 11, 24, 19, 2, 24, 21, 11, 16, 11, 190, 24, 59, 24, 19, 17, 16, 6, 25, 24, 61, 43, 24, 19, 30, 16, 18, 94, 24, 22, 1, 24, 54, 25, 16, 10, 40, 24, 34, 39, 24, 17, 12, 24, 27, 218, 24, 19, 72, 24, 17, 6, 24, 34, 166, 16, 9, 16, 7, 29, 16, 5, 119, 24, 17, 47, 24, 14, 1, 16, 11, 16, 16, 10, 24, 15, 10, 16, 7, 18, 16, 4, 155, 24, 7, 7, 24, 12, 3, 16, 5, 160, 16, 9, 207, 16, 9, 253, 24, 14, 6, 16, 9, 12, 16, 9, 45, 16, 6, 258, 24, 20, 48, 16, 10, 24, 23, 17, 16, 8, 185, 24, 28, 14, 16, 8, 113, 24, 28, 16, 24, 14, 50, 24, 23, 152, 24, 18, 124, 16, 6, 24, 13, 255, 24, 10, 32, 24, 18, 1, 16, 7, 238, 16, 8, 16, 10, 60, 16, 8, 16, 11, 16, 9, 16, 9, 208, 16, 7, 186, 16, 10, 180, 16, 11, 16, 12, 16, 9, 91, 16, 9, 219, 24, 80, 42, 16, 8, 148, 24, 21, 13, 16, 8, 255, 16, 8, 22, 24, 60, 30, 24, 25, 58, 16, 7, 249, 16, 8, 78, 24, 15, 138, 24, 31, 132, 24, 43, 29, 16, 9, 10, 24, 15, 83, 24, 16, 7, 16, 30, 106, 24, 13, 10, 24, 13, 17, 16, 10, 84, 16, 11, 82, 16, 6, 26, 16, 8, 14, 16, 11, 157, 16, 9, 79, 16, 11, 111, 24, 59, 37, 16, 12, 155, 16, 11, 7, 16, 11, 20, 16, 8, 16, 18, 66, 16, 8, 114, 16, 14, 115, 16, 13, 199, 16, 12, 44, 16, 8, 185, 16, 8, 198, 16, 6, 16, 10, 193, 16, 13, 101, 16, 10, 210, 16, 5, 8, 16, 226, 23, 16, 8, 232, 16, 4, 1, 24, 22, 24, 32, 28, 24, 42, 239, 24, 27, 105, 24, 13, 33, 24, 21, 62, 24, 15, 65, 24, 18, 3, 24, 12, 77, 24, 18, 9, 16, 7, 137, 16, 250, 119, 24, 17, 13, 16, 8, 1, 24, 67, 57, 24, 17, 162, 16, 8, 12, 16, 11, 47, 24, 16, 42, 16, 6, 139, 16, 8, 66, 24, 81, 25, 16, 10, 16, 17, 9, 16, 4, 10, 24, 139, 41, 16, 11, 140, 16, 11, 111, 16, 12, 51, 16, 10, 44, 16, 7, 29, 16, 9, 13, 16, 9, 18, 16, 11, 80, 16, 6, 261, 16, 10, 233, 24, 17, 5, 24, 15, 1, 16, 7, 83, 16, 8, 16, 7, 78, 24, 17, 19, 16, 13, 12, 16, 19, 8, 16, 7, 228, 24, 16, 33, 16, 11, 157, 16, 10, 17, 24, 22, 227, 24, 15, 16, 9, 25, 24, 22, 14, 16, 16, 90, 24, 14, 3, 16, 7, 64, 24, 13, 57, 24, 14, 22, 24, 11, 36, 24, 11, 4, 24, 15, 170, 24, 74, 27, 24, 20, 6, 24, 24, 14, 16, 10, 32, 24, 11, 250, 16, 7, 243, 16, 14, 225, 16, 13, 16, 7, 190, 16, 10, 24, 13, 8, 24, 13, 106, 24, 106, 51, 24, 20, 32, 24, 34, 12, 24, 15, 3, 16, 7, 11, 16, 6, 219, 24, 18, 5, 16, 7, 149, 16, 8, 8, 16, 15, 16, 9, 261, 16, 9, 119, 16, 9, 130, 16, 9, 16, 12, 183, 16, 10, 63, 16, 15, 55, 24, 37, 160, 24, 15, 234, 16, 11, 165, 24, 34, 45, 16, 14, 10, 24, 20, 5, 16, 7, 11, 16, 9, 112, 16, 12, 62, 16, 10, 257, 16, 8, 113, 24, 20, 60, 24, 21, 67, 16, 6, 69, 16, 5, 209, 16, 14, 122, 16, 26, 76, 16, 6, 236, 24, 23, 122, 16, 10, 24, 20, 2, 24, 33, 11, 24, 28, 15, 24, 32, 7, 24, 46, 92, 24, 31, 42, 16, 13, 159, 16, 4, 108, 16, 9, 160, 24, 15, 56, 24, 20, 8, 16, 13, 11, 16, 11, 253, 24, 19, 30, 16, 4, 16, 16, 166, 16, 11, 12, 16, 8, 8, 16, 11, 211, 16, 9, 106, 16, 9, 16, 9, 203, 24, 63, 101, 16, 14, 198, 16, 10, 82, 16, 8, 259, 16, 15, 24, 21, 11, 16, 6, 24, 31, 103, 16, 20, 247, 16, 9, 16, 8, 238, 16, 7, 16, 9, 16, 8, 215, 24, 17, 11, 16, 8, 111, 24, 13, 4, 16, 8, 84, 24, 13, 84, 16, 10, 78, 24, 24, 1, 16, 7, 72, 16, 13, 139, 16, 7, 142, 16, 8, 1, 16, 9, 51, 16, 14, 16, 23, 12, 24, 25, 5, 16, 11, 40, 16, 11, 30, 16, 29, 11, 16, 6, 19, 16, 24, 87, 24, 18, 71, 16, 21, 9, 16, 11, 21, 16, 18, 16, 14, 11, 16, 16, 66, 24, 45, 57, 24, 16, 4, 24, 26, 13, 16, 8, 71, 16, 10, 20, 16, 5, 16, 19, 114, 16, 6, 244, 16, 9, 246, 24, 29, 49, 16, 9, 35, 16, 7, 186, 24, 29, 24, 28, 19, 24, 40, 31, 16, 16, 35, 24, 30, 1, 24, 50, 1, 24, 31, 8, 24, 23, 1, 24, 20, 6, 16, 13, 178, 24, 43, 31, 16, 11, 94, 24, 21, 20, 24, 11, 15, 24, 28, 1, 16, 9, 94, 24, 14, 9, 16, 14, 71, 16, 5, 109, 24, 20, 17, 24, 35, 243, 16, 8, 230, 24, 25, 70, 16, 9, 146, 16, 26, 13, 16, 19, 75, 16, 10, 165, 16, 30, 128, 16, 6, 83, 24, 44, 98, 24, 18, 1, 24, 20, 30, 16, 10, 38, 24, 18, 15, 24, 28, 16, 24, 15, 21, 24, 16, 186, 24, 23, 148, 16, 10, 232, 24, 27, 11, 16, 9, 122, 16, 9, 24, 20, 12, 24, 18, 5, 24, 13, 121, 24, 37, 28, 24, 14, 9, 16, 14, 195, 24, 16, 5, 24, 30, 11, 24, 12, 189, 16, 6, 105, 24, 32, 11, 24, 22, 23, 16, 10, 244, 24, 15, 166, 16, 9, 16, 13, 257, 24, 17, 206, 24, 135, 58, 16, 9, 120, 16, 8, 102, 16, 16, 121, 16, 11, 16, 16, 250, 16, 11, 187, 24, 13, 221, 16, 9, 16, 8, 24, 12, 78, 16, 13, 24, 14, 12, 16, 6, 250, 24, 31, 1, 24, 19, 35, 16, 13, 100, 16, 8, 149, 16, 6, 4, 16, 13, 52, 16, 10, 180, 24, 27, 15, 16, 11, 52, 16, 10, 205, 16, 11, 26, 16, 9, 16, 19, 7, 16, 9, 192, 16, 9, 25, 16, 6, 182, 16, 15, 6, 16, 17, 10, 16, 11, 218, 24, 9, 69, 16, 8, 20, 24, 31, 19, 24, 17, 61, 24, 12, 124, 24, 13, 102, 24, 12, 156, 16, 10, 16, 28, 154], "type": "scatter"}, {"customdata": [23, 38, 62, 80, 98, 101, 125, 239, 245, 277, 301, 403, 473, 476, 485, 494, 521, 608, 755, 788, 797, 902, 923, 926, 965, 974, 983, 1025, 1031, 1040, 1199, 1211, 1247, 1268, 1307, 1322, 1337, 1385, 1397, 1403, 1409, 1412, 1415, 1418, 1541, 1568, 1583, 1619, 1622, 1630, 1637, 1655, 1670, 1691, 1778, 1832, 1877, 1895, 1937, 1964, 1979, 1985, 1988, 1991, 2003, 2006, 2102, 2126, 2150, 2216, 2267, 2297, 2351, 2357, 2393, 2405, 2474, 2519, 2537, 2555, 2561, 2570, 2576, 2579, 2621, 2654, 2678, 2699, 2822, 2867, 2888, 2900, 2903, 2909, 2951, 2996], "hovertemplate": "Index: %{customdata}<br>Value: %{y:.2f}<extra></extra>", "marker": {"color": "#d66a6a", "size": 10, "symbol": "x"}, "mode": "markers", "name": "Outliers (96)", "x": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95], "y": [267, 287, 365, 304, 291, 297, 304, 295, 328, 275, 274, 323, 350, 304, 323, 298, 281, 297, 262, 291, 280, 299, 293, 354, 306, 340, 343, 385, 277, 309, 306, 285, 274, 268, 325, 270, 298, 288, 264, 309, 342, 310, 277, 420, 388, 354, 368, 371, 359, 295, 272, 268, 268, 329, 262, 339, 421, 393, 402, 331, 289, 282, 360, 338, 410, 290, 268, 314, 267, 263, 385, 368, 298, 299, 328, 267, 345, 281, 396, 352, 294, 264, 269, 403, 307, 338, 345, 318, 326, 271, 367, 314, 296, 337, 287, 274], "type": "scatter"}];
            const layout = Object.assign({}, {"template": {"data": {"histogram2dcontour": [{"type": "histogram2dcontour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "choropleth": [{"type": "choropleth", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "histogram2d": [{"type": "histogram2d", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "heatmap": [{"type": "heatmap", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "contourcarpet": [{"type": "contourcarpet", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "contour": [{"type": "contour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "surface": [{"type": "surface", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "mesh3d": [{"type": "mesh3d", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "scatter": [{"fillpattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}, "type": "scatter"}], "parcoords": [{"type": "parcoords", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolargl": [{"type": "scatterpolargl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "bar"}], "scattergeo": [{"type": "scattergeo", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolar": [{"type": "scatterpolar", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "histogram": [{"marker": {"pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "histogram"}], "scattergl": [{"type": "scattergl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatter3d": [{"type": "scatter3d", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermap": [{"type": "scattermap", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermapbox": [{"type": "scattermapbox", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterternary": [{"type": "scatterternary", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattercarpet": [{"type": "scattercarpet", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "barpolar"}], "pie": [{"automargin": true, "type": "pie"}]}, "layout": {"autotypenumbers": "strict", "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "hovermode": "closest", "hoverlabel": {"align": "left"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"bgcolor": "#E5ECF6", "angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "ternary": {"bgcolor": "#E5ECF6", "aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]]}, "xaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "yaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "geo": {"bgcolor": "white", "landcolor": "#E5ECF6", "subunitcolor": "white", "showland": true, "showlakes": true, "lakecolor": "white"}, "title": {"x": 0.05}, "mapbox": {"style": "light"}}}, "shapes": [{"line": {"color": "#e0b854", "dash": "dash"}, "type": "line", "x0": 0, "x1": 1, "xref": "x domain", "y0": 261.12229316230554, "y1": 261.12229316230554, "yref": "y"}, {"line": {"color": "#e0b854", "dash": "dash"}, "type": "line", "x0": 0, "x1": 1, "xref": "x domain", "y0": -168.57895982897222, "y1": -168.57895982897222, "yref": "y"}, {"line": {"color": "#7cb97c", "dash": "dot"}, "type": "line", "x0": 0, "x1": 1, "xref": "x domain", "y0": 46.27166666666667, "y1": 46.27166666666667, "yref": "y"}], "annotations": [{"font": {"color": "#a39e93"}, "showarrow": false, "text": "Upper bound (261.1)", "x": 1, "xanchor": "right", "xref": "x domain", "y": 261.12229316230554, "yanchor": "bottom", "yref": "y"}, {"font": {"color": "#a39e93"}, "showarrow": false, "text": "Lower bound (-168.6)", "x": 1, "xanchor": "right", "xref": "x domain", "y": -168.57895982897222, "yanchor": "bottom", "yref": "y"}, {"font": {"color": "#a39e93"}, "showarrow": false, "text": "Mean (46.3)", "x": 1, "xanchor": "right", "xref": "x domain", "y": 46.27166666666667, "yanchor": "bottom", "yref": "y"}], "margin": {"l": 50, "r": 30, "t": 30, "b": 50}, "font": {"color": "#a39e93"}, "xaxis": {"title": {"text": "Sample Index"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "yaxis": {"title": {"text": "Length Word Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "legend": {"font": {"color": "#a39e93"}, "orientation": "h", "yanchor": "bottom", "y": 1.02, "xanchor": "right", "x": 1}, "title": {}, "height": 400, "paper_bgcolor": "rgba(0,0,0,0)", "plot_bgcolor": "rgba(0,0,0,0)"}, darkLayout);
            Plotly.newPlot('anomaly_chart_0', data, layout, plotlyConfig);
        })();
        
        (function() {
            const data = [{"hovertemplate": "Index: %{x}<br>Value: %{y:.2f}<extra></extra>", "marker": {"color": "#6b9bd1", "opacity": 0.6, "size": 6}, "mode": "markers", "name": "Normal", "x": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902], "y": [18, 7, 147, 18, 7, 63, 18, 7, 247, 18, 7, 262, 18, 24, 147, 18, 190, 103, 27, 19, 175, 18, 20, 309, 27, 22, 25, 18, 6, 148, 18, 7, 8, 18, 14, 151, 18, 7, 27, 22, 14, 18, 10, 203, 18, 7, 199, 27, 40, 103, 18, 11, 235, 18, 12, 18, 18, 6, 36, 18, 12, 18, 7, 68, 27, 36, 17, 18, 8, 95, 18, 8, 155, 18, 8, 20, 18, 10, 27, 17, 83, 18, 12, 37, 18, 9, 318, 27, 20, 7, 18, 12, 79, 18, 7, 18, 11, 27, 24, 6, 27, 31, 52, 18, 35, 61, 27, 68, 25, 18, 152, 23, 18, 7, 153, 18, 21, 91, 18, 9, 27, 20, 122, 18, 22, 149, 27, 18, 129, 18, 10, 133, 18, 8, 308, 27, 74, 52, 27, 49, 49, 27, 34, 15, 18, 8, 94, 18, 152, 10, 18, 38, 35, 18, 17, 102, 18, 16, 103, 18, 8, 98, 27, 31, 25, 18, 7, 80, 18, 11, 12, 18, 10, 113, 18, 77, 124, 18, 8, 16, 27, 22, 11, 27, 16, 7, 27, 19, 289, 27, 16, 24, 27, 61, 33, 27, 19, 20, 27, 22, 303, 27, 16, 5, 27, 16, 16, 27, 15, 51, 18, 16, 133, 27, 23, 6, 27, 19, 58, 27, 15, 6, 18, 12, 27, 25, 15, 18, 28, 137, 18, 15, 27, 34, 1, 27, 18, 18, 10, 299, 18, 10, 252, 18, 12, 206, 18, 10, 245, 18, 154, 63, 27, 19, 55, 27, 21, 8, 18, 13, 252, 27, 24, 11, 27, 23, 141, 18, 102, 18, 11, 180, 27, 13, 5, 18, 6, 173, 18, 17, 64, 18, 6, 7, 27, 24, 1, 18, 9, 227, 18, 21, 27, 25, 28, 18, 8, 44, 18, 12, 73, 18, 13, 276, 27, 48, 27, 18, 8, 84, 18, 13, 22, 27, 34, 14, 27, 24, 13, 27, 23, 18, 27, 22, 20, 27, 20, 6, 27, 16, 6, 27, 53, 29, 27, 19, 46, 27, 18, 12, 27, 18, 83, 27, 18, 22, 27, 16, 14, 27, 97, 55, 18, 7, 91, 27, 17, 13, 18, 12, 65, 27, 12, 54, 27, 21, 89, 27, 16, 8, 18, 9, 84, 27, 28, 7, 27, 24, 155, 18, 47, 296, 27, 20, 14, 27, 48, 5, 18, 9, 6, 18, 48, 27, 21, 5, 18, 14, 19, 27, 18, 47, 18, 18, 164, 27, 21, 7, 27, 21, 50, 18, 11, 59, 27, 27, 71, 27, 33, 12, 18, 12, 162, 18, 19, 101, 27, 30, 153, 18, 16, 181, 18, 8, 237, 27, 34, 159, 18, 20, 255, 27, 33, 139, 27, 38, 11, 27, 43, 263, 18, 29, 307, 18, 67, 18, 25, 210, 18, 13, 18, 9, 27, 16, 9, 18, 258, 60, 27, 17, 27, 20, 121, 18, 31, 24, 27, 15, 27, 17, 21, 18, 8, 24, 18, 8, 42, 18, 10, 151, 27, 42, 161, 18, 32, 20, 27, 13, 11, 27, 19, 6, 27, 27, 27, 19, 8, 27, 13, 240, 27, 14, 134, 27, 21, 6, 27, 34, 22, 27, 25, 14, 27, 23, 11, 27, 17, 58, 27, 15, 125, 27, 13, 14, 27, 37, 1, 27, 64, 25, 27, 25, 10, 27, 20, 7, 18, 11, 59, 27, 21, 8, 27, 19, 8, 27, 23, 20, 27, 14, 35, 27, 19, 9, 27, 35, 44, 18, 15, 147, 27, 21, 36, 18, 10, 215, 18, 12, 18, 18, 10, 174, 18, 10, 134, 27, 23, 6, 18, 15, 18, 10, 194, 18, 7, 276, 18, 9, 12, 18, 9, 132, 27, 16, 15, 27, 36, 9, 27, 13, 213, 27, 135, 18, 27, 32, 261, 27, 33, 44, 18, 11, 94, 18, 19, 10, 18, 9, 200, 27, 28, 158, 27, 37, 98, 18, 6, 18, 27, 89, 12, 18, 5, 254, 27, 28, 2, 18, 10, 19, 27, 18, 6, 18, 16, 75, 18, 8, 118, 18, 9, 255, 18, 9, 177, 27, 20, 60, 18, 5, 16, 18, 30, 51, 18, 6, 145, 18, 6, 50, 18, 11, 24, 27, 20, 42, 18, 10, 204, 18, 13, 129, 27, 21, 8, 18, 8, 53, 18, 10, 23, 18, 14, 69, 18, 11, 36, 18, 12, 17, 18, 10, 30, 18, 13, 143, 18, 8, 81, 18, 10, 48, 18, 17, 8, 18, 9, 115, 18, 23, 127, 18, 10, 82, 18, 14, 18, 13, 293, 18, 13, 264, 18, 11, 300, 18, 10, 32, 18, 13, 27, 18, 19, 281, 18, 9, 218, 18, 13, 92, 18, 12, 291, 18, 13, 278, 18, 10, 18, 6, 21, 18, 28, 42, 18, 11, 27, 25, 9, 27, 22, 13, 18, 34, 176, 18, 7, 306, 18, 10, 233, 18, 9, 87, 18, 8, 24, 27, 22, 104, 27, 78, 25, 18, 18, 50, 27, 22, 9, 27, 44, 2, 27, 23, 9, 27, 102, 37, 18, 11, 229, 18, 11, 146, 27, 24, 18, 27, 28, 10, 27, 81, 39, 18, 74, 172, 18, 16, 90, 27, 19, 11, 27, 34, 84, 18, 19, 54, 27, 16, 82, 27, 19, 25, 27, 22, 56, 27, 17, 110, 27, 14, 276, 27, 32, 20, 27, 23, 244, 18, 11, 147, 18, 17, 18, 18, 11, 240, 27, 17, 27, 25, 84, 27, 17, 4, 27, 18, 23, 27, 14, 6, 27, 13, 44, 27, 17, 186, 27, 25, 18, 10, 18, 8, 101, 18, 9, 179, 18, 6, 266, 18, 6, 104, 18, 8, 109, 27, 20, 26, 18, 6, 139, 18, 11, 251, 18, 7, 163, 18, 9, 253, 18, 8, 170, 18, 9, 280, 27, 26, 27, 104, 108, 18, 37, 37, 18, 14, 18, 15, 34, 27, 28, 79, 18, 7, 27, 21, 1, 27, 60, 49, 18, 11, 272, 27, 20, 8, 27, 89, 50, 18, 10, 13, 18, 8, 243, 27, 17, 39, 18, 7, 292, 27, 97, 64, 18, 8, 173, 27, 17, 7, 18, 27, 69, 18, 13, 18, 12, 239, 18, 6, 27, 17, 282, 18, 11, 124, 18, 9, 18, 12, 167, 18, 6, 137, 27, 39, 1, 27, 82, 56, 27, 41, 24, 27, 17, 105, 27, 44, 13, 27, 15, 107, 27, 26, 129, 27, 26, 28, 27, 17, 28, 18, 13, 110, 27, 24, 8, 27, 34, 88, 27, 44, 38, 27, 27, 18, 18, 11, 6, 18, 10, 171, 27, 22, 8, 27, 19, 9, 27, 18, 7, 18, 9, 221, 18, 8, 10, 27, 18, 6, 18, 13, 219, 18, 10, 251, 27, 23, 121, 18, 11, 135, 18, 5, 12, 18, 8, 144, 18, 16, 126, 18, 13, 46, 18, 7, 237, 18, 12, 41, 18, 11, 254, 18, 21, 101, 18, 10, 112, 18, 10, 209, 18, 11, 138, 18, 10, 233, 18, 7, 211, 18, 5, 86, 27, 50, 13, 18, 9, 12, 27, 13, 18, 27, 42, 25, 18, 7, 27, 27, 79, 51, 27, 14, 14, 18, 8, 9, 18, 36, 84, 18, 11, 12, 18, 6, 18, 8, 23, 18, 40, 281, 18, 16, 7, 18, 7, 18, 12, 203, 18, 13, 2, 18, 7, 10, 18, 15, 14, 27, 19, 35, 18, 6, 19, 18, 12, 24, 18, 8, 72, 18, 13, 22, 18, 5, 21, 18, 7, 16, 18, 12, 18, 10, 218, 18, 6, 243, 18, 13, 185, 18, 11, 225, 18, 10, 293, 18, 8, 229, 18, 12, 18, 10, 95, 27, 18, 11, 18, 9, 221, 18, 9, 110, 27, 16, 40, 27, 18, 5, 27, 24, 12, 27, 19, 8, 27, 23, 2, 27, 21, 38, 27, 20, 9, 27, 19, 109, 27, 17, 27, 15, 2, 18, 10, 316, 18, 8, 197, 27, 49, 36, 18, 7, 303, 18, 5, 255, 18, 9, 118, 27, 33, 14, 18, 17, 66, 18, 10, 18, 8, 284, 18, 11, 43, 18, 11, 18, 7, 172, 27, 28, 18, 18, 14, 17, 27, 25, 48, 18, 7, 219, 18, 10, 276, 18, 14, 68, 18, 11, 151, 18, 11, 154, 18, 14, 227, 18, 16, 54, 18, 7, 248, 18, 13, 18, 12, 15, 18, 7, 230, 27, 16, 73, 18, 13, 315, 18, 12, 109, 18, 10, 27, 53, 70, 18, 8, 18, 9, 18, 12, 314, 18, 12, 27, 30, 20, 18, 11, 20, 18, 14, 204, 27, 72, 32, 27, 14, 151, 27, 25, 57, 27, 18, 15, 18, 7, 171, 27, 23, 111, 18, 9, 113, 18, 22, 139, 18, 15, 146, 27, 26, 12, 18, 9, 44, 18, 13, 61, 18, 8, 141, 18, 25, 26, 18, 9, 42, 18, 10, 48, 18, 8, 16, 27, 18, 6, 27, 17, 21, 27, 18, 8, 18, 9, 27, 27, 25, 14, 18, 10, 140, 27, 28, 16, 18, 16, 67, 18, 8, 186, 27, 20, 6, 27, 21, 55, 18, 10, 250, 18, 11, 155, 27, 14, 2, 18, 8, 219, 18, 9, 197, 27, 40, 36, 27, 44, 288, 18, 19, 125, 18, 15, 208, 18, 10, 18, 8, 13, 27, 48, 42, 27, 124, 79, 27, 36, 13, 18, 7, 50, 18, 13, 16, 27, 14, 39, 27, 90, 9, 18, 12, 18, 8, 234, 27, 17, 42, 18, 14, 62, 18, 28, 68, 18, 12, 18, 20, 225, 18, 8, 224, 18, 9, 15, 18, 9, 263, 18, 14, 192, 18, 7, 297, 18, 9, 131, 18, 19, 18, 18, 15, 277, 27, 18, 51, 18, 5, 12, 18, 10, 18, 15, 27, 17, 11, 18, 10, 104, 18, 15, 27, 15, 67, 18, 14, 18, 9, 191, 18, 9, 141, 18, 5, 20, 18, 13, 11, 18, 7, 183, 18, 10, 18, 16, 59, 18, 11, 206, 18, 11, 281, 18, 7, 164, 18, 13, 320, 18, 17, 10, 18, 8, 283, 18, 7, 119, 18, 20, 12, 18, 17, 37, 18, 12, 165, 18, 12, 27, 30, 124, 27, 26, 97, 18, 10, 289, 18, 14, 41, 27, 15, 188, 27, 24, 33, 18, 14, 90, 18, 18, 31, 18, 11, 169, 18, 14, 44, 27, 21, 13, 27, 19, 27, 27, 25, 111, 27, 20, 23, 27, 21, 10, 27, 20, 11, 27, 14, 222, 27, 26, 119, 27, 33, 14, 27, 58, 4, 27, 22, 11, 27, 26, 92, 18, 13, 21, 27, 46, 50, 27, 25, 10, 18, 10, 53, 27, 29, 20, 27, 28, 15, 18, 26, 291, 18, 8, 200, 18, 12, 122, 18, 44, 35, 18, 57, 74, 18, 24, 200, 18, 13, 124, 18, 9, 230, 18, 6, 287, 18, 11, 50, 27, 18, 25, 18, 10, 246, 27, 20, 3, 27, 20, 9, 27, 30, 16, 27, 24, 9, 27, 24, 14, 18, 11, 272, 27, 67, 27, 23, 19, 18, 6, 35, 27, 71, 51, 27, 22, 34, 18, 21, 138, 27, 26, 6, 27, 67, 36, 18, 13, 48, 27, 43, 64, 27, 28, 16, 27, 34, 277, 27, 22, 101, 27, 21, 7, 27, 39, 192, 18, 9, 18, 10, 56, 18, 5, 148, 27, 20, 72, 27, 19, 4, 18, 11, 32, 18, 12, 27, 37, 21, 18, 8, 22, 18, 8, 179, 27, 15, 12, 27, 14, 4, 18, 6, 205, 18, 10, 256, 18, 10, 307, 27, 22, 15, 18, 10, 18, 18, 10, 51, 18, 7, 299, 27, 27, 65, 18, 11, 27, 34, 30, 18, 9, 211, 27, 35, 18, 18, 12, 131, 27, 32, 19, 27, 17, 81, 27, 28, 192, 27, 21, 136, 18, 8, 27, 15, 293, 27, 13, 42, 27, 24, 3, 18, 8, 293, 18, 11, 18, 11, 66, 18, 10, 18, 12, 18, 11, 18, 10, 230, 18, 7, 213, 18, 11, 208, 18, 12, 18, 13, 18, 11, 120, 18, 12, 245, 27, 89, 47, 18, 10, 289, 27, 30, 14, 18, 8, 286, 18, 11, 26, 27, 67, 33, 27, 31, 71, 18, 8, 277, 18, 8, 149, 27, 29, 240, 27, 52, 203, 27, 52, 32, 18, 10, 13, 27, 19, 118, 27, 19, 8, 18, 60, 220, 27, 17, 13, 27, 17, 25, 18, 11, 145, 18, 12, 132, 18, 8, 55, 18, 11, 20, 18, 14, 190, 18, 12, 101, 18, 12, 152, 27, 71, 41, 18, 14, 179, 18, 13, 8, 18, 13, 47, 18, 10, 18, 41, 108, 18, 9, 133, 18, 15, 130, 18, 14, 261, 18, 13, 62, 18, 10, 244, 18, 9, 223, 18, 7, 18, 12, 251, 18, 29, 252, 18, 12, 268, 18, 7, 12, 18, 292, 36, 18, 10, 321, 18, 8, 9, 27, 25, 27, 36, 34, 27, 47, 279, 27, 34, 119, 27, 20, 45, 27, 25, 81, 27, 18, 74, 27, 21, 4, 27, 15, 85, 27, 21, 11, 18, 9, 189, 18, 318, 162, 27, 20, 18, 18, 10, 3, 27, 86, 71, 27, 21, 240, 18, 10, 16, 18, 14, 78, 27, 21, 45, 18, 7, 161, 18, 11, 102, 27, 101, 28, 18, 11, 307, 18, 19, 10, 18, 5, 13, 27, 178, 57, 18, 13, 161, 18, 13, 127, 18, 13, 70, 18, 11, 57, 18, 8, 37, 18, 10, 16, 18, 10, 19, 18, 12, 100, 18, 8, 18, 12, 296, 27, 20, 6, 27, 18, 1, 18, 10, 114, 18, 9, 18, 8, 98, 27, 20, 24, 18, 16, 15, 18, 22, 9, 18, 9, 274, 27, 20, 40, 18, 14, 206, 18, 14, 26, 27, 27, 275, 27, 27, 18, 10, 28, 27, 26, 15, 18, 18, 104, 27, 17, 4, 18, 8, 82, 27, 16, 86, 27, 20, 42, 27, 14, 38, 27, 17, 6, 27, 18, 27, 81, 32, 27, 23, 7, 27, 27, 21, 18, 11, 41, 27, 13, 18, 8, 283, 18, 15, 259, 18, 14, 18, 8, 254, 18, 11, 27, 20, 11, 27, 22, 130, 27, 122, 57, 27, 28, 37, 27, 38, 14, 27, 20, 4, 18, 8, 21, 18, 7, 242, 27, 21, 6, 18, 8, 181, 18, 9, 16, 18, 18, 18, 10, 18, 11, 137, 18, 10, 177, 18, 11, 320, 18, 21, 261, 18, 11, 70, 18, 17, 70, 27, 54, 209, 27, 17, 274, 18, 14, 189, 27, 48, 53, 18, 15, 15, 27, 23, 6, 18, 9, 17, 18, 10, 126, 18, 14, 77, 18, 11, 303, 18, 9, 136, 27, 33, 86, 27, 30, 101, 18, 7, 81, 18, 6, 242, 18, 20, 168, 18, 29, 83, 18, 7, 271, 27, 26, 153, 18, 10, 27, 22, 3, 27, 40, 17, 27, 35, 22, 27, 39, 9, 27, 59, 109, 27, 39, 51, 18, 14, 178, 18, 8, 129, 18, 11, 192, 27, 18, 73, 27, 23, 9, 18, 14, 15, 18, 16, 306, 27, 27, 50, 18, 8, 18, 19, 230, 18, 20, 22, 18, 9, 13, 18, 13, 257, 18, 10, 121, 18, 11, 18, 10, 240, 27, 75, 112, 18, 15, 18, 11, 96, 18, 9, 312, 18, 17, 27, 27, 15, 18, 7, 27, 39, 127, 18, 22, 293, 18, 10, 18, 9, 292, 18, 8, 312, 18, 10, 18, 15, 260, 27, 23, 14, 18, 10, 143, 27, 18, 5, 18, 10, 92, 27, 15, 96, 18, 11, 88, 27, 31, 2, 18, 8, 85, 18, 16, 164, 18, 9, 166, 18, 9, 6, 18, 11, 69, 18, 16, 18, 28, 13, 27, 30, 6, 18, 14, 55, 18, 13, 40, 18, 33, 13, 18, 6, 28, 18, 29, 115, 27, 25, 81, 18, 27, 14, 18, 12, 33, 18, 18, 18, 15, 19, 18, 18, 74, 27, 50, 71, 27, 21, 8, 27, 34, 16, 18, 12, 99, 18, 12, 51, 18, 7, 18, 59, 274, 18, 7, 291, 18, 10, 290, 27, 37, 56, 18, 14, 52, 18, 8, 229, 27, 33, 27, 34, 21, 27, 51, 35, 18, 19, 40, 27, 40, 1, 27, 61, 3, 27, 37, 9, 27, 28, 2, 27, 25, 7, 18, 14, 200, 27, 48, 33, 18, 14, 111, 27, 24, 24, 27, 13, 16, 27, 42, 3, 18, 11, 104, 27, 19, 11, 18, 17, 83, 18, 5, 129, 27, 26, 19, 27, 45, 285, 18, 9, 292, 27, 31, 79, 18, 10, 190, 18, 30, 15, 18, 20, 83, 18, 11, 200, 18, 36, 181, 18, 7, 98, 27, 52, 135, 27, 21, 3, 27, 27, 41, 18, 14, 55, 27, 21, 17, 27, 35, 17, 27, 18, 43, 27, 20, 217, 27, 26, 160, 18, 13, 267, 27, 35, 12, 18, 10, 136, 18, 11, 27, 24, 17, 27, 25, 10, 27, 17, 144, 27, 43, 31, 27, 18, 14, 18, 16, 238, 27, 19, 7, 27, 34, 14, 27, 14, 210, 18, 7, 120, 27, 37, 19, 27, 25, 25, 18, 11, 27, 25, 276, 18, 10, 18, 14, 307, 27, 19, 242, 27, 158, 71, 18, 10, 144, 18, 9, 157, 18, 17, 144, 18, 12, 18, 17, 281, 18, 12, 220, 27, 15, 250, 18, 10, 18, 9, 27, 14, 109, 18, 14, 27, 18, 18, 18, 7, 27, 45, 3, 27, 25, 47, 18, 22, 183, 18, 9, 176, 18, 8, 6, 18, 14, 75, 18, 15, 269, 27, 32, 17, 18, 12, 64, 18, 13, 234, 18, 13, 29, 18, 10, 18, 23, 9, 18, 10, 236, 18, 12, 57, 18, 8, 211, 18, 17, 7, 18, 21, 14, 18, 12, 246, 27, 13, 80, 18, 10, 35, 27, 45, 26, 27, 20, 74, 27, 14, 142, 27, 15, 141, 27, 15, 201, 18, 10, 18, 35, 174], "type": "scatter"}, {"customdata": [38, 62, 80, 98, 101, 125, 230, 239, 245, 277, 301, 403, 466, 473, 476, 485, 494, 521, 608, 755, 788, 797, 902, 923, 926, 965, 974, 983, 1025, 1031, 1040, 1199, 1211, 1247, 1268, 1307, 1337, 1346, 1385, 1403, 1409, 1412, 1418, 1541, 1568, 1583, 1619, 1622, 1630, 1637, 1655, 1691, 1832, 1877, 1895, 1937, 1964, 1979, 1985, 1988, 1991, 2003, 2006, 2102, 2126, 2150, 2252, 2267, 2297, 2327, 2342, 2351, 2357, 2393, 2396, 2474, 2519, 2537, 2546, 2555, 2561, 2570, 2579, 2621, 2654, 2678, 2699, 2822, 2861, 2867, 2888, 2900, 2903, 2909, 2915, 2951, 2996], "hovertemplate": "Index: %{customdata}<br>Value: %{y:.2f}<extra></extra>", "marker": {"color": "#d66a6a", "size": 10, "symbol": "x"}, "mode": "markers", "name": "Outliers (97)", "x": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96], "y": [352, 423, 388, 361, 359, 354, 325, 334, 376, 348, 348, 423, 327, 399, 394, 368, 349, 340, 343, 336, 339, 355, 355, 346, 419, 354, 396, 405, 436, 331, 372, 369, 355, 341, 328, 428, 344, 340, 333, 359, 496, 390, 499, 469, 407, 473, 421, 427, 359, 331, 333, 376, 379, 496, 458, 457, 395, 356, 339, 407, 405, 500, 346, 331, 379, 355, 323, 486, 499, 503, 357, 354, 331, 495, 330, 397, 350, 465, 328, 402, 344, 329, 473, 353, 470, 431, 367, 392, 322, 326, 429, 365, 371, 424, 371, 350, 323], "type": "scatter"}];
            const layout = Object.assign({}, {"template": {"data": {"histogram2dcontour": [{"type": "histogram2dcontour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "choropleth": [{"type": "choropleth", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "histogram2d": [{"type": "histogram2d", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "heatmap": [{"type": "heatmap", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "contourcarpet": [{"type": "contourcarpet", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "contour": [{"type": "contour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "surface": [{"type": "surface", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "mesh3d": [{"type": "mesh3d", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "scatter": [{"fillpattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}, "type": "scatter"}], "parcoords": [{"type": "parcoords", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolargl": [{"type": "scatterpolargl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "bar"}], "scattergeo": [{"type": "scattergeo", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolar": [{"type": "scatterpolar", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "histogram": [{"marker": {"pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "histogram"}], "scattergl": [{"type": "scattergl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatter3d": [{"type": "scatter3d", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermap": [{"type": "scattermap", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermapbox": [{"type": "scattermapbox", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterternary": [{"type": "scatterternary", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattercarpet": [{"type": "scattercarpet", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "barpolar"}], "pie": [{"automargin": true, "type": "pie"}]}, "layout": {"autotypenumbers": "strict", "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "hovermode": "closest", "hoverlabel": {"align": "left"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"bgcolor": "#E5ECF6", "angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "ternary": {"bgcolor": "#E5ECF6", "aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]]}, "xaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "yaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "geo": {"bgcolor": "white", "landcolor": "#E5ECF6", "subunitcolor": "white", "showland": true, "showlakes": true, "lakecolor": "white"}, "title": {"x": 0.05}, "mapbox": {"style": "light"}}}, "shapes": [{"line": {"color": "#e0b854", "dash": "dash"}, "type": "line", "x0": 0, "x1": 1, "xref": "x domain", "y0": 321.32074355503676, "y1": 321.32074355503676, "yref": "y"}, {"line": {"color": "#e0b854", "dash": "dash"}, "type": "line", "x0": 0, "x1": 1, "xref": "x domain", "y0": -207.53607688837008, "y1": -207.53607688837008, "yref": "y"}, {"line": {"color": "#7cb97c", "dash": "dot"}, "type": "line", "x0": 0, "x1": 1, "xref": "x domain", "y0": 56.89233333333333, "y1": 56.89233333333333, "yref": "y"}], "annotations": [{"font": {"color": "#a39e93"}, "showarrow": false, "text": "Upper bound (321.3)", "x": 1, "xanchor": "right", "xref": "x domain", "y": 321.32074355503676, "yanchor": "bottom", "yref": "y"}, {"font": {"color": "#a39e93"}, "showarrow": false, "text": "Lower bound (-207.5)", "x": 1, "xanchor": "right", "xref": "x domain", "y": -207.53607688837008, "yanchor": "bottom", "yref": "y"}, {"font": {"color": "#a39e93"}, "showarrow": false, "text": "Mean (56.9)", "x": 1, "xanchor": "right", "xref": "x domain", "y": 56.89233333333333, "yanchor": "bottom", "yref": "y"}], "margin": {"l": 50, "r": 30, "t": 30, "b": 50}, "font": {"color": "#a39e93"}, "xaxis": {"title": {"text": "Sample Index"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "yaxis": {"title": {"text": "Length Token Count"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "legend": {"font": {"color": "#a39e93"}, "orientation": "h", "yanchor": "bottom", "y": 1.02, "xanchor": "right", "x": 1}, "title": {}, "height": 400, "paper_bgcolor": "rgba(0,0,0,0)", "plot_bgcolor": "rgba(0,0,0,0)"}, darkLayout);
            Plotly.newPlot('anomaly_chart_1', data, layout, plotlyConfig);
        })();
        
        (function() {
            const data = [{"hovertemplate": "Index: %{x}<br>Value: %{y:.2f}<extra></extra>", "marker": {"color": "#6b9bd1", "opacity": 0.6, "size": 6}, "mode": "markers", "name": "Normal", "x": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987], "y": [0.875, 1.0, 0.743801652892562, 0.875, 1.0, 0.6792452830188679, 0.875, 1.0, 0.45933014354066987, 0.875, 1.0, 0.6620370370370371, 0.875, 0.8095238095238095, 0.5714285714285714, 0.875, 0.768, 0.8507462686567164, 0.8333333333333334, 1.0, 0.5161290322580645, 0.875, 0.9473684210526315, 0.5280898876404494, 0.8333333333333334, 0.9444444444444444, 0.9047619047619048, 0.875, 1.0, 0.6869565217391305, 0.875, 1.0, 1.0, 0.875, 0.8571428571428571, 0.6935483870967742, 0.875, 0.8571428571428571, 0.578397212543554, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.6449704142011834, 0.875, 1.0, 0.622093023255814, 0.8333333333333334, 0.8695652173913043, 0.5909090909090909, 0.875, 1.0, 0.6153846153846154, 0.875, 1.0, 0.8181818181818182, 0.875, 1.0, 0.9230769230769231, 0.875, 0.8181818181818182, 0.4849315068493151, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9375, 0.9375, 0.875, 1.0, 0.75, 0.875, 1.0, 0.5454545454545454, 0.875, 1.0, 0.9411764705882353, 0.875, 1.0, 0.5592105263157895, 0.8333333333333334, 1.0, 0.5172413793103449, 0.875, 1.0, 0.88, 0.875, 1.0, 0.45454545454545453, 0.8333333333333334, 0.8823529411764706, 0.8333333333333334, 0.875, 0.9090909090909091, 1.0, 0.875, 1.0, 0.5979381443298969, 0.875, 0.9, 0.5993265993265994, 0.8333333333333334, 0.9, 1.0, 0.8333333333333334, 0.8846153846153846, 0.7027027027027027, 0.875, 0.5555555555555556, 0.7857142857142857, 0.8333333333333334, 0.8166666666666667, 0.9523809523809523, 0.875, 0.7619047619047619, 0.9523809523809523, 0.875, 1.0, 0.6776859504132231, 0.875, 0.9166666666666666, 0.6923076923076923, 0.875, 1.0, 0.5197368421052632, 0.8333333333333334, 0.9411764705882353, 0.7058823529411765, 0.875, 1.0, 0.5092592592592593, 0.8333333333333334, 0.9375, 0.6886792452830188, 0.875, 1.0, 0.7522123893805309, 0.875, 1.0, 0.5593869731800766, 0.8333333333333334, 0.828125, 0.8541666666666666, 0.8333333333333334, 0.8604651162790697, 0.8717948717948718, 0.8333333333333334, 0.7741935483870968, 0.7857142857142857, 0.875, 1.0, 0.6923076923076923, 0.875, 0.7619047619047619, 1.0, 0.875, 0.8823529411764706, 0.9166666666666666, 0.875, 0.8666666666666667, 0.5657894736842105, 0.875, 0.9285714285714286, 0.5342465753424658, 0.875, 1.0, 0.6071428571428571, 0.8333333333333334, 1.0, 0.9230769230769231, 0.875, 1.0, 0.7619047619047619, 0.875, 1.0, 1.0, 0.875, 1.0, 0.7341772151898734, 0.875, 0.7540983606557377, 0.5393258426966292, 0.875, 1.0, 0.9230769230769231, 0.8333333333333334, 0.9411764705882353, 0.8888888888888888, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.9333333333333333, 0.6233183856502242, 0.8333333333333334, 0.9230769230769231, 0.8823529411764706, 0.8333333333333334, 0.8181818181818182, 0.8125, 0.8333333333333334, 0.9411764705882353, 0.9411764705882353, 0.8333333333333334, 0.9, 0.5907335907335908, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9230769230769231, 0.7619047619047619, 0.875, 1.0, 0.6063829787234043, 0.8333333333333334, 0.95, 1.0, 0.8333333333333334, 0.9375, 0.7021276595744681, 0.8333333333333334, 0.9230769230769231, 1.0, 0.875, 0.9090909090909091, 0.6363636363636364, 0.8333333333333334, 0.9130434782608695, 0.8461538461538461, 0.875, 0.875, 0.72, 0.875, 0.9166666666666666, 0.48135593220338985, 0.8333333333333334, 0.8888888888888888, 1.0, 0.8333333333333334, 0.9333333333333333, 0.5, 0.875, 1.0, 0.5525291828793775, 0.875, 1.0, 0.5972850678733032, 0.875, 1.0, 0.4603174603174603, 0.875, 1.0, 0.6650246305418719, 0.875, 0.7698412698412699, 0.8269230769230769, 0.8333333333333334, 1.0, 0.9142857142857143, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 0.9166666666666666, 0.6009615384615384, 0.8333333333333334, 0.8636363636363636, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.7203389830508474, 0.875, 0.6509090909090909, 0.7721518987341772, 0.875, 1.0, 0.92, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.6265822784810127, 0.875, 1.0, 0.6226415094339622, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8, 1.0, 0.875, 1.0, 0.6167664670658682, 0.875, 0.656934306569343, 1.0, 0.8333333333333334, 1.0, 0.7333333333333333, 0.875, 1.0, 0.8823529411764706, 0.875, 1.0, 0.7272727272727273, 0.875, 1.0, 0.5550660792951542, 0.8333333333333334, 0.825, 0.8076923076923077, 0.875, 1.0, 1.0, 0.875, 1.0, 0.9473684210526315, 0.8333333333333334, 0.8709677419354839, 1.0, 0.8333333333333334, 0.9047619047619048, 1.0, 0.8333333333333334, 0.8095238095238095, 0.875, 0.8333333333333334, 0.8823529411764706, 1.0, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.7727272727272727, 0.8148148148148148, 0.8333333333333334, 0.9333333333333333, 0.8421052631578947, 0.8333333333333334, 0.9375, 0.9090909090909091, 0.8333333333333334, 0.9285714285714286, 0.6825396825396826, 0.8333333333333334, 0.8571428571428571, 0.95, 0.8333333333333334, 0.9230769230769231, 0.9090909090909091, 0.8333333333333334, 0.7162162162162162, 0.8409090909090909, 0.875, 1.0, 0.6521739130434783, 0.8333333333333334, 1.0, 0.9090909090909091, 0.875, 0.875, 1.0, 0.8333333333333334, 1.0, 0.9302325581395349, 0.8333333333333334, 0.9166666666666666, 0.6290322580645161, 0.8333333333333334, 0.9230769230769231, 1.0, 0.875, 0.875, 0.7878787878787878, 0.8333333333333334, 0.8695652173913043, 1.0, 0.8333333333333334, 0.9375, 0.5833333333333334, 0.875, 0.7333333333333333, 0.45698924731182794, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.782608695652174, 1.0, 0.875, 1.0, 1.0, 0.875, 0.6346749226006192, 0.9333333333333333, 0.8333333333333334, 0.8333333333333334, 1.0, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 0.6756756756756757, 0.875, 0.9333333333333333, 0.7898550724637681, 0.8333333333333334, 0.8421052631578947, 1.0, 0.8333333333333334, 0.9444444444444444, 0.75, 0.875, 1.0, 0.8571428571428571, 0.8333333333333334, 0.8636363636363636, 0.734375, 0.8333333333333334, 0.9259259259259259, 0.9090909090909091, 0.875, 0.9090909090909091, 0.6388888888888888, 0.875, 0.8235294117647058, 0.6714285714285714, 0.8333333333333334, 1.0, 0.7047619047619048, 0.875, 0.9333333333333333, 0.6756756756756757, 0.875, 1.0, 0.5185185185185185, 0.8333333333333334, 1.0, 0.7446808510638298, 0.875, 1.0, 0.480225988700565, 0.8333333333333334, 0.95, 0.5346534653465347, 0.8333333333333334, 0.9523809523809523, 0.8888888888888888, 0.8333333333333334, 0.9565217391304348, 0.48520710059171596, 0.875, 0.84, 0.4933920704845815, 0.875, 0.6708333333333333, 0.74, 0.875, 0.9090909090909091, 0.5774647887323944, 0.875, 0.8333333333333334, 0.875, 0.8888888888888888, 0.6118421052631579, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 0.6470588235294118, 0.8490566037735849, 0.8333333333333334, 0.9333333333333333, 0.5510835913312694, 0.8333333333333334, 0.9444444444444444, 0.8552631578947368, 0.875, 0.9130434782608695, 1.0, 0.8333333333333334, 1.0, 0.5503355704697986, 0.8333333333333334, 0.9230769230769231, 0.9285714285714286, 0.875, 1.0, 0.9473684210526315, 0.875, 1.0, 1.0, 0.875, 0.875, 0.6343283582089553, 0.8333333333333334, 0.8611111111111112, 0.6267605633802817, 0.875, 0.9, 0.9411764705882353, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9130434782608695, 0.5871886120996441, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 0.5721153846153846, 0.8333333333333334, 0.9090909090909091, 0.638095238095238, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 0.9583333333333334, 0.8888888888888888, 0.8333333333333334, 0.85, 1.0, 0.8333333333333334, 0.9, 0.9, 0.8333333333333334, 0.9230769230769231, 0.8163265306122449, 0.8333333333333334, 0.9230769230769231, 0.6634615384615384, 0.8333333333333334, 0.9090909090909091, 1.0, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.7543859649122807, 0.8333333333333334, 0.8333333333333334, 0.9545454545454546, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.9047619047619048, 0.8333333333333334, 0.9333333333333333, 1.0, 0.8333333333333334, 0.875, 1.0, 0.8333333333333334, 0.75, 0.8666666666666667, 0.8333333333333334, 1.0, 0.9090909090909091, 0.8333333333333334, 0.875, 0.875, 0.8333333333333334, 0.9523809523809523, 0.7333333333333333, 0.875, 0.9166666666666666, 0.711864406779661, 0.8333333333333334, 1.0, 0.8, 0.875, 0.7777777777777778, 0.6737967914438503, 0.875, 0.8888888888888888, 0.9285714285714286, 0.875, 0.8888888888888888, 0.7019867549668874, 0.875, 1.0, 0.7254901960784313, 0.8333333333333334, 0.9, 1.0, 0.875, 0.8571428571428571, 0.5791245791245792, 0.875, 0.8888888888888888, 0.6325301204819277, 0.875, 0.8333333333333334, 0.5390946502057613, 0.875, 1.0, 1.0, 0.875, 0.875, 0.8333333333333334, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9032258064516129, 0.875, 0.8333333333333334, 1.0, 0.8333333333333334, 0.7053571428571429, 1.0, 0.8333333333333334, 0.8260869565217391, 0.589041095890411, 0.8333333333333334, 1.0, 0.8461538461538461, 0.875, 1.0, 0.7209302325581395, 0.875, 1.0, 1.0, 0.875, 0.75, 0.8333333333333334, 0.8888888888888888, 0.4774774774774775, 0.8333333333333334, 0.8387096774193549, 0.7023809523809523, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8591549295774648, 1.0, 0.875, 1.0, 0.5688073394495413, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.9230769230769231, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.8548387096774194, 0.875, 1.0, 0.7142857142857143, 0.875, 0.875, 0.5920398009950248, 0.875, 1.0, 0.7304964539007093, 0.8333333333333334, 0.9333333333333333, 0.75, 0.875, 1.0, 0.9230769230769231, 0.875, 0.96, 0.8, 0.875, 1.0, 0.6106870229007634, 0.875, 1.0, 0.8095238095238095, 0.875, 0.9, 0.7619047619047619, 0.8333333333333334, 0.9411764705882353, 0.8205128205128205, 0.875, 0.875, 0.512987012987013, 0.875, 1.0, 0.7019230769230769, 0.8333333333333334, 0.9473684210526315, 1.0, 0.875, 1.0, 0.7674418604651163, 0.875, 1.0, 1.0, 0.875, 0.8461538461538461, 0.8035714285714286, 0.875, 0.9, 0.9333333333333333, 0.875, 0.875, 0.9, 0.875, 1.0, 0.5909090909090909, 0.875, 1.0, 0.4854368932038835, 0.875, 1.0, 0.8181818181818182, 0.875, 1.0, 0.8837209302325582, 0.875, 0.9285714285714286, 1.0, 0.875, 1.0, 0.6105263157894737, 0.875, 0.8571428571428571, 0.5169491525423728, 0.875, 0.7777777777777778, 0.7777777777777778, 0.875, 0.8333333333333334, 0.6030534351145038, 0.875, 1.0, 0.5604838709677419, 0.875, 1.0, 0.5789473684210527, 0.875, 0.875, 0.5511811023622047, 0.875, 0.8888888888888888, 0.9285714285714286, 0.875, 1.0, 0.95, 0.875, 1.0, 0.6272727272727273, 0.875, 1.0, 0.6032608695652174, 0.875, 0.8888888888888888, 0.6666666666666666, 0.875, 0.9, 0.5418502202643172, 0.875, 0.9090909090909091, 0.5825688073394495, 0.875, 1.0, 0.5429553264604811, 0.875, 1.0, 1.0, 0.875, 0.8571428571428571, 0.7916666666666666, 0.875, 1.0, 0.6035714285714285, 0.8333333333333334, 0.7142857142857143, 1.0, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.875, 0.8260869565217391, 0.5137614678899083, 0.875, 1.0, 0.5936254980079682, 0.875, 0.9, 0.5693069306930693, 0.875, 1.0, 0.8333333333333334, 0.875, 1.0, 0.8636363636363636, 0.8333333333333334, 0.9375, 0.8235294117647058, 0.8333333333333334, 0.8833333333333333, 1.0, 0.875, 0.9166666666666666, 0.918918918918919, 0.8333333333333334, 0.9444444444444444, 1.0, 0.8333333333333334, 0.96875, 1.0, 0.8333333333333334, 0.9473684210526315, 1.0, 0.8333333333333334, 0.6666666666666666, 0.8709677419354839, 0.875, 1.0, 0.6418918918918919, 0.875, 0.9, 0.6967213114754098, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.88, 1.0, 0.8333333333333334, 0.696969696969697, 0.8181818181818182, 0.875, 0.8125, 0.5819672131147541, 0.875, 0.9285714285714286, 0.8157894736842105, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.8260869565217391, 0.6521739130434783, 0.875, 1.0, 0.8636363636363636, 0.8333333333333334, 0.9166666666666666, 0.7702702702702703, 0.8333333333333334, 0.9375, 0.8888888888888888, 0.8333333333333334, 1.0, 0.7741935483870968, 0.8333333333333334, 0.7692307692307693, 0.6702127659574468, 0.8333333333333334, 0.9090909090909091, 0.6063348416289592, 0.8333333333333334, 0.9259259259259259, 0.9444444444444444, 0.8333333333333334, 0.95, 0.5233644859813084, 0.875, 1.0, 0.6610169491525424, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5362318840579711, 0.8333333333333334, 1.0, 0.5217391304347826, 0.8333333333333334, 0.8823529411764706, 0.6349206349206349, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9, 1.0, 0.8333333333333334, 1.0, 0.5757575757575758, 0.8333333333333334, 0.8571428571428571, 0.5938566552901023, 0.875, 1.0, 0.6016949152542372, 0.875, 1.0, 0.7721518987341772, 0.875, 1.0, 0.6618705035971223, 0.875, 1.0, 0.6612903225806451, 0.875, 1.0, 0.8089887640449438, 0.875, 1.0, 0.7021276595744681, 0.8333333333333334, 0.875, 0.7222222222222222, 0.875, 1.0, 0.672566371681416, 0.875, 1.0, 0.5767441860465117, 0.875, 1.0, 0.6259541984732825, 0.875, 1.0, 0.6635071090047393, 0.875, 1.0, 0.7058823529411765, 0.875, 1.0, 0.6061946902654868, 0.8333333333333334, 0.782608695652174, 0.49019607843137253, 0.8333333333333334, 0.7840909090909091, 0.7956989247311828, 0.875, 0.95, 1.0, 0.875, 1.0, 0.5264705882352941, 0.875, 0.9230769230769231, 0.9032258064516129, 0.8333333333333334, 0.782608695652174, 0.6031746031746031, 0.875, 1.0, 0.5685131195335277, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.7692307692307693, 0.7857142857142857, 0.875, 1.0, 0.5892116182572614, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 0.7066666666666667, 0.8444444444444444, 0.875, 1.0, 1.0, 0.875, 1.0, 0.551219512195122, 0.8333333333333334, 0.8461538461538461, 0.9, 0.875, 1.0, 0.5475113122171946, 0.8333333333333334, 0.759493670886076, 0.7962962962962963, 0.875, 1.0, 0.625, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.9444444444444444, 0.875, 1.0, 0.4831168831168831, 0.875, 0.9090909090909091, 0.6225490196078431, 0.875, 1.0, 0.5523465703971119, 0.8333333333333334, 0.8461538461538461, 0.625531914893617, 0.875, 0.9, 0.6976744186046512, 0.875, 1.0, 0.6601941747572816, 0.875, 1.0, 0.6511627906976745, 0.875, 1.0, 0.7049180327868853, 0.8333333333333334, 0.9090909090909091, 1.0, 0.8333333333333334, 0.8852459016393442, 0.8837209302325582, 0.8333333333333334, 0.8571428571428571, 0.9444444444444444, 0.8333333333333334, 0.9333333333333333, 0.7340425531914894, 0.8333333333333334, 0.825, 0.9166666666666666, 0.8333333333333334, 1.0, 0.7142857142857143, 0.8333333333333334, 0.9565217391304348, 0.7477477477477478, 0.8333333333333334, 0.95, 0.9, 0.8333333333333334, 1.0, 0.88, 0.875, 1.0, 0.7634408602150538, 0.8333333333333334, 0.9444444444444444, 1.0, 0.8333333333333334, 0.7857142857142857, 0.6714285714285714, 0.8333333333333334, 0.9, 0.8387096774193549, 0.8333333333333334, 0.9090909090909091, 0.9333333333333333, 0.875, 0.9, 1.0, 0.875, 1.0, 0.678082191780822, 0.8333333333333334, 0.8421052631578947, 0.8571428571428571, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.5957446808510638, 0.875, 0.8571428571428571, 1.0, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.6548223350253807, 0.875, 1.0, 0.5668202764976958, 0.8333333333333334, 1.0, 0.6161616161616161, 0.875, 1.0, 0.6785714285714286, 0.875, 1.0, 1.0, 0.875, 1.0, 0.7589285714285714, 0.875, 1.0, 0.7387387387387387, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5072463768115942, 0.875, 0.9090909090909091, 0.7878787878787878, 0.875, 1.0, 0.5141509433962265, 0.875, 0.8888888888888888, 0.47435897435897434, 0.875, 0.8571428571428571, 0.7444444444444445, 0.875, 1.0, 0.48044692737430167, 0.875, 1.0, 0.6178861788617886, 0.875, 1.0, 0.5194174757281553, 0.875, 1.0, 0.6608187134502924, 0.875, 1.0, 0.7246376811594203, 0.8333333333333334, 0.7441860465116279, 0.9166666666666666, 0.875, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8611111111111112, 0.8260869565217391, 0.875, 1.0, 0.9473684210526315, 0.8333333333333334, 0.7692307692307693, 0.8181818181818182, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 1.0, 0.875, 0.8666666666666667, 0.6271186440677966, 0.875, 0.9, 1.0, 0.875, 1.0, 0.5261437908496732, 0.875, 1.0, 0.75, 0.875, 0.5161290322580645, 0.875, 0.8571428571428571, 1.0, 0.875, 1.0, 0.6, 0.875, 1.0, 0.7159090909090909, 0.875, 0.8333333333333334, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9285714285714286, 0.84, 0.875, 1.0, 0.8, 0.875, 0.9, 0.8421052631578947, 0.875, 1.0, 0.8064516129032258, 0.875, 0.9090909090909091, 0.9473684210526315, 0.875, 1.0, 1.0, 0.875, 1.0, 0.9285714285714286, 0.875, 1.0, 0.6715328467153284, 0.875, 1.0, 0.5760869565217391, 0.875, 1.0, 0.5741626794258373, 0.875, 1.0, 0.788135593220339, 0.875, 1.0, 0.5024390243902439, 0.875, 0.8571428571428571, 0.5819672131147541, 0.875, 1.0, 0.6030927835051546, 0.875, 1.0, 0.5522388059701493, 0.875, 1.0, 0.7469879518072289, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.5536723163841808, 0.875, 0.875, 0.9787234042553191, 0.8333333333333334, 1.0, 0.9354838709677419, 0.8333333333333334, 0.9333333333333333, 1.0, 0.8333333333333334, 0.9, 1.0, 0.8333333333333334, 1.0, 0.8333333333333334, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 0.9444444444444444, 0.7272727272727273, 0.8333333333333334, 0.8666666666666667, 0.875, 0.8333333333333334, 0.9285714285714286, 0.7127659574468085, 0.8333333333333334, 1.0, 0.49538461538461537, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.875, 1.0, 0.6589595375722543, 0.8333333333333334, 0.8064516129032258, 0.8214285714285714, 0.875, 1.0, 0.5407407407407407, 0.875, 1.0, 0.6359223300970874, 0.875, 1.0, 0.7052631578947368, 0.8333333333333334, 0.8260869565217391, 1.0, 0.875, 0.8, 0.6730769230769231, 0.875, 1.0, 0.5335570469798657, 0.875, 1.0, 0.569620253164557, 0.875, 0.8571428571428571, 0.8709677419354839, 0.875, 0.8571428571428571, 0.5310077519379846, 0.875, 1.0, 0.6875, 0.8333333333333334, 0.8260869565217391, 0.9411764705882353, 0.875, 0.9166666666666666, 0.9230769230769231, 0.8333333333333334, 0.9523809523809523, 0.825, 0.875, 1.0, 0.6270270270270271, 0.875, 1.0, 0.6830357142857143, 0.875, 0.8181818181818182, 0.5357142857142857, 0.875, 0.8888888888888888, 0.8804347826086957, 0.875, 1.0, 0.6370967741935484, 0.875, 1.0, 0.572139303482587, 0.875, 0.8666666666666667, 0.7872340425531915, 0.875, 1.0, 0.5203252032520326, 0.875, 1.0, 0.5486111111111112, 0.875, 0.9090909090909091, 1.0, 0.875, 1.0, 0.605, 0.8333333333333334, 1.0, 0.7272727272727273, 0.875, 0.9090909090909091, 0.625, 0.875, 0.9090909090909091, 0.6875, 0.875, 1.0, 0.517799352750809, 0.8333333333333334, 0.7954545454545454, 0.75, 0.875, 1.0, 0.49707602339181284, 0.875, 1.0, 0.5290322580645161, 0.875, 1.0, 0.5415162454873647, 0.875, 1.0, 0.4642857142857143, 0.8333333333333334, 0.9090909090909091, 0.9411764705882353, 0.875, 0.8181818181818182, 1.0, 0.875, 0.7857142857142857, 0.5782312925170068, 0.8333333333333334, 0.8135593220338984, 0.8928571428571429, 0.8333333333333334, 0.9090909090909091, 0.6766917293233082, 0.8333333333333334, 0.9523809523809523, 0.9090909090909091, 0.8333333333333334, 0.875, 0.9090909090909091, 0.875, 1.0, 0.6163522012578616, 0.8333333333333334, 0.9444444444444444, 0.6956521739130435, 0.875, 1.0, 0.7340425531914894, 0.875, 0.9333333333333333, 0.7107438016528925, 0.875, 1.0, 0.6404494382022472, 0.8333333333333334, 0.782608695652174, 0.8181818181818182, 0.875, 1.0, 0.8529411764705882, 0.875, 1.0, 0.8653846153846154, 0.875, 0.8571428571428571, 0.6355140186915887, 0.875, 0.9444444444444444, 0.7142857142857143, 0.875, 0.8333333333333334, 0.96875, 0.875, 1.0, 0.7804878048780488, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 1.0, 0.6666666666666666, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.5454545454545454, 0.8333333333333334, 0.9545454545454546, 0.8333333333333334, 0.875, 0.8888888888888888, 0.7297297297297297, 0.8333333333333334, 0.64, 0.8461538461538461, 0.875, 0.9333333333333333, 0.9019607843137255, 0.875, 1.0, 0.6580645161290323, 0.8333333333333334, 0.9375, 0.8, 0.8333333333333334, 1.0, 0.7894736842105263, 0.875, 0.8888888888888888, 0.5161290322580645, 0.875, 1.0, 0.6691176470588235, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.6902173913043478, 0.875, 1.0, 0.6047904191616766, 0.8333333333333334, 0.9642857142857143, 0.8571428571428571, 0.8333333333333334, 0.967741935483871, 0.6198347107438017, 0.875, 0.8823529411764706, 0.6136363636363636, 0.875, 1.0, 0.6704545454545454, 0.875, 1.0, 0.5128865979381443, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8780487804878049, 0.8857142857142857, 0.8333333333333334, 0.7904761904761904, 0.8676470588235294, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 0.918918918918919, 0.875, 0.9, 0.9090909090909091, 0.8333333333333334, 1.0, 0.65625, 0.8333333333333334, 0.8108108108108109, 1.0, 0.875, 0.9090909090909091, 0.5169491525423728, 0.875, 1.0, 0.5882352941176471, 0.8333333333333334, 1.0, 0.7941176470588235, 0.875, 1.0, 0.7931034482758621, 0.875, 0.9166666666666666, 0.8518518518518519, 0.875, 0.8, 0.5978260869565217, 0.875, 0.9230769230769231, 0.5114503816793893, 0.875, 1.0, 0.654054054054054, 0.875, 0.875, 1.0, 0.875, 1.0, 0.6716417910447762, 0.875, 1.0, 0.5531914893617021, 0.875, 1.0, 0.484, 0.875, 0.875, 0.6666666666666666, 0.875, 0.8823529411764706, 1.0, 0.875, 0.9230769230769231, 0.49795918367346936, 0.8333333333333334, 0.9230769230769231, 0.8372093023255814, 0.875, 1.0, 1.0, 0.875, 0.8888888888888888, 0.5256064690026954, 0.875, 0.9230769230769231, 0.5626740947075209, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 0.6555555555555556, 0.875, 0.6, 0.8461538461538461, 0.8333333333333334, 0.9166666666666666, 0.7368421052631579, 0.875, 0.9090909090909091, 0.5772058823529411, 0.875, 1.0, 0.7151898734177216, 0.875, 1.0, 0.6347826086956522, 0.875, 1.0, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 0.6666666666666666, 0.875, 0.8888888888888888, 0.6044776119402985, 0.875, 1.0, 0.8723404255319149, 0.875, 0.9, 0.6054054054054054, 0.875, 1.0, 0.5179282868525896, 0.875, 1.0, 0.796875, 0.875, 0.7692307692307693, 0.5522388059701493, 0.875, 0.9375, 1.0, 0.875, 1.0, 0.5897435897435898, 0.875, 0.8571428571428571, 0.7037037037037037, 0.875, 0.7894736842105263, 0.8181818181818182, 0.875, 0.9333333333333333, 0.9285714285714286, 0.875, 1.0, 0.7112676056338029, 0.875, 0.9090909090909091, 0.5562310030395137, 0.8333333333333334, 0.9473684210526315, 0.6605504587155964, 0.8333333333333334, 0.8636363636363636, 0.6781609195402298, 0.875, 1.0, 0.6244897959183674, 0.875, 0.9230769230769231, 0.7631578947368421, 0.8333333333333334, 0.9090909090909091, 0.6896551724137931, 0.8333333333333334, 0.9545454545454546, 0.9642857142857143, 0.875, 1.0, 0.7530864197530864, 0.875, 0.9411764705882353, 0.896551724137931, 0.875, 1.0, 0.6923076923076923, 0.875, 0.9230769230769231, 0.9117647058823529, 0.8333333333333334, 0.8823529411764706, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9545454545454546, 0.65625, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9166666666666666, 0.578125, 0.8333333333333334, 0.9047619047619048, 0.6226415094339622, 0.8333333333333334, 0.8387096774193549, 1.0, 0.8333333333333334, 0.7647058823529411, 1.0, 0.8333333333333334, 0.8947368421052632, 0.9, 0.8333333333333334, 0.9565217391304348, 0.7590361445783133, 0.875, 1.0, 0.7777777777777778, 0.8333333333333334, 0.8095238095238095, 0.9090909090909091, 0.8333333333333334, 0.8695652173913043, 0.8888888888888888, 0.875, 1.0, 0.775, 0.8333333333333334, 0.8846153846153846, 0.9375, 0.8333333333333334, 0.9130434782608695, 1.0, 0.875, 0.9130434782608695, 0.5267175572519084, 0.875, 1.0, 0.6882352941176471, 0.875, 1.0, 0.7425742574257426, 0.875, 0.7297297297297297, 0.8064516129032258, 0.875, 0.8085106382978723, 0.7846153846153846, 0.875, 0.9047619047619048, 0.6086956521739131, 0.875, 0.9090909090909091, 0.6991150442477876, 0.875, 1.0, 0.580952380952381, 0.875, 1.0, 0.5822784810126582, 0.875, 0.9, 0.8478260869565217, 0.8333333333333334, 0.9375, 0.8571428571428571, 0.875, 1.0, 0.6570048309178744, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8846153846153846, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9047619047619048, 1.0, 0.875, 0.9090909090909091, 0.4631578947368421, 0.8333333333333334, 0.847457627118644, 0.5014749262536873, 0.8333333333333334, 0.9473684210526315, 1.0, 0.875, 1.0, 0.96, 0.8333333333333334, 0.8524590163934426, 0.8604651162790697, 0.8333333333333334, 0.9473684210526315, 0.9, 0.875, 0.7777777777777778, 0.6276595744680851, 0.8333333333333334, 0.9090909090909091, 1.0, 0.8333333333333334, 0.7962962962962963, 0.88, 0.875, 0.8, 0.8, 0.8333333333333334, 0.8235294117647058, 0.48717948717948717, 0.8333333333333334, 1.0, 0.9166666666666666, 0.8333333333333334, 0.7037037037037037, 0.6192660550458715, 0.8333333333333334, 0.9473684210526315, 0.8472222222222222, 0.8333333333333334, 0.8823529411764706, 1.0, 0.8333333333333334, 0.8529411764705882, 0.6265060240963856, 0.875, 0.8888888888888888, 0.5296912114014252, 0.875, 0.8571428571428571, 0.8275862068965517, 0.875, 1.0, 0.7815126050420168, 0.8333333333333334, 1.0, 0.6595744680851063, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5292620865139949, 0.8333333333333334, 0.6666666666666666, 1.0, 0.875, 1.0, 0.9444444444444444, 0.875, 1.0, 0.5806451612903226, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.675, 0.875, 0.8888888888888888, 0.5942028985507246, 0.875, 1.0, 0.5138339920948617, 0.8333333333333334, 1.0, 1.0, 0.875, 0.8888888888888888, 1.0, 0.875, 1.0, 0.8666666666666667, 0.875, 1.0, 0.5658914728682171, 0.8333333333333334, 0.9, 0.6666666666666666, 0.875, 1.0, 0.8333333333333334, 0.9130434782608695, 0.8235294117647058, 0.875, 1.0, 0.5513513513513514, 0.8333333333333334, 0.8928571428571429, 0.9285714285714286, 0.875, 1.0, 0.6991150442477876, 0.8333333333333334, 0.8928571428571429, 0.875, 0.8333333333333334, 1.0, 0.56, 0.8333333333333334, 0.9130434782608695, 0.6447368421052632, 0.8333333333333334, 0.9444444444444444, 0.5725806451612904, 0.875, 0.8333333333333334, 0.525679758308157, 0.8333333333333334, 0.9230769230769231, 0.6078431372549019, 0.8333333333333334, 1.0, 0.9375, 0.8333333333333334, 0.9444444444444444, 1.0, 0.875, 1.0, 0.5504201680672269, 0.875, 1.0, 0.6262975778546713, 0.875, 1.0, 0.7666666666666667, 0.875, 0.875, 0.5957446808510638, 0.875, 1.0, 0.4638888888888889, 0.875, 1.0, 0.5621301775147929, 0.875, 0.8888888888888888, 0.46634615384615385, 0.875, 1.0, 0.5913978494623656, 0.875, 1.0, 0.6388888888888888, 0.875, 0.9090909090909091, 0.5317073170731708, 0.875, 0.9166666666666666, 0.5448275862068965, 0.875, 1.0, 0.6593406593406593, 0.875, 1.0, 0.6027397260273972, 0.8333333333333334, 0.7125, 0.7857142857142857, 0.875, 1.0, 0.9527027027027027, 0.8333333333333334, 1.0, 0.9230769230769231, 0.875, 1.0, 0.5372549019607843, 0.875, 1.0, 0.8181818181818182, 0.8333333333333334, 0.8, 0.9333333333333333, 0.8333333333333334, 0.96, 0.8275862068965517, 0.875, 1.0, 0.4578313253012048, 0.875, 0.875, 0.6794871794871795, 0.8333333333333334, 0.9333333333333333, 0.4855072463768116, 0.8333333333333334, 0.7741935483870968, 0.4621212121212121, 0.8333333333333334, 0.6976744186046512, 0.7586206896551724, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 0.4578313253012048, 0.8333333333333334, 0.9375, 1.0, 0.875, 0.7, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.8690476190476191, 0.875, 0.9090909090909091, 0.6829268292682927, 0.875, 1.0, 0.9615384615384616, 0.875, 1.0, 0.9285714285714286, 0.875, 0.9090909090909091, 0.5796178343949044, 0.875, 0.8888888888888888, 0.759493670886076, 0.875, 0.9090909090909091, 0.5495495495495496, 0.8333333333333334, 0.8135593220338984, 0.7567567567567568, 0.875, 1.0, 0.6967741935483871, 0.875, 0.9090909090909091, 1.0, 0.875, 1.0, 1.0, 0.875, 0.75, 0.5373134328358209, 0.875, 0.9444444444444444, 0.7575757575757576, 0.875, 1.0, 0.6929824561403509, 0.875, 0.8571428571428571, 0.7478260869565218, 0.875, 0.9230769230769231, 0.6984924623115578, 0.875, 1.0, 0.6363636363636364, 0.875, 0.875, 0.6324324324324324, 0.875, 0.875, 0.5505050505050505, 0.875, 1.0, 0.5636942675159236, 0.875, 0.9, 0.48704663212435234, 0.875, 0.9230769230769231, 0.6831683168316832, 0.875, 0.9, 0.5428571428571428, 0.875, 1.0, 1.0, 0.875, 0.6769911504424779, 0.8260869565217391, 0.875, 0.875, 0.6206896551724138, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9090909090909091, 0.5430711610486891, 0.8333333333333334, 0.875, 0.9285714285714286, 0.8333333333333334, 0.8333333333333334, 0.5104602510460251, 0.8333333333333334, 0.9259259259259259, 0.6952380952380952, 0.8333333333333334, 1.0, 0.6060606060606061, 0.8333333333333334, 1.0, 0.6129032258064516, 0.8333333333333334, 0.9333333333333333, 0.8307692307692308, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 0.7662337662337663, 0.8333333333333334, 0.9444444444444444, 1.0, 0.875, 1.0, 0.6058394160583942, 0.875, 0.64, 0.5966386554621849, 0.8333333333333334, 0.7647058823529411, 0.8461538461538461, 0.875, 1.0, 1.0, 0.8333333333333334, 0.7761194029850746, 0.7368421052631579, 0.8333333333333334, 1.0, 0.5370370370370371, 0.875, 1.0, 1.0, 0.875, 1.0, 0.8297872340425532, 0.8333333333333334, 0.875, 0.8809523809523809, 0.875, 1.0, 0.6906474820143885, 0.875, 1.0, 0.696969696969697, 0.8333333333333334, 0.7901234567901234, 0.88, 0.875, 0.8, 0.6007604562737643, 0.875, 1.0, 1.0, 0.875, 1.0, 1.0, 0.8333333333333334, 0.762589928057554, 0.7073170731707317, 0.875, 1.0, 0.7, 0.875, 1.0, 0.6576576576576577, 0.875, 1.0, 0.8235294117647058, 0.875, 1.0, 0.6590909090909091, 0.875, 1.0, 0.9310344827586207, 0.875, 0.8888888888888888, 1.0, 0.875, 1.0, 0.8888888888888888, 0.875, 1.0, 0.625, 0.875, 1.0, 0.5402298850574713, 0.875, 1.0, 0.6781115879828327, 0.8333333333333334, 0.8823529411764706, 0.8, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.7710843373493976, 0.875, 1.0, 0.5012987012987012, 0.875, 1.0, 0.7692307692307693, 0.8333333333333334, 0.8235294117647058, 0.6842105263157895, 0.875, 1.0, 0.9166666666666666, 0.875, 0.8421052631578947, 0.875, 0.875, 1.0, 0.6096491228070176, 0.8333333333333334, 1.0, 0.8787878787878788, 0.875, 1.0, 0.6305732484076433, 0.875, 1.0, 0.9411764705882353, 0.8333333333333334, 1.0, 0.5374449339207048, 0.8333333333333334, 1.0, 0.875, 1.0, 0.96, 0.8333333333333334, 0.9090909090909091, 0.8571428571428571, 0.875, 0.9375, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.78125, 0.8333333333333334, 0.9230769230769231, 0.7192982456140351, 0.8333333333333334, 1.0, 0.8636363636363636, 0.8333333333333334, 1.0, 0.7777777777777778, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 0.7, 0.8333333333333334, 0.7837837837837838, 0.9629629629629629, 0.8333333333333334, 0.95, 1.0, 0.8333333333333334, 0.875, 0.9285714285714286, 0.875, 1.0, 0.90625, 0.8333333333333334, 1.0, 0.58, 0.875, 1.0, 0.5349794238683128, 0.875, 0.9285714285714286, 0.64, 0.875, 0.9230769230769231, 0.6342281879194631, 0.875, 0.8571428571428571, 0.5578947368421052, 0.875, 1.0, 0.5284280936454849, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 0.6792452830188679, 0.8333333333333334, 0.6509433962264151, 0.8627450980392157, 0.8333333333333334, 0.8, 0.875, 0.8333333333333334, 0.8235294117647058, 0.8333333333333334, 0.8333333333333334, 0.9333333333333333, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 0.547945205479452, 0.8333333333333334, 0.8888888888888888, 0.8, 0.875, 1.0, 0.7114093959731543, 0.875, 1.0, 1.0, 0.875, 0.9333333333333333, 0.5853658536585366, 0.875, 1.0, 0.5325670498084292, 0.875, 0.8888888888888888, 0.5798319327731093, 0.875, 1.0, 0.5923076923076923, 0.875, 1.0, 0.5805243445692884, 0.875, 1.0, 0.4918032786885246, 0.875, 1.0, 0.7301587301587301, 0.875, 0.8, 0.8, 0.8333333333333334, 0.7027027027027027, 0.59375, 0.8333333333333334, 1.0, 0.6025641025641025, 0.875, 0.9090909090909091, 0.6424242424242425, 0.8333333333333334, 0.9411764705882353, 0.7111111111111111, 0.875, 1.0, 1.0, 0.8333333333333334, 0.85, 1.0, 0.875, 0.8571428571428571, 1.0, 0.875, 0.8888888888888888, 0.6071428571428571, 0.875, 1.0, 0.5483870967741935, 0.875, 0.8, 0.5603112840466926, 0.875, 1.0, 0.6194690265486725, 0.8333333333333334, 0.9, 0.6333333333333333, 0.8333333333333334, 0.9047619047619048, 0.6567164179104478, 0.875, 1.0, 0.7101449275362319, 0.875, 1.0, 0.49760765550239233, 0.875, 1.0, 0.47540983606557374, 0.875, 0.9230769230769231, 0.7894736842105263, 0.875, 0.8333333333333334, 0.5127118644067796, 0.8333333333333334, 0.782608695652174, 0.6639344262295082, 0.875, 0.9, 0.5072463768115942, 0.8333333333333334, 0.95, 1.0, 0.8333333333333334, 0.9393939393939394, 1.0, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.8695652173913043, 0.717391304347826, 0.8333333333333334, 0.9354838709677419, 0.7619047619047619, 0.875, 0.9230769230769231, 0.6918238993710691, 0.875, 1.0, 0.6481481481481481, 0.875, 1.0, 0.675, 0.8333333333333334, 1.0, 0.5714285714285714, 0.8333333333333334, 0.95, 1.0, 0.875, 0.9230769230769231, 1.0, 0.875, 1.0, 0.5533596837944664, 0.8333333333333334, 0.8421052631578947, 0.7333333333333333, 0.875, 1.0, 0.5231316725978647, 0.875, 1.0, 0.6927710843373494, 0.875, 1.0, 1.0, 0.875, 0.875, 1.0, 0.875, 0.9090909090909091, 0.5829383886255924, 0.875, 0.8888888888888888, 0.7264150943396226, 0.875, 0.8888888888888888, 0.5075757575757576, 0.875, 0.8888888888888888, 0.6206896551724138, 0.8333333333333334, 0.8571428571428571, 0.6633663366336634, 0.875, 0.9285714285714286, 0.875, 0.7, 0.7804878048780488, 0.875, 1.0, 0.6525096525096525, 0.875, 0.8, 0.5568181818181818, 0.8333333333333334, 0.9047619047619048, 0.9090909090909091, 0.875, 1.0, 0.6598639455782312, 0.8333333333333334, 0.6774193548387096, 0.4563106796116505, 0.875, 0.95, 0.5101214574898786, 0.875, 1.0, 0.6401515151515151, 0.875, 1.0, 0.542016806722689, 0.875, 1.0, 0.5687732342007435, 0.875, 0.8888888888888888, 0.456575682382134, 0.875, 1.0, 0.6232558139534884, 0.8333333333333334, 0.9411764705882353, 1.0, 0.875, 1.0, 0.6846846846846847, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.7738095238095238, 0.8333333333333334, 1.0, 0.7023809523809523, 0.875, 1.0, 0.7435897435897436, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 0.7638888888888888, 0.875, 1.0, 0.6330935251798561, 0.875, 1.0, 0.6830985915492958, 0.875, 0.875, 1.0, 0.875, 1.0, 0.7647058823529411, 0.875, 0.8571428571428571, 0.5570032573289903, 0.875, 0.8695652173913043, 0.9166666666666666, 0.8333333333333334, 0.96, 1.0, 0.875, 1.0, 0.825, 0.875, 1.0, 0.9, 0.875, 0.8620689655172413, 1.0, 0.875, 1.0, 0.9473684210526315, 0.875, 0.8333333333333334, 0.5057471264367817, 0.8333333333333334, 0.9444444444444444, 0.7183098591549296, 0.875, 0.8571428571428571, 1.0, 0.875, 1.0, 0.8095238095238095, 0.875, 0.8888888888888888, 0.4940828402366864, 0.875, 1.0, 1.0, 0.875, 0.9375, 0.7727272727272727, 0.8333333333333334, 0.8222222222222222, 0.8070175438596491, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8461538461538461, 1.0, 0.875, 1.0, 0.647887323943662, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5565217391304348, 0.875, 0.8947368421052632, 0.875, 1.0, 0.6434426229508197, 0.875, 1.0, 0.6300813008130082, 0.8333333333333334, 0.7586206896551724, 0.4897959183673469, 0.875, 1.0, 0.7428571428571429, 0.875, 1.0, 0.6505376344086021, 0.8333333333333334, 0.8620689655172413, 0.5754716981132075, 0.8333333333333334, 0.9642857142857143, 0.9473684210526315, 0.8333333333333334, 0.95, 0.8709677419354839, 0.875, 0.9375, 0.9428571428571428, 0.8333333333333334, 0.8666666666666667, 1.0, 0.8333333333333334, 0.84, 1.0, 0.8333333333333334, 0.8709677419354839, 1.0, 0.8333333333333334, 0.9130434782608695, 1.0, 0.8333333333333334, 0.9, 0.8333333333333334, 0.875, 0.9230769230769231, 0.601123595505618, 0.8333333333333334, 0.7906976744186046, 0.7741935483870968, 0.875, 1.0, 0.7659574468085106, 0.8333333333333334, 0.9523809523809523, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8214285714285714, 1.0, 0.875, 1.0, 0.7446808510638298, 0.8333333333333334, 1.0, 0.6666666666666666, 0.875, 0.7857142857142857, 0.7323943661971831, 0.875, 1.0, 0.6330275229357798, 0.8333333333333334, 0.95, 0.8235294117647058, 0.8333333333333334, 0.8857142857142857, 0.5061728395061729, 0.875, 0.875, 0.6652173913043479, 0.8333333333333334, 0.92, 0.7428571428571429, 0.875, 1.0, 0.7123287671232876, 0.875, 0.9615384615384616, 1.0, 0.875, 0.8947368421052632, 0.7333333333333333, 0.875, 1.0, 0.6363636363636364, 0.875, 0.9, 0.6796875, 0.875, 1.0, 0.7710843373493976, 0.8333333333333334, 0.9772727272727273, 0.7755102040816326, 0.8333333333333334, 0.8888888888888888, 1.0, 0.8333333333333334, 0.95, 0.9333333333333333, 0.875, 1.0, 0.6842105263157895, 0.8333333333333334, 0.8333333333333334, 0.9333333333333333, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.9333333333333333, 0.8571428571428571, 0.8333333333333334, 1.0, 0.5698924731182796, 0.8333333333333334, 0.8260869565217391, 0.5878378378378378, 0.875, 1.0, 0.6077586206896551, 0.8333333333333334, 0.8148148148148148, 1.0, 0.875, 1.0, 0.7049180327868853, 0.875, 0.8888888888888888, 0.5276073619631901, 0.8333333333333334, 0.85, 0.9166666666666666, 0.8333333333333334, 0.8333333333333334, 1.0, 0.8333333333333334, 0.9230769230769231, 0.743801652892562, 0.8333333333333334, 0.8918918918918919, 0.9642857142857143, 0.8333333333333334, 1.0, 1.0, 0.875, 0.9285714285714286, 0.5846153846153846, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.9, 0.9090909090909091, 0.8333333333333334, 1.0, 0.6137566137566137, 0.875, 1.0, 0.6666666666666666, 0.8333333333333334, 0.875, 1.0, 0.8333333333333334, 0.9090909090909091, 0.8695652173913043, 0.875, 0.9, 0.5655737704918032, 0.8333333333333334, 1.0, 0.6204819277108434, 0.875, 1.0, 0.5867158671586716, 0.875, 1.0, 0.6303501945525292, 0.8333333333333334, 0.8823529411764706, 0.5631067961165048, 0.8333333333333334, 0.7333333333333333, 0.8448275862068966, 0.875, 1.0, 0.75, 0.875, 1.0, 0.696078431372549, 0.875, 1.0, 0.45454545454545453, 0.875, 1.0, 0.5068119891008175, 0.875, 0.875, 0.54, 0.875, 1.0, 0.45454545454545453, 0.8333333333333334, 0.9230769230769231, 0.5701357466063348, 0.875, 1.0, 0.5031847133757962, 0.875, 1.0, 0.5945945945945946, 0.8333333333333334, 1.0, 0.7307692307692307, 0.875, 0.8461538461538461, 0.5786350148367952, 0.8333333333333334, 0.9285714285714286, 0.9166666666666666, 0.875, 1.0, 0.8333333333333334, 0.9032258064516129, 1.0, 0.8333333333333334, 0.7894736842105263, 0.7428571428571429, 0.875, 1.0, 0.61, 0.875, 1.0, 0.6510067114093959, 0.875, 1.0, 1.0, 0.875, 0.8461538461538461, 0.6153846153846154, 0.875, 0.9, 0.5722222222222222, 0.8333333333333334, 0.9259259259259259, 1.0, 0.875, 1.0, 0.6346153846153846, 0.875, 1.0, 0.5756097560975609, 0.875, 0.9090909090909091, 0.8846153846153846, 0.875, 1.0, 0.627177700348432, 0.875, 0.9473684210526315, 1.0, 0.875, 0.8888888888888888, 0.6145833333333334, 0.875, 1.0, 1.0, 0.875, 1.0, 0.6373626373626373, 0.875, 0.9333333333333333, 1.0, 0.875, 0.8823529411764706, 0.8, 0.875, 1.0, 0.5963302752293578, 0.8333333333333334, 1.0, 0.7536231884057971, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8709677419354839, 0.8947368421052632, 0.8333333333333334, 0.9411764705882353, 0.5901639344262295, 0.8333333333333334, 0.9166666666666666, 0.6048387096774194, 0.8333333333333334, 0.8461538461538461, 0.803921568627451, 0.8333333333333334, 1.0, 0.6602564102564102, 0.875, 0.9, 0.48905109489051096, 0.875, 0.8571428571428571, 0.6233766233766234], "type": "scatter"}, {"customdata": [473, 629, 647, 1205, 1313, 1937, 2060, 2297, 2306, 2546, 2681, 2915], "hovertemplate": "Index: %{customdata}<br>Value: %{y:.2f}<extra></extra>", "marker": {"color": "#d66a6a", "size": 10, "symbol": "x"}, "mode": "markers", "name": "Outliers (12)", "x": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "y": [0.44285714285714284, 0.44936708860759494, 0.45, 0.40828402366863903, 0.4481132075471698, 0.43532338308457713, 0.4339622641509434, 0.3532608695652174, 0.4444444444444444, 0.35858585858585856, 0.41228070175438597, 0.356], "type": "scatter"}];
            const layout = Object.assign({}, {"template": {"data": {"histogram2dcontour": [{"type": "histogram2dcontour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "choropleth": [{"type": "choropleth", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "histogram2d": [{"type": "histogram2d", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "heatmap": [{"type": "heatmap", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "contourcarpet": [{"type": "contourcarpet", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "contour": [{"type": "contour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "surface": [{"type": "surface", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "mesh3d": [{"type": "mesh3d", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "scatter": [{"fillpattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}, "type": "scatter"}], "parcoords": [{"type": "parcoords", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolargl": [{"type": "scatterpolargl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "bar"}], "scattergeo": [{"type": "scattergeo", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolar": [{"type": "scatterpolar", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "histogram": [{"marker": {"pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "histogram"}], "scattergl": [{"type": "scattergl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatter3d": [{"type": "scatter3d", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermap": [{"type": "scattermap", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermapbox": [{"type": "scattermapbox", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterternary": [{"type": "scatterternary", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattercarpet": [{"type": "scattercarpet", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "barpolar"}], "pie": [{"automargin": true, "type": "pie"}]}, "layout": {"autotypenumbers": "strict", "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "hovermode": "closest", "hoverlabel": {"align": "left"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"bgcolor": "#E5ECF6", "angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "ternary": {"bgcolor": "#E5ECF6", "aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]]}, "xaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "yaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "geo": {"bgcolor": "white", "landcolor": "#E5ECF6", "subunitcolor": "white", "showland": true, "showlakes": true, "lakecolor": "white"}, "title": {"x": 0.05}, "mapbox": {"style": "light"}}}, "shapes": [{"line": {"color": "#e0b854", "dash": "dash"}, "type": "line", "x0": 0, "x1": 1, "xref": "x domain", "y0": 1.2488747554565234, "y1": 1.2488747554565234, "yref": "y"}, {"line": {"color": "#e0b854", "dash": "dash"}, "type": "line", "x0": 0, "x1": 1, "xref": "x domain", "y0": 0.4514311854758205, "y1": 0.4514311854758205, "yref": "y"}, {"line": {"color": "#7cb97c", "dash": "dot"}, "type": "line", "x0": 0, "x1": 1, "xref": "x domain", "y0": 0.8501529704661719, "y1": 0.8501529704661719, "yref": "y"}], "annotations": [{"font": {"color": "#a39e93"}, "showarrow": false, "text": "Upper bound (1.2)", "x": 1, "xanchor": "right", "xref": "x domain", "y": 1.2488747554565234, "yanchor": "bottom", "yref": "y"}, {"font": {"color": "#a39e93"}, "showarrow": false, "text": "Lower bound (0.5)", "x": 1, "xanchor": "right", "xref": "x domain", "y": 0.4514311854758205, "yanchor": "bottom", "yref": "y"}, {"font": {"color": "#a39e93"}, "showarrow": false, "text": "Mean (0.9)", "x": 1, "xanchor": "right", "xref": "x domain", "y": 0.8501529704661719, "yanchor": "bottom", "yref": "y"}], "margin": {"l": 50, "r": 30, "t": 30, "b": 50}, "font": {"color": "#a39e93"}, "xaxis": {"title": {"text": "Sample Index"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "yaxis": {"title": {"text": "Diversity Unique Words Ratio"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "legend": {"font": {"color": "#a39e93"}, "orientation": "h", "yanchor": "bottom", "y": 1.02, "xanchor": "right", "x": 1}, "title": {}, "height": 400, "paper_bgcolor": "rgba(0,0,0,0)", "plot_bgcolor": "rgba(0,0,0,0)"}, darkLayout);
            Plotly.newPlot('anomaly_chart_2', data, layout, plotlyConfig);
        })();
        
        (function() {
            const data = [{"hovertemplate": "Index: %{x}<br>Value: %{y:.2f}<extra></extra>", "marker": {"color": "#6b9bd1", "opacity": 0.6, "size": 6}, "mode": "markers", "name": "Normal", "x": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987], "y": [0.875, 1.0, 0.743801652892562, 0.875, 1.0, 0.6792452830188679, 0.875, 1.0, 0.45933014354066987, 0.875, 1.0, 0.6620370370370371, 0.875, 0.8095238095238095, 0.5714285714285714, 0.875, 0.768, 0.8507462686567164, 0.8333333333333334, 1.0, 0.5161290322580645, 0.875, 0.9473684210526315, 0.5280898876404494, 0.8333333333333334, 0.9444444444444444, 0.9047619047619048, 0.875, 1.0, 0.6869565217391305, 0.875, 1.0, 1.0, 0.875, 0.8571428571428571, 0.6935483870967742, 0.875, 0.8571428571428571, 0.578397212543554, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.6449704142011834, 0.875, 1.0, 0.622093023255814, 0.8333333333333334, 0.8695652173913043, 0.5909090909090909, 0.875, 1.0, 0.6153846153846154, 0.875, 1.0, 0.8181818181818182, 0.875, 1.0, 0.9230769230769231, 0.875, 0.8181818181818182, 0.4849315068493151, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9375, 0.9375, 0.875, 1.0, 0.75, 0.875, 1.0, 0.5454545454545454, 0.875, 1.0, 0.9411764705882353, 0.875, 1.0, 0.5592105263157895, 0.8333333333333334, 1.0, 0.5172413793103449, 0.875, 1.0, 0.88, 0.875, 1.0, 0.45454545454545453, 0.8333333333333334, 0.8823529411764706, 0.8333333333333334, 0.875, 0.9090909090909091, 1.0, 0.875, 1.0, 0.5979381443298969, 0.875, 0.9, 0.5993265993265994, 0.8333333333333334, 0.9, 1.0, 0.8333333333333334, 0.8846153846153846, 0.7027027027027027, 0.875, 0.5555555555555556, 0.7857142857142857, 0.8333333333333334, 0.8166666666666667, 0.9523809523809523, 0.875, 0.7619047619047619, 0.9523809523809523, 0.875, 1.0, 0.6776859504132231, 0.875, 0.9166666666666666, 0.6923076923076923, 0.875, 1.0, 0.5197368421052632, 0.8333333333333334, 0.9411764705882353, 0.7058823529411765, 0.875, 1.0, 0.5092592592592593, 0.8333333333333334, 0.9375, 0.6886792452830188, 0.875, 1.0, 0.7522123893805309, 0.875, 1.0, 0.5593869731800766, 0.8333333333333334, 0.828125, 0.8541666666666666, 0.8333333333333334, 0.8604651162790697, 0.8717948717948718, 0.8333333333333334, 0.7741935483870968, 0.7857142857142857, 0.875, 1.0, 0.6923076923076923, 0.875, 0.7619047619047619, 1.0, 0.875, 0.8823529411764706, 0.9166666666666666, 0.875, 0.8666666666666667, 0.5657894736842105, 0.875, 0.9285714285714286, 0.5342465753424658, 0.875, 1.0, 0.6071428571428571, 0.8333333333333334, 1.0, 0.9230769230769231, 0.875, 1.0, 0.7619047619047619, 0.875, 1.0, 1.0, 0.875, 1.0, 0.7341772151898734, 0.875, 0.7540983606557377, 0.5393258426966292, 0.875, 1.0, 0.9230769230769231, 0.8333333333333334, 0.9411764705882353, 0.8888888888888888, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.9333333333333333, 0.6233183856502242, 0.8333333333333334, 0.9230769230769231, 0.8823529411764706, 0.8333333333333334, 0.8181818181818182, 0.8125, 0.8333333333333334, 0.9411764705882353, 0.9411764705882353, 0.8333333333333334, 0.9, 0.5907335907335908, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9230769230769231, 0.7619047619047619, 0.875, 1.0, 0.6063829787234043, 0.8333333333333334, 0.95, 1.0, 0.8333333333333334, 0.9375, 0.7021276595744681, 0.8333333333333334, 0.9230769230769231, 1.0, 0.875, 0.9090909090909091, 0.6363636363636364, 0.8333333333333334, 0.9130434782608695, 0.8461538461538461, 0.875, 0.875, 0.72, 0.875, 0.9166666666666666, 0.48135593220338985, 0.8333333333333334, 0.8888888888888888, 1.0, 0.8333333333333334, 0.9333333333333333, 0.5, 0.875, 1.0, 0.5525291828793775, 0.875, 1.0, 0.5972850678733032, 0.875, 1.0, 0.4603174603174603, 0.875, 1.0, 0.6650246305418719, 0.875, 0.7698412698412699, 0.8269230769230769, 0.8333333333333334, 1.0, 0.9142857142857143, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 0.9166666666666666, 0.6009615384615384, 0.8333333333333334, 0.8636363636363636, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.7203389830508474, 0.875, 0.6509090909090909, 0.7721518987341772, 0.875, 1.0, 0.92, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.6265822784810127, 0.875, 1.0, 0.6226415094339622, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8, 1.0, 0.875, 1.0, 0.6167664670658682, 0.875, 0.656934306569343, 1.0, 0.8333333333333334, 1.0, 0.7333333333333333, 0.875, 1.0, 0.8823529411764706, 0.875, 1.0, 0.7272727272727273, 0.875, 1.0, 0.5550660792951542, 0.8333333333333334, 0.825, 0.8076923076923077, 0.875, 1.0, 1.0, 0.875, 1.0, 0.9473684210526315, 0.8333333333333334, 0.8709677419354839, 1.0, 0.8333333333333334, 0.9047619047619048, 1.0, 0.8333333333333334, 0.8095238095238095, 0.875, 0.8333333333333334, 0.8823529411764706, 1.0, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.7727272727272727, 0.8148148148148148, 0.8333333333333334, 0.9333333333333333, 0.8421052631578947, 0.8333333333333334, 0.9375, 0.9090909090909091, 0.8333333333333334, 0.9285714285714286, 0.6825396825396826, 0.8333333333333334, 0.8571428571428571, 0.95, 0.8333333333333334, 0.9230769230769231, 0.9090909090909091, 0.8333333333333334, 0.7162162162162162, 0.8409090909090909, 0.875, 1.0, 0.6521739130434783, 0.8333333333333334, 1.0, 0.9090909090909091, 0.875, 0.875, 1.0, 0.8333333333333334, 1.0, 0.9302325581395349, 0.8333333333333334, 0.9166666666666666, 0.6290322580645161, 0.8333333333333334, 0.9230769230769231, 1.0, 0.875, 0.875, 0.7878787878787878, 0.8333333333333334, 0.8695652173913043, 1.0, 0.8333333333333334, 0.9375, 0.5833333333333334, 0.875, 0.7333333333333333, 0.45698924731182794, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.782608695652174, 1.0, 0.875, 1.0, 1.0, 0.875, 0.6346749226006192, 0.9333333333333333, 0.8333333333333334, 0.8333333333333334, 1.0, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 0.6756756756756757, 0.875, 0.9333333333333333, 0.7898550724637681, 0.8333333333333334, 0.8421052631578947, 1.0, 0.8333333333333334, 0.9444444444444444, 0.75, 0.875, 1.0, 0.8571428571428571, 0.8333333333333334, 0.8636363636363636, 0.734375, 0.8333333333333334, 0.9259259259259259, 0.9090909090909091, 0.875, 0.9090909090909091, 0.6388888888888888, 0.875, 0.8235294117647058, 0.6714285714285714, 0.8333333333333334, 1.0, 0.7047619047619048, 0.875, 0.9333333333333333, 0.6756756756756757, 0.875, 1.0, 0.5185185185185185, 0.8333333333333334, 1.0, 0.7446808510638298, 0.875, 1.0, 0.480225988700565, 0.8333333333333334, 0.95, 0.5346534653465347, 0.8333333333333334, 0.9523809523809523, 0.8888888888888888, 0.8333333333333334, 0.9565217391304348, 0.48520710059171596, 0.875, 0.84, 0.4933920704845815, 0.875, 0.6708333333333333, 0.74, 0.875, 0.9090909090909091, 0.5774647887323944, 0.875, 0.8333333333333334, 0.875, 0.8888888888888888, 0.6118421052631579, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 0.6470588235294118, 0.8490566037735849, 0.8333333333333334, 0.9333333333333333, 0.5510835913312694, 0.8333333333333334, 0.9444444444444444, 0.8552631578947368, 0.875, 0.9130434782608695, 1.0, 0.8333333333333334, 1.0, 0.5503355704697986, 0.8333333333333334, 0.9230769230769231, 0.9285714285714286, 0.875, 1.0, 0.9473684210526315, 0.875, 1.0, 1.0, 0.875, 0.875, 0.6343283582089553, 0.8333333333333334, 0.8611111111111112, 0.6267605633802817, 0.875, 0.9, 0.9411764705882353, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9130434782608695, 0.5871886120996441, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 0.5721153846153846, 0.8333333333333334, 0.9090909090909091, 0.638095238095238, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 0.9583333333333334, 0.8888888888888888, 0.8333333333333334, 0.85, 1.0, 0.8333333333333334, 0.9, 0.9, 0.8333333333333334, 0.9230769230769231, 0.8163265306122449, 0.8333333333333334, 0.9230769230769231, 0.6634615384615384, 0.8333333333333334, 0.9090909090909091, 1.0, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.7543859649122807, 0.8333333333333334, 0.8333333333333334, 0.9545454545454546, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.9047619047619048, 0.8333333333333334, 0.9333333333333333, 1.0, 0.8333333333333334, 0.875, 1.0, 0.8333333333333334, 0.75, 0.8666666666666667, 0.8333333333333334, 1.0, 0.9090909090909091, 0.8333333333333334, 0.875, 0.875, 0.8333333333333334, 0.9523809523809523, 0.7333333333333333, 0.875, 0.9166666666666666, 0.711864406779661, 0.8333333333333334, 1.0, 0.8, 0.875, 0.7777777777777778, 0.6737967914438503, 0.875, 0.8888888888888888, 0.9285714285714286, 0.875, 0.8888888888888888, 0.7019867549668874, 0.875, 1.0, 0.7254901960784313, 0.8333333333333334, 0.9, 1.0, 0.875, 0.8571428571428571, 0.5791245791245792, 0.875, 0.8888888888888888, 0.6325301204819277, 0.875, 0.8333333333333334, 0.5390946502057613, 0.875, 1.0, 1.0, 0.875, 0.875, 0.8333333333333334, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9032258064516129, 0.875, 0.8333333333333334, 1.0, 0.8333333333333334, 0.7053571428571429, 1.0, 0.8333333333333334, 0.8260869565217391, 0.589041095890411, 0.8333333333333334, 1.0, 0.8461538461538461, 0.875, 1.0, 0.7209302325581395, 0.875, 1.0, 1.0, 0.875, 0.75, 0.8333333333333334, 0.8888888888888888, 0.4774774774774775, 0.8333333333333334, 0.8387096774193549, 0.7023809523809523, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8591549295774648, 1.0, 0.875, 1.0, 0.5688073394495413, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.9230769230769231, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.8548387096774194, 0.875, 1.0, 0.7142857142857143, 0.875, 0.875, 0.5920398009950248, 0.875, 1.0, 0.7304964539007093, 0.8333333333333334, 0.9333333333333333, 0.75, 0.875, 1.0, 0.9230769230769231, 0.875, 0.96, 0.8, 0.875, 1.0, 0.6106870229007634, 0.875, 1.0, 0.8095238095238095, 0.875, 0.9, 0.7619047619047619, 0.8333333333333334, 0.9411764705882353, 0.8205128205128205, 0.875, 0.875, 0.512987012987013, 0.875, 1.0, 0.7019230769230769, 0.8333333333333334, 0.9473684210526315, 1.0, 0.875, 1.0, 0.7674418604651163, 0.875, 1.0, 1.0, 0.875, 0.8461538461538461, 0.8035714285714286, 0.875, 0.9, 0.9333333333333333, 0.875, 0.875, 0.9, 0.875, 1.0, 0.5909090909090909, 0.875, 1.0, 0.4854368932038835, 0.875, 1.0, 0.8181818181818182, 0.875, 1.0, 0.8837209302325582, 0.875, 0.9285714285714286, 1.0, 0.875, 1.0, 0.6105263157894737, 0.875, 0.8571428571428571, 0.5169491525423728, 0.875, 0.7777777777777778, 0.7777777777777778, 0.875, 0.8333333333333334, 0.6030534351145038, 0.875, 1.0, 0.5604838709677419, 0.875, 1.0, 0.5789473684210527, 0.875, 0.875, 0.5511811023622047, 0.875, 0.8888888888888888, 0.9285714285714286, 0.875, 1.0, 0.95, 0.875, 1.0, 0.6272727272727273, 0.875, 1.0, 0.6032608695652174, 0.875, 0.8888888888888888, 0.6666666666666666, 0.875, 0.9, 0.5418502202643172, 0.875, 0.9090909090909091, 0.5825688073394495, 0.875, 1.0, 0.5429553264604811, 0.875, 1.0, 1.0, 0.875, 0.8571428571428571, 0.7916666666666666, 0.875, 1.0, 0.6035714285714285, 0.8333333333333334, 0.7142857142857143, 1.0, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.875, 0.8260869565217391, 0.5137614678899083, 0.875, 1.0, 0.5936254980079682, 0.875, 0.9, 0.5693069306930693, 0.875, 1.0, 0.8333333333333334, 0.875, 1.0, 0.8636363636363636, 0.8333333333333334, 0.9375, 0.8235294117647058, 0.8333333333333334, 0.8833333333333333, 1.0, 0.875, 0.9166666666666666, 0.918918918918919, 0.8333333333333334, 0.9444444444444444, 1.0, 0.8333333333333334, 0.96875, 1.0, 0.8333333333333334, 0.9473684210526315, 1.0, 0.8333333333333334, 0.6666666666666666, 0.8709677419354839, 0.875, 1.0, 0.6418918918918919, 0.875, 0.9, 0.6967213114754098, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.88, 1.0, 0.8333333333333334, 0.696969696969697, 0.8181818181818182, 0.875, 0.8125, 0.5819672131147541, 0.875, 0.9285714285714286, 0.8157894736842105, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.8260869565217391, 0.6521739130434783, 0.875, 1.0, 0.8636363636363636, 0.8333333333333334, 0.9166666666666666, 0.7702702702702703, 0.8333333333333334, 0.9375, 0.8888888888888888, 0.8333333333333334, 1.0, 0.7741935483870968, 0.8333333333333334, 0.7692307692307693, 0.6702127659574468, 0.8333333333333334, 0.9090909090909091, 0.6063348416289592, 0.8333333333333334, 0.9259259259259259, 0.9444444444444444, 0.8333333333333334, 0.95, 0.5233644859813084, 0.875, 1.0, 0.6610169491525424, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5362318840579711, 0.8333333333333334, 1.0, 0.5217391304347826, 0.8333333333333334, 0.8823529411764706, 0.6349206349206349, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9, 1.0, 0.8333333333333334, 1.0, 0.5757575757575758, 0.8333333333333334, 0.8571428571428571, 0.5938566552901023, 0.875, 1.0, 0.6016949152542372, 0.875, 1.0, 0.7721518987341772, 0.875, 1.0, 0.6618705035971223, 0.875, 1.0, 0.6612903225806451, 0.875, 1.0, 0.8089887640449438, 0.875, 1.0, 0.7021276595744681, 0.8333333333333334, 0.875, 0.7222222222222222, 0.875, 1.0, 0.672566371681416, 0.875, 1.0, 0.5767441860465117, 0.875, 1.0, 0.6259541984732825, 0.875, 1.0, 0.6635071090047393, 0.875, 1.0, 0.7058823529411765, 0.875, 1.0, 0.6061946902654868, 0.8333333333333334, 0.782608695652174, 0.49019607843137253, 0.8333333333333334, 0.7840909090909091, 0.7956989247311828, 0.875, 0.95, 1.0, 0.875, 1.0, 0.5264705882352941, 0.875, 0.9230769230769231, 0.9032258064516129, 0.8333333333333334, 0.782608695652174, 0.6031746031746031, 0.875, 1.0, 0.5685131195335277, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.7692307692307693, 0.7857142857142857, 0.875, 1.0, 0.5892116182572614, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 0.7066666666666667, 0.8444444444444444, 0.875, 1.0, 1.0, 0.875, 1.0, 0.551219512195122, 0.8333333333333334, 0.8461538461538461, 0.9, 0.875, 1.0, 0.5475113122171946, 0.8333333333333334, 0.759493670886076, 0.7962962962962963, 0.875, 1.0, 0.625, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.9444444444444444, 0.875, 1.0, 0.4831168831168831, 0.875, 0.9090909090909091, 0.6225490196078431, 0.875, 1.0, 0.5523465703971119, 0.8333333333333334, 0.8461538461538461, 0.625531914893617, 0.875, 0.9, 0.6976744186046512, 0.875, 1.0, 0.6601941747572816, 0.875, 1.0, 0.6511627906976745, 0.875, 1.0, 0.7049180327868853, 0.8333333333333334, 0.9090909090909091, 1.0, 0.8333333333333334, 0.8852459016393442, 0.8837209302325582, 0.8333333333333334, 0.8571428571428571, 0.9444444444444444, 0.8333333333333334, 0.9333333333333333, 0.7340425531914894, 0.8333333333333334, 0.825, 0.9166666666666666, 0.8333333333333334, 1.0, 0.7142857142857143, 0.8333333333333334, 0.9565217391304348, 0.7477477477477478, 0.8333333333333334, 0.95, 0.9, 0.8333333333333334, 1.0, 0.88, 0.875, 1.0, 0.7634408602150538, 0.8333333333333334, 0.9444444444444444, 1.0, 0.8333333333333334, 0.7857142857142857, 0.6714285714285714, 0.8333333333333334, 0.9, 0.8387096774193549, 0.8333333333333334, 0.9090909090909091, 0.9333333333333333, 0.875, 0.9, 1.0, 0.875, 1.0, 0.678082191780822, 0.8333333333333334, 0.8421052631578947, 0.8571428571428571, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.5957446808510638, 0.875, 0.8571428571428571, 1.0, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.6548223350253807, 0.875, 1.0, 0.5668202764976958, 0.8333333333333334, 1.0, 0.6161616161616161, 0.875, 1.0, 0.6785714285714286, 0.875, 1.0, 1.0, 0.875, 1.0, 0.7589285714285714, 0.875, 1.0, 0.7387387387387387, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5072463768115942, 0.875, 0.9090909090909091, 0.7878787878787878, 0.875, 1.0, 0.5141509433962265, 0.875, 0.8888888888888888, 0.47435897435897434, 0.875, 0.8571428571428571, 0.7444444444444445, 0.875, 1.0, 0.48044692737430167, 0.875, 1.0, 0.6178861788617886, 0.875, 1.0, 0.5194174757281553, 0.875, 1.0, 0.6608187134502924, 0.875, 1.0, 0.7246376811594203, 0.8333333333333334, 0.7441860465116279, 0.9166666666666666, 0.875, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8611111111111112, 0.8260869565217391, 0.875, 1.0, 0.9473684210526315, 0.8333333333333334, 0.7692307692307693, 0.8181818181818182, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 1.0, 0.875, 0.8666666666666667, 0.6271186440677966, 0.875, 0.9, 1.0, 0.875, 1.0, 0.5261437908496732, 0.875, 1.0, 0.75, 0.875, 0.5161290322580645, 0.875, 0.8571428571428571, 1.0, 0.875, 1.0, 0.6, 0.875, 1.0, 0.7159090909090909, 0.875, 0.8333333333333334, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9285714285714286, 0.84, 0.875, 1.0, 0.8, 0.875, 0.9, 0.8421052631578947, 0.875, 1.0, 0.8064516129032258, 0.875, 0.9090909090909091, 0.9473684210526315, 0.875, 1.0, 1.0, 0.875, 1.0, 0.9285714285714286, 0.875, 1.0, 0.6715328467153284, 0.875, 1.0, 0.5760869565217391, 0.875, 1.0, 0.5741626794258373, 0.875, 1.0, 0.788135593220339, 0.875, 1.0, 0.5024390243902439, 0.875, 0.8571428571428571, 0.5819672131147541, 0.875, 1.0, 0.6030927835051546, 0.875, 1.0, 0.5522388059701493, 0.875, 1.0, 0.7469879518072289, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.5536723163841808, 0.875, 0.875, 0.9787234042553191, 0.8333333333333334, 1.0, 0.9354838709677419, 0.8333333333333334, 0.9333333333333333, 1.0, 0.8333333333333334, 0.9, 1.0, 0.8333333333333334, 1.0, 0.8333333333333334, 0.8333333333333334, 0.9411764705882353, 1.0, 0.8333333333333334, 0.9444444444444444, 0.7272727272727273, 0.8333333333333334, 0.8666666666666667, 0.875, 0.8333333333333334, 0.9285714285714286, 0.7127659574468085, 0.8333333333333334, 1.0, 0.49538461538461537, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.875, 1.0, 0.6589595375722543, 0.8333333333333334, 0.8064516129032258, 0.8214285714285714, 0.875, 1.0, 0.5407407407407407, 0.875, 1.0, 0.6359223300970874, 0.875, 1.0, 0.7052631578947368, 0.8333333333333334, 0.8260869565217391, 1.0, 0.875, 0.8, 0.6730769230769231, 0.875, 1.0, 0.5335570469798657, 0.875, 1.0, 0.569620253164557, 0.875, 0.8571428571428571, 0.8709677419354839, 0.875, 0.8571428571428571, 0.5310077519379846, 0.875, 1.0, 0.6875, 0.8333333333333334, 0.8260869565217391, 0.9411764705882353, 0.875, 0.9166666666666666, 0.9230769230769231, 0.8333333333333334, 0.9523809523809523, 0.825, 0.875, 1.0, 0.6270270270270271, 0.875, 1.0, 0.6830357142857143, 0.875, 0.8181818181818182, 0.5357142857142857, 0.875, 0.8888888888888888, 0.8804347826086957, 0.875, 1.0, 0.6370967741935484, 0.875, 1.0, 0.572139303482587, 0.875, 0.8666666666666667, 0.7872340425531915, 0.875, 1.0, 0.5203252032520326, 0.875, 1.0, 0.5486111111111112, 0.875, 0.9090909090909091, 1.0, 0.875, 1.0, 0.605, 0.8333333333333334, 1.0, 0.7272727272727273, 0.875, 0.9090909090909091, 0.625, 0.875, 0.9090909090909091, 0.6875, 0.875, 1.0, 0.517799352750809, 0.8333333333333334, 0.7954545454545454, 0.75, 0.875, 1.0, 0.49707602339181284, 0.875, 1.0, 0.5290322580645161, 0.875, 1.0, 0.5415162454873647, 0.875, 1.0, 0.4642857142857143, 0.8333333333333334, 0.9090909090909091, 0.9411764705882353, 0.875, 0.8181818181818182, 1.0, 0.875, 0.7857142857142857, 0.5782312925170068, 0.8333333333333334, 0.8135593220338984, 0.8928571428571429, 0.8333333333333334, 0.9090909090909091, 0.6766917293233082, 0.8333333333333334, 0.9523809523809523, 0.9090909090909091, 0.8333333333333334, 0.875, 0.9090909090909091, 0.875, 1.0, 0.6163522012578616, 0.8333333333333334, 0.9444444444444444, 0.6956521739130435, 0.875, 1.0, 0.7340425531914894, 0.875, 0.9333333333333333, 0.7107438016528925, 0.875, 1.0, 0.6404494382022472, 0.8333333333333334, 0.782608695652174, 0.8181818181818182, 0.875, 1.0, 0.8529411764705882, 0.875, 1.0, 0.8653846153846154, 0.875, 0.8571428571428571, 0.6355140186915887, 0.875, 0.9444444444444444, 0.7142857142857143, 0.875, 0.8333333333333334, 0.96875, 0.875, 1.0, 0.7804878048780488, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 1.0, 0.6666666666666666, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.5454545454545454, 0.8333333333333334, 0.9545454545454546, 0.8333333333333334, 0.875, 0.8888888888888888, 0.7297297297297297, 0.8333333333333334, 0.64, 0.8461538461538461, 0.875, 0.9333333333333333, 0.9019607843137255, 0.875, 1.0, 0.6580645161290323, 0.8333333333333334, 0.9375, 0.8, 0.8333333333333334, 1.0, 0.7894736842105263, 0.875, 0.8888888888888888, 0.5161290322580645, 0.875, 1.0, 0.6691176470588235, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.6902173913043478, 0.875, 1.0, 0.6047904191616766, 0.8333333333333334, 0.9642857142857143, 0.8571428571428571, 0.8333333333333334, 0.967741935483871, 0.6198347107438017, 0.875, 0.8823529411764706, 0.6136363636363636, 0.875, 1.0, 0.6704545454545454, 0.875, 1.0, 0.5128865979381443, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8780487804878049, 0.8857142857142857, 0.8333333333333334, 0.7904761904761904, 0.8676470588235294, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 0.918918918918919, 0.875, 0.9, 0.9090909090909091, 0.8333333333333334, 1.0, 0.65625, 0.8333333333333334, 0.8108108108108109, 1.0, 0.875, 0.9090909090909091, 0.5169491525423728, 0.875, 1.0, 0.5882352941176471, 0.8333333333333334, 1.0, 0.7941176470588235, 0.875, 1.0, 0.7931034482758621, 0.875, 0.9166666666666666, 0.8518518518518519, 0.875, 0.8, 0.5978260869565217, 0.875, 0.9230769230769231, 0.5114503816793893, 0.875, 1.0, 0.654054054054054, 0.875, 0.875, 1.0, 0.875, 1.0, 0.6716417910447762, 0.875, 1.0, 0.5531914893617021, 0.875, 1.0, 0.484, 0.875, 0.875, 0.6666666666666666, 0.875, 0.8823529411764706, 1.0, 0.875, 0.9230769230769231, 0.49795918367346936, 0.8333333333333334, 0.9230769230769231, 0.8372093023255814, 0.875, 1.0, 1.0, 0.875, 0.8888888888888888, 0.5256064690026954, 0.875, 0.9230769230769231, 0.5626740947075209, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 0.6555555555555556, 0.875, 0.6, 0.8461538461538461, 0.8333333333333334, 0.9166666666666666, 0.7368421052631579, 0.875, 0.9090909090909091, 0.5772058823529411, 0.875, 1.0, 0.7151898734177216, 0.875, 1.0, 0.6347826086956522, 0.875, 1.0, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 0.6666666666666666, 0.875, 0.8888888888888888, 0.6044776119402985, 0.875, 1.0, 0.8723404255319149, 0.875, 0.9, 0.6054054054054054, 0.875, 1.0, 0.5179282868525896, 0.875, 1.0, 0.796875, 0.875, 0.7692307692307693, 0.5522388059701493, 0.875, 0.9375, 1.0, 0.875, 1.0, 0.5897435897435898, 0.875, 0.8571428571428571, 0.7037037037037037, 0.875, 0.7894736842105263, 0.8181818181818182, 0.875, 0.9333333333333333, 0.9285714285714286, 0.875, 1.0, 0.7112676056338029, 0.875, 0.9090909090909091, 0.5562310030395137, 0.8333333333333334, 0.9473684210526315, 0.6605504587155964, 0.8333333333333334, 0.8636363636363636, 0.6781609195402298, 0.875, 1.0, 0.6244897959183674, 0.875, 0.9230769230769231, 0.7631578947368421, 0.8333333333333334, 0.9090909090909091, 0.6896551724137931, 0.8333333333333334, 0.9545454545454546, 0.9642857142857143, 0.875, 1.0, 0.7530864197530864, 0.875, 0.9411764705882353, 0.896551724137931, 0.875, 1.0, 0.6923076923076923, 0.875, 0.9230769230769231, 0.9117647058823529, 0.8333333333333334, 0.8823529411764706, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9545454545454546, 0.65625, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9166666666666666, 0.578125, 0.8333333333333334, 0.9047619047619048, 0.6226415094339622, 0.8333333333333334, 0.8387096774193549, 1.0, 0.8333333333333334, 0.7647058823529411, 1.0, 0.8333333333333334, 0.8947368421052632, 0.9, 0.8333333333333334, 0.9565217391304348, 0.7590361445783133, 0.875, 1.0, 0.7777777777777778, 0.8333333333333334, 0.8095238095238095, 0.9090909090909091, 0.8333333333333334, 0.8695652173913043, 0.8888888888888888, 0.875, 1.0, 0.775, 0.8333333333333334, 0.8846153846153846, 0.9375, 0.8333333333333334, 0.9130434782608695, 1.0, 0.875, 0.9130434782608695, 0.5267175572519084, 0.875, 1.0, 0.6882352941176471, 0.875, 1.0, 0.7425742574257426, 0.875, 0.7297297297297297, 0.8064516129032258, 0.875, 0.8085106382978723, 0.7846153846153846, 0.875, 0.9047619047619048, 0.6086956521739131, 0.875, 0.9090909090909091, 0.6991150442477876, 0.875, 1.0, 0.580952380952381, 0.875, 1.0, 0.5822784810126582, 0.875, 0.9, 0.8478260869565217, 0.8333333333333334, 0.9375, 0.8571428571428571, 0.875, 1.0, 0.6570048309178744, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8846153846153846, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9047619047619048, 1.0, 0.875, 0.9090909090909091, 0.4631578947368421, 0.8333333333333334, 0.847457627118644, 0.5014749262536873, 0.8333333333333334, 0.9473684210526315, 1.0, 0.875, 1.0, 0.96, 0.8333333333333334, 0.8524590163934426, 0.8604651162790697, 0.8333333333333334, 0.9473684210526315, 0.9, 0.875, 0.7777777777777778, 0.6276595744680851, 0.8333333333333334, 0.9090909090909091, 1.0, 0.8333333333333334, 0.7962962962962963, 0.88, 0.875, 0.8, 0.8, 0.8333333333333334, 0.8235294117647058, 0.48717948717948717, 0.8333333333333334, 1.0, 0.9166666666666666, 0.8333333333333334, 0.7037037037037037, 0.6192660550458715, 0.8333333333333334, 0.9473684210526315, 0.8472222222222222, 0.8333333333333334, 0.8823529411764706, 1.0, 0.8333333333333334, 0.8529411764705882, 0.6265060240963856, 0.875, 0.8888888888888888, 0.5296912114014252, 0.875, 0.8571428571428571, 0.8275862068965517, 0.875, 1.0, 0.7815126050420168, 0.8333333333333334, 1.0, 0.6595744680851063, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5292620865139949, 0.8333333333333334, 0.6666666666666666, 1.0, 0.875, 1.0, 0.9444444444444444, 0.875, 1.0, 0.5806451612903226, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.675, 0.875, 0.8888888888888888, 0.5942028985507246, 0.875, 1.0, 0.5138339920948617, 0.8333333333333334, 1.0, 1.0, 0.875, 0.8888888888888888, 1.0, 0.875, 1.0, 0.8666666666666667, 0.875, 1.0, 0.5658914728682171, 0.8333333333333334, 0.9, 0.6666666666666666, 0.875, 1.0, 0.8333333333333334, 0.9130434782608695, 0.8235294117647058, 0.875, 1.0, 0.5513513513513514, 0.8333333333333334, 0.8928571428571429, 0.9285714285714286, 0.875, 1.0, 0.6991150442477876, 0.8333333333333334, 0.8928571428571429, 0.875, 0.8333333333333334, 1.0, 0.56, 0.8333333333333334, 0.9130434782608695, 0.6447368421052632, 0.8333333333333334, 0.9444444444444444, 0.5725806451612904, 0.875, 0.8333333333333334, 0.525679758308157, 0.8333333333333334, 0.9230769230769231, 0.6078431372549019, 0.8333333333333334, 1.0, 0.9375, 0.8333333333333334, 0.9444444444444444, 1.0, 0.875, 1.0, 0.5504201680672269, 0.875, 1.0, 0.6262975778546713, 0.875, 1.0, 0.7666666666666667, 0.875, 0.875, 0.5957446808510638, 0.875, 1.0, 0.4638888888888889, 0.875, 1.0, 0.5621301775147929, 0.875, 0.8888888888888888, 0.46634615384615385, 0.875, 1.0, 0.5913978494623656, 0.875, 1.0, 0.6388888888888888, 0.875, 0.9090909090909091, 0.5317073170731708, 0.875, 0.9166666666666666, 0.5448275862068965, 0.875, 1.0, 0.6593406593406593, 0.875, 1.0, 0.6027397260273972, 0.8333333333333334, 0.7125, 0.7857142857142857, 0.875, 1.0, 0.9527027027027027, 0.8333333333333334, 1.0, 0.9230769230769231, 0.875, 1.0, 0.5372549019607843, 0.875, 1.0, 0.8181818181818182, 0.8333333333333334, 0.8, 0.9333333333333333, 0.8333333333333334, 0.96, 0.8275862068965517, 0.875, 1.0, 0.4578313253012048, 0.875, 0.875, 0.6794871794871795, 0.8333333333333334, 0.9333333333333333, 0.4855072463768116, 0.8333333333333334, 0.7741935483870968, 0.4621212121212121, 0.8333333333333334, 0.6976744186046512, 0.7586206896551724, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 0.4578313253012048, 0.8333333333333334, 0.9375, 1.0, 0.875, 0.7, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.8690476190476191, 0.875, 0.9090909090909091, 0.6829268292682927, 0.875, 1.0, 0.9615384615384616, 0.875, 1.0, 0.9285714285714286, 0.875, 0.9090909090909091, 0.5796178343949044, 0.875, 0.8888888888888888, 0.759493670886076, 0.875, 0.9090909090909091, 0.5495495495495496, 0.8333333333333334, 0.8135593220338984, 0.7567567567567568, 0.875, 1.0, 0.6967741935483871, 0.875, 0.9090909090909091, 1.0, 0.875, 1.0, 1.0, 0.875, 0.75, 0.5373134328358209, 0.875, 0.9444444444444444, 0.7575757575757576, 0.875, 1.0, 0.6929824561403509, 0.875, 0.8571428571428571, 0.7478260869565218, 0.875, 0.9230769230769231, 0.6984924623115578, 0.875, 1.0, 0.6363636363636364, 0.875, 0.875, 0.6324324324324324, 0.875, 0.875, 0.5505050505050505, 0.875, 1.0, 0.5636942675159236, 0.875, 0.9, 0.48704663212435234, 0.875, 0.9230769230769231, 0.6831683168316832, 0.875, 0.9, 0.5428571428571428, 0.875, 1.0, 1.0, 0.875, 0.6769911504424779, 0.8260869565217391, 0.875, 0.875, 0.6206896551724138, 0.875, 1.0, 1.0, 0.8333333333333334, 0.9090909090909091, 0.5430711610486891, 0.8333333333333334, 0.875, 0.9285714285714286, 0.8333333333333334, 0.8333333333333334, 0.5104602510460251, 0.8333333333333334, 0.9259259259259259, 0.6952380952380952, 0.8333333333333334, 1.0, 0.6060606060606061, 0.8333333333333334, 1.0, 0.6129032258064516, 0.8333333333333334, 0.9333333333333333, 0.8307692307692308, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 0.7662337662337663, 0.8333333333333334, 0.9444444444444444, 1.0, 0.875, 1.0, 0.6058394160583942, 0.875, 0.64, 0.5966386554621849, 0.8333333333333334, 0.7647058823529411, 0.8461538461538461, 0.875, 1.0, 1.0, 0.8333333333333334, 0.7761194029850746, 0.7368421052631579, 0.8333333333333334, 1.0, 0.5370370370370371, 0.875, 1.0, 1.0, 0.875, 1.0, 0.8297872340425532, 0.8333333333333334, 0.875, 0.8809523809523809, 0.875, 1.0, 0.6906474820143885, 0.875, 1.0, 0.696969696969697, 0.8333333333333334, 0.7901234567901234, 0.88, 0.875, 0.8, 0.6007604562737643, 0.875, 1.0, 1.0, 0.875, 1.0, 1.0, 0.8333333333333334, 0.762589928057554, 0.7073170731707317, 0.875, 1.0, 0.7, 0.875, 1.0, 0.6576576576576577, 0.875, 1.0, 0.8235294117647058, 0.875, 1.0, 0.6590909090909091, 0.875, 1.0, 0.9310344827586207, 0.875, 0.8888888888888888, 1.0, 0.875, 1.0, 0.8888888888888888, 0.875, 1.0, 0.625, 0.875, 1.0, 0.5402298850574713, 0.875, 1.0, 0.6781115879828327, 0.8333333333333334, 0.8823529411764706, 0.8, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.7710843373493976, 0.875, 1.0, 0.5012987012987012, 0.875, 1.0, 0.7692307692307693, 0.8333333333333334, 0.8235294117647058, 0.6842105263157895, 0.875, 1.0, 0.9166666666666666, 0.875, 0.8421052631578947, 0.875, 0.875, 1.0, 0.6096491228070176, 0.8333333333333334, 1.0, 0.8787878787878788, 0.875, 1.0, 0.6305732484076433, 0.875, 1.0, 0.9411764705882353, 0.8333333333333334, 1.0, 0.5374449339207048, 0.8333333333333334, 1.0, 0.875, 1.0, 0.96, 0.8333333333333334, 0.9090909090909091, 0.8571428571428571, 0.875, 0.9375, 0.8333333333333334, 0.9285714285714286, 1.0, 0.875, 1.0, 0.78125, 0.8333333333333334, 0.9230769230769231, 0.7192982456140351, 0.8333333333333334, 1.0, 0.8636363636363636, 0.8333333333333334, 1.0, 0.7777777777777778, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.9333333333333333, 0.7, 0.8333333333333334, 0.7837837837837838, 0.9629629629629629, 0.8333333333333334, 0.95, 1.0, 0.8333333333333334, 0.875, 0.9285714285714286, 0.875, 1.0, 0.90625, 0.8333333333333334, 1.0, 0.58, 0.875, 1.0, 0.5349794238683128, 0.875, 0.9285714285714286, 0.64, 0.875, 0.9230769230769231, 0.6342281879194631, 0.875, 0.8571428571428571, 0.5578947368421052, 0.875, 1.0, 0.5284280936454849, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 1.0, 0.6792452830188679, 0.8333333333333334, 0.6509433962264151, 0.8627450980392157, 0.8333333333333334, 0.8, 0.875, 0.8333333333333334, 0.8235294117647058, 0.8333333333333334, 0.8333333333333334, 0.9333333333333333, 1.0, 0.875, 1.0, 1.0, 0.875, 1.0, 0.547945205479452, 0.8333333333333334, 0.8888888888888888, 0.8, 0.875, 1.0, 0.7114093959731543, 0.875, 1.0, 1.0, 0.875, 0.9333333333333333, 0.5853658536585366, 0.875, 1.0, 0.5325670498084292, 0.875, 0.8888888888888888, 0.5798319327731093, 0.875, 1.0, 0.5923076923076923, 0.875, 1.0, 0.5805243445692884, 0.875, 1.0, 0.4918032786885246, 0.875, 1.0, 0.7301587301587301, 0.875, 0.8, 0.8, 0.8333333333333334, 0.7027027027027027, 0.59375, 0.8333333333333334, 1.0, 0.6025641025641025, 0.875, 0.9090909090909091, 0.6424242424242425, 0.8333333333333334, 0.9411764705882353, 0.7111111111111111, 0.875, 1.0, 1.0, 0.8333333333333334, 0.85, 1.0, 0.875, 0.8571428571428571, 1.0, 0.875, 0.8888888888888888, 0.6071428571428571, 0.875, 1.0, 0.5483870967741935, 0.875, 0.8, 0.5603112840466926, 0.875, 1.0, 0.6194690265486725, 0.8333333333333334, 0.9, 0.6333333333333333, 0.8333333333333334, 0.9047619047619048, 0.6567164179104478, 0.875, 1.0, 0.7101449275362319, 0.875, 1.0, 0.49760765550239233, 0.875, 1.0, 0.47540983606557374, 0.875, 0.9230769230769231, 0.7894736842105263, 0.875, 0.8333333333333334, 0.5127118644067796, 0.8333333333333334, 0.782608695652174, 0.6639344262295082, 0.875, 0.9, 0.5072463768115942, 0.8333333333333334, 0.95, 1.0, 0.8333333333333334, 0.9393939393939394, 1.0, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.8695652173913043, 0.717391304347826, 0.8333333333333334, 0.9354838709677419, 0.7619047619047619, 0.875, 0.9230769230769231, 0.6918238993710691, 0.875, 1.0, 0.6481481481481481, 0.875, 1.0, 0.675, 0.8333333333333334, 1.0, 0.5714285714285714, 0.8333333333333334, 0.95, 1.0, 0.875, 0.9230769230769231, 1.0, 0.875, 1.0, 0.5533596837944664, 0.8333333333333334, 0.8421052631578947, 0.7333333333333333, 0.875, 1.0, 0.5231316725978647, 0.875, 1.0, 0.6927710843373494, 0.875, 1.0, 1.0, 0.875, 0.875, 1.0, 0.875, 0.9090909090909091, 0.5829383886255924, 0.875, 0.8888888888888888, 0.7264150943396226, 0.875, 0.8888888888888888, 0.5075757575757576, 0.875, 0.8888888888888888, 0.6206896551724138, 0.8333333333333334, 0.8571428571428571, 0.6633663366336634, 0.875, 0.9285714285714286, 0.875, 0.7, 0.7804878048780488, 0.875, 1.0, 0.6525096525096525, 0.875, 0.8, 0.5568181818181818, 0.8333333333333334, 0.9047619047619048, 0.9090909090909091, 0.875, 1.0, 0.6598639455782312, 0.8333333333333334, 0.6774193548387096, 0.4563106796116505, 0.875, 0.95, 0.5101214574898786, 0.875, 1.0, 0.6401515151515151, 0.875, 1.0, 0.542016806722689, 0.875, 1.0, 0.5687732342007435, 0.875, 0.8888888888888888, 0.456575682382134, 0.875, 1.0, 0.6232558139534884, 0.8333333333333334, 0.9411764705882353, 1.0, 0.875, 1.0, 0.6846846846846847, 0.8333333333333334, 1.0, 1.0, 0.875, 1.0, 0.7738095238095238, 0.8333333333333334, 1.0, 0.7023809523809523, 0.875, 1.0, 0.7435897435897436, 0.8333333333333334, 0.9166666666666666, 1.0, 0.875, 1.0, 0.7638888888888888, 0.875, 1.0, 0.6330935251798561, 0.875, 1.0, 0.6830985915492958, 0.875, 0.875, 1.0, 0.875, 1.0, 0.7647058823529411, 0.875, 0.8571428571428571, 0.5570032573289903, 0.875, 0.8695652173913043, 0.9166666666666666, 0.8333333333333334, 0.96, 1.0, 0.875, 1.0, 0.825, 0.875, 1.0, 0.9, 0.875, 0.8620689655172413, 1.0, 0.875, 1.0, 0.9473684210526315, 0.875, 0.8333333333333334, 0.5057471264367817, 0.8333333333333334, 0.9444444444444444, 0.7183098591549296, 0.875, 0.8571428571428571, 1.0, 0.875, 1.0, 0.8095238095238095, 0.875, 0.8888888888888888, 0.4940828402366864, 0.875, 1.0, 1.0, 0.875, 0.9375, 0.7727272727272727, 0.8333333333333334, 0.8222222222222222, 0.8070175438596491, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8461538461538461, 1.0, 0.875, 1.0, 0.647887323943662, 0.875, 1.0, 1.0, 0.875, 1.0, 0.5565217391304348, 0.875, 0.8947368421052632, 0.875, 1.0, 0.6434426229508197, 0.875, 1.0, 0.6300813008130082, 0.8333333333333334, 0.7586206896551724, 0.4897959183673469, 0.875, 1.0, 0.7428571428571429, 0.875, 1.0, 0.6505376344086021, 0.8333333333333334, 0.8620689655172413, 0.5754716981132075, 0.8333333333333334, 0.9642857142857143, 0.9473684210526315, 0.8333333333333334, 0.95, 0.8709677419354839, 0.875, 0.9375, 0.9428571428571428, 0.8333333333333334, 0.8666666666666667, 1.0, 0.8333333333333334, 0.84, 1.0, 0.8333333333333334, 0.8709677419354839, 1.0, 0.8333333333333334, 0.9130434782608695, 1.0, 0.8333333333333334, 0.9, 0.8333333333333334, 0.875, 0.9230769230769231, 0.601123595505618, 0.8333333333333334, 0.7906976744186046, 0.7741935483870968, 0.875, 1.0, 0.7659574468085106, 0.8333333333333334, 0.9523809523809523, 1.0, 0.8333333333333334, 1.0, 1.0, 0.8333333333333334, 0.8214285714285714, 1.0, 0.875, 1.0, 0.7446808510638298, 0.8333333333333334, 1.0, 0.6666666666666666, 0.875, 0.7857142857142857, 0.7323943661971831, 0.875, 1.0, 0.6330275229357798, 0.8333333333333334, 0.95, 0.8235294117647058, 0.8333333333333334, 0.8857142857142857, 0.5061728395061729, 0.875, 0.875, 0.6652173913043479, 0.8333333333333334, 0.92, 0.7428571428571429, 0.875, 1.0, 0.7123287671232876, 0.875, 0.9615384615384616, 1.0, 0.875, 0.8947368421052632, 0.7333333333333333, 0.875, 1.0, 0.6363636363636364, 0.875, 0.9, 0.6796875, 0.875, 1.0, 0.7710843373493976, 0.8333333333333334, 0.9772727272727273, 0.7755102040816326, 0.8333333333333334, 0.8888888888888888, 1.0, 0.8333333333333334, 0.95, 0.9333333333333333, 0.875, 1.0, 0.6842105263157895, 0.8333333333333334, 0.8333333333333334, 0.9333333333333333, 0.8333333333333334, 0.9285714285714286, 1.0, 0.8333333333333334, 0.9333333333333333, 0.8571428571428571, 0.8333333333333334, 1.0, 0.5698924731182796, 0.8333333333333334, 0.8260869565217391, 0.5878378378378378, 0.875, 1.0, 0.6077586206896551, 0.8333333333333334, 0.8148148148148148, 1.0, 0.875, 1.0, 0.7049180327868853, 0.875, 0.8888888888888888, 0.5276073619631901, 0.8333333333333334, 0.85, 0.9166666666666666, 0.8333333333333334, 0.8333333333333334, 1.0, 0.8333333333333334, 0.9230769230769231, 0.743801652892562, 0.8333333333333334, 0.8918918918918919, 0.9642857142857143, 0.8333333333333334, 1.0, 1.0, 0.875, 0.9285714285714286, 0.5846153846153846, 0.8333333333333334, 0.9375, 1.0, 0.8333333333333334, 0.9, 0.9090909090909091, 0.8333333333333334, 1.0, 0.6137566137566137, 0.875, 1.0, 0.6666666666666666, 0.8333333333333334, 0.875, 1.0, 0.8333333333333334, 0.9090909090909091, 0.8695652173913043, 0.875, 0.9, 0.5655737704918032, 0.8333333333333334, 1.0, 0.6204819277108434, 0.875, 1.0, 0.5867158671586716, 0.875, 1.0, 0.6303501945525292, 0.8333333333333334, 0.8823529411764706, 0.5631067961165048, 0.8333333333333334, 0.7333333333333333, 0.8448275862068966, 0.875, 1.0, 0.75, 0.875, 1.0, 0.696078431372549, 0.875, 1.0, 0.45454545454545453, 0.875, 1.0, 0.5068119891008175, 0.875, 0.875, 0.54, 0.875, 1.0, 0.45454545454545453, 0.8333333333333334, 0.9230769230769231, 0.5701357466063348, 0.875, 1.0, 0.5031847133757962, 0.875, 1.0, 0.5945945945945946, 0.8333333333333334, 1.0, 0.7307692307692307, 0.875, 0.8461538461538461, 0.5786350148367952, 0.8333333333333334, 0.9285714285714286, 0.9166666666666666, 0.875, 1.0, 0.8333333333333334, 0.9032258064516129, 1.0, 0.8333333333333334, 0.7894736842105263, 0.7428571428571429, 0.875, 1.0, 0.61, 0.875, 1.0, 0.6510067114093959, 0.875, 1.0, 1.0, 0.875, 0.8461538461538461, 0.6153846153846154, 0.875, 0.9, 0.5722222222222222, 0.8333333333333334, 0.9259259259259259, 1.0, 0.875, 1.0, 0.6346153846153846, 0.875, 1.0, 0.5756097560975609, 0.875, 0.9090909090909091, 0.8846153846153846, 0.875, 1.0, 0.627177700348432, 0.875, 0.9473684210526315, 1.0, 0.875, 0.8888888888888888, 0.6145833333333334, 0.875, 1.0, 1.0, 0.875, 1.0, 0.6373626373626373, 0.875, 0.9333333333333333, 1.0, 0.875, 0.8823529411764706, 0.8, 0.875, 1.0, 0.5963302752293578, 0.8333333333333334, 1.0, 0.7536231884057971, 0.875, 1.0, 1.0, 0.8333333333333334, 0.8709677419354839, 0.8947368421052632, 0.8333333333333334, 0.9411764705882353, 0.5901639344262295, 0.8333333333333334, 0.9166666666666666, 0.6048387096774194, 0.8333333333333334, 0.8461538461538461, 0.803921568627451, 0.8333333333333334, 1.0, 0.6602564102564102, 0.875, 0.9, 0.48905109489051096, 0.875, 0.8571428571428571, 0.6233766233766234], "type": "scatter"}, {"customdata": [473, 629, 647, 1205, 1313, 1937, 2060, 2297, 2306, 2546, 2681, 2915], "hovertemplate": "Index: %{customdata}<br>Value: %{y:.2f}<extra></extra>", "marker": {"color": "#d66a6a", "size": 10, "symbol": "x"}, "mode": "markers", "name": "Outliers (12)", "x": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "y": [0.44285714285714284, 0.44936708860759494, 0.45, 0.40828402366863903, 0.4481132075471698, 0.43532338308457713, 0.4339622641509434, 0.3532608695652174, 0.4444444444444444, 0.35858585858585856, 0.41228070175438597, 0.356], "type": "scatter"}];
            const layout = Object.assign({}, {"template": {"data": {"histogram2dcontour": [{"type": "histogram2dcontour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "choropleth": [{"type": "choropleth", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "histogram2d": [{"type": "histogram2d", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "heatmap": [{"type": "heatmap", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "contourcarpet": [{"type": "contourcarpet", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "contour": [{"type": "contour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "surface": [{"type": "surface", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "mesh3d": [{"type": "mesh3d", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "scatter": [{"fillpattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}, "type": "scatter"}], "parcoords": [{"type": "parcoords", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolargl": [{"type": "scatterpolargl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "bar"}], "scattergeo": [{"type": "scattergeo", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolar": [{"type": "scatterpolar", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "histogram": [{"marker": {"pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "histogram"}], "scattergl": [{"type": "scattergl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatter3d": [{"type": "scatter3d", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermap": [{"type": "scattermap", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermapbox": [{"type": "scattermapbox", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterternary": [{"type": "scatterternary", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattercarpet": [{"type": "scattercarpet", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "barpolar"}], "pie": [{"automargin": true, "type": "pie"}]}, "layout": {"autotypenumbers": "strict", "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "hovermode": "closest", "hoverlabel": {"align": "left"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"bgcolor": "#E5ECF6", "angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "ternary": {"bgcolor": "#E5ECF6", "aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]]}, "xaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "yaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "geo": {"bgcolor": "white", "landcolor": "#E5ECF6", "subunitcolor": "white", "showland": true, "showlakes": true, "lakecolor": "white"}, "title": {"x": 0.05}, "mapbox": {"style": "light"}}}, "shapes": [{"line": {"color": "#e0b854", "dash": "dash"}, "type": "line", "x0": 0, "x1": 1, "xref": "x domain", "y0": 1.2488747554565234, "y1": 1.2488747554565234, "yref": "y"}, {"line": {"color": "#e0b854", "dash": "dash"}, "type": "line", "x0": 0, "x1": 1, "xref": "x domain", "y0": 0.4514311854758205, "y1": 0.4514311854758205, "yref": "y"}, {"line": {"color": "#7cb97c", "dash": "dot"}, "type": "line", "x0": 0, "x1": 1, "xref": "x domain", "y0": 0.8501529704661719, "y1": 0.8501529704661719, "yref": "y"}], "annotations": [{"font": {"color": "#a39e93"}, "showarrow": false, "text": "Upper bound (1.2)", "x": 1, "xanchor": "right", "xref": "x domain", "y": 1.2488747554565234, "yanchor": "bottom", "yref": "y"}, {"font": {"color": "#a39e93"}, "showarrow": false, "text": "Lower bound (0.5)", "x": 1, "xanchor": "right", "xref": "x domain", "y": 0.4514311854758205, "yanchor": "bottom", "yref": "y"}, {"font": {"color": "#a39e93"}, "showarrow": false, "text": "Mean (0.9)", "x": 1, "xanchor": "right", "xref": "x domain", "y": 0.8501529704661719, "yanchor": "bottom", "yref": "y"}], "margin": {"l": 50, "r": 30, "t": 30, "b": 50}, "font": {"color": "#a39e93"}, "xaxis": {"title": {"text": "Sample Index"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "yaxis": {"title": {"text": "Diversity Type Token Ratio"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "legend": {"font": {"color": "#a39e93"}, "orientation": "h", "yanchor": "bottom", "y": 1.02, "xanchor": "right", "x": 1}, "title": {}, "height": 400, "paper_bgcolor": "rgba(0,0,0,0)", "plot_bgcolor": "rgba(0,0,0,0)"}, darkLayout);
            Plotly.newPlot('anomaly_chart_3', data, layout, plotlyConfig);
        })();
        
        (function() {
            const data = [{"hovertemplate": "Index: %{x}<br>Value: %{y:.2f}<extra></extra>", "marker": {"color": "#6b9bd1", "opacity": 0.6, "size": 6}, "mode": "markers", "name": "Normal", "x": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960], "y": [3.5, 2.4494897427831783, 8.181818181818182, 3.5, 2.4494897427831783, 4.944980302152805, 3.5, 2.4494897427831783, 6.640458853114317, 3.5, 2.4494897427831783, 9.729917589388736, 3.5, 3.709704134011871, 6.590035768383312, 3.5, 8.586501033599191, 6.963658328309397, 4.08248290463863, 3.464101615137755, 5.747369664856797, 3.5, 4.129483209670111, 8.629059865205674, 4.08248290463863, 4.00693842672377, 4.1461399144838555, 3.5, 2.23606797749979, 7.366787985098479, 3.5, 2.4494897427831783, 2.6457513110645903, 3.5, 3.2071349029490928, 7.723027987151322, 3.5, 2.2677868380553634, 9.798670179275858, 4.08248290463863, 3.8729833462074166, 2.82842712474619, 3.5, 3.162277660168379, 8.384615384615385, 3.5, 2.6457513110645903, 8.15867351279435, 4.08248290463863, 4.170288281141495, 4.800568148193976, 3.5, 3.162277660168379, 8.875203139603666, 3.5, 3.162277660168379, 2.7136021011998728, 3.5, 2.23606797749979, 4.706787243316417, 3.5, 2.7136021011998728, 9.264603429846783, 3.5, 2.4494897427831783, 5.2915026221291805, 4.08248290463863, 5.303300858899106, 3.75, 3.5, 2.6457513110645903, 6.7082039324993685, 3.5, 2.4494897427831783, 6.2667956144051224, 3.5, 2.6457513110645903, 3.8805700005813275, 3.5, 3.0, 9.750168689498874, 4.08248290463863, 3.7416573867739413, 3.9391929857916765, 3.5, 3.3166247903554, 4.4, 3.5, 2.82842712474619, 7.229988054812212, 4.08248290463863, 3.6380343755449944, 2.041241452319315, 3.5, 3.015113445777636, 5.477225575051661, 3.5, 2.23606797749979, 3.5, 2.846049894151541, 4.08248290463863, 4.024922359499621, 2.23606797749979, 4.08248290463863, 4.510671108178233, 4.27437366993929, 3.5, 2.8867513459481287, 5.092010548749033, 4.08248290463863, 6.325872798805447, 4.364357804719848, 3.5, 8.55235974119758, 4.364357804719848, 3.5, 2.4494897427831783, 7.454545454545454, 3.5, 3.1754264805429417, 4.323460152737352, 3.5, 2.6457513110645903, 9.061921487887188, 4.08248290463863, 3.8805700005813275, 7.129062309432055, 3.5, 3.8729833462074166, 5.292377467571569, 4.08248290463863, 3.75, 7.090386795208028, 3.5, 3.0, 7.996127381260576, 3.5, 2.6457513110645903, 9.037173124616524, 4.08248290463863, 6.625, 5.917840259193665, 4.08248290463863, 5.642447102306373, 5.4443572293729625, 4.08248290463863, 4.310527248642598, 2.9398736610366685, 3.5, 2.4494897427831783, 6.114295984380816, 3.5, 8.55235974119758, 2.6457513110645903, 3.5, 5.144957554275265, 4.4907311951024935, 3.5, 3.3565855667130946, 4.932438278217077, 3.5, 3.474396144861517, 4.564604740649092, 3.5, 2.6457513110645903, 5.564556201017806, 4.08248290463863, 4.123105625617661, 3.328201177351375, 3.5, 2.4494897427831783, 6.047431568147635, 3.5, 2.82842712474619, 2.82842712474619, 3.5, 2.82842712474619, 6.525509825370939, 3.5, 5.889696476913215, 5.087989824030529, 3.5, 2.4494897427831783, 3.328201177351375, 4.08248290463863, 3.8805700005813275, 2.6666666666666665, 4.08248290463863, 3.474396144861517, 2.4494897427831783, 4.08248290463863, 3.6147844564602556, 9.308128469535708, 4.08248290463863, 3.328201177351375, 3.6380343755449944, 4.08248290463863, 6.067798762169179, 4.596194077712559, 4.08248290463863, 3.8805700005813275, 3.8805700005813275, 4.08248290463863, 4.024922359499621, 9.506957419818361, 4.08248290463863, 3.474396144861517, 2.0, 4.08248290463863, 3.7416573867739413, 3.3166247903554, 4.08248290463863, 3.328201177351375, 4.937707198786941, 3.5, 3.464101615137755, 5.879101103675122, 4.08248290463863, 4.2485291572496005, 2.23606797749979, 4.08248290463863, 3.75, 4.813544719430521, 4.08248290463863, 3.328201177351375, 1.7320508075688774, 3.5, 3.015113445777636, 9.899494936611665, 4.08248290463863, 4.37880269519857, 3.05085107923876, 3.5, 4.286607049870562, 8.049844718999243, 3.5, 3.1754264805429417, 8.267559638302064, 4.08248290463863, 4.618802153517006, 1.0, 4.08248290463863, 3.6147844564602556, 9.055385138137416, 3.5, 3.0, 8.857716634035635, 3.5, 2.6457513110645903, 8.879280880751324, 3.5, 3.3166247903554, 6.328318816843779, 3.5, 3.0, 9.475142485638552, 3.5, 8.641446821835055, 5.963027109421214, 4.08248290463863, 3.8729833462074166, 5.408987230262506, 4.08248290463863, 3.474396144861517, 2.23606797749979, 3.5, 3.1754264805429417, 8.667190566019205, 4.08248290463863, 4.050813610756598, 2.6666666666666665, 4.08248290463863, 4.00693842672377, 7.824884252135749, 3.5, 6.863036195648746, 3.5, 3.162277660168379, 9.2, 4.08248290463863, 3.3166247903554, 1.7320508075688774, 3.5, 2.23606797749979, 7.876017113339727, 3.5, 3.464101615137755, 4.532898610306738, 3.5, 2.23606797749979, 2.4494897427831783, 4.08248290463863, 3.5777087639996634, 1.0, 3.5, 2.82842712474619, 7.970379295101609, 3.5, 4.123105625617661, 4.08248290463863, 4.358898943540673, 2.840187787218772, 3.5, 2.4494897427831783, 5.144957554275265, 3.5, 3.3166247903554, 5.908391567007971, 3.5, 3.464101615137755, 8.362913726159647, 4.08248290463863, 5.217758139277826, 4.118438837901865, 3.5, 2.6457513110645903, 6.324555320336758, 3.5, 3.162277660168379, 4.129483209670111, 4.08248290463863, 4.849343154722923, 3.6055512754639896, 4.08248290463863, 4.1461399144838555, 3.3166247903554, 4.08248290463863, 3.709704134011871, 3.5, 4.08248290463863, 3.6380343755449944, 3.464101615137755, 4.08248290463863, 3.8805700005813275, 2.23606797749979, 4.08248290463863, 3.6055512754639896, 2.23606797749979, 4.08248290463863, 5.125692857821981, 4.233901974057256, 4.08248290463863, 3.6147844564602556, 5.191085476184401, 4.08248290463863, 3.75, 3.015113445777636, 4.08248290463863, 3.474396144861517, 5.417490779798923, 4.08248290463863, 3.2071349029490928, 4.2485291572496005, 4.08248290463863, 3.328201177351375, 3.015113445777636, 4.08248290463863, 6.161124853422422, 5.577959874688627, 3.5, 2.0, 4.423258684646914, 4.08248290463863, 3.464101615137755, 3.015113445777636, 3.5, 2.4748737341529163, 5.2915026221291805, 4.08248290463863, 3.0, 6.099942813304187, 4.08248290463863, 3.1754264805429417, 4.953004953007429, 4.08248290463863, 3.328201177351375, 2.6457513110645903, 3.5, 2.4748737341529163, 6.4007575309253015, 4.08248290463863, 4.170288281141495, 2.4494897427831783, 4.08248290463863, 3.75, 5.715476066494083, 3.5, 4.016632088371218, 6.232502388407515, 4.08248290463863, 4.123105625617661, 3.3166247903554, 4.08248290463863, 3.753259453027346, 1.7320508075688774, 3.5, 2.82842712474619, 2.23606797749979, 3.5, 5.11207720338155, 4.08248290463863, 3.5355339059327378, 2.0, 3.5, 3.464101615137755, 3.162277660168379, 4.08248290463863, 3.6147844564602556, 4.109974682633933, 3.5, 3.6147844564602556, 9.27869618527036, 4.08248290463863, 3.670651741928988, 2.4494897427831783, 4.08248290463863, 4.00693842672377, 4.743416490252569, 3.5, 2.82842712474619, 6.0, 4.08248290463863, 4.050813610756598, 5.875, 4.08248290463863, 4.811252243246881, 3.015113445777636, 3.5, 3.015113445777636, 6.639528095680696, 3.5, 3.395498750508662, 5.61757446387165, 4.08248290463863, 3.464101615137755, 7.221660539819146, 3.5, 3.6147844564602556, 7.118684968143743, 3.5, 2.4494897427831783, 6.599663291074443, 4.08248290463863, 4.358898943540673, 7.219948723811553, 3.5, 4.358898943540673, 6.388991238024046, 4.08248290463863, 4.2485291572496005, 5.3732008271339415, 4.08248290463863, 4.364357804719848, 2.6666666666666665, 4.08248290463863, 4.587317109255645, 6.3076923076923075, 3.5, 4.2, 7.433701089919686, 3.5, 5.232590180780451, 3.5, 4.264014327112209, 6.88128713803285, 3.5, 2.886751345948129, 8.285098499285155, 3.5, 2.6666666666666665, 4.08248290463863, 3.1754264805429417, 2.82842712474619, 3.5, 9.619220954147268, 6.181225377691006, 4.08248290463863, 3.6147844564602556, 9.904184936528898, 4.08248290463863, 4.00693842672377, 7.456011350793257, 3.5, 4.37880269519857, 4.123105625617661, 4.08248290463863, 3.6055512754639896, 9.500264920361273, 4.08248290463863, 3.328201177351375, 3.474396144861517, 3.5, 2.82842712474619, 4.129483209670111, 3.5, 2.6457513110645903, 4.47213595499958, 3.5, 2.4748737341529163, 7.342881617441561, 4.08248290463863, 5.166666666666667, 7.4687140888405334, 3.5, 4.929503017546495, 3.8805700005813275, 4.08248290463863, 3.3166247903554, 2.23606797749979, 4.08248290463863, 3.7416573867739413, 2.0, 4.08248290463863, 4.37880269519857, 9.843074773486244, 4.08248290463863, 4.0, 2.6457513110645903, 4.08248290463863, 3.3166247903554, 8.251165418850283, 4.08248290463863, 3.015113445777636, 6.538530488755173, 4.08248290463863, 3.8805700005813275, 2.23606797749979, 4.08248290463863, 4.694855340334425, 3.771236166328254, 4.08248290463863, 3.801315561749642, 2.82842712474619, 4.08248290463863, 4.024922359499621, 2.846049894151541, 4.08248290463863, 3.328201177351375, 5.714285714285714, 4.08248290463863, 3.328201177351375, 6.766006662267349, 4.08248290463863, 3.015113445777636, 3.3166247903554, 4.08248290463863, 5.303300858899106, 1.0, 4.08248290463863, 5.695489135379688, 4.08248290463863, 4.08248290463863, 4.477215043467819, 3.0, 4.08248290463863, 4.0, 2.4494897427831783, 3.5, 3.162277660168379, 5.863527298559492, 4.08248290463863, 3.6147844564602556, 2.23606797749979, 4.08248290463863, 3.5, 2.6457513110645903, 4.08248290463863, 3.3541019662496843, 3.3565855667130946, 4.08248290463863, 3.464101615137755, 5.222329678670935, 4.08248290463863, 3.5, 2.4748737341529163, 4.08248290463863, 4.364357804719848, 4.016632088371218, 3.5, 3.1754264805429417, 7.732826790345916, 4.08248290463863, 4.0, 4.381780460041329, 3.5, 2.3333333333333335, 9.214032544001846, 3.5, 2.6666666666666665, 3.474396144861517, 3.5, 2.6666666666666665, 8.62615766297429, 3.5, 2.82842712474619, 7.32709181802739, 4.08248290463863, 4.024922359499621, 2.23606797749979, 3.5, 3.2071349029490928, 9.980452274793343, 3.5, 2.6666666666666665, 8.149580519916494, 3.5, 2.041241452319315, 8.403653918204553, 3.5, 2.6457513110645903, 2.6457513110645903, 3.5, 2.4748737341529163, 7.905694150420948, 4.08248290463863, 3.6055512754639896, 3.162277660168379, 4.08248290463863, 5.028948456749697, 2.4748737341529163, 4.08248290463863, 3.162277660168379, 5.648456717647683, 4.08248290463863, 7.464798341932237, 4.0, 4.08248290463863, 3.9617738670844207, 8.717012181353368, 4.08248290463863, 5.2915026221291805, 5.284229075567875, 3.5, 3.162277660168379, 6.685631938613182, 3.5, 3.7416573867739413, 3.0, 3.5, 2.1213203435596424, 6.037383539249432, 4.08248290463863, 3.771236166328254, 5.030537377488245, 4.08248290463863, 4.669737852696148, 6.437427761961776, 3.5, 2.0, 3.3166247903554, 4.08248290463863, 7.239368114982505, 3.162277660168379, 3.5, 2.0, 8.398339722334596, 4.08248290463863, 4.47213595499958, 1.0, 3.5, 3.0, 3.328201177351375, 4.08248290463863, 3.7416573867739413, 2.23606797749979, 3.5, 3.3166247903554, 6.731006731010097, 3.5, 2.6457513110645903, 7.0710678118654755, 3.5, 2.4748737341529163, 8.39361282871732, 3.5, 2.4494897427831783, 8.674164786985145, 4.08248290463863, 3.6147844564602556, 4.9749371855331, 3.5, 2.0, 3.328201177351375, 3.5, 4.8, 5.059644256269407, 3.5, 2.23606797749979, 6.989632453288303, 3.5, 2.23606797749979, 5.246313898711125, 3.5, 2.846049894151541, 3.4914862437758782, 4.08248290463863, 3.8805700005813275, 5.124100921762788, 3.5, 2.4748737341529163, 6.366001415800504, 3.5, 3.3166247903554, 7.158238932543718, 4.08248290463863, 4.129483209670111, 2.6457513110645903, 3.5, 2.6457513110645903, 5.032452820975954, 3.5, 2.82842712474619, 4.58257569495584, 3.5, 3.05085107923876, 6.013377943029549, 3.5, 2.846049894151541, 5.11207720338155, 3.5, 2.4748737341529163, 2.846049894151541, 3.5, 2.82842712474619, 2.7716093126229358, 3.5, 3.162277660168379, 4.926646390821466, 3.5, 2.23606797749979, 6.646940512883967, 3.5, 3.0, 5.794945672638978, 3.5, 3.474396144861517, 2.6457513110645903, 3.5, 2.6457513110645903, 5.950674442093894, 3.5, 3.927922024247863, 5.615505169179773, 3.5, 2.3333333333333335, 6.599663291074444, 3.5, 2.886751345948129, 9.761272598800403, 3.5, 3.464101615137755, 8.82650882651324, 3.5, 3.162277660168379, 8.369745012779504, 3.5, 2.4748737341529163, 8.784381271934219, 3.5, 2.6666666666666665, 4.913538149119954, 3.5, 3.464101615137755, 4.2485291572496005, 3.5, 3.3166247903554, 9.303958101992741, 3.5, 2.6457513110645903, 8.183028566596791, 3.5, 2.6666666666666665, 6.0, 3.5, 2.846049894151541, 8.163796732679655, 3.5, 3.015113445777636, 8.601525360778174, 3.5, 3.0, 9.262124031816677, 3.5, 2.23606797749979, 3.6055512754639896, 3.5, 3.2071349029490928, 3.878358759406699, 3.5, 3.162277660168379, 10.099681748875627, 4.08248290463863, 3.2732683535398857, 2.4494897427831783, 4.08248290463863, 4.00693842672377, 2.6666666666666665, 3.5, 3.9617738670844207, 5.3638271972384475, 3.5, 2.6457513110645903, 9.404796606157268, 3.5, 2.846049894151541, 8.091371764398357, 3.5, 2.82842712474619, 7.0710678118654755, 3.5, 2.82842712474619, 4.050813610756598, 4.08248290463863, 3.75, 7.592566023652966, 4.08248290463863, 6.84227057829977, 4.69041575982343, 3.5, 3.1754264805429417, 5.589565568382148, 4.08248290463863, 4.00693842672377, 2.0, 4.08248290463863, 5.480077554195743, 1.0, 4.08248290463863, 4.129483209670111, 2.82842712474619, 4.08248290463863, 6.324555320336758, 4.849343154722923, 3.5, 2.82842712474619, 7.808951897004472, 3.5, 2.846049894151541, 7.6955384136140745, 4.08248290463863, 3.474396144861517, 3.0, 4.08248290463863, 4.4, 3.0, 4.08248290463863, 5.662208585049306, 4.700096710803842, 3.5, 5.629165124598852, 6.428037969018815, 3.5, 3.474396144861517, 7.111887749987414, 4.08248290463863, 3.75, 2.4494897427831783, 4.08248290463863, 3.9617738670844207, 5.417363388859614, 3.5, 3.464101615137755, 4.050813610756598, 4.08248290463863, 3.1754264805429417, 6.626115408397699, 4.08248290463863, 3.75, 3.771236166328254, 4.08248290463863, 3.7416573867739413, 4.310527248642598, 4.08248290463863, 2.773500981126146, 6.497953851430398, 4.08248290463863, 3.015113445777636, 9.013815439550587, 4.08248290463863, 4.811252243246881, 4.00693842672377, 4.08248290463863, 4.2485291572496005, 7.656162382676229, 3.5, 3.162277660168379, 7.180482019606923, 3.5, 3.8729833462074166, 3.162277660168379, 3.5, 3.0, 7.715033320111766, 4.08248290463863, 3.464101615137755, 9.021712938673348, 4.08248290463863, 3.6380343755449944, 5.039526306789696, 4.08248290463863, 3.6055512754639896, 1.7320508075688774, 4.08248290463863, 3.6147844564602556, 3.6055512754639896, 4.08248290463863, 3.464101615137755, 2.23606797749979, 4.08248290463863, 2.846049894151541, 4.123105625617661, 4.08248290463863, 3.7416573867739413, 7.395739969534468, 4.08248290463863, 3.927922024247863, 3.5, 3.0, 3.5, 2.23606797749979, 6.863036195648746, 3.5, 2.82842712474619, 7.803338153055733, 3.5, 2.0, 9.018797573813227, 3.5, 2.23606797749979, 7.631984736045792, 3.5, 2.23606797749979, 6.807380225308036, 4.08248290463863, 3.5, 3.0641293851417064, 3.5, 2.23606797749979, 7.149478599715339, 3.5, 3.162277660168379, 8.456729809433872, 3.5, 2.4494897427831783, 7.164373264620511, 3.5, 2.23606797749979, 9.6379974715012, 3.5, 2.6457513110645903, 8.231932086840423, 3.5, 2.82842712474619, 9.113104441757029, 4.08248290463863, 3.753259453027346, 8.574929257125442, 4.08248290463863, 7.35542471426856, 7.673442541005147, 3.5, 4.2485291572496005, 4.358898943540673, 3.5, 3.6055512754639896, 9.707637987384864, 3.5, 3.328201177351375, 5.028948456749697, 4.08248290463863, 3.753259453027346, 4.787549991450211, 3.5, 2.4494897427831783, 4.08248290463863, 4.123105625617661, 1.0, 4.08248290463863, 5.547001962252292, 5.092010548749033, 3.5, 3.162277660168379, 9.147024094891798, 4.08248290463863, 3.8805700005813275, 2.6457513110645903, 4.08248290463863, 6.119912853410033, 5.664705542999467, 3.5, 2.82842712474619, 3.3166247903554, 3.5, 2.6457513110645903, 7.892262342196234, 4.08248290463863, 3.05085107923876, 4.929503017546495, 3.5, 2.6457513110645903, 8.139340807355381, 4.08248290463863, 6.750527405556143, 5.851558829982037, 3.5, 2.6457513110645903, 7.2886898685566255, 4.08248290463863, 3.474396144861517, 2.4494897427831783, 3.5, 2.23606797749979, 4.00693842672377, 3.5, 3.464101615137755, 9.479437760739836, 3.5, 3.015113445777636, 8.891778533577861, 3.5, 2.4494897427831783, 9.192879052329477, 4.08248290463863, 3.05085107923876, 9.589222673885601, 3.5, 2.846049894151541, 6.469966392206305, 3.5, 2.82842712474619, 3.5, 3.162277660168379, 6.038635299392551, 3.5, 2.4494897427831783, 7.786074159656593, 4.08248290463863, 5.222329678670935, 1.0, 4.08248290463863, 6.913991516376383, 5.794945672638978, 4.08248290463863, 3.927922024247863, 4.00693842672377, 4.08248290463863, 3.6147844564602556, 7.116806599185674, 4.08248290463863, 5.217758139277826, 3.1754264805429417, 4.08248290463863, 3.6055512754639896, 6.546536707079771, 4.08248290463863, 4.587317109255645, 7.878011364745742, 4.08248290463863, 4.2485291572496005, 4.024922359499621, 4.08248290463863, 3.6055512754639896, 4.4, 3.5, 3.464101615137755, 7.3623570325860195, 4.08248290463863, 4.00693842672377, 2.4494897427831783, 4.08248290463863, 4.157609203101499, 5.61757446387165, 4.08248290463863, 5.692099788303082, 4.669737852696148, 4.08248290463863, 4.264014327112209, 3.6147844564602556, 3.5, 2.846049894151541, 1.7320508075688774, 3.5, 3.0, 8.193298297163443, 4.08248290463863, 3.670651741928988, 2.2677868380553634, 4.08248290463863, 3.75, 2.82842712474619, 4.08248290463863, 3.8729833462074166, 2.4494897427831783, 3.5, 2.82842712474619, 8.168439523882096, 3.5, 2.2677868380553634, 2.4494897427831783, 4.08248290463863, 3.474396144861517, 2.23606797749979, 3.5, 3.162277660168379, 9.190869448440344, 3.5, 3.0, 8.349784069616208, 4.08248290463863, 4.242640687119286, 6.130730673081194, 3.5, 3.162277660168379, 7.181324987175317, 3.5, 2.0, 2.4494897427831783, 3.5, 2.6457513110645903, 8.031745051446078, 3.5, 3.464101615137755, 7.783095565170492, 3.5, 2.82842712474619, 4.47213595499958, 3.5, 2.4494897427831783, 7.298004491997617, 3.5, 3.015113445777636, 4.526019054848144, 3.5, 3.162277660168379, 7.486150735203552, 3.5, 3.771236166328254, 4.1894250263350035, 3.5, 2.2677868380553634, 7.0624201077093804, 3.5, 3.0, 6.427941797666649, 3.5, 2.82842712474619, 6.852689223472485, 3.5, 3.0, 7.4550432529203094, 3.5, 2.4494897427831783, 8.64132597579116, 3.5, 2.0, 6.01929265428846, 4.08248290463863, 4.879954250643349, 3.1754264805429417, 3.5, 2.82842712474619, 3.0, 4.08248290463863, 3.162277660168379, 3.3166247903554, 4.08248290463863, 5.166666666666667, 3.9617738670844207, 3.5, 2.4494897427831783, 4.129483209670111, 4.08248290463863, 6.201736729460423, 4.700096710803842, 4.08248290463863, 3.1754264805429417, 3.3166247903554, 3.5, 2.6457513110645903, 2.82842712474619, 3.5, 4.74692883171144, 4.816989706290483, 3.5, 2.846049894151541, 2.82842712474619, 3.5, 2.23606797749979, 9.203757402647975, 3.5, 2.6457513110645903, 3.3541019662496843, 3.5, 2.8736848324283986, 5.3076923076923075, 3.5, 3.2071349029490928, 2.4494897427831783, 3.5, 2.4494897427831783, 3.5, 3.3166247903554, 9.497607354199554, 3.5, 2.886751345948129, 1.0, 3.5, 2.23606797749979, 2.4494897427831783, 3.5, 3.3166247903554, 2.4494897427831783, 4.08248290463863, 3.474396144861517, 4.2, 3.5, 2.23606797749979, 3.0983866769659336, 3.5, 2.846049894151541, 3.670651741928988, 3.5, 2.6457513110645903, 6.350006350009525, 3.5, 3.015113445777636, 4.129483209670111, 3.5, 2.0, 3.162277660168379, 3.5, 2.0, 3.474396144861517, 3.5, 3.0, 3.5, 2.82842712474619, 7.8144236762095485, 3.5, 2.23606797749979, 8.300573566392895, 3.5, 3.464101615137755, 8.561343946454407, 3.5, 3.162277660168379, 7.193832046426655, 3.5, 2.2677868380553634, 9.090618475235615, 3.5, 2.6457513110645903, 8.400110455827535, 3.5, 2.82842712474619, 9.040538882366587, 3.5, 3.0, 6.805384119360801, 4.08248290463863, 3.474396144861517, 2.82842712474619, 3.5, 2.82842712474619, 7.366131074427723, 3.5, 2.4748737341529163, 6.70978960890315, 4.08248290463863, 3.162277660168379, 5.208553758776472, 4.08248290463863, 3.6147844564602556, 1.7320508075688774, 4.08248290463863, 4.024922359499621, 3.3166247903554, 4.08248290463863, 4.0, 2.041241452319315, 4.08248290463863, 3.8805700005813275, 1.0, 4.08248290463863, 4.00693842672377, 4.177863742936748, 4.08248290463863, 3.3565855667130946, 2.4748737341529163, 4.08248290463863, 3.474396144861517, 6.910522349933915, 4.08248290463863, 3.6055512754639896, 8.930673159226188, 4.08248290463863, 3.3166247903554, 1.0, 3.5, 2.82842712474619, 6.524626787562728, 3.5, 2.6457513110645903, 8.667259502474643, 4.08248290463863, 4.490132550669373, 4.3465914396061125, 3.5, 2.4494897427831783, 8.885277043972694, 3.5, 2.0, 9.127202487220192, 3.5, 2.6457513110645903, 6.874054958970533, 4.08248290463863, 3.9617738670844207, 2.82842712474619, 3.5, 3.0983866769659336, 4.853626716970755, 3.5, 3.0, 9.210622697179527, 3.5, 2.6457513110645903, 8.769192333232017, 3.5, 2.2677868380553634, 4.849343154722923, 3.5, 2.2677868380553634, 8.529247447196258, 3.5, 2.0, 8.25, 4.08248290463863, 3.9617738670844207, 3.8805700005813275, 3.5, 3.1754264805429417, 3.328201177351375, 4.08248290463863, 4.364357804719848, 5.217758139277826, 3.5, 2.4494897427831783, 8.52848961628817, 3.5, 2.82842712474619, 3.5, 2.7136021011998728, 4.008918628686366, 3.5, 2.6666666666666665, 8.44483376931153, 3.5, 3.0, 7.094409430057609, 3.5, 3.6055512754639896, 8.11147458237388, 3.5, 3.3565855667130946, 5.3970046854220985, 3.5, 2.4494897427831783, 5.770685661871567, 3.5, 3.3166247903554, 9.310239285622876, 3.5, 3.015113445777636, 3.0, 3.5, 2.4494897427831783, 8.555992052357224, 4.08248290463863, 3.464101615137755, 5.908391567007971, 3.5, 3.015113445777636, 3.5, 3.015113445777636, 6.736096792653741, 3.5, 3.0, 9.102081983817188, 4.08248290463863, 5.276448530110864, 6.0, 3.5, 2.6457513110645903, 9.192547197409878, 3.5, 2.82842712474619, 9.314574081651863, 3.5, 3.3166247903554, 9.012626521891645, 3.5, 3.3166247903554, 9.5150257112482, 4.08248290463863, 4.264014327112209, 3.8805700005813275, 3.5, 2.7136021011998728, 3.162277660168379, 3.5, 2.9398736610366685, 7.010681840159742, 4.08248290463863, 6.249067727079546, 4.72455591261534, 4.08248290463863, 3.015113445777636, 7.80398972571708, 4.08248290463863, 4.364357804719848, 5.222329678670935, 4.08248290463863, 3.5, 3.015113445777636, 3.5, 2.4494897427831783, 7.771905540037812, 4.08248290463863, 4.00693842672377, 5.778520948116921, 3.5, 2.82842712474619, 7.116806599185674, 3.5, 3.6147844564602556, 7.818181818181818, 3.5, 3.7416573867739413, 6.041987916036252, 4.08248290463863, 3.753259453027346, 2.7136021011998728, 3.5, 2.82842712474619, 4.973458969132756, 3.5, 3.464101615137755, 6.240377207533828, 3.5, 2.2677868380553634, 6.573808125510512, 3.5, 4.00693842672377, 1.889822365046136, 3.5, 2.041241452319315, 5.480077554195743, 3.5, 2.6457513110645903, 4.997560380435394, 3.5, 2.4494897427831783, 2.82842712474619, 4.08248290463863, 3.474396144861517, 2.0, 4.08248290463863, 3.464101615137755, 2.3094010767585034, 4.08248290463863, 4.0, 2.6457513110645903, 3.5, 2.82842712474619, 2.5584085962673253, 4.08248290463863, 4.477215043467819, 2.886751345948129, 3.5, 2.6666666666666665, 7.688179765595242, 4.08248290463863, 3.2, 3.05085107923876, 3.5, 3.6147844564602556, 6.441288386528845, 3.5, 2.6457513110645903, 8.192837154805488, 4.08248290463863, 3.75, 1.7888543819998317, 4.08248290463863, 3.6055512754639896, 4.8666426339228765, 3.5, 2.6666666666666665, 7.6030554129838634, 3.5, 3.162277660168379, 7.8031856239841515, 4.08248290463863, 3.464101615137755, 1.0, 3.5, 2.6457513110645903, 9.362564215835969, 3.5, 2.82842712474619, 7.815614648594782, 4.08248290463863, 5.102520385624567, 4.535573676110727, 4.08248290463863, 5.388159060803248, 9.642365197998375, 3.5, 3.6380343755449944, 5.7564193416014815, 3.5, 3.6055512754639896, 8.894584665044027, 3.5, 3.0, 10.102694343079511, 3.5, 2.4494897427831783, 3.162277660168379, 4.08248290463863, 5.622255427989819, 5.239956379316803, 4.08248290463863, 8.099970605472826, 7.154800938571823, 4.08248290463863, 4.4907311951024935, 3.162277660168379, 3.5, 2.4494897427831783, 5.589565568382148, 3.5, 2.846049894151541, 3.015113445777636, 4.08248290463863, 3.162277660168379, 3.7123106012293743, 4.08248290463863, 6.974858324629157, 2.4494897427831783, 3.5, 3.015113445777636, 9.72634026318503, 3.5, 2.6457513110645903, 8.40168050416806, 4.08248290463863, 3.7416573867739413, 4.630461798847739, 3.5, 2.23606797749979, 4.270992778072193, 3.5, 4.4907311951024935, 6.2598071204459, 3.5, 2.5298221281347035, 3.5, 3.328201177351375, 5.853817179628954, 3.5, 2.6457513110645903, 8.896096927335073, 3.5, 2.4748737341529163, 2.82842712474619, 3.5, 2.82842712474619, 9.522165814091077, 3.5, 2.6457513110645903, 5.363390480545726, 3.5, 2.23606797749979, 7.652711937607478, 3.5, 2.4748737341529163, 7.211102550927978, 3.5, 3.6380343755449944, 3.7416573867739413, 3.5, 3.328201177351375, 7.794294092999267, 4.08248290463863, 3.328201177351375, 5.489948531973768, 3.5, 2.0, 3.162277660168379, 3.5, 2.6666666666666665, 3.5, 3.328201177351375, 4.08248290463863, 3.1754264805429417, 2.6457513110645903, 3.5, 2.6457513110645903, 6.219146064997813, 3.5, 3.05085107923876, 4.08248290463863, 3.1754264805429417, 4.542199791661352, 3.5, 3.015113445777636, 9.519523282676069, 3.5, 2.82842712474619, 8.989797311185749, 3.5, 2.6457513110645903, 6.8072851001542904, 3.5, 2.0, 3.162277660168379, 3.5, 3.3166247903554, 3.162277660168379, 3.5, 2.4494897427831783, 7.916228058025278, 3.5, 2.6666666666666665, 9.895724992860723, 3.5, 3.7416573867739413, 5.980464651413677, 3.5, 2.846049894151541, 8.234403767450646, 3.5, 3.162277660168379, 8.205527240271442, 3.5, 2.4494897427831783, 9.015611460128481, 3.5, 2.773500981126146, 9.040538882366587, 3.5, 3.75, 3.0, 3.5, 2.6457513110645903, 9.021342216356466, 3.5, 2.2677868380553634, 7.31310340973526, 3.5, 3.4412360080584263, 2.7136021011998728, 3.5, 3.6147844564602556, 4.913538149119954, 3.5, 3.3166247903554, 8.47573171879656, 3.5, 3.015113445777636, 10.089116589485474, 4.08248290463863, 4.129483209670111, 6.89634925359229, 4.08248290463863, 4.050813610756598, 6.325463955542989, 3.5, 3.0, 9.77481144449908, 3.5, 3.328201177351375, 4.704421212792114, 4.08248290463863, 3.015113445777636, 8.304547985373997, 4.08248290463863, 4.477215043467819, 5.102520385624567, 3.5, 3.6055512754639896, 6.777777777777778, 3.5, 3.8805700005813275, 4.828078792603349, 3.5, 3.162277660168379, 8.278795899070198, 3.5, 3.328201177351375, 5.316456139417774, 4.08248290463863, 3.6380343755449944, 3.464101615137755, 4.08248290463863, 3.464101615137755, 4.47213595499958, 4.08248290463863, 4.477215043467819, 6.429910574805843, 4.08248290463863, 4.123105625617661, 4.47213595499958, 4.08248290463863, 4.123105625617661, 3.0, 4.08248290463863, 3.8729833462074166, 2.4494897427831783, 4.08248290463863, 3.1754264805429417, 8.010734985006058, 4.08248290463863, 4.1461399144838555, 6.410486691557943, 4.08248290463863, 4.669737852696148, 3.3166247903554, 4.08248290463863, 5.461092327709238, 1.7320508075688774, 4.08248290463863, 3.9000674757995495, 2.846049894151541, 4.08248290463863, 4.587317109255645, 6.915148379350492, 3.5, 3.464101615137755, 2.3333333333333335, 4.08248290463863, 5.246313898711125, 6.030226891555272, 4.08248290463863, 4.170288281141495, 2.6666666666666665, 3.5, 2.82842712474619, 4.901530373260988, 4.08248290463863, 4.510671108178233, 3.75, 4.08248290463863, 4.37880269519857, 3.3166247903554, 3.5, 4.37880269519857, 8.525668472369972, 3.5, 2.6457513110645903, 8.973490369514234, 3.5, 3.3166247903554, 7.462778926574919, 3.5, 4.438772657244647, 4.490132550669373, 3.5, 5.542869676919993, 6.325771464049632, 3.5, 4.1461399144838555, 8.25674954467424, 3.5, 3.015113445777636, 7.431694860230418, 3.5, 2.6457513110645903, 8.418799823976721, 3.5, 2.23606797749979, 8.964063273970506, 3.5, 2.846049894151541, 5.750236290040988, 4.08248290463863, 3.75, 3.927922024247863, 3.5, 3.0, 9.452653437254055, 4.08248290463863, 4.0, 1.0, 4.08248290463863, 4.0, 2.82842712474619, 4.08248290463863, 4.510671108178233, 3.3166247903554, 4.08248290463863, 4.358898943540673, 1.414213562373095, 4.08248290463863, 4.1461399144838555, 3.3166247903554, 3.5, 3.015113445777636, 6.384191000968102, 4.08248290463863, 6.509445549041193, 9.233132592090664, 4.08248290463863, 4.129483209670111, 4.123105625617661, 3.5, 2.4494897427831783, 4.8, 4.08248290463863, 6.657917756510591, 5.642447102306373, 4.08248290463863, 4.129483209670111, 4.929503017546495, 3.5, 3.299831645537222, 6.085385352926881, 4.08248290463863, 4.264014327112209, 1.0, 4.08248290463863, 5.851558829982037, 4.4, 3.5, 2.5298221281347035, 5.059644256269407, 4.08248290463863, 4.801960383990247, 3.0424349222966556, 4.08248290463863, 4.123105625617661, 3.1754264805429417, 4.08248290463863, 3.65655170486763, 9.143353729961051, 4.08248290463863, 4.129483209670111, 7.188918942063234, 4.08248290463863, 3.6380343755449944, 2.4494897427831783, 4.08248290463863, 4.973458969132756, 8.071965467345862, 3.5, 2.6666666666666665, 3.5, 2.2677868380553634, 4.456688116249245, 3.5, 2.23606797749979, 8.525296022362365, 4.08248290463863, 4.123105625617661, 4.521814736434731, 4.08248290463863, 3.7416573867739413, 1.0, 3.5, 3.3166247903554, 4.0, 3.5, 3.162277660168379, 4.08248290463863, 2.581988897471611, 3.162277660168379, 3.5, 2.6457513110645903, 4.00693842672377, 3.5, 2.0, 7.22897396012249, 4.08248290463863, 2.6457513110645903, 2.6457513110645903, 4.08248290463863, 3.464101615137755, 1.7320508075688774, 3.5, 2.23606797749979, 8.538149682454623, 3.5, 2.6666666666666665, 8.549090976340064, 3.5, 3.0, 8.17302997500511, 4.08248290463863, 3.7416573867739413, 2.4494897427831783, 3.5, 2.6666666666666665, 3.464101615137755, 3.5, 3.0, 5.813776741499453, 3.5, 2.4494897427831783, 9.08956297292448, 4.08248290463863, 4.024922359499621, 4.618802153517007, 3.5, 3.162277660168379, 8.728206690941787, 4.08248290463863, 4.37880269519857, 3.395498750508662, 3.5, 2.82842712474619, 7.499189145356839, 4.08248290463863, 4.72455591261534, 3.474396144861517, 3.5, 2.82842712474619, 7.431694860230418, 4.08248290463863, 4.72455591261534, 3.5, 4.08248290463863, 3.7416573867739413, 3.959797974644666, 4.08248290463863, 4.37880269519857, 7.948849635407365, 4.08248290463863, 4.00693842672377, 6.375988221950509, 3.5, 2.041241452319315, 9.563904952770041, 4.08248290463863, 3.328201177351375, 9.706476511819817, 4.08248290463863, 3.162277660168379, 5.303300858899106, 4.08248290463863, 4.00693842672377, 1.0, 3.5, 2.6457513110645903, 8.491468778533354, 3.5, 2.82842712474619, 3.5, 3.162277660168379, 5.938574464184706, 3.5, 2.4748737341529163, 10.00425441414695, 3.5, 3.3166247903554, 8.80167282080199, 3.5, 3.0, 3.5, 2.6666666666666665, 6.725739879230903, 3.5, 2.6457513110645903, 8.065591326174431, 3.5, 3.162277660168379, 8.571593913749194, 3.5, 3.015113445777636, 3.5, 3.1754264805429417, 9.278079468332315, 3.5, 3.0, 6.28970902033151, 3.5, 3.0, 8.919733394873214, 4.08248290463863, 6.3727937358744, 5.092010548749033, 3.5, 2.82842712474619, 4.08248290463863, 4.58257569495584, 3.328201177351375, 3.5, 2.82842712474619, 8.579272787866548, 3.5, 2.82842712474619, 3.8376128944009875, 4.08248290463863, 6.196773353931867, 5.11207720338155, 4.08248290463863, 4.8, 6.302708777266682, 3.5, 2.6457513110645903, 7.224456455979048, 3.5, 2.4748737341529163, 6.001068280966357, 4.08248290463863, 3.6147844564602556, 5.703418756083616, 4.08248290463863, 4.310527248642598, 5.309368506648784, 4.08248290463863, 4.57495710997814, 4.085297439895141, 3.5, 3.0, 3.162277660168379, 4.08248290463863, 3.6147844564602556, 4.171041879608233, 4.08248290463863, 3.75, 2.6457513110645903, 3.5, 3.8340579025361627, 4.467914966843415, 4.08248290463863, 3.6055512754639896, 3.162277660168379, 4.08248290463863, 3.6055512754639896, 4.123105625617661, 3.5, 3.162277660168379, 7.964952993613722, 3.5, 3.015113445777636, 6.184165460191406, 3.5, 2.4494897427831783, 4.902903378454601, 3.5, 2.82842712474619, 3.474396144861517, 3.5, 3.015113445777636, 7.262590648655362, 3.5, 2.6666666666666665, 6.750527405556143, 3.5, 3.015113445777636, 5.789863774090244, 4.08248290463863, 6.249067727079546, 4.603171644550004, 3.5, 3.464101615137755, 8.674768752146987, 3.5, 3.015113445777636, 2.6457513110645903, 3.5, 3.3166247903554, 4.47213595499958, 3.5, 2.1213203435596424, 8.796199993653977, 3.5, 4.00693842672377, 6.154574548966636, 3.5, 2.82842712474619, 7.3990279114953825, 3.5, 3.2071349029490928, 8.0195413508667, 3.5, 3.328201177351375, 9.853448749615868, 3.5, 3.464101615137755, 4.221158824088691, 3.5, 2.4748737341529163, 8.60201107849755, 3.5, 2.4748737341529163, 7.746292694253846, 3.5, 2.4494897427831783, 9.988687869300877, 3.5, 2.846049894151541, 6.7662680570377285, 3.5, 3.328201177351375, 6.8657566124489255, 3.5, 2.846049894151541, 7.866747376502838, 3.5, 2.23606797749979, 2.82842712474619, 3.5, 3.9617738670844207, 3.5, 2.4748737341529163, 9.454063165900024, 3.5, 2.0, 1.0, 4.08248290463863, 4.264014327112209, 8.873855889750516, 4.08248290463863, 4.949747468305833, 4.913538149119954, 4.08248290463863, 5.400617248673217, 7.89152397370844, 4.08248290463863, 4.811252243246881, 7.124070532524293, 4.08248290463863, 3.6055512754639896, 3.481553119113957, 4.08248290463863, 4.58257569495584, 4.826004826007239, 4.08248290463863, 3.6147844564602556, 6.697875667817257, 4.08248290463863, 4.242640687119286, 1.7320508075688774, 4.08248290463863, 3.464101615137755, 6.723674011118638, 4.08248290463863, 4.00693842672377, 3.0, 3.5, 2.6457513110645903, 7.091168559049116, 3.5, 6.508559328900301, 4.08248290463863, 3.1529631254723287, 3.05085107923876, 3.5, 2.82842712474619, 1.0, 4.08248290463863, 6.352811106527871, 5.563035899673184, 4.08248290463863, 4.123105625617661, 6.83536555146996, 3.5, 2.82842712474619, 3.464101615137755, 3.5, 3.3166247903554, 5.688734668417887, 4.08248290463863, 3.5, 5.7092239485974, 3.5, 2.4494897427831783, 8.142613724927722, 3.5, 2.82842712474619, 5.662208585049306, 4.08248290463863, 7.111111111111111, 4.4, 3.5, 2.5298221281347035, 9.742697372455677, 3.5, 4.123105625617661, 3.0, 3.5, 2.0, 3.162277660168379, 4.08248290463863, 8.990802654607693, 4.529039094769576, 3.5, 3.3166247903554, 8.282511696339462, 3.5, 3.3166247903554, 6.928853368993243, 3.5, 3.464101615137755, 5.881176352917641, 3.5, 3.162277660168379, 4.371914496377572, 3.5, 2.6457513110645903, 5.0137741307804005, 3.5, 2.6666666666666665, 3.6055512754639896, 3.5, 3.0, 3.771236166328254, 3.5, 3.3166247903554, 5.590169943749474, 3.5, 2.4494897427831783, 8.727680894321438, 3.5, 3.162277660168379, 4.08248290463863, 3.6380343755449944, 1.7888543819998317, 4.08248290463863, 3.8729833462074166, 1.0, 3.5, 2.6457513110645903, 7.024912639340182, 3.5, 2.82842712474619, 9.836190794746173, 3.5, 2.6457513110645903, 6.793662204867574, 4.08248290463863, 3.395498750508662, 2.9824045403173027, 3.5, 3.6055512754639896, 3.1754264805429417, 3.5, 3.670651741928988, 2.4748737341529163, 3.5, 2.6457513110645903, 9.205499881602055, 4.08248290463863, 4.0, 5.048252022715237, 3.5, 3.3166247903554, 7.90106015622946, 3.5, 3.162277660168379, 3.8805700005813275, 4.08248290463863, 4.69041575982343, 8.097424401519659, 4.08248290463863, 3.8729833462074166, 6.776718456854931, 3.5, 3.0, 4.8, 4.08248290463863, 4.264014327112209, 3.2071349029490928, 3.5, 3.75, 4.216370213557839, 4.08248290463863, 3.474396144861517, 1.7320508075688774, 3.5, 2.6457513110645903, 6.25, 4.08248290463863, 3.328201177351375, 5.430582663966679, 4.08248290463863, 3.7416573867739413, 4.050813610756598, 4.08248290463863, 3.3166247903554, 4.666666666666667, 4.08248290463863, 3.3166247903554, 2.0, 4.08248290463863, 3.6147844564602556, 9.126883367283707, 4.08248290463863, 6.742363047141518, 5.003702332976757, 4.08248290463863, 4.2485291572496005, 2.4494897427831783, 4.08248290463863, 4.286607049870562, 3.474396144861517, 3.5, 3.162277660168379, 5.126524163602469, 4.08248290463863, 3.3166247903554, 9.170605214488301, 3.5, 2.6457513110645903, 8.339503888294594, 3.5, 3.474396144861517, 9.6, 3.5, 3.328201177351375, 3.5, 2.2677868380553634, 7.6900482511661234, 3.5, 3.162277660168379, 9.137375925066596, 4.08248290463863, 3.6055512754639896, 2.82842712474619, 4.08248290463863, 3.6055512754639896, 6.993258208972302, 4.08248290463863, 6.701872450265123, 6.161232369723243, 4.08248290463863, 3.5777087639996634, 4.949747468305833, 4.08248290463863, 4.801960383990247, 2.886751345948129, 4.08248290463863, 3.6147844564602556, 1.7320508075688774, 3.5, 2.6457513110645903, 3.3166247903554, 3.5, 2.4494897427831783, 8.108848540793831, 4.08248290463863, 3.771236166328254, 1.7888543819998317, 3.5, 2.6457513110645903, 8.68385835750183, 3.5, 2.82842712474619, 2.82842712474619, 3.5, 3.6147844564602556, 3.5, 3.0, 8.603884002203404, 3.5, 2.6666666666666665, 6.325219629494658, 3.5, 3.0, 6.753346748664125, 3.5, 3.0, 9.485845951112621, 3.5, 3.464101615137755, 6.6529914385911555, 3.5, 3.162277660168379, 5.795455252808151, 3.5, 3.0983866769659336, 5.932958789676531, 4.08248290463863, 4.27437366993929, 7.510409442899901, 4.08248290463863, 3.8729833462074166, 9.217458351494649, 3.5, 3.015113445777636, 8.252088808112143, 4.08248290463863, 5.487954724560282, 4.7702783519995515, 3.5, 3.7416573867739413, 3.162277660168379, 4.08248290463863, 3.801315561749642, 2.23606797749979, 3.5, 2.2677868380553634, 3.3166247903554, 3.5, 2.6666666666666665, 6.425396041156863, 3.5, 3.464101615137755, 4.318004318006477, 3.5, 2.5298221281347035, 8.982473206345997, 3.5, 2.82842712474619, 6.58504607868518, 4.08248290463863, 4.024922359499621, 4.905778905196061, 4.08248290463863, 4.1461399144838555, 5.37545555167743, 3.5, 2.4494897427831783, 5.898906801202691, 3.5, 2.23606797749979, 7.193830424207176, 3.5, 3.7416573867739413, 5.251073270466074, 3.5, 4.706787243316417, 6.882472016116853, 3.5, 2.041241452319315, 7.876429114339844, 4.08248290463863, 3.753259453027346, 7.333395429444001, 3.5, 2.846049894151541, 9.421683286017897, 4.08248290463863, 4.2485291572496005, 1.414213562373095, 4.08248290463863, 5.396407334626633, 3.3166247903554, 4.08248290463863, 4.913538149119954, 3.8729833462074166, 4.08248290463863, 5.303300858899106, 2.6457513110645903, 4.08248290463863, 5.897678246195885, 6.880975663883468, 4.08248290463863, 5.208553758776472, 4.937707198786941, 3.5, 3.328201177351375, 8.723567442899585, 3.5, 2.0, 6.735753140545634, 3.5, 3.0, 8.538149682454623, 4.08248290463863, 3.8729833462074166, 4.27617987059879, 4.08248290463863, 4.2485291572496005, 2.82842712474619, 3.5, 3.328201177351375, 3.3166247903554, 3.5, 3.3166247903554, 8.80172458846704, 4.08248290463863, 3.670651741928988, 4.016632088371218, 3.5, 2.0, 8.769284798196836, 3.5, 4.0, 8.925731045622827, 3.5, 3.3166247903554, 3.464101615137755, 3.5, 2.4748737341529163, 2.82842712474619, 3.5, 3.015113445777636, 8.467669207104626, 3.5, 2.6666666666666665, 7.478901140150934, 3.5, 2.6666666666666665, 10.100630043355082, 3.5, 2.6666666666666665, 8.843466319929314, 4.08248290463863, 6.80336051416609, 6.666749174406927, 3.5, 3.474396144861517, 5.045750287082781, 3.5, 2.2135943621178655, 7.067617668790178, 3.5, 2.82842712474619, 3.5, 3.0983866769659336, 4.08248290463863, 4.1461399144838555, 3.015113445777636, 3.5, 2.4494897427831783, 4.08248290463863, 3.771711342562273, 4.631047607372178, 3.5, 4.2485291572496005, 8.017188013494799, 3.5, 3.0, 3.5, 2.82842712474619, 8.36182803382292, 3.5, 2.6457513110645903, 9.328574641000293, 3.5, 2.6666666666666665, 9.165692857515609, 3.5, 2.82842712474619, 9.138724148904345, 4.08248290463863, 3.8805700005813275, 3.3166247903554, 3.5, 2.82842712474619, 7.213600767718993, 4.08248290463863, 3.6055512754639896, 2.0, 3.5, 2.82842712474619, 7.092081432669753, 4.08248290463863, 3.6055512754639896, 6.437427761961776, 3.5, 3.162277660168379, 6.567206798038654, 4.08248290463863, 4.4907311951024935, 1.0, 3.5, 2.6457513110645903, 6.481812160876686, 3.5, 3.6055512754639896, 7.464062581183745, 3.5, 2.6457513110645903, 8.140059175477884, 3.5, 2.4748737341529163, 1.0, 3.5, 3.0, 5.461092327709238, 3.5, 3.2071349029490928, 9.759485488654478, 3.5, 4.170288281141495, 3.1754264805429417, 4.08248290463863, 4.8, 2.23606797749979, 3.5, 3.3166247903554, 5.217758139277826, 3.5, 3.3166247903554, 4.929503017546495, 3.5, 4.642383454426297, 3.3166247903554, 3.5, 2.4494897427831783, 4.129483209670111, 3.5, 4.08248290463863, 4.717295153286297, 4.08248290463863, 4.00693842672377, 6.052586456788652, 3.5, 3.927922024247863, 3.0, 3.5, 3.3166247903554, 3.709704134011871, 3.5, 3.771236166328254, 9.083602496781033, 3.5, 3.7416573867739413, 3.3166247903554, 3.5, 3.75, 6.277666039945969, 4.08248290463863, 5.515634344499481, 6.092848842499201, 4.08248290463863, 4.0, 2.0, 4.08248290463863, 4.314554973040049, 3.6055512754639896, 3.5, 2.82842712474619, 5.459195627691725, 3.5, 3.162277660168379, 4.47213595499958, 3.5, 2.23606797749979, 3.5, 3.9000674757995495, 4.401953314433961, 3.5, 2.4494897427831783, 10.050895074732335, 3.5, 3.0, 9.882439052481743, 4.08248290463863, 4.085297439895141, 3.4285714285714284, 3.5, 3.0, 4.394802124588286, 3.5, 2.6457513110645903, 8.872150458791875, 4.08248290463863, 4.642383454426297, 4.08248290463863, 5.102520385624567, 4.129483209670111, 4.08248290463863, 6.00832755431992, 4.849343154722923, 3.5, 3.75, 5.578018081208209, 4.08248290463863, 4.74692883171144, 1.0, 4.08248290463863, 5.939696961966999, 1.0, 4.08248290463863, 4.849343154722923, 2.82842712474619, 4.08248290463863, 4.37880269519857, 1.0, 4.08248290463863, 4.024922359499621, 2.041241452319315, 3.5, 3.328201177351375, 8.019989072255717, 4.08248290463863, 5.184951391308559, 4.310527248642598, 3.5, 3.3166247903554, 7.426232973063312, 4.08248290463863, 4.364357804719848, 4.47213595499958, 4.08248290463863, 3.3166247903554, 3.8729833462074166, 4.08248290463863, 4.3465914396061125, 1.0, 3.5, 3.0, 7.219948723811553, 4.08248290463863, 3.7416573867739413, 2.0, 3.5, 2.9398736610366685, 6.171264622608037, 3.5, 2.23606797749979, 6.609001368025944, 4.08248290463863, 4.2485291572496005, 3.395498750508662, 4.08248290463863, 5.239956379316803, 7.890453678924885, 3.5, 2.4748737341529163, 10.08852124295554, 4.08248290463863, 4.6, 6.215188768538847, 3.5, 3.0, 8.607101241464626, 3.5, 4.902903378454601, 3.6055512754639896, 3.5, 3.9000674757995495, 6.350852961085883, 3.5, 3.162277660168379, 8.17423891369599, 3.5, 4.929503017546495, 7.689786245403703, 3.5, 2.4494897427831783, 7.024912639340182, 4.08248290463863, 6.482493908421918, 7.677159338596802, 4.08248290463863, 3.771236166328254, 1.0, 4.08248290463863, 4.2485291572496005, 5.11207720338155, 3.5, 3.162277660168379, 4.217756949399826, 4.08248290463863, 3.5355339059327378, 3.6147844564602556, 4.08248290463863, 4.913538149119954, 4.0, 4.08248290463863, 3.6147844564602556, 3.927922024247863, 4.08248290463863, 4.0, 7.772297096131725, 4.08248290463863, 3.9617738670844207, 7.151355947783043, 3.5, 3.162277660168379, 9.25710351661044, 4.08248290463863, 4.233901974057256, 3.3166247903554, 3.5, 3.0, 7.786074159656593, 3.5, 2.6666666666666665, 9.526198940693433, 4.08248290463863, 3.801315561749642, 3.1754264805429417, 4.08248290463863, 3.5355339059327378, 2.23606797749979, 4.08248290463863, 3.328201177351375, 8.181818181818182, 4.08248290463863, 5.425166581076791, 5.102520385624567, 4.08248290463863, 3.7416573867739413, 3.0, 3.5, 3.474396144861517, 8.163709564049535, 4.08248290463863, 3.75, 2.23606797749979, 4.08248290463863, 4.929503017546495, 3.015113445777636, 4.08248290463863, 3.464101615137755, 8.437758422458371, 3.5, 2.4494897427831783, 6.831300510639733, 4.08248290463863, 4.949747468305833, 3.3166247903554, 4.08248290463863, 4.264014327112209, 4.170288281141495, 3.5, 2.846049894151541, 8.834544715369823, 4.08248290463863, 3.8729833462074166, 7.994350414775228, 3.5, 3.0, 9.658562153769513, 3.5, 3.6055512754639896, 10.105282357139245, 4.08248290463863, 3.6380343755449944, 8.082102965782765, 4.08248290463863, 8.520563361656318, 6.434015210126405, 3.5, 3.0, 8.215838362577491, 3.5, 2.82842712474619, 7.030047555134388, 3.5, 4.0, 5.0, 3.5, 3.3166247903554, 9.709120968076979, 3.5, 3.5, 8.538149682454625, 3.5, 3.3166247903554, 6.215815605080611, 4.08248290463863, 3.328201177351375, 8.475677204353536, 3.5, 3.0, 8.916455838132986, 3.5, 2.82842712474619, 4.08248290463863, 3.464101615137755, 6.453979094624195, 3.5, 3.05085107923876, 4.08248290463863, 3.474396144861517, 3.1754264805429417, 3.5, 2.4494897427831783, 5.628854235099715, 4.08248290463863, 5.028948456749697, 1.0, 4.08248290463863, 3.4412360080584263, 4.394802124588286, 3.5, 3.6055512754639896, 6.1, 3.5, 2.82842712474619, 7.946549629034693, 3.5, 2.4494897427831783, 2.0, 3.5, 3.05085107923876, 4.437601569801833, 3.5, 2.846049894151541, 7.677166722749278, 4.08248290463863, 4.811252243246881, 3.8729833462074166, 3.5, 3.3166247903554, 4.576276618858141, 3.5, 3.162277660168379, 8.241477490081023, 3.5, 3.015113445777636, 4.510671108178233, 3.5, 3.0, 3.5, 4.129483209670111, 2.6457513110645903, 3.5, 2.6666666666666665, 8.51591647054698, 3.5, 3.0, 5.0, 3.5, 2.4494897427831783, 8.598492073268774, 3.5, 3.6147844564602556, 2.4494897427831783, 3.5, 3.6380343755449944, 2.5298221281347035, 3.5, 3.3166247903554, 8.804710999221752, 4.08248290463863, 3.0, 6.260064360459999, 3.5, 2.82842712474619, 4.47213595499958, 4.08248290463863, 4.849343154722923, 3.9000674757995495, 4.08248290463863, 3.8805700005813275, 4.609327677584255, 4.08248290463863, 3.1754264805429417, 6.735198826004059, 4.08248290463863, 3.05085107923876, 8.11920985240873, 4.08248290463863, 3.464101615137755, 8.246599920961987, 3.5, 2.846049894151541, 8.095236050624372, 3.5, 4.535573676110727, 7.735900454643651], "type": "scatter"}, {"customdata": [98, 101, 277, 301, 403, 466, 476, 923, 926, 983, 1040, 1211, 1247, 1364, 1397, 1583, 1619, 1622, 1630, 1877, 1895, 1979, 1991, 2003, 2018, 2140, 2182, 2255, 2351, 2393, 2552, 2555, 2561, 2570, 2678, 2699, 2903, 2909, 2951], "hovertemplate": "Index: %{customdata}<br>Value: %{y:.2f}<extra></extra>", "marker": {"color": "#d66a6a", "size": 10, "symbol": "x"}, "mode": "markers", "name": "Outliers (39)", "x": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38], "y": [10.200060642633556, 10.328607586704738, 10.794106135883936, 10.874197679943185, 11.406505123530474, 10.392505312323236, 10.66783162498112, 10.165188538363557, 11.320822273543232, 10.529010319542758, 11.605154529366915, 10.129165809680478, 11.115846517275257, 10.222742503150233, 10.15504800579495, 11.468292773139114, 10.123895567197717, 10.661152242179043, 10.305338422390601, 10.86835498787732, 10.492212063950621, 10.647058823529411, 10.334637571188003, 10.766252603480527, 11.59012860502769, 10.17740861013741, 10.119288512538814, 10.350924156870612, 10.948476036270003, 10.601426503185268, 10.50114904542028, 10.446835101424911, 11.314309764284204, 10.401230987753616, 10.336932519516777, 10.26213041988441, 10.229792209456097, 10.622326858705444, 10.625064049817196], "type": "scatter"}];
            const layout = Object.assign({}, {"template": {"data": {"histogram2dcontour": [{"type": "histogram2dcontour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "choropleth": [{"type": "choropleth", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "histogram2d": [{"type": "histogram2d", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "heatmap": [{"type": "heatmap", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "contourcarpet": [{"type": "contourcarpet", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "contour": [{"type": "contour", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "surface": [{"type": "surface", "colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}], "mesh3d": [{"type": "mesh3d", "colorbar": {"outlinewidth": 0, "ticks": ""}}], "scatter": [{"fillpattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}, "type": "scatter"}], "parcoords": [{"type": "parcoords", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolargl": [{"type": "scatterpolargl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "bar"}], "scattergeo": [{"type": "scattergeo", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterpolar": [{"type": "scatterpolar", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "histogram": [{"marker": {"pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "histogram"}], "scattergl": [{"type": "scattergl", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatter3d": [{"type": "scatter3d", "line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermap": [{"type": "scattermap", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattermapbox": [{"type": "scattermapbox", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scatterternary": [{"type": "scatterternary", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "scattercarpet": [{"type": "scattercarpet", "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}, "pattern": {"fillmode": "overlay", "size": 10, "solidity": 0.2}}, "type": "barpolar"}], "pie": [{"automargin": true, "type": "pie"}]}, "layout": {"autotypenumbers": "strict", "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "hovermode": "closest", "hoverlabel": {"align": "left"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"bgcolor": "#E5ECF6", "angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "ternary": {"bgcolor": "#E5ECF6", "aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]]}, "xaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "yaxis": {"gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "automargin": true, "zerolinewidth": 2}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white", "gridwidth": 2}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "geo": {"bgcolor": "white", "landcolor": "#E5ECF6", "subunitcolor": "white", "showland": true, "showlakes": true, "lakecolor": "white"}, "title": {"x": 0.05}, "mapbox": {"style": "light"}}}, "shapes": [{"line": {"color": "#e0b854", "dash": "dash"}, "type": "line", "x0": 0, "x1": 1, "xref": "x domain", "y0": 10.113905963858514, "y1": 10.113905963858514, "yref": "y"}, {"line": {"color": "#e0b854", "dash": "dash"}, "type": "line", "x0": 0, "x1": 1, "xref": "x domain", "y0": -1.3948739607074865, "y1": -1.3948739607074865, "yref": "y"}, {"line": {"color": "#7cb97c", "dash": "dot"}, "type": "line", "x0": 0, "x1": 1, "xref": "x domain", "y0": 4.359516001575513, "y1": 4.359516001575513, "yref": "y"}], "annotations": [{"font": {"color": "#a39e93"}, "showarrow": false, "text": "Upper bound (10.1)", "x": 1, "xanchor": "right", "xref": "x domain", "y": 10.113905963858514, "yanchor": "bottom", "yref": "y"}, {"font": {"color": "#a39e93"}, "showarrow": false, "text": "Lower bound (-1.4)", "x": 1, "xanchor": "right", "xref": "x domain", "y": -1.3948739607074865, "yanchor": "bottom", "yref": "y"}, {"font": {"color": "#a39e93"}, "showarrow": false, "text": "Mean (4.4)", "x": 1, "xanchor": "right", "xref": "x domain", "y": 4.359516001575513, "yanchor": "bottom", "yref": "y"}], "margin": {"l": 50, "r": 30, "t": 30, "b": 50}, "font": {"color": "#a39e93"}, "xaxis": {"title": {"text": "Sample Index"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "yaxis": {"title": {"text": "Diversity Vocabulary Richness"}, "gridcolor": "rgba(255,255,255,0.05)", "color": "#a39e93"}, "legend": {"font": {"color": "#a39e93"}, "orientation": "h", "yanchor": "bottom", "y": 1.02, "xanchor": "right", "x": 1}, "title": {}, "height": 400, "paper_bgcolor": "rgba(0,0,0,0)", "plot_bgcolor": "rgba(0,0,0,0)"}, darkLayout);
            Plotly.newPlot('anomaly_chart_4', data, layout, plotlyConfig);
        })();
        
    </script>
    

    <!-- Conversation Modal Scripts -->
    
    <script>
        // Conversation data for each recommendation
        const conversationData = {
            
            'rec_0': {
                title: "Outliers detected in length word count",
                conversations: [{"conversation_id": "7", "flagged_count": 1, "turns": [{"index": 21, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 22, "is_flagged": false, "role": "user", "text": "Write a short story in third person narration about a protagonist who has to make an important career decision."}, {"index": 23, "is_flagged": true, "metric_value": "267", "role": "assistant", "text": "Sophie sat at her desk, staring blankly at the computer screen. Her mind was racing as she weighed the options in front of her. She had been offered a promotion to a senior management position within her company, but it meant moving across the country and leaving her friends and family behind. At the same time, her dream job at a rival company had finally opened up, and she had been invited for an interview. \n\nThe thought of leaving her comfortable life behind, to take a chance on something new and uncertain, made Sophie feel anxious, but also excited. She had always been one to take risks, and it had paid off in the past. But now, with such a big decision to make, Sophie couldn\u0027t help but feel overwhelmed.\n\nAfter much deliberation, Sophie made her choice. She packed her bags, said goodbye to her loved ones, and moved across the country for the new job. It was a difficult first few months, adjusting to a new city and a new company culture. But as time went by, Sophie realized that she had made the best decision for her career. The job was challenging, but also rewarding, and she was surrounded by passionate, driven colleagues who inspired her to grow and develop.\n\nIn the end, Sophie\u0027s bold move paid off. She was thriving in her new role, and had made new friends and connections in her new city. Looking back, Sophie was grateful for the opportunity to take a risk and make a change, and was confident that she had made the right decision for her career."}]}, {"conversation_id": "12", "flagged_count": 1, "turns": [{"index": 36, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 37, "is_flagged": false, "role": "user", "text": "Discuss the causes of the Great Depression"}, {"index": 38, "is_flagged": true, "metric_value": "287", "role": "assistant", "text": "The Great Depression was a period of economic decline that lasted from 1929-1939, making it the longest-lasting depression in modern history. It affected not only the United States but countries around the world, causing substantial social and economic upheaval.\n\nThere were several major causes of the Great Depression, which include:\n\n1. Stock Market Crash of 1929 - In October of 1929, the stock market experienced a significant fall that wiped out millions of investors. This event is considered by many to be the initial trigger of the Great Depression.\n\n2. Overproduction - During the 1920s, many industries produced more goods than consumers wanted or could afford. This ultimately led to a decline in demand for goods, causing job loss, lower wages, and business failure.\n\n3. High Tariffs and War Debts - Protectionist trade policies in the form of high tariffs led to a decline in global trade, as other countries retaliated with tariffs of their own. Additionally, many countries were struggling to repay war debts, which led to economic instability.\n\n4. Bank Failures - As demand for goods declined, many banks began to fail, causing a loss of confidence in the banking system. This led to a massive withdrawal of money from banks, causing even more banks to fail.\n\n5. Drought Conditions - The Dust Bowl was a severe drought and dust storm that hit the Great Plains region of the United States in the 1930s. This had a significant impact on agriculture, causing many farmers to lose their land and livelihoods which worsened the effects of the depression.\n\nOverall, the Great Depression was caused by a combination of factors, including economic, environmental, and political factors. Its impact was widespread, affecting millions of people around the world."}]}, {"conversation_id": "20", "flagged_count": 1, "turns": [{"index": 60, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 61, "is_flagged": false, "role": "user", "text": "Compare and contrast the Cuban Missile Crisis and the Vietnam War."}, {"index": 62, "is_flagged": true, "metric_value": "365", "role": "assistant", "text": "The Cuban Missile Crisis (1962) and the Vietnam War (1955-1975) were two pivotal events in US history that occurred during the Cold War. Although both events were characterized by heightened tensions between the United States and Communist powers, they had some key differences that set them apart.\n\nThe Cuban Missile Crisis was a 13-day political and military standoff between the United States and the Soviet Union over the installation of Soviet nuclear missiles in Cuba, just 90 miles from US shores. In contrast, the Vietnam War was a prolonged conflict that lasted for almost two decades, involving the US, South Vietnam, North Vietnam, and other Communist allies. The war was a result of the US attempting to contain the spread of communism in Southeast Asia.\n\nThe Cuban Missile Crisis is often considered the closest the world ever came to nuclear war, with the US and the Soviet Union coming to the brink of a full-scale confrontation. Ultimately, a peaceful resolution was reached, with the Soviet Union agreeing to remove the missiles from Cuba in exchange for a US promise not to invade Cuba and a secret agreement to remove US missiles from Turkey. In contrast, the Vietnam War resulted in significant devastation, with millions of lives lost, many more injured, and widespread destruction of infrastructure.\n\nAnother major difference between the two events was domestic support. During the Cuban Missile Crisis, there was widespread public support for the US government\u0027s actions, and President Kennedy\u0027s handling of the situation is often cited as one of his greatest accomplishments. However, the Vietnam War was deeply unpopular among the American public, and anti-war sentiment grew steadily over the years. The war resulted in a significant loss of faith in the US government and military, and it is still remembered as one of the most divisive conflicts in American history.\n\nIn conclusion, while the Cuban Missile Crisis and the Vietnam War were both major events in the Cold War, they differed in their duration, outcome, and domestic support. The Cuban Missile Crisis was a short, intense standoff that was resolved peacefully, whereas the Vietnam War was a prolonged conflict that resulted in widespread devastation and was deeply unpopular at home."}]}, {"conversation_id": "26", "flagged_count": 1, "turns": [{"index": 78, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 79, "is_flagged": false, "role": "user", "text": "Describe the life and reign of King Charles II."}, {"index": 80, "is_flagged": true, "metric_value": "304", "role": "assistant", "text": "King Charles II was the monarch of England, Scotland, and Ireland from 1660 to 1685. Born on May 29, 1630, he was the son of King Charles I and Queen Henrietta Maria. During his father\u0027s reign, England was embroiled in civil war between the royalists who supported the monarchy and the parliamentarians who sought to limit its powers. Eventually, the parliamentarians, led by Oliver Cromwell, emerged victorious, and Charles I was executed in 1649.\n\nCharles II went into exile and lived for a time in France and the Netherlands. In 1660, following the death of Cromwell and the collapse of the Commonwealth government, Charles was invited back to England and restored to the throne.\n\nHis reign, often referred to as the Restoration, was marked by a period of relative stability and prosperity. Charles re-established the Church of England as the national church, and he worked to rebuild the country after the devastation of the civil wars. He also pursued a policy of religious tolerance, issuing the Declaration of Indulgence in 1672, which granted freedom of worship to non-Anglican Protestants.\n\nCharles was also known for his interest in science and the arts, and he was a patron of the Royal Society, a scientific organization founded in 1660. His court was known for its extravagance and hedonism, and Charles himself had a reputation as a womanizer.\n\nIn the later years of his reign, Charles faced increasing opposition from parliament, particularly over his pro-Catholic policies and his attempts to bypass their authority. In 1678, the Test Act was passed, requiring all officeholders to swear an oath against Catholicism, which Charles reluctantly signed.\n\nCharles died on February 6, 1685, and was succeeded by his brother James II. He is remembered as a popular and charismatic monarch who helped to restore stability to England after a period of turmoil."}]}, {"conversation_id": "32", "flagged_count": 1, "turns": [{"index": 96, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 97, "is_flagged": false, "role": "user", "text": "Outline the consequences of deforestation."}, {"index": 98, "is_flagged": true, "metric_value": "291", "role": "assistant", "text": "Deforestation, or the large-scale removal of trees from a forested area, has numerous consequences that can have significant impacts on the environment, economy, and local communities. Some of the consequences of deforestation include:\n\n1. Loss of biodiversity: Forests are home to a wide variety of plant and animal species, many of which rely on the forest for their survival. Removing large areas of trees can destroy their habitat, leading to a loss of biodiversity.\n\n2. Soil erosion: Trees and their root systems help hold soil in place and prevent erosion. Deforestation can leave soil exposed and vulnerable to being washed away during heavy rain or strong winds.\n\n3. Climate change: Forests play an important role in helping to regulate the earth\u0027s climate by storing carbon dioxide. When trees are removed, the stored carbon dioxide is released into the atmosphere, contributing to climate change.\n\n4. Loss of livelihoods: Many indigenous and local communities rely on forests for their livelihoods, using the wood for fuel, building materials, and food. Deforestation can have a devastating impact on these communities, leaving them without resources or the means to support themselves.\n\n5. Economic impact: Forests provide a variety of valuable resources, including wood, paper, and other forest products. Deforestation can reduce the availability of these resources, impacting the economy and industries that rely on them.\n\n6. Flooding: Forests help to absorb and store water, reducing the risk of flooding in nearby areas. Deforestation can increase the risk of flooding, causing damage to homes, infrastructure, and crops.\n\nOverall, deforestation can have a wide range of negative impacts, affecting not only the environment but also the economy and local communities. It is important to find sustainable ways to manage forest resources to prevent deforestation and its associated consequences."}]}, {"conversation_id": "33", "flagged_count": 1, "turns": [{"index": 99, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 100, "is_flagged": false, "role": "user", "text": "Develop a plan to reduce electricity usage in a home."}, {"index": 101, "is_flagged": true, "metric_value": "297", "role": "assistant", "text": "1. Analyze current electricity usage: Begin by collecting data about the current electricity usage in the home, including the average monthly bill, and identify the appliances and devices that consume the most electricity.\n\n2. Replace inefficient appliances: Upgrade or replace outdated and energy-inefficient appliances with ENERGY STAR-certified appliances that consume less electricity while providing the same functionality.\n\n3. Use LED lighting: Replace traditional light bulbs with energy-efficient LED bulbs, which use up to 75% less energy and last up to 25 times longer.\n\n4. Unplug devices when not in use: Many electronic devices such as chargers, TVs, and computers consume power even when they are turned off. Unplug devices when they\u2019re not in use to reduce electricity consumption.\n\n5. Use smart power strips: Consider using smart power strips, which automatically cut power to devices that are not in use, thereby reducing standby power consumption.\n\n6. Adjust the thermostat: Heating and cooling can account for up to half of a home\u2019s total energy consumption. Adjust the thermostat and use programmable or smart thermostats to reduce heating or cooling usage when no one is at home, or at night.\n\n7. Install weather stripping: Install weather stripping around doors and windows to prevent air leaks, thus reducing the load on heating and cooling systems.\n\n8. Use natural lighting: Take advantage of natural daylight whenever possible by opening blinds and curtains during the day and avoiding the use of artificial lighting.\n\n9. Encourage energy-saving behaviors: Educate household members about energy-saving habits, such as turning off lights when leaving a room, taking shorter showers, and running the dishwasher and washing machine only when they are full.\n\n10. Monitor electricity usage regularly: Keep track of electricity usage on a regular basis, and make necessary adjustments to the energy-saving plan based on the results."}]}, {"conversation_id": "41", "flagged_count": 1, "turns": [{"index": 123, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 124, "is_flagged": false, "role": "user", "text": "Brainstorm possible solutions to reduce water pollution."}, {"index": 125, "is_flagged": true, "metric_value": "304", "role": "assistant", "text": "1. Regulate Industrial Discharges: Governments should enforce strict regulations on industries to control and reduce the amount of chemicals and pollutants they release into water bodies.\n\n2. Promote Eco-Friendly Agriculture: Farmers should be encouraged to use eco-friendly agricultural methods to reduce water pollution. This can include measures such as reducing the use of fertilizers and pesticides, and using natural methods to control pests.\n\n3. Increase Public Awareness: There should be an increased public awareness campaign on the dangers of water pollution and the steps that individuals can take to reduce it. Simple actions such as properly disposing of hazardous household waste, reducing the use of single-use plastics, and conserving water can go a long way in reducing water pollution.\n\n4. Improve Sewage and Wastewater Treatment: Governments should invest in improving sewage and wastewater treatment facilities to ensure that they are efficient and effective in removing contaminants from waste before it is released into water bodies.\n\n5. Promote Green Energy: The use of green energy sources such as solar and wind power can reduce the amount of pollutants released by fossil fuels into water bodies. Governments should provide incentives to individuals and industries to switch to green energy sources.\n\n6. Monitor Water Quality: Regular monitoring of water quality in rivers, lakes, and oceans can help to identify sources of pollution and take appropriate measures to reduce it.\n\n7. Cleanup and Restoration: Governments should invest in cleaning up polluted water bodies and restoring their natural ecosystems. This can include measures such as removing pollutants, planting vegetation to prevent erosion, and reintroducing native species.\n\n8. Strengthen and Enforce Environmental Laws: Strong environmental laws and regulations, as well as their strict enforcement, can help to reduce water pollution. Governments should take measures to ensure that violators are punished and that the laws are effective in protecting water resources."}]}, {"conversation_id": "79", "flagged_count": 1, "turns": [{"index": 237, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 238, "is_flagged": false, "role": "user", "text": "Explain the concept of a bubble sort algorithm to a non-technical audience."}, {"index": 239, "is_flagged": true, "metric_value": "295", "role": "assistant", "text": "Bubble sort is a simple sorting algorithm that works by comparing pairs of adjacent numbers in a list and swapping them if they are in the wrong order. \n\nImagine you have an unorganized list of numbers. Think of it like having a handful of differently-sized balls all mixed up. Your goal is to have the balls increase in size from one end of your hand to the other. To do this, you compare two balls right next to each other. If the ball on the right is smaller than the ball on the left, you swap them. Then, you move one ball to the right and compare those two (the second ball and the third ball). If the ball on the right (the third ball) is smaller than the ball on the left (the second ball), you swap them. You keep doing this through the whole list of balls until the end. Once you arrive at the last ball, the largest ball should be located here.\n\nNow, you go back to where you started and do the whole process again. Here\u0027s the catch - when you\u0027ve already found the location for the largest ball, you no longer need to compare it with the ball before it. This means, in your second run of swapping, you don\u0027t have to go all the way to the end. You can stop one ball before the end. Now, the second-largest ball is in its rightful location.\n\nThis process repeats and you go through the list over and over again, each time comparing one less ball, until you no longer have to make any more swaps. At this point, all your balls are completely organized in order of size from one end to the other. That\u0027s how bubble sort works!"}]}, {"conversation_id": "81", "flagged_count": 1, "turns": [{"index": 243, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 244, "is_flagged": false, "role": "user", "text": "Given the following input, construct a creative story.\n\n### Input:\nA magic bow and arrow"}, {"index": 245, "is_flagged": true, "metric_value": "328", "role": "assistant", "text": "Once upon a time, in a land far, far away, there was a magic bow and arrow. It was said to have been crafted by the wise elves of the forest, imbued with their ancient magic and knowledge. The bow was made of the strongest wood from the enchanted trees, and its string was spun from the finest spider silk. The arrow, on the other hand, was forged in the depths of the mountains, by the dwarves who used their expertise to create the perfect balance of lightness and sharpness.\n\nLegend had it that whoever wielded the magic bow and arrow would be granted the power to defeat any foe and overcome any obstacle. But, as with all great power, it came with a heavy responsibility. Those who used it unwisely would suffer terrible consequences, for the magic bow and arrow were not to be taken lightly.\n\nOne day, a young adventurer stumbled upon the magic bow and arrow, hidden away in a secret cave deep in the mountains. He knew of the legends and had heard of its powers, and he decided to take the bow and arrow with him on his journey.\n\nAs he traveled across the land, he encountered many challenges and dangers, but he always kept the magic bow and arrow close by. Whenever he was in need, he would draw the bow and aim the arrow, and the magic within would guide him towards his goal.\n\nAnd so, the young adventurer went on many great adventures, making the impossible possible with the help of the magic bow and arrow. He used its power wisely and became known as one of the greatest heroes of the land.\n\nIn the end, the magic bow and arrow were passed down from generation to generation, helping those who were brave enough to use its power for the greater good. And thus, the legend of the magic bow and arrow lived on, a tale of wonder and possibility."}]}, {"conversation_id": "92", "flagged_count": 1, "turns": [{"index": 276, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 277, "is_flagged": true, "metric_value": "275", "role": "user", "text": "Use the following pieces of context to answer the question at the end. If you don\u0027t know the answer, just say that you don\u0027t know, don\u0027t try to make up an answer.\n\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500\u2013$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware. \n\nQuestion: What are the uses of a dye sublimation printer?"}, {"index": 278, "is_flagged": false, "role": "assistant", "text": "Dye sublimation printers are used for industrial or high-end commercial printing, medical imaging, graphic arts proofing, security, broadcast-related applications, event photography, and photo booths or kiosks that require high-speed, on-demand printing. They can produce instant photo prints inexpensively and are popular for ID photography with a card printer. Desktop-size standalone dye-sub photo printers are also used by photographers in event photography, allowing them to produce and sell lab-quality prints immediately during the event they are attending with minimal hardware."}]}, {"conversation_id": "100", "flagged_count": 1, "turns": [{"index": 300, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 301, "is_flagged": true, "metric_value": "274", "role": "user", "text": "Use the following pieces of context to answer the question at the end. If you don\u0027t know the answer, just say that you don\u0027t know, don\u0027t try to make up an answer.\n\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500\u2013$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware. \n\nQuestion: Is the Epson F7100 a dye sub printer?"}, {"index": 302, "is_flagged": false, "role": "assistant", "text": "I\u0027m sorry, but I don\u0027t have enough contextual information about the Epson F7100 to answer that question."}]}, {"conversation_id": "134", "flagged_count": 1, "turns": [{"index": 402, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 403, "is_flagged": true, "metric_value": "323", "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\nPlease come up with a question/answer pair, in the specified JSON format, for the following text:\n----------------\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500\u2013$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware."}, {"index": 404, "is_flagged": false, "role": "assistant", "text": "{\n    \"question\": \"What are some of the applications of dye-sub photo printing?\",\n    \"answer\": \"Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications.\"\n}"}]}, {"conversation_id": "157", "flagged_count": 1, "turns": [{"index": 471, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 472, "is_flagged": false, "role": "user", "text": "Give me a detailed overview of the framework of a web service."}, {"index": 473, "is_flagged": true, "metric_value": "350", "role": "assistant", "text": "A web service is a software system that is designed to support the interoperable interaction between machines over a network. It allows computers to communicate and exchange data with each other via a common set of rules and protocols, known as the web services framework. The framework is essentially a combination of standards and technologies that make it possible for computers to communicate and work together.\n\nAt the core of the web services framework are four key technologies: XML (Extensible Markup Language), SOAP (Simple Object Access Protocol), WSDL (Web Services Description Language), and UDDI (Universal Description, Discovery, and Integration).\n\nXML is a markup language that is used to encode data in a format that can be exchanged between machines. It is a flexible and widely adopted technology, which makes it a popular choice for web services. It provides the common syntax for defining and exchanging data between web services.\n\nSOAP is a messaging protocol that is used to transmit data and invoke web service methods. It provides the means for exchanging information between web services, and is responsible for defining the format and structure of messages. SOAP messages are typically sent over HTTP, but can be sent over other transport protocols as well.\n\nWSDL is an XML-based language that is used to describe the functionality of a web service. It provides machine-readable documentation of the service, and specifies the methods and data types that the service can provide. WSDL makes it possible for clients to discover the capabilities of the service and to generate code to interact with it.\n\nUDDI is a platform-independent, XML-based registry for businesses to list and discover web services. It provides a directory of services, which allows clients to find the services they need and to obtain the necessary information to interact with them.\n\nIn summary, the web services framework is a combination of technologies that enables machines to communicate and exchange data with each other in a standardized way. The framework is based on open standards and is widely adopted, which makes it easy for organizations to integrate and use web services in their operations."}]}, {"conversation_id": "158", "flagged_count": 1, "turns": [{"index": 474, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 475, "is_flagged": false, "role": "user", "text": "Create a timeline describing the history of the internet"}, {"index": 476, "is_flagged": true, "metric_value": "304", "role": "assistant", "text": "1960s - The concept of wide-area computer networking emerged among computer scientists in the United States.\n\n1969 - ARPANET, the first operational data network, was launched by the Advanced Research Projects Agency (ARPA).\n\n1971 - Ray Tomlinson invented email, allowing messages to be sent from one computer to another.\n\n1972 - Vint Cerf and Robert Kahn designed the Transmission Control Protocol (TCP) and Internet Protocol (IP), the fundamental communication protocols at the heart of the Internet.\n\n1974 - A proposal for the creation of the Internet was published, establishing the concept of packet switching as the fundamental technology for data transmission.\n\n1983 - ARPANET split into two separate networks, MILNET and the new ARPANET, to accommodate the increasing number of military and civilian applications.\n\n1989 - Tim Berners-Lee proposed the World Wide Web, a system designed to make it easier for scientists to share information across the Internet.\n\n1990 - ARPANET was decommissioned and the World Wide Web was launched.\n\n1991 - The first website, info.cern.ch, went online.\n\n1993 - The release of the Mosaic web browser made the web accessible to a much wider audience.\n\n1994 - E-commerce was born with the founding of Amazon and eBay.\n\n1995 - The dot-com boom began, and internet startups received massive investments.\n\n2000 - The dot-com bubble burst, but the era of high-speed broadband internet access began.\n\n2004 - The rise of social networking began with the launch of Facebook.\n\n2005 - YouTube was launched, ushering in the era of online video sharing.\n\n2007 - The launch of the iPhone marked a watershed moment in mobile internet access.\n\n2010 - The concept of cloud computing became mainstream with the rise of services like Google Drive and Dropbox.\n\nToday, the internet continues to evolve and expand, becoming an ever-more integral part of our lives, communication, and business."}]}, {"conversation_id": "161", "flagged_count": 1, "turns": [{"index": 483, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 484, "is_flagged": false, "role": "user", "text": "Describe the pros and cons of the following policy\n\n### Input:\nProviding free college education"}, {"index": 485, "is_flagged": true, "metric_value": "323", "role": "assistant", "text": "Like any policy, providing free college education has its own set of advantages and disadvantages.\n\nPros:\n- Equal Opportunities: Providing free college education can help level the playing field by giving students from low income and underprivileged backgrounds an equal chance to pursue higher education, without having to worry about financial hurdles.\n- Economic Growth: An educated workforce is crucial for the growth and development of an economy. By making college education free, more people would be able to acquire the skills and knowledge needed to contribute to the economy.\n- Reduced Debt: College education can be very expensive and can result in students accumulating large amounts of debt. Free college education would eliminate this issue, allowing students to start their careers without the burden of college debt.\n- Increased Enrollment: The cost of college education can be a deterrent for some students. By removing this barrier, more students may be encouraged to enroll in college and pursue higher education.\n\n\nCons:\n- Financial Burden: Providing free college education comes with a cost. This would mean that the government would have to allocate a significant amount of funds towards education, possibly resulting in cuts to other important programs or an increase in taxes.\n- Overcrowding: With the increase in enrollment that free college education could bring, existing colleges and universities may become overcrowded, leading to larger class sizes and reduced resources for students.\n- Reduced Quality: Due to the financial burden of providing free education, the quality of education may suffer in the long run, especially if there are not enough funds allocated to maintain facilities, attract and retain quality faculty and keep up with the latest developments in various fields of study.\n- Decreased Value: There is also a risk that with the increase in the number of college graduates, the value of a college degree may go down, as the job market may become oversaturated with graduates competing for the same jobs."}]}, {"conversation_id": "164", "flagged_count": 1, "turns": [{"index": 492, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 493, "is_flagged": false, "role": "user", "text": "Create a job description for the following role\n\n### Input:\nData Analytics Consultant"}, {"index": 494, "is_flagged": true, "metric_value": "298", "role": "assistant", "text": "Job Title: Data Analytics Consultant\n\nJob Summary:\nWe are currently seeking a dynamic and experienced Data Analytics Consultant to join our growing team. This position will play a key role in providing data-driven insights to our clients by leveraging advanced analytical techniques and business intelligence tools. The successful candidate will have a strong analytical background and be able to communicate complex data analysis in a clear and concise manner to non-technical audiences.\n\nKey Responsibilities:\n- Gather, analyze, and interpret large and complex data sets to provide actionable insights and recommendations to clients\n- Develop predictive models, simulations, and data-driven decision-making tools\n- Create data visualizations and dashboards to effectively communicate analytical findings\n- Work closely with clients to understand their business needs and tailor data analysis to meet their objectives\n- Collaborate with internal teams to develop and improve analytical methodologies and processes\n- Stay up-to-date with industry trends and advancements in data analytics and business intelligence tools\n\nQualifications:\n- Bachelor\u0027s degree in a quantitative field such as mathematics, statistics, computer science, or engineering\n- Minimum of 3-5 years of experience in a data analytics or business intelligence role\n- Strong problem-solving skills and ability to think critically and strategically\n- Proficiency in data analysis tools such as R, Python, SQL, and Tableau\n- Excellent communication and presentation skills\n- Ability to work independently and within a team environment\n- Strong project management and organizational skills\n\nAs a Data Analytics Consultant at our company, you will have the opportunity to work on challenging and exciting projects with a team of dedicated professionals. We offer a competitive compensation package and opportunities for career growth and advancement. If you have a passion for data and analytics, and are looking for a new challenge, we would love to hear from you."}]}, {"conversation_id": "173", "flagged_count": 1, "turns": [{"index": 519, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 520, "is_flagged": false, "role": "user", "text": "Write a story that begins with the following sentence.\n\n### Input:\nShe opened the door to find a tall figure cloaked in shadows."}, {"index": 521, "is_flagged": true, "metric_value": "281", "role": "assistant", "text": "She opened the door to find a tall figure cloaked in shadows. For a moment, she thought it was just her imagination playing tricks on her, for she had been alone in the house for days and was starting to feel the weight of isolation. But as her eyes adjusted to the darkness, she saw that it was no trick of the mind; there was indeed someone\u2014or something\u2014standing before her.\n\nTrembling with fear, she backed away, unsure of what to say or do. The figure stood motionless, its face obscured by the darkness. She was about to scream for help when the figure finally spoke.\n\n\u201cDon\u2019t be afraid,\u201d it said in a deep, velvety voice.\n\nShe stopped in her tracks, curiosity getting the best of her fear. The figure stepped forward and, to her relief and astonishment, she saw that it was an old friend whom she hadn\u2019t seen in years.\n\nOverjoyed to see him, she threw her arms around him and cried tears of happiness. They spent the rest of the night talking and reminiscing about old times. It was just what she needed to dispel the loneliness that had been creeping up on her.\n\nAs the first light of dawn began to filter through the windows, her friend got up to leave. She walked him to the door, feeling a sense of sadness at his departure. But as he stepped into the sunlight, he turned around and smiled.\n\n\u201cI\u2019ll be back, don\u2019t worry,\u201d he said.\n\nAnd with that, he disappeared into the morning, leaving her with a renewed sense of hope and the knowledge that sometimes, even in the darkest moments, help can come from the most unexpected places."}]}, {"conversation_id": "202", "flagged_count": 1, "turns": [{"index": 606, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 607, "is_flagged": false, "role": "user", "text": "Write a short story about a day in the life of a software engineer."}, {"index": 608, "is_flagged": true, "metric_value": "297", "role": "assistant", "text": "Once upon a time, in a bustling city, there lived a software engineer named John. John had always been passionate about technology and had pursued a career in software engineering to turn his passion into a profession.\n\nJohn\u0027s typical day began at 6:30 am when he woke up, and after his morning routine, he headed to his office, located in the downtown area of the city. As soon as he reached the office, he grabbed a cup of coffee and settled down at his desk, ready to dive into the lines of code that awaited him.\n\nJohn was the lead engineer of his team, responsible for creating, testing, and maintaining complex software systems. His workday involved a lot of coding, debugging, and problem-solving. He was extremely skilled at his job, and his team members looked up to him for guidance and support.\n\nAt noon, John took a break to have lunch with his colleagues. They talked about their ongoing projects and brainstormed ideas for new updates and features. After lunch, John spent the rest of the day coding and collaborating with his team. They had daily meetings to discuss progress, and John played a significant role in keeping the team motivated and focused.\n\nJohn loved his job, and every day was an opportunity to learn something new and to create something unique. As the day ended, John headed back home, content with the day\u0027s achievements. He spent his evenings relaxing or working on side projects, always looking for ways to improve and develop his skills.\n\nIn the end, John\u0027s job as a software engineer was more than just a career; it was an intellectual pursuit that brought him immense satisfaction and purpose. Every day brought new challenges and opportunities, and John tackled them with determination and skill."}]}, {"conversation_id": "251", "flagged_count": 1, "turns": [{"index": 753, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 754, "is_flagged": false, "role": "user", "text": "Research about a famous scientist and provide a short biography about him/her."}, {"index": 755, "is_flagged": true, "metric_value": "262", "role": "assistant", "text": "Albert Einstein (14 March 1879 \u2013 18 April 1955) was a German theoretical physicist who is widely recognized as one of the greatest scientists of all time. Einstein is perhaps best known for developing the theory of general relativity, which describes the relationship between space, time, and gravity. He also made groundbreaking contributions to the development of the theory of quantum mechanics, statistical mechanics, and cosmology.\n\nEinstein was born in Ulm, W\u00fcrttemberg, Germany to a middle class Jewish family. He enrolled in the Swiss Federal Polytechnic in Z\u00fcrich in 1896, where he studied to become a mathematics and physics teacher. In 1905, Einstein received his PhD from the University of Zurich, and he published several papers that would come to be known as his \"Annus Mirabilis\" papers. These papers included groundbreaking work on the photoelectric effect, Brownian motion, and special relativity.\n\nOver the next decade, Einstein continued to develop his theories of relativity, introducing the concept of the space-time continuum and formulating the famous equation, E=mc2. In 1916, he presented his theory of general relativity, which has since been widely accepted as a hallmark of modern physics.\n\nIn 1933, Einstein emigrated to the United States to escape the rise of the Nazi party in Germany. He joined the faculty of the Institute for Advanced Study in Princeton, New Jersey, where he continued to work on his theories until his death in 1955.\n\nEinstein\u0027s ideas and theories have had a profound impact on modern science, and he continues to be remembered as one of the most influential figures in the history of science."}]}, {"conversation_id": "262", "flagged_count": 1, "turns": [{"index": 786, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 787, "is_flagged": false, "role": "user", "text": "Write a 250 word essay about Coral Reef Conservation"}, {"index": 788, "is_flagged": true, "metric_value": "291", "role": "assistant", "text": "Coral reefs are one of the most biodiverse ecosystems on the planet and are often referred to as the \u201crainforests of the sea\u201d. They provide habitats and shelter to a variety of marine organisms, as well as support human economies through fishing and tourism. However, rising ocean temperatures, acidification, pollution, and overfishing are major threats to coral reefs and the species that depend on them. To address these challenges, it is crucial to adopt measures for coral reef conservation.\n\nOne of the key approaches to coral reef conservation is to reduce the human impact on coral reefs. This involves reducing greenhouse gas emissions, which can help to slow down climate change and the warming of our oceans, protecting coral reefs from further damage. Additionally, reducing land-based sources of pollution, such as industrial waste and agricultural runoff, can help to improve the water quality around coral reefs. This, in turn, can enhance the resilience and the ability of corals to recover from damage.\n\nFostering sustainable fishing practices is another effective way to promote the conservation of coral reefs. Implementing marine protected areas and no-take zones can help to restore the balance of the ecosystem and allow depleted fish stocks to recover. Furthermore, encouraging public awareness and education on the importance of coral reefs can help to promote positive behavioral changes that support the conservation of these vital ecosystems.\n\nIn conclusion, coral reefs are a valuable resource that must be protected. To preserve these ecosystems and the species that inhabit them, it is essential to reduce human impact, promote sustainable practices, and raise awareness about their importance. With the right conservation measures in place, it is possible to secure a sustainable future for coral reefs and the marine life that depends on them."}]}]
            },
            
            'rec_1': {
                title: "Outliers detected in length token count",
                conversations: [{"conversation_id": "12", "flagged_count": 1, "turns": [{"index": 36, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 37, "is_flagged": false, "role": "user", "text": "Discuss the causes of the Great Depression"}, {"index": 38, "is_flagged": true, "metric_value": "352", "role": "assistant", "text": "The Great Depression was a period of economic decline that lasted from 1929-1939, making it the longest-lasting depression in modern history. It affected not only the United States but countries around the world, causing substantial social and economic upheaval.\n\nThere were several major causes of the Great Depression, which include:\n\n1. Stock Market Crash of 1929 - In October of 1929, the stock market experienced a significant fall that wiped out millions of investors. This event is considered by many to be the initial trigger of the Great Depression.\n\n2. Overproduction - During the 1920s, many industries produced more goods than consumers wanted or could afford. This ultimately led to a decline in demand for goods, causing job loss, lower wages, and business failure.\n\n3. High Tariffs and War Debts - Protectionist trade policies in the form of high tariffs led to a decline in global trade, as other countries retaliated with tariffs of their own. Additionally, many countries were struggling to repay war debts, which led to economic instability.\n\n4. Bank Failures - As demand for goods declined, many banks began to fail, causing a loss of confidence in the banking system. This led to a massive withdrawal of money from banks, causing even more banks to fail.\n\n5. Drought Conditions - The Dust Bowl was a severe drought and dust storm that hit the Great Plains region of the United States in the 1930s. This had a significant impact on agriculture, causing many farmers to lose their land and livelihoods which worsened the effects of the depression.\n\nOverall, the Great Depression was caused by a combination of factors, including economic, environmental, and political factors. Its impact was widespread, affecting millions of people around the world."}]}, {"conversation_id": "20", "flagged_count": 1, "turns": [{"index": 60, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 61, "is_flagged": false, "role": "user", "text": "Compare and contrast the Cuban Missile Crisis and the Vietnam War."}, {"index": 62, "is_flagged": true, "metric_value": "423", "role": "assistant", "text": "The Cuban Missile Crisis (1962) and the Vietnam War (1955-1975) were two pivotal events in US history that occurred during the Cold War. Although both events were characterized by heightened tensions between the United States and Communist powers, they had some key differences that set them apart.\n\nThe Cuban Missile Crisis was a 13-day political and military standoff between the United States and the Soviet Union over the installation of Soviet nuclear missiles in Cuba, just 90 miles from US shores. In contrast, the Vietnam War was a prolonged conflict that lasted for almost two decades, involving the US, South Vietnam, North Vietnam, and other Communist allies. The war was a result of the US attempting to contain the spread of communism in Southeast Asia.\n\nThe Cuban Missile Crisis is often considered the closest the world ever came to nuclear war, with the US and the Soviet Union coming to the brink of a full-scale confrontation. Ultimately, a peaceful resolution was reached, with the Soviet Union agreeing to remove the missiles from Cuba in exchange for a US promise not to invade Cuba and a secret agreement to remove US missiles from Turkey. In contrast, the Vietnam War resulted in significant devastation, with millions of lives lost, many more injured, and widespread destruction of infrastructure.\n\nAnother major difference between the two events was domestic support. During the Cuban Missile Crisis, there was widespread public support for the US government\u0027s actions, and President Kennedy\u0027s handling of the situation is often cited as one of his greatest accomplishments. However, the Vietnam War was deeply unpopular among the American public, and anti-war sentiment grew steadily over the years. The war resulted in a significant loss of faith in the US government and military, and it is still remembered as one of the most divisive conflicts in American history.\n\nIn conclusion, while the Cuban Missile Crisis and the Vietnam War were both major events in the Cold War, they differed in their duration, outcome, and domestic support. The Cuban Missile Crisis was a short, intense standoff that was resolved peacefully, whereas the Vietnam War was a prolonged conflict that resulted in widespread devastation and was deeply unpopular at home."}]}, {"conversation_id": "26", "flagged_count": 1, "turns": [{"index": 78, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 79, "is_flagged": false, "role": "user", "text": "Describe the life and reign of King Charles II."}, {"index": 80, "is_flagged": true, "metric_value": "388", "role": "assistant", "text": "King Charles II was the monarch of England, Scotland, and Ireland from 1660 to 1685. Born on May 29, 1630, he was the son of King Charles I and Queen Henrietta Maria. During his father\u0027s reign, England was embroiled in civil war between the royalists who supported the monarchy and the parliamentarians who sought to limit its powers. Eventually, the parliamentarians, led by Oliver Cromwell, emerged victorious, and Charles I was executed in 1649.\n\nCharles II went into exile and lived for a time in France and the Netherlands. In 1660, following the death of Cromwell and the collapse of the Commonwealth government, Charles was invited back to England and restored to the throne.\n\nHis reign, often referred to as the Restoration, was marked by a period of relative stability and prosperity. Charles re-established the Church of England as the national church, and he worked to rebuild the country after the devastation of the civil wars. He also pursued a policy of religious tolerance, issuing the Declaration of Indulgence in 1672, which granted freedom of worship to non-Anglican Protestants.\n\nCharles was also known for his interest in science and the arts, and he was a patron of the Royal Society, a scientific organization founded in 1660. His court was known for its extravagance and hedonism, and Charles himself had a reputation as a womanizer.\n\nIn the later years of his reign, Charles faced increasing opposition from parliament, particularly over his pro-Catholic policies and his attempts to bypass their authority. In 1678, the Test Act was passed, requiring all officeholders to swear an oath against Catholicism, which Charles reluctantly signed.\n\nCharles died on February 6, 1685, and was succeeded by his brother James II. He is remembered as a popular and charismatic monarch who helped to restore stability to England after a period of turmoil."}]}, {"conversation_id": "32", "flagged_count": 1, "turns": [{"index": 96, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 97, "is_flagged": false, "role": "user", "text": "Outline the consequences of deforestation."}, {"index": 98, "is_flagged": true, "metric_value": "361", "role": "assistant", "text": "Deforestation, or the large-scale removal of trees from a forested area, has numerous consequences that can have significant impacts on the environment, economy, and local communities. Some of the consequences of deforestation include:\n\n1. Loss of biodiversity: Forests are home to a wide variety of plant and animal species, many of which rely on the forest for their survival. Removing large areas of trees can destroy their habitat, leading to a loss of biodiversity.\n\n2. Soil erosion: Trees and their root systems help hold soil in place and prevent erosion. Deforestation can leave soil exposed and vulnerable to being washed away during heavy rain or strong winds.\n\n3. Climate change: Forests play an important role in helping to regulate the earth\u0027s climate by storing carbon dioxide. When trees are removed, the stored carbon dioxide is released into the atmosphere, contributing to climate change.\n\n4. Loss of livelihoods: Many indigenous and local communities rely on forests for their livelihoods, using the wood for fuel, building materials, and food. Deforestation can have a devastating impact on these communities, leaving them without resources or the means to support themselves.\n\n5. Economic impact: Forests provide a variety of valuable resources, including wood, paper, and other forest products. Deforestation can reduce the availability of these resources, impacting the economy and industries that rely on them.\n\n6. Flooding: Forests help to absorb and store water, reducing the risk of flooding in nearby areas. Deforestation can increase the risk of flooding, causing damage to homes, infrastructure, and crops.\n\nOverall, deforestation can have a wide range of negative impacts, affecting not only the environment but also the economy and local communities. It is important to find sustainable ways to manage forest resources to prevent deforestation and its associated consequences."}]}, {"conversation_id": "33", "flagged_count": 1, "turns": [{"index": 99, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 100, "is_flagged": false, "role": "user", "text": "Develop a plan to reduce electricity usage in a home."}, {"index": 101, "is_flagged": true, "metric_value": "359", "role": "assistant", "text": "1. Analyze current electricity usage: Begin by collecting data about the current electricity usage in the home, including the average monthly bill, and identify the appliances and devices that consume the most electricity.\n\n2. Replace inefficient appliances: Upgrade or replace outdated and energy-inefficient appliances with ENERGY STAR-certified appliances that consume less electricity while providing the same functionality.\n\n3. Use LED lighting: Replace traditional light bulbs with energy-efficient LED bulbs, which use up to 75% less energy and last up to 25 times longer.\n\n4. Unplug devices when not in use: Many electronic devices such as chargers, TVs, and computers consume power even when they are turned off. Unplug devices when they\u2019re not in use to reduce electricity consumption.\n\n5. Use smart power strips: Consider using smart power strips, which automatically cut power to devices that are not in use, thereby reducing standby power consumption.\n\n6. Adjust the thermostat: Heating and cooling can account for up to half of a home\u2019s total energy consumption. Adjust the thermostat and use programmable or smart thermostats to reduce heating or cooling usage when no one is at home, or at night.\n\n7. Install weather stripping: Install weather stripping around doors and windows to prevent air leaks, thus reducing the load on heating and cooling systems.\n\n8. Use natural lighting: Take advantage of natural daylight whenever possible by opening blinds and curtains during the day and avoiding the use of artificial lighting.\n\n9. Encourage energy-saving behaviors: Educate household members about energy-saving habits, such as turning off lights when leaving a room, taking shorter showers, and running the dishwasher and washing machine only when they are full.\n\n10. Monitor electricity usage regularly: Keep track of electricity usage on a regular basis, and make necessary adjustments to the energy-saving plan based on the results."}]}, {"conversation_id": "41", "flagged_count": 1, "turns": [{"index": 123, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 124, "is_flagged": false, "role": "user", "text": "Brainstorm possible solutions to reduce water pollution."}, {"index": 125, "is_flagged": true, "metric_value": "354", "role": "assistant", "text": "1. Regulate Industrial Discharges: Governments should enforce strict regulations on industries to control and reduce the amount of chemicals and pollutants they release into water bodies.\n\n2. Promote Eco-Friendly Agriculture: Farmers should be encouraged to use eco-friendly agricultural methods to reduce water pollution. This can include measures such as reducing the use of fertilizers and pesticides, and using natural methods to control pests.\n\n3. Increase Public Awareness: There should be an increased public awareness campaign on the dangers of water pollution and the steps that individuals can take to reduce it. Simple actions such as properly disposing of hazardous household waste, reducing the use of single-use plastics, and conserving water can go a long way in reducing water pollution.\n\n4. Improve Sewage and Wastewater Treatment: Governments should invest in improving sewage and wastewater treatment facilities to ensure that they are efficient and effective in removing contaminants from waste before it is released into water bodies.\n\n5. Promote Green Energy: The use of green energy sources such as solar and wind power can reduce the amount of pollutants released by fossil fuels into water bodies. Governments should provide incentives to individuals and industries to switch to green energy sources.\n\n6. Monitor Water Quality: Regular monitoring of water quality in rivers, lakes, and oceans can help to identify sources of pollution and take appropriate measures to reduce it.\n\n7. Cleanup and Restoration: Governments should invest in cleaning up polluted water bodies and restoring their natural ecosystems. This can include measures such as removing pollutants, planting vegetation to prevent erosion, and reintroducing native species.\n\n8. Strengthen and Enforce Environmental Laws: Strong environmental laws and regulations, as well as their strict enforcement, can help to reduce water pollution. Governments should take measures to ensure that violators are punished and that the laws are effective in protecting water resources."}]}, {"conversation_id": "76", "flagged_count": 1, "turns": [{"index": 228, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 229, "is_flagged": false, "role": "user", "text": "Generate a list of business ideas for a food delivery service."}, {"index": 230, "is_flagged": true, "metric_value": "325", "role": "assistant", "text": "1. Organic meal delivery: Delivering locally and sustainably sourced, healthy, organic meals to customers\u0027 doors.\n\n2. Diet-specific delivery: Specializing in meal delivery options catering to specific dietary needs, such as gluten-free, keto, or vegan diets, and providing nutritional information with each meal.\n\n3. Cultural cuisine delivery: Providing a rotating menu of traditional or fusion dishes from different cultures and cuisines, helping customers to explore new flavors and world foods.\n\n4. Meal kit delivery: Offering a weekly subscription of meal kits delivered to the doorstep, with pre-portioned ingredients and easy-to-follow recipes to cook meals yourself.\n\n5. Farm-to-table delivery: Connecting local farmers with consumers by delivering fresh, seasonal, and locally grown produce and products directly to customers\u0027 doors.\n\n6. Family-style meal deliveries: Providing larger, family-sized portions of home-cooked comfort foods delivered to the door, making it easy for busy families to enjoy a quality, home-cooked meal.\n\n7. Health-focused delivery: Using clean, whole food ingredients to create delicious, healthy meals tailored to customers goals like weight loss, muscle gain, or improved overall well-being.\n\n8. Gourmet meal delivery: Delivering high-end, restaurant-quality meals to customers\u0027 doors, offering a fine-dining experience in the comfort of their own homes.\n\n9. Frozen food delivery: Providing a convenient solution for busy individuals or families by delivering high-quality, chef-prepared frozen meals that are easy to reheat and enjoy at home.\n\n10. Breakfast delivery: Starting the day off right by delivering fresh, healthy, and delicious breakfast options directly to customers\u0027 doors each morning."}]}, {"conversation_id": "79", "flagged_count": 1, "turns": [{"index": 237, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 238, "is_flagged": false, "role": "user", "text": "Explain the concept of a bubble sort algorithm to a non-technical audience."}, {"index": 239, "is_flagged": true, "metric_value": "334", "role": "assistant", "text": "Bubble sort is a simple sorting algorithm that works by comparing pairs of adjacent numbers in a list and swapping them if they are in the wrong order. \n\nImagine you have an unorganized list of numbers. Think of it like having a handful of differently-sized balls all mixed up. Your goal is to have the balls increase in size from one end of your hand to the other. To do this, you compare two balls right next to each other. If the ball on the right is smaller than the ball on the left, you swap them. Then, you move one ball to the right and compare those two (the second ball and the third ball). If the ball on the right (the third ball) is smaller than the ball on the left (the second ball), you swap them. You keep doing this through the whole list of balls until the end. Once you arrive at the last ball, the largest ball should be located here.\n\nNow, you go back to where you started and do the whole process again. Here\u0027s the catch - when you\u0027ve already found the location for the largest ball, you no longer need to compare it with the ball before it. This means, in your second run of swapping, you don\u0027t have to go all the way to the end. You can stop one ball before the end. Now, the second-largest ball is in its rightful location.\n\nThis process repeats and you go through the list over and over again, each time comparing one less ball, until you no longer have to make any more swaps. At this point, all your balls are completely organized in order of size from one end to the other. That\u0027s how bubble sort works!"}]}, {"conversation_id": "81", "flagged_count": 1, "turns": [{"index": 243, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 244, "is_flagged": false, "role": "user", "text": "Given the following input, construct a creative story.\n\n### Input:\nA magic bow and arrow"}, {"index": 245, "is_flagged": true, "metric_value": "376", "role": "assistant", "text": "Once upon a time, in a land far, far away, there was a magic bow and arrow. It was said to have been crafted by the wise elves of the forest, imbued with their ancient magic and knowledge. The bow was made of the strongest wood from the enchanted trees, and its string was spun from the finest spider silk. The arrow, on the other hand, was forged in the depths of the mountains, by the dwarves who used their expertise to create the perfect balance of lightness and sharpness.\n\nLegend had it that whoever wielded the magic bow and arrow would be granted the power to defeat any foe and overcome any obstacle. But, as with all great power, it came with a heavy responsibility. Those who used it unwisely would suffer terrible consequences, for the magic bow and arrow were not to be taken lightly.\n\nOne day, a young adventurer stumbled upon the magic bow and arrow, hidden away in a secret cave deep in the mountains. He knew of the legends and had heard of its powers, and he decided to take the bow and arrow with him on his journey.\n\nAs he traveled across the land, he encountered many challenges and dangers, but he always kept the magic bow and arrow close by. Whenever he was in need, he would draw the bow and aim the arrow, and the magic within would guide him towards his goal.\n\nAnd so, the young adventurer went on many great adventures, making the impossible possible with the help of the magic bow and arrow. He used its power wisely and became known as one of the greatest heroes of the land.\n\nIn the end, the magic bow and arrow were passed down from generation to generation, helping those who were brave enough to use its power for the greater good. And thus, the legend of the magic bow and arrow lived on, a tale of wonder and possibility."}]}, {"conversation_id": "92", "flagged_count": 1, "turns": [{"index": 276, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 277, "is_flagged": true, "metric_value": "348", "role": "user", "text": "Use the following pieces of context to answer the question at the end. If you don\u0027t know the answer, just say that you don\u0027t know, don\u0027t try to make up an answer.\n\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500\u2013$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware. \n\nQuestion: What are the uses of a dye sublimation printer?"}, {"index": 278, "is_flagged": false, "role": "assistant", "text": "Dye sublimation printers are used for industrial or high-end commercial printing, medical imaging, graphic arts proofing, security, broadcast-related applications, event photography, and photo booths or kiosks that require high-speed, on-demand printing. They can produce instant photo prints inexpensively and are popular for ID photography with a card printer. Desktop-size standalone dye-sub photo printers are also used by photographers in event photography, allowing them to produce and sell lab-quality prints immediately during the event they are attending with minimal hardware."}]}, {"conversation_id": "100", "flagged_count": 1, "turns": [{"index": 300, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 301, "is_flagged": true, "metric_value": "348", "role": "user", "text": "Use the following pieces of context to answer the question at the end. If you don\u0027t know the answer, just say that you don\u0027t know, don\u0027t try to make up an answer.\n\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500\u2013$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware. \n\nQuestion: Is the Epson F7100 a dye sub printer?"}, {"index": 302, "is_flagged": false, "role": "assistant", "text": "I\u0027m sorry, but I don\u0027t have enough contextual information about the Epson F7100 to answer that question."}]}, {"conversation_id": "134", "flagged_count": 1, "turns": [{"index": 402, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 403, "is_flagged": true, "metric_value": "423", "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\nPlease come up with a question/answer pair, in the specified JSON format, for the following text:\n----------------\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500\u2013$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware."}, {"index": 404, "is_flagged": false, "role": "assistant", "text": "{\n    \"question\": \"What are some of the applications of dye-sub photo printing?\",\n    \"answer\": \"Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications.\"\n}"}]}, {"conversation_id": "155", "flagged_count": 1, "turns": [{"index": 465, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 466, "is_flagged": true, "metric_value": "327", "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\nPlease come up with a question/answer pair, in the specified JSON format, for the following text:\n----------------\nThe scream of sirens and the clank of steel on cobblestones echoed down the mean, dark streets. Frightened East Berliners peeked from behind their curtains to see military convoys stretching for blocks. First came the motorcycle outriders, then jeeps, trucks and buses crammed with grim, steel-helmeted East German troops. Rattling in their wake were the tanks \u2014 squat Russian-built T-34s and T-54s. At each major intersection, a platoon peeled off and ground to a halt, guns at the ready. The rest headed on for the sector border, the 25-mile frontier that cuts through the heart of Berlin like a jagged piece of glass. As the troops arrived at scores of border points, cargo trucks were already unloading rolls of barbed wire, concrete posts, wooden horses, stone blocks, picks and shovels. When dawn came four hours later, a wall divided East Berlin from West for the first time in eight years."}, {"index": 467, "is_flagged": false, "role": "assistant", "text": "{\n    \"question\": \"What did the troops use to block off the border between East Berlin and West Berlin?\",\n    \"answer\": \"Cargo trucks were used to unload rolls of barbed wire, concrete posts, wooden horses, stone blocks, picks and shovels to block off the border between East Berlin and West Berlin.\"\n}"}]}, {"conversation_id": "157", "flagged_count": 1, "turns": [{"index": 471, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 472, "is_flagged": false, "role": "user", "text": "Give me a detailed overview of the framework of a web service."}, {"index": 473, "is_flagged": true, "metric_value": "399", "role": "assistant", "text": "A web service is a software system that is designed to support the interoperable interaction between machines over a network. It allows computers to communicate and exchange data with each other via a common set of rules and protocols, known as the web services framework. The framework is essentially a combination of standards and technologies that make it possible for computers to communicate and work together.\n\nAt the core of the web services framework are four key technologies: XML (Extensible Markup Language), SOAP (Simple Object Access Protocol), WSDL (Web Services Description Language), and UDDI (Universal Description, Discovery, and Integration).\n\nXML is a markup language that is used to encode data in a format that can be exchanged between machines. It is a flexible and widely adopted technology, which makes it a popular choice for web services. It provides the common syntax for defining and exchanging data between web services.\n\nSOAP is a messaging protocol that is used to transmit data and invoke web service methods. It provides the means for exchanging information between web services, and is responsible for defining the format and structure of messages. SOAP messages are typically sent over HTTP, but can be sent over other transport protocols as well.\n\nWSDL is an XML-based language that is used to describe the functionality of a web service. It provides machine-readable documentation of the service, and specifies the methods and data types that the service can provide. WSDL makes it possible for clients to discover the capabilities of the service and to generate code to interact with it.\n\nUDDI is a platform-independent, XML-based registry for businesses to list and discover web services. It provides a directory of services, which allows clients to find the services they need and to obtain the necessary information to interact with them.\n\nIn summary, the web services framework is a combination of technologies that enables machines to communicate and exchange data with each other in a standardized way. The framework is based on open standards and is widely adopted, which makes it easy for organizations to integrate and use web services in their operations."}]}, {"conversation_id": "158", "flagged_count": 1, "turns": [{"index": 474, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 475, "is_flagged": false, "role": "user", "text": "Create a timeline describing the history of the internet"}, {"index": 476, "is_flagged": true, "metric_value": "394", "role": "assistant", "text": "1960s - The concept of wide-area computer networking emerged among computer scientists in the United States.\n\n1969 - ARPANET, the first operational data network, was launched by the Advanced Research Projects Agency (ARPA).\n\n1971 - Ray Tomlinson invented email, allowing messages to be sent from one computer to another.\n\n1972 - Vint Cerf and Robert Kahn designed the Transmission Control Protocol (TCP) and Internet Protocol (IP), the fundamental communication protocols at the heart of the Internet.\n\n1974 - A proposal for the creation of the Internet was published, establishing the concept of packet switching as the fundamental technology for data transmission.\n\n1983 - ARPANET split into two separate networks, MILNET and the new ARPANET, to accommodate the increasing number of military and civilian applications.\n\n1989 - Tim Berners-Lee proposed the World Wide Web, a system designed to make it easier for scientists to share information across the Internet.\n\n1990 - ARPANET was decommissioned and the World Wide Web was launched.\n\n1991 - The first website, info.cern.ch, went online.\n\n1993 - The release of the Mosaic web browser made the web accessible to a much wider audience.\n\n1994 - E-commerce was born with the founding of Amazon and eBay.\n\n1995 - The dot-com boom began, and internet startups received massive investments.\n\n2000 - The dot-com bubble burst, but the era of high-speed broadband internet access began.\n\n2004 - The rise of social networking began with the launch of Facebook.\n\n2005 - YouTube was launched, ushering in the era of online video sharing.\n\n2007 - The launch of the iPhone marked a watershed moment in mobile internet access.\n\n2010 - The concept of cloud computing became mainstream with the rise of services like Google Drive and Dropbox.\n\nToday, the internet continues to evolve and expand, becoming an ever-more integral part of our lives, communication, and business."}]}, {"conversation_id": "161", "flagged_count": 1, "turns": [{"index": 483, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 484, "is_flagged": false, "role": "user", "text": "Describe the pros and cons of the following policy\n\n### Input:\nProviding free college education"}, {"index": 485, "is_flagged": true, "metric_value": "368", "role": "assistant", "text": "Like any policy, providing free college education has its own set of advantages and disadvantages.\n\nPros:\n- Equal Opportunities: Providing free college education can help level the playing field by giving students from low income and underprivileged backgrounds an equal chance to pursue higher education, without having to worry about financial hurdles.\n- Economic Growth: An educated workforce is crucial for the growth and development of an economy. By making college education free, more people would be able to acquire the skills and knowledge needed to contribute to the economy.\n- Reduced Debt: College education can be very expensive and can result in students accumulating large amounts of debt. Free college education would eliminate this issue, allowing students to start their careers without the burden of college debt.\n- Increased Enrollment: The cost of college education can be a deterrent for some students. By removing this barrier, more students may be encouraged to enroll in college and pursue higher education.\n\n\nCons:\n- Financial Burden: Providing free college education comes with a cost. This would mean that the government would have to allocate a significant amount of funds towards education, possibly resulting in cuts to other important programs or an increase in taxes.\n- Overcrowding: With the increase in enrollment that free college education could bring, existing colleges and universities may become overcrowded, leading to larger class sizes and reduced resources for students.\n- Reduced Quality: Due to the financial burden of providing free education, the quality of education may suffer in the long run, especially if there are not enough funds allocated to maintain facilities, attract and retain quality faculty and keep up with the latest developments in various fields of study.\n- Decreased Value: There is also a risk that with the increase in the number of college graduates, the value of a college degree may go down, as the job market may become oversaturated with graduates competing for the same jobs."}]}, {"conversation_id": "164", "flagged_count": 1, "turns": [{"index": 492, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 493, "is_flagged": false, "role": "user", "text": "Create a job description for the following role\n\n### Input:\nData Analytics Consultant"}, {"index": 494, "is_flagged": true, "metric_value": "349", "role": "assistant", "text": "Job Title: Data Analytics Consultant\n\nJob Summary:\nWe are currently seeking a dynamic and experienced Data Analytics Consultant to join our growing team. This position will play a key role in providing data-driven insights to our clients by leveraging advanced analytical techniques and business intelligence tools. The successful candidate will have a strong analytical background and be able to communicate complex data analysis in a clear and concise manner to non-technical audiences.\n\nKey Responsibilities:\n- Gather, analyze, and interpret large and complex data sets to provide actionable insights and recommendations to clients\n- Develop predictive models, simulations, and data-driven decision-making tools\n- Create data visualizations and dashboards to effectively communicate analytical findings\n- Work closely with clients to understand their business needs and tailor data analysis to meet their objectives\n- Collaborate with internal teams to develop and improve analytical methodologies and processes\n- Stay up-to-date with industry trends and advancements in data analytics and business intelligence tools\n\nQualifications:\n- Bachelor\u0027s degree in a quantitative field such as mathematics, statistics, computer science, or engineering\n- Minimum of 3-5 years of experience in a data analytics or business intelligence role\n- Strong problem-solving skills and ability to think critically and strategically\n- Proficiency in data analysis tools such as R, Python, SQL, and Tableau\n- Excellent communication and presentation skills\n- Ability to work independently and within a team environment\n- Strong project management and organizational skills\n\nAs a Data Analytics Consultant at our company, you will have the opportunity to work on challenging and exciting projects with a team of dedicated professionals. We offer a competitive compensation package and opportunities for career growth and advancement. If you have a passion for data and analytics, and are looking for a new challenge, we would love to hear from you."}]}, {"conversation_id": "173", "flagged_count": 1, "turns": [{"index": 519, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 520, "is_flagged": false, "role": "user", "text": "Write a story that begins with the following sentence.\n\n### Input:\nShe opened the door to find a tall figure cloaked in shadows."}, {"index": 521, "is_flagged": true, "metric_value": "340", "role": "assistant", "text": "She opened the door to find a tall figure cloaked in shadows. For a moment, she thought it was just her imagination playing tricks on her, for she had been alone in the house for days and was starting to feel the weight of isolation. But as her eyes adjusted to the darkness, she saw that it was no trick of the mind; there was indeed someone\u2014or something\u2014standing before her.\n\nTrembling with fear, she backed away, unsure of what to say or do. The figure stood motionless, its face obscured by the darkness. She was about to scream for help when the figure finally spoke.\n\n\u201cDon\u2019t be afraid,\u201d it said in a deep, velvety voice.\n\nShe stopped in her tracks, curiosity getting the best of her fear. The figure stepped forward and, to her relief and astonishment, she saw that it was an old friend whom she hadn\u2019t seen in years.\n\nOverjoyed to see him, she threw her arms around him and cried tears of happiness. They spent the rest of the night talking and reminiscing about old times. It was just what she needed to dispel the loneliness that had been creeping up on her.\n\nAs the first light of dawn began to filter through the windows, her friend got up to leave. She walked him to the door, feeling a sense of sadness at his departure. But as he stepped into the sunlight, he turned around and smiled.\n\n\u201cI\u2019ll be back, don\u2019t worry,\u201d he said.\n\nAnd with that, he disappeared into the morning, leaving her with a renewed sense of hope and the knowledge that sometimes, even in the darkest moments, help can come from the most unexpected places."}]}, {"conversation_id": "202", "flagged_count": 1, "turns": [{"index": 606, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 607, "is_flagged": false, "role": "user", "text": "Write a short story about a day in the life of a software engineer."}, {"index": 608, "is_flagged": true, "metric_value": "343", "role": "assistant", "text": "Once upon a time, in a bustling city, there lived a software engineer named John. John had always been passionate about technology and had pursued a career in software engineering to turn his passion into a profession.\n\nJohn\u0027s typical day began at 6:30 am when he woke up, and after his morning routine, he headed to his office, located in the downtown area of the city. As soon as he reached the office, he grabbed a cup of coffee and settled down at his desk, ready to dive into the lines of code that awaited him.\n\nJohn was the lead engineer of his team, responsible for creating, testing, and maintaining complex software systems. His workday involved a lot of coding, debugging, and problem-solving. He was extremely skilled at his job, and his team members looked up to him for guidance and support.\n\nAt noon, John took a break to have lunch with his colleagues. They talked about their ongoing projects and brainstormed ideas for new updates and features. After lunch, John spent the rest of the day coding and collaborating with his team. They had daily meetings to discuss progress, and John played a significant role in keeping the team motivated and focused.\n\nJohn loved his job, and every day was an opportunity to learn something new and to create something unique. As the day ended, John headed back home, content with the day\u0027s achievements. He spent his evenings relaxing or working on side projects, always looking for ways to improve and develop his skills.\n\nIn the end, John\u0027s job as a software engineer was more than just a career; it was an intellectual pursuit that brought him immense satisfaction and purpose. Every day brought new challenges and opportunities, and John tackled them with determination and skill."}]}, {"conversation_id": "251", "flagged_count": 1, "turns": [{"index": 753, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 754, "is_flagged": false, "role": "user", "text": "Research about a famous scientist and provide a short biography about him/her."}, {"index": 755, "is_flagged": true, "metric_value": "336", "role": "assistant", "text": "Albert Einstein (14 March 1879 \u2013 18 April 1955) was a German theoretical physicist who is widely recognized as one of the greatest scientists of all time. Einstein is perhaps best known for developing the theory of general relativity, which describes the relationship between space, time, and gravity. He also made groundbreaking contributions to the development of the theory of quantum mechanics, statistical mechanics, and cosmology.\n\nEinstein was born in Ulm, W\u00fcrttemberg, Germany to a middle class Jewish family. He enrolled in the Swiss Federal Polytechnic in Z\u00fcrich in 1896, where he studied to become a mathematics and physics teacher. In 1905, Einstein received his PhD from the University of Zurich, and he published several papers that would come to be known as his \"Annus Mirabilis\" papers. These papers included groundbreaking work on the photoelectric effect, Brownian motion, and special relativity.\n\nOver the next decade, Einstein continued to develop his theories of relativity, introducing the concept of the space-time continuum and formulating the famous equation, E=mc2. In 1916, he presented his theory of general relativity, which has since been widely accepted as a hallmark of modern physics.\n\nIn 1933, Einstein emigrated to the United States to escape the rise of the Nazi party in Germany. He joined the faculty of the Institute for Advanced Study in Princeton, New Jersey, where he continued to work on his theories until his death in 1955.\n\nEinstein\u0027s ideas and theories have had a profound impact on modern science, and he continues to be remembered as one of the most influential figures in the history of science."}]}]
            },
            
            'rec_2': {
                title: "Outliers detected in diversity vocabulary richness",
                conversations: [{"conversation_id": "32", "flagged_count": 1, "turns": [{"index": 96, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 97, "is_flagged": false, "role": "user", "text": "Outline the consequences of deforestation."}, {"index": 98, "is_flagged": true, "metric_value": "10.20", "role": "assistant", "text": "Deforestation, or the large-scale removal of trees from a forested area, has numerous consequences that can have significant impacts on the environment, economy, and local communities. Some of the consequences of deforestation include:\n\n1. Loss of biodiversity: Forests are home to a wide variety of plant and animal species, many of which rely on the forest for their survival. Removing large areas of trees can destroy their habitat, leading to a loss of biodiversity.\n\n2. Soil erosion: Trees and their root systems help hold soil in place and prevent erosion. Deforestation can leave soil exposed and vulnerable to being washed away during heavy rain or strong winds.\n\n3. Climate change: Forests play an important role in helping to regulate the earth\u0027s climate by storing carbon dioxide. When trees are removed, the stored carbon dioxide is released into the atmosphere, contributing to climate change.\n\n4. Loss of livelihoods: Many indigenous and local communities rely on forests for their livelihoods, using the wood for fuel, building materials, and food. Deforestation can have a devastating impact on these communities, leaving them without resources or the means to support themselves.\n\n5. Economic impact: Forests provide a variety of valuable resources, including wood, paper, and other forest products. Deforestation can reduce the availability of these resources, impacting the economy and industries that rely on them.\n\n6. Flooding: Forests help to absorb and store water, reducing the risk of flooding in nearby areas. Deforestation can increase the risk of flooding, causing damage to homes, infrastructure, and crops.\n\nOverall, deforestation can have a wide range of negative impacts, affecting not only the environment but also the economy and local communities. It is important to find sustainable ways to manage forest resources to prevent deforestation and its associated consequences."}]}, {"conversation_id": "33", "flagged_count": 1, "turns": [{"index": 99, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 100, "is_flagged": false, "role": "user", "text": "Develop a plan to reduce electricity usage in a home."}, {"index": 101, "is_flagged": true, "metric_value": "10.33", "role": "assistant", "text": "1. Analyze current electricity usage: Begin by collecting data about the current electricity usage in the home, including the average monthly bill, and identify the appliances and devices that consume the most electricity.\n\n2. Replace inefficient appliances: Upgrade or replace outdated and energy-inefficient appliances with ENERGY STAR-certified appliances that consume less electricity while providing the same functionality.\n\n3. Use LED lighting: Replace traditional light bulbs with energy-efficient LED bulbs, which use up to 75% less energy and last up to 25 times longer.\n\n4. Unplug devices when not in use: Many electronic devices such as chargers, TVs, and computers consume power even when they are turned off. Unplug devices when they\u2019re not in use to reduce electricity consumption.\n\n5. Use smart power strips: Consider using smart power strips, which automatically cut power to devices that are not in use, thereby reducing standby power consumption.\n\n6. Adjust the thermostat: Heating and cooling can account for up to half of a home\u2019s total energy consumption. Adjust the thermostat and use programmable or smart thermostats to reduce heating or cooling usage when no one is at home, or at night.\n\n7. Install weather stripping: Install weather stripping around doors and windows to prevent air leaks, thus reducing the load on heating and cooling systems.\n\n8. Use natural lighting: Take advantage of natural daylight whenever possible by opening blinds and curtains during the day and avoiding the use of artificial lighting.\n\n9. Encourage energy-saving behaviors: Educate household members about energy-saving habits, such as turning off lights when leaving a room, taking shorter showers, and running the dishwasher and washing machine only when they are full.\n\n10. Monitor electricity usage regularly: Keep track of electricity usage on a regular basis, and make necessary adjustments to the energy-saving plan based on the results."}]}, {"conversation_id": "92", "flagged_count": 1, "turns": [{"index": 276, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 277, "is_flagged": true, "metric_value": "10.79", "role": "user", "text": "Use the following pieces of context to answer the question at the end. If you don\u0027t know the answer, just say that you don\u0027t know, don\u0027t try to make up an answer.\n\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500\u2013$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware. \n\nQuestion: What are the uses of a dye sublimation printer?"}, {"index": 278, "is_flagged": false, "role": "assistant", "text": "Dye sublimation printers are used for industrial or high-end commercial printing, medical imaging, graphic arts proofing, security, broadcast-related applications, event photography, and photo booths or kiosks that require high-speed, on-demand printing. They can produce instant photo prints inexpensively and are popular for ID photography with a card printer. Desktop-size standalone dye-sub photo printers are also used by photographers in event photography, allowing them to produce and sell lab-quality prints immediately during the event they are attending with minimal hardware."}]}, {"conversation_id": "100", "flagged_count": 1, "turns": [{"index": 300, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 301, "is_flagged": true, "metric_value": "10.87", "role": "user", "text": "Use the following pieces of context to answer the question at the end. If you don\u0027t know the answer, just say that you don\u0027t know, don\u0027t try to make up an answer.\n\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500\u2013$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware. \n\nQuestion: Is the Epson F7100 a dye sub printer?"}, {"index": 302, "is_flagged": false, "role": "assistant", "text": "I\u0027m sorry, but I don\u0027t have enough contextual information about the Epson F7100 to answer that question."}]}, {"conversation_id": "134", "flagged_count": 1, "turns": [{"index": 402, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 403, "is_flagged": true, "metric_value": "11.41", "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\nPlease come up with a question/answer pair, in the specified JSON format, for the following text:\n----------------\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500\u2013$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware."}, {"index": 404, "is_flagged": false, "role": "assistant", "text": "{\n    \"question\": \"What are some of the applications of dye-sub photo printing?\",\n    \"answer\": \"Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications.\"\n}"}]}, {"conversation_id": "155", "flagged_count": 1, "turns": [{"index": 465, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 466, "is_flagged": true, "metric_value": "10.39", "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\nPlease come up with a question/answer pair, in the specified JSON format, for the following text:\n----------------\nThe scream of sirens and the clank of steel on cobblestones echoed down the mean, dark streets. Frightened East Berliners peeked from behind their curtains to see military convoys stretching for blocks. First came the motorcycle outriders, then jeeps, trucks and buses crammed with grim, steel-helmeted East German troops. Rattling in their wake were the tanks \u2014 squat Russian-built T-34s and T-54s. At each major intersection, a platoon peeled off and ground to a halt, guns at the ready. The rest headed on for the sector border, the 25-mile frontier that cuts through the heart of Berlin like a jagged piece of glass. As the troops arrived at scores of border points, cargo trucks were already unloading rolls of barbed wire, concrete posts, wooden horses, stone blocks, picks and shovels. When dawn came four hours later, a wall divided East Berlin from West for the first time in eight years."}, {"index": 467, "is_flagged": false, "role": "assistant", "text": "{\n    \"question\": \"What did the troops use to block off the border between East Berlin and West Berlin?\",\n    \"answer\": \"Cargo trucks were used to unload rolls of barbed wire, concrete posts, wooden horses, stone blocks, picks and shovels to block off the border between East Berlin and West Berlin.\"\n}"}]}, {"conversation_id": "158", "flagged_count": 1, "turns": [{"index": 474, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 475, "is_flagged": false, "role": "user", "text": "Create a timeline describing the history of the internet"}, {"index": 476, "is_flagged": true, "metric_value": "10.67", "role": "assistant", "text": "1960s - The concept of wide-area computer networking emerged among computer scientists in the United States.\n\n1969 - ARPANET, the first operational data network, was launched by the Advanced Research Projects Agency (ARPA).\n\n1971 - Ray Tomlinson invented email, allowing messages to be sent from one computer to another.\n\n1972 - Vint Cerf and Robert Kahn designed the Transmission Control Protocol (TCP) and Internet Protocol (IP), the fundamental communication protocols at the heart of the Internet.\n\n1974 - A proposal for the creation of the Internet was published, establishing the concept of packet switching as the fundamental technology for data transmission.\n\n1983 - ARPANET split into two separate networks, MILNET and the new ARPANET, to accommodate the increasing number of military and civilian applications.\n\n1989 - Tim Berners-Lee proposed the World Wide Web, a system designed to make it easier for scientists to share information across the Internet.\n\n1990 - ARPANET was decommissioned and the World Wide Web was launched.\n\n1991 - The first website, info.cern.ch, went online.\n\n1993 - The release of the Mosaic web browser made the web accessible to a much wider audience.\n\n1994 - E-commerce was born with the founding of Amazon and eBay.\n\n1995 - The dot-com boom began, and internet startups received massive investments.\n\n2000 - The dot-com bubble burst, but the era of high-speed broadband internet access began.\n\n2004 - The rise of social networking began with the launch of Facebook.\n\n2005 - YouTube was launched, ushering in the era of online video sharing.\n\n2007 - The launch of the iPhone marked a watershed moment in mobile internet access.\n\n2010 - The concept of cloud computing became mainstream with the rise of services like Google Drive and Dropbox.\n\nToday, the internet continues to evolve and expand, becoming an ever-more integral part of our lives, communication, and business."}]}, {"conversation_id": "307", "flagged_count": 1, "turns": [{"index": 921, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 922, "is_flagged": false, "role": "user", "text": "Analyze the given film and explain why it should be included in the top 10 list.\n\n### Input:\nThe film Parasite"}, {"index": 923, "is_flagged": true, "metric_value": "10.17", "role": "assistant", "text": "Parasite, directed by Bong Joon-ho, is an exceptional film that deserves to be included in the top 10 list. The film tells the story of a lower-class family who connives their way into working for a wealthy family. It explores themes of class conflict and social inequality and offers a biting satire of contemporary society.\n\nOne reason why Parasite should be in the top 10 list is its brilliant script. The film masterfully combines elements of different genres \u2013 it\u0027s part drama, part comedy, and part thriller. The screenplay is full of twists and surprises and keeps the audience on edge from start to finish.\n\nAnother reason why Parasite stands out is its incredible ensemble cast. The performances are universally strong and the actors fully embody their characters. The film is both funny and heartbreaking, thanks in large part to the actors\u0027 ability to convey complex emotions.\n\nParasite is also visually stunning. The film boasts gorgeous cinematography, with each shot carefully composed and lit. The production design is impeccable, with the contrasting worlds of the rich and the poor vividly brought to life.\n\nFinally, Parasite should be included in the top 10 list because of its cultural significance. The film won the Palme d\u0027Or at the Cannes Film Festival and went on to win four Oscars, including Best Picture. It has been widely acclaimed by critics and audiences alike, and its success has paved the way for more diverse stories to be told on the global stage.\n\nOverall, Parasite is a masterful film that combines razor-sharp social commentary with thrilling storytelling. It is not only highly entertaining, but it also challenges and provokes its audience in all the best ways. For these reasons, it should undoubtedly be included in the top 10 list."}]}, {"conversation_id": "308", "flagged_count": 1, "turns": [{"index": 924, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 925, "is_flagged": false, "role": "user", "text": "Explain the implications of artificial intelligence in health care."}, {"index": 926, "is_flagged": true, "metric_value": "11.32", "role": "assistant", "text": "Artificial intelligence (AI) is rapidly transforming the field of healthcare, bringing about a vast array of implications for patients, medical professionals and the industry as a whole. AI techniques including machine learning, natural language processing, and computer vision, are empowering numerous innovations that could have far-reaching impacts on patient care and medical research. Some of these implications include:\n\n1. Improved accuracy in diagnosis: AI has the ability to analyze large sets of patient data alongside a patient\u0027s symptoms, medical history and test results to provide improved diagnosis, reducing the likelihood of misdiagnosis or delayed diagnosis and leading to better patient outcomes.\n\n2. Enhanced treatment plans: Through big data analysis, AI can assess patient-specific information to propose more personalized treatment plans. This approach could be far more effective than the \"one-size-fits-all\" model of care delivery often used today, as it is tailored to the individual needs of each patient.\n\n3. Increased efficiency: AI can help automate routine tasks such as data entry, appointment scheduling, and the analysis of medical scans, freeing up more time for medical professionals to focus on patient care. AI-powered chatbots and virtual assistants can also take on the role of triage nurses, helping to answer patient queries and providing personalized health advice.\n\n4. Advanced drug discovery: AI is being used to accelerate drug discovery, helping to identify new drug candidates much more quickly and cost-effectively than traditional methods. This could greatly reduce the time and resources required to bring new treatments to market, potentially enhancing the speed and quality of healthcare delivery.\n\n5. Predictive analytics: AI is being used to develop algorithms that can predict a patient\u0027s likelihood of suffering certain conditions, or their risk of being readmitted following a hospital stay, allowing for earlier intervention and better health outcomes.\n\nThese are just some of the many ways that AI is revolutionizing healthcare, potentially leading to more accurate diagnosis, enhanced treatment, and overall improved patient care. However, care must be taken to ensure that AI is developed and used ethically, and that the technology does not widen health disparities or reduce the human touch that is so important in medical care."}]}, {"conversation_id": "327", "flagged_count": 1, "turns": [{"index": 981, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 982, "is_flagged": false, "role": "user", "text": "Describe the effects of climate change."}, {"index": 983, "is_flagged": true, "metric_value": "10.53", "role": "assistant", "text": "Climate change is a broad term that refers to long-term changes in the Earth\u0027s climate system, including shifts in temperature, precipitation patterns, and weather events. Over the past century, human activities have led to an increase in greenhouse gases in the atmosphere, leading to a process known as global warming. Here are some of the key effects of climate change:\n\n1. Rising temperatures: One of the most significant effects of climate change is the increase in global temperatures. According to data from NASA, the average global temperature has increased by about 1 degree Celsius since the late 19th century, and most of this change has occurred over the past four decades.\n\n2. Changing precipitation patterns: Climate change is also affecting the amount, intensity, and distribution of precipitation. Some areas are experiencing more droughts, while others are seeing an increase in the frequency and intensity of heavy rainfall events.\n\n3. More frequent and intense weather events: Climate change is increasing the frequency and intensity of extreme weather events such as heatwaves, floods, and hurricanes.\n\n4. Shrinking ice caps and rising sea levels: The Earth\u0027s polar ice caps are melting at an alarming rate due to rising temperatures. As a result, sea levels are rising, and this poses a threat to coastal cities and small island nations.\n\n5. Impacts on ecosystems and wildlife: Changes in climate are affecting the natural habitats of many species, forcing them to adapt, migrate, or risk extinction. Coral reefs, for example, are highly sensitive to changes in temperature and are experiencing mass bleaching events, which can result in the loss of entire ecosystems.\n\n6. Effects on human health: Climate change is also having an impact on human health. The increase in heatwaves, for example, can cause heat stress and dehydration, while the worsening air quality can lead to respiratory problems.\n\nThese are just a few examples of the many effects of climate change. This complex and far-reaching issue poses significant challenges for the future, and it is important that we take action to mitigate its impact before it\u0027s too late."}]}, {"conversation_id": "346", "flagged_count": 1, "turns": [{"index": 1038, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1039, "is_flagged": false, "role": "user", "text": "Write a Satire article about the coronavirus pandemic"}, {"index": 1040, "is_flagged": true, "metric_value": "11.61", "role": "assistant", "text": "It is the year 2021 and the world is thriving in this new era of the Coronavirus pandemic. The streets are empty, masks have become a fashion statement and the term \u2018social distancing\u2019 is the new cool. Who would have thought that staying six feet apart and avoiding close contact would become a trend?\n\nWell, let\u2019s say we saw this coming. All those sci-fi movies predicting global pandemics have indeed come true. After all, what\u2019s not to love about people hoarding toilet paper like there\u2019s no tomorrow, working in sweatpants (without actually breaking a sweat), and spending quality time with family (which we were dying to get rid of).\n\nThe economic crisis? A thing of the past! Governments are now offering stimulus checks to their citizens to help them through these trying times. Of course, the fact that the money is coming from their own pockets doesn\u2019t matter. It\u2019s the thought that counts, right? Plus, there\u2019s no better time to go bankrupt than during a global pandemic.\n\nLet\u2019s not forget the creative minds who turned quarantine into an opportunity. From the Instagram bakers to the TikTok dancers, the arts are thriving. Why bother going to the theatre or the museum when you can have all the entertainment in the palm of your hand?\n\nAnd yes, some might argue that precious lives have been lost and countless people have suffered due to this pandemic, but let\u2019s focus on the positive. Pollution levels have gone down, wild animals can roam free, and for the first time in forever, humanity is united in a common cause. Just like that movie Contagion. How exciting!\n\nSo, let\u0027s raise a glass (from a safe distance) and toast to the Coronavirus pandemic. May it continue to bless our lives with unexpected twists and turns. After all, who needs normalcy when you have a global health crisis?"}]}, {"conversation_id": "403", "flagged_count": 1, "turns": [{"index": 1209, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1210, "is_flagged": false, "role": "user", "text": "Explain the uses of artificial intelligence."}, {"index": 1211, "is_flagged": true, "metric_value": "10.13", "role": "assistant", "text": "Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to act, think, and learn like humans. AI has made a profound impact on various industries and is rapidly changing the way we interact with technology. There are numerous uses and applications of AI across multiple fields, including:\n\n1. Healthcare: AI is helping to improve diagnostics, patient care, and drug development, reducing the time and cost while increasing the effectiveness of treatment.\n\n2. Transportation: With the help of AI, self-driving cars, trucks, and drones are becoming a reality, revolutionizing the way goods and people are transported.\n\n3. Business: AI is being used in business to automate processes, improve decision-making, enhance customer service, and personalize marketing campaigns.\n\n4. Finance: AI is used in the finance industry to detect fraud, manage risk, and make investment decisions.\n\n5. Education: AI is being used in education to personalize learning and help students learn at their own pace.\n\n6. Retail: AI is transforming the retail industry by predicting consumer behavior, customizing the shopping experience, and managing inventory.\n\n7. Manufacturing: AI is being used in manufacturing to increase efficiency, minimize downtime, and reduce costs.\n\n8. Agriculture: AI is being used in agriculture to optimize crop growth, predict weather patterns, and automate tasks.\n\n9. Telecommunications: AI is being used in telecommunications to improve network optimization, predict equipment failure, and personalize customer service.\n\n10. Entertainment: AI is being used in the entertainment industry to create personalized recommendations, generate content, and enhance the gaming experience.\n\nIn conclusion, AI is changing the way we live, work, and interact with technology. Its uses are numerous and varied, and its potential is almost limitless, with new applications being discovered all the time."}]}, {"conversation_id": "415", "flagged_count": 1, "turns": [{"index": 1245, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1246, "is_flagged": false, "role": "user", "text": "Generate a list of fashion trends for Spring 2021."}, {"index": 1247, "is_flagged": true, "metric_value": "11.12", "role": "assistant", "text": "1. Oversized Boyfriend Jackets: Pairing this trend with tailored pieces, such as trousers or a structured skirt, would give a chic well put-together look.\n\n2. Black Face Masks: No surprise here that face masks make it to the list. Black masks serve as a sleek and stylish alternative to the other disposable options - they are easy to coordinate with outfits.\n\n3. Head Scarfs: Make your average hair updo interesting with the retro silk headscarf look. There are countless Youtube tutorials available to flawlessly tie this into any hair type.\n\n4. Sorbet Pastel Tones: An assortment of sorbet pastel tones is fashionable this spring as these colors radiate positivity and are soothing to the eyes.\n\n5. Yellow Bags: Accessories are a great way to add a splash of color to your outfit. A yellow bag inspires optimism and enthusiasm for life, plus, it brightens up any outfit.\n\n6. Folk Inspired Coats: Intricate embroidery and exaggerated fringes in folk-inspired coats add dimension and character to any outfit.\n\n7. White Knee High Boots: Oh so retro, yet oh so chic, white knee-high boots are in and never fail to make a statement.\n\n8. Pop Blue Accessories: Even though blue is commonly associated with the winter season, this spring, you\u0027ll see a pop of electric blue accessories trending. \n\n9. Fringing on Bags: Look no further for the most playful trend of the season! Movement in an outfit is just the best when paired with a fringe bag.\n\n10. Wide Leg Trousers: Say goodbye to tight bottoms and hello to full comfort in wide leg trousers. Pair these trousers with a fitted top to even out the proportions."}]}, {"conversation_id": "454", "flagged_count": 1, "turns": [{"index": 1362, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1363, "is_flagged": false, "role": "user", "text": "Brainstorm creative ideas for designing a conference room."}, {"index": 1364, "is_flagged": true, "metric_value": "10.22", "role": "assistant", "text": "1. Incorporating nature: Having green walls or a small indoor garden can help create a calm and inviting atmosphere for meetings and conferences.\n2. High-tech features: Installing smart boards, touch screens, and video conferencing systems can facilitate better communication and collaboration among participants.\n3. Comfortable and ergonomic seating: Using adjustable chairs and tables can provide a comfortable setting for long meetings.\n4. Adequate lighting: Natural light is ideal, but if that\u0027s not possible, installing ambient lighting that can be adjusted to the time of day or mood can help keep attendees focused and alert.\n5. Colorful and themed decor: Adding some color and an attractive theme can create a vibrant and engaging atmosphere for brainstorming and discussion.\n6. Soundproofing: Ensuring the conference room is well soundproofed can prevent interruptions and distractions from outside noise.\n7. Small breakout areas: Having smaller areas within the conference room where participants can have one-on-one discussions or work in smaller groups can promote better communication and collaboration.\n8. Refreshment station: Providing a small area for refreshments can keep attendees hydrated, energized, and focused throughout the meeting.\n9. Artwork: Displaying artwork or inspirational quotes on the walls can spark creativity and provide visual interest.\n10. Flexibility: Using modular furniture and movable partitions can allow for the conference room to be rearranged to suit the needs of different meetings and events."}]}, {"conversation_id": "465", "flagged_count": 1, "turns": [{"index": 1395, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1396, "is_flagged": false, "role": "user", "text": "Propose a transportation solution for a city with rapid population growth."}, {"index": 1397, "is_flagged": true, "metric_value": "10.16", "role": "assistant", "text": "As an AI assistant, I can recommend several transportation solutions for a city experiencing rapid population growth.\n\n1. Expand public transit: One of the best ways to manage an urban population boom is to expand the city\u0027s public transit system with additional buses, subway lines or light rail systems. This will reduce the number of cars on the roads and help to ease traffic congestion.\n\n2. Promote carpooling and ride-sharing: Encouraging carpooling and ride-sharing can significantly reduce the number of cars on the road. Many cities now have specific carpool lanes that provide incentives for people to share their rides.\n\n3. Implement smart traffic management systems: A smart traffic management system uses real-time data and analytics to optimize traffic flow and reduce congestion. This might include altering traffic signal timings, providing real-time traffic information to motorists, or redirecting traffic to less congested routes.\n\n4. Develop bike-sharing programs: Bike-sharing programs are becoming increasingly popular in urban areas. These programs allow people to quickly and easily rent a bike for short trips, providing an alternative mode of transportation that is both eco-friendly and affordable.\n\n5. Encourage telecommuting and flexible work schedules: Encouraging remote working and flexible schedules can reduce the number of people commuting during peak hours. This can help to ease traffic congestion and reduce air pollution.\n\nTo sum up, a multimodal approach that combines various transportation strategies is crucial in addressing the transportation needs of a rapidly growing city. With the right solutions and incentives in place, it is possible to ensure that all residents have access to safe, efficient, and affordable transportation options."}]}, {"conversation_id": "527", "flagged_count": 1, "turns": [{"index": 1581, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1582, "is_flagged": false, "role": "user", "text": "Outline the events of the following movie: The Great Gatsby."}, {"index": 1583, "is_flagged": true, "metric_value": "11.47", "role": "assistant", "text": "The Great Gatsby is a 2013 movie adaptation of the F. Scott Fitzgerald novel of the same name, directed by Baz Luhrmann. The movie follows the events of the book, which is set in the year 1922 in Long Island, New York.\n\nThe movie is told from the perspective of Nick Carraway (Tobey Maguire) who narrates the story. Nick, who moves to a small house in West Egg, Long Island, next to the mansion of the mysterious millionaire Jay Gatsby (Leonardo DiCaprio).\n\nNick\u0027s cousin, Daisy Buchanan (Carey Mulligan), lives across the bay in the more fashionable East Egg, with her husband, Tom (Joel Edgerton), a wealthy aristocrat. Nick soon learns that Tom is having an affair with Myrtle Wilson (Isla Fisher), who is married to struggling garage owner, George (Jason Clarke).\n\nGatsby befriends Nick and invites him to one of his lavish parties, where he meets the enigmatic and beautiful Jordan Baker (Elizabeth Debicki), with whom he soon becomes romantically involved. Gatsby confides in Nick that he\u0027s deeply in love with Daisy, and has been for years, despite her now being married to Tom.\n\nGatsby and Daisy eventually reunite and start an affair, with Gatsby revealing to her his past; he was originally poor and had made his wealth through his association with dishonest bootleggers. Despite this, Daisy is unable to leave her husband.\n\nTensions boil over during a heated confrontation between Tom, Gatsby, Daisy and Jordan at the Plaza Hotel. Daisy admits she loved Gatsby, but confesses she still loves her husband. Tom, in turn, tells Gatsby he\u0027s done research on his past and Gatsby is revealed to be a fraud.\n\nDevastated, Gatsby and Daisy drive off in the car, with Daisy at the wheel, and accidentally hits and kills Myrtle. Tom tells George that Gatsby was driving the car, and George, assuming reckless driving and that Gatsby is Myrtle\u0027s lover, ends up shooting and killing Gatsby before turning the gun on himself.\n\nNick is left in charge of Gatsby\u0027s funeral but very few people attend, reflecting the emptiness of Gatsby\u0027s life. Nick leaves, disillusioned with the 1920s lifestyle and writes his story about his summer with Gatsby, a man who had everything, and yet had nothing."}]}, {"conversation_id": "539", "flagged_count": 1, "turns": [{"index": 1617, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1618, "is_flagged": false, "role": "user", "text": "Put together a business plan for a new restaurant."}, {"index": 1619, "is_flagged": true, "metric_value": "10.12", "role": "assistant", "text": "Executive Summary:\n\nOur new restaurant will be a unique dining establishment that combines delectable cuisine, efficient service, and an inspiring atmosphere. The goal of our restaurant is to provide our customers with an unforgettable dining experience that will delight their senses.\n\nCompany Description:\n\nOur restauranwill be a limited liability company, owned and operated by a team of experienced industry professionals. Our management team has extensive experience in the restaurant and hospitality industry, and we are committed to creating a special dining experience for our customers.\n\nMarket Analysis:\n\nThe restaurant industry is highly competitive, and our market analysis has identified several opportunities that we can leverage to differentiate ourselves from other establishments. We have chosen an up-and-coming neighborhood for our location, which will appeal to trendy, young professionals looking for a unique dining experience. Our target market is people who value quality food, excellent service, and an inspiring atmosphere.\n\nMenu and Services:\n\nOur menu will consist of a wide variety of dishes that are inspired by flavors from around the world. Our dishes will be made from the freshest ingredients and prepared by our skilled chefs. We will offer efficient service in a friendly and professional manner. In addition, we will provide additional services such as catering and special events to further enhance our customer\u0027s experience.\n\nMarketing and Advertising:\n\nWe will implement a comprehensive marketing and advertising campaign to generate awareness and interest in our restaurant. Our campaign will consist of targeted online ads, social media promotions, and partnerships with local businesses. We will also generate word-of-mouth by providing our customers with an exceptional dining experience.\n\nFinancial Projections:\n\nWe project that our restaurant will generate strong revenues and profitability within its first year of operation. Our financial projections include detailed assumptions about revenue growth, operating expenses, and capital expenditures. We will carefully manage our cash flow and finances to ensure that our restaurant is sustainable and profitable.\n\nConclusion:\n\nOur new restaurant is a unique and exciting concept that combines delicious cuisine, efficient service, and an inspiring atmosphere. We are confident that our business plan provides a solid foundation for the successful launch and growth of our restaurant. We look forward to serving our customers and providing them with an unforgettable dining experience."}]}, {"conversation_id": "540", "flagged_count": 1, "turns": [{"index": 1620, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1621, "is_flagged": false, "role": "user", "text": "Devise a strategic plan for a fashion brand to increase its online presence."}, {"index": 1622, "is_flagged": true, "metric_value": "10.66", "role": "assistant", "text": "In today\u0027s digital era, having a prominent online presence is crucial for any fashion brand. Following is a strategic plan that could help a fashion brand bolster its online presence: \n\n1. **Revamp website:** The website is a direct representation of the brand and acts as the epicenter of all online activities. Start by giving a facelift to the website with visually appealing graphics and make it more user-friendly with easy navigation and quick load times. Ensure that the website is optimized for mobile devices.\n\n2. **Leverage social media:** Social media platforms like Instagram and Facebook are powerful tools for fashion brands to showcase their collection and connect with their audience. Share high-quality images and videos of the latest collection and behind-the-scenes content to keep the audience engaged. Collaborate with influencers and social media personalities to expand the brand\u0027s reach.\n\n3. **Improve search engine rankings:** Making it to the top of the search engine results page is crucial to increase the brand\u0027s visibility. Optimize the website for search engines by incorporating relevant keywords, creating high-quality content, and building backlinks from reputed websites.\n\n4. **Content marketing:** Content marketing is an effective way to reach and engage with the target audience. Create a blog section on the website and share informative and interesting articles about the latest fashion trends, styling tips, and news. Share the content on social media and other online platforms to drive traffic to the website.\n\n5. **Email marketing:** Email marketing is a great way to keep the customers engaged and informed about the latest collection, sales, and promotions. Build an email list and send regular newsletters and promotional emails.\n\n6. **Paid advertising:** Invest in paid online advertising like Google AdWords, Facebook Ads, and Instagram Ads to reach a larger audience. These platforms allow precise targeting, helping the brand reach its ideal audience.\n\n7. **Analyze and optimize:** Monitor the analytics to measure the success of the online marketing efforts. Identify the strategies that are working and the ones that need improvement. Regularly optimize the strategies to ensure the best results.\n\nIn conclusion, by implementing these strategies, a fashion brand can significantly increase its online presence and attract more customers."}]}, {"conversation_id": "543", "flagged_count": 1, "turns": [{"index": 1629, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1630, "is_flagged": true, "metric_value": "10.31", "role": "user", "text": "Given the following context, answer the question at the end. If you do not know the answer, just say you don\u0027t know the answer given the context. Do not make up an answer.\n\nWhat is Roll-to-Roll Printing? \nWhat a fantastic question. This printing system is where you utilize a full roll of media (DTF film), rather than a single sheet of media, that feeds onto another empty roll at the end of the system. It is a really efficient system if you need to print constantly. It\u2019s almost a set-and-forget type of printer that will print until you run out of media or ink, which is already faster than single feed sheets that cannot be stacked.\nWhen compared to standard printing methods, the process improves printer life cycle costs while also making it easy to scale up your operation. Efficiency is also maximized when manufacturing requires multiple prints. A well-designed roll-to-roll system can make manufacturing quicker and cheaper without compromising quality. \n\nWith individual sheets your prints are limited by the size of the media and if you have a small format printer the prints are even smaller. The only limitation on size for roll printers is, again the size of the media. So instead of a max length of 20 inches for a single sheet we can make a print that is 328 feet long (almost a full football field from post to post). \n\nWe\u2019ve released and promoted several different machines which already utilize this system, like the 24\u201d STS 628D and Prestige XL2 and the large format 1682D. However, there is nothing available for the smaller format printing (under 24\u201d) and this leaves either smaller shops or the hobbyist with not many options under $10,000.\n\nQuestion: What is the price of the Prestige XL2?"}, {"index": 1631, "is_flagged": false, "role": "assistant", "text": "The context does not provide information about the price of the Prestige XL2."}]}, {"conversation_id": "625", "flagged_count": 1, "turns": [{"index": 1875, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1876, "is_flagged": false, "role": "user", "text": "Write a story about a doctor and his patient"}, {"index": 1877, "is_flagged": true, "metric_value": "10.87", "role": "assistant", "text": "Dr. Richard was a prominent neurosurgeon, known for his remarkable skills and ability to turn difficult cases into success stories. He had a long list of patients who had been successfully treated by him and were now living normal lives. One day, he was assigned a new patient, Emily, who had been in a terrible car accident and was suffering from a severe head injury.\n\nEmily was a young and vibrant woman in her mid-twenties. She had been in a coma for two weeks following the accident, and her family was desperate for her to wake up. When Dr. Richard first saw her, he could see the pain and anguish in the eyes of her family members. He could feel their hope, and he knew that he had to do everything in his power to help Emily.\n\nDr. Richard went through Emily\u0027s medical records and ordered a series of tests to understand the extent of her injuries. The results showed that she had severe swelling in her brain, which was causing pressure on the brainstem. Dr. Richard knew that this was a critical condition, which required immediate surgery.\n\nHe explained the situation to Emily\u0027s family and, with their consent, proceeded with the surgery. It was a long and complicated procedure, but Dr. Richard\u0027s skillful hands worked tirelessly to remove the damaged tissue and relieve the pressure on Emily\u0027s brain.\n\nWhen the surgery was over, Emily\u0027s family waited anxiously outside the operating room. Dr. Richard emerged several hours later, looking exhausted but relieved. He told them that the surgery had gone well, and Emily was now in stable condition.\n\nOver the next few days, Emily started showing signs of improvement. She finally woke up from her coma and slowly began to regain her strength. Dr. Richard visited her every day, monitoring her progress and offering her words of encouragement.\n\nAs the days went by, Emily\u0027s condition improved dramatically. She was soon able to walk and talk again, and her memory started coming back. Her family was overjoyed, and they were grateful to Dr. Richard for his dedication and skill in saving their beloved daughter\u0027s life.\n\nDr. Richard\u0027s relationship with Emily didn\u0027t end when she was discharged from the hospital. He continued to monitor her progress, and she became one of his success stories - a testament to his skill as a neurosurgeon. Emily\u0027s family would always remember him as the doctor who gave them their daughter back, and Dr. Richard would always remember Emily as the patient who reminded him why he"}]}]
            },
            
            'rec_3': {
                title: "Outliers detected in training quality instruction word count",
                conversations: [{"conversation_id": "5", "flagged_count": 1, "turns": [{"index": 15, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 16, "is_flagged": true, "metric_value": "125.00", "role": "user", "text": "Write a concise summary of the following:\n\"Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. \n\nAdditionally, the Commodore 64 dominated the market with between 30% and 40% share and 2 million units sold per year, outselling the IBM PC clones, Apple Computers, and Atari computers. Adding to their success, Sam Tramiel (former Atari president), during an interview in 1989, said they were building 400,000 C64s a month for a couple of years. \""}, {"index": 17, "is_flagged": false, "role": "assistant", "text": "The Commodore 64 was a highly successful 8-bit home computer manufactured by Commodore Business Machine (CBM) in 1982, with sales amounting to approximately 17 million units sold between 1983-1986. It dominated the market with between 30% and 40% share and outsold its competitors, including IBM PC clones, Apple Computers, and Atari computers. At its peak, CBM was building 400,000 C64s a month for a couple of years."}]}, {"conversation_id": "38", "flagged_count": 1, "turns": [{"index": 114, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 115, "is_flagged": true, "metric_value": "126.00", "role": "user", "text": "Use the following pieces of context to answer the question at the end. If you don\u0027t know the answer, just say that you don\u0027t know, don\u0027t try to make up an answer.\n\nFounded in the 13th century, Berlin has had an eventful history. Excavations from 2008 suggest that the city may be even older than was previously assumed: state archaeologists have discovered an oak beam that probably dates back to 1183.\n\nAlmost no other metropolis has experienced such frequent, radical change transforming the face of the city. Although Berlin saw steady growth in its importance, dazzling epochs alternated with darker eras. Nevertheless, the formerly divided city has succeeded in becoming a vibrant metropolis in the heart of Europe.\n\nQuestion: What is the best restaurant in Berlin?"}, {"index": 116, "is_flagged": false, "role": "assistant", "text": "I\u0027m sorry, I cannot answer that question as the context given does not provide information about the best restaurant in Berlin."}]}, {"conversation_id": "51", "flagged_count": 1, "turns": [{"index": 153, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 154, "is_flagged": true, "metric_value": "126.00", "role": "user", "text": "Use the following pieces of context to answer the question at the end. If you don\u0027t know the answer, just say that you don\u0027t know, don\u0027t try to make up an answer.\n\nFounded in the 13th century, Berlin has had an eventful history. Excavations from 2008 suggest that the city may be even older than was previously assumed: state archaeologists have discovered an oak beam that probably dates back to 1183.\n\nAlmost no other metropolis has experienced such frequent, radical change transforming the face of the city. Although Berlin saw steady growth in its importance, dazzling epochs alternated with darker eras. Nevertheless, the formerly divided city has succeeded in becoming a vibrant metropolis in the heart of Europe.\n\nQuestion: What is the best restaurant in Berlin?"}, {"index": 155, "is_flagged": false, "role": "assistant", "text": "Berlin was founded in the 13th century."}]}, {"conversation_id": "86", "flagged_count": 1, "turns": [{"index": 258, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 259, "is_flagged": true, "metric_value": "126.00", "role": "user", "text": "Use the following pieces of context to answer the question at the end. If you don\u0027t know the answer, just say that you don\u0027t know, don\u0027t try to make up an answer.\n\nFounded in the 13th century, Berlin has had an eventful history. Excavations from 2008 suggest that the city may be even older than was previously assumed: state archaeologists have discovered an oak beam that probably dates back to 1183.\n\nAlmost no other metropolis has experienced such frequent, radical change transforming the face of the city. Although Berlin saw steady growth in its importance, dazzling epochs alternated with darker eras. Nevertheless, the formerly divided city has succeeded in becoming a vibrant metropolis in the heart of Europe.\n\nQuestion: What events occurred in 20th century Berlin?"}, {"index": 260, "is_flagged": false, "role": "assistant", "text": "The context provided does not provide specific information about the events that occurred in 20th century Berlin. However, as Berlin has had an eventful history, it is known that the city has undergone frequent and radical changes throughout its history, including in the 20th century, which saw both bright and dark periods."}]}, {"conversation_id": "92", "flagged_count": 1, "turns": [{"index": 276, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 277, "is_flagged": true, "metric_value": "275.00", "role": "user", "text": "Use the following pieces of context to answer the question at the end. If you don\u0027t know the answer, just say that you don\u0027t know, don\u0027t try to make up an answer.\n\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500\u2013$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware. \n\nQuestion: What are the uses of a dye sublimation printer?"}, {"index": 278, "is_flagged": false, "role": "assistant", "text": "Dye sublimation printers are used for industrial or high-end commercial printing, medical imaging, graphic arts proofing, security, broadcast-related applications, event photography, and photo booths or kiosks that require high-speed, on-demand printing. They can produce instant photo prints inexpensively and are popular for ID photography with a card printer. Desktop-size standalone dye-sub photo printers are also used by photographers in event photography, allowing them to produce and sell lab-quality prints immediately during the event they are attending with minimal hardware."}]}, {"conversation_id": "100", "flagged_count": 1, "turns": [{"index": 300, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 301, "is_flagged": true, "metric_value": "274.00", "role": "user", "text": "Use the following pieces of context to answer the question at the end. If you don\u0027t know the answer, just say that you don\u0027t know, don\u0027t try to make up an answer.\n\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500\u2013$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware. \n\nQuestion: Is the Epson F7100 a dye sub printer?"}, {"index": 302, "is_flagged": false, "role": "assistant", "text": "I\u0027m sorry, but I don\u0027t have enough contextual information about the Epson F7100 to answer that question."}]}, {"conversation_id": "134", "flagged_count": 1, "turns": [{"index": 402, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 403, "is_flagged": true, "metric_value": "323.00", "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\nPlease come up with a question/answer pair, in the specified JSON format, for the following text:\n----------------\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500\u2013$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware."}, {"index": 404, "is_flagged": false, "role": "assistant", "text": "{\n    \"question\": \"What are some of the applications of dye-sub photo printing?\",\n    \"answer\": \"Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications.\"\n}"}]}, {"conversation_id": "155", "flagged_count": 1, "turns": [{"index": 465, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 466, "is_flagged": true, "metric_value": "240.00", "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\nPlease come up with a question/answer pair, in the specified JSON format, for the following text:\n----------------\nThe scream of sirens and the clank of steel on cobblestones echoed down the mean, dark streets. Frightened East Berliners peeked from behind their curtains to see military convoys stretching for blocks. First came the motorcycle outriders, then jeeps, trucks and buses crammed with grim, steel-helmeted East German troops. Rattling in their wake were the tanks \u2014 squat Russian-built T-34s and T-54s. At each major intersection, a platoon peeled off and ground to a halt, guns at the ready. The rest headed on for the sector border, the 25-mile frontier that cuts through the heart of Berlin like a jagged piece of glass. As the troops arrived at scores of border points, cargo trucks were already unloading rolls of barbed wire, concrete posts, wooden horses, stone blocks, picks and shovels. When dawn came four hours later, a wall divided East Berlin from West for the first time in eight years."}, {"index": 467, "is_flagged": false, "role": "assistant", "text": "{\n    \"question\": \"What did the troops use to block off the border between East Berlin and West Berlin?\",\n    \"answer\": \"Cargo trucks were used to unload rolls of barbed wire, concrete posts, wooden horses, stone blocks, picks and shovels to block off the border between East Berlin and West Berlin.\"\n}"}]}, {"conversation_id": "160", "flagged_count": 1, "turns": [{"index": 480, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 481, "is_flagged": true, "metric_value": "221.00", "role": "user", "text": "Use the following pieces of context to answer the question at the end. If you don\u0027t know the answer, just say that you don\u0027t know, don\u0027t try to make up an answer.\n\nA state actor\u0027s involvement in the blast of the Nord Stream pipelines last year is the \"absolute main scenario\", though confirming identity will prove difficult, the Swedish prosecutor investigating the attack said on Thursday.\n\nIn September 2022, several unexplained underwater explosions ruptured the Nord Stream 1 and newly-built Nord Stream 2 pipelines that link Russia and Germany across the Baltic Sea. The blasts occurred in the economic zones of Sweden and Denmark and both countries say the explosions were deliberate, but have yet to determine who was responsible.\n\nThe blast in the Swedish zone occurred at a depth of 80 meters, which the Swedish prosecutor said made it complicated to investigate.\n\n\"We believe it will be rather difficult to determine who did this,\" prosecutor Mats Ljungqvist told Reuters in a phone interview.\n\n\"The people who did this have probably been aware that they would leave clues behind and probably took care so that the evidence would not point in one direction, but in several directions,\" he added.\n\n\"That makes it difficult to clearly point to one actor.\"\n\nThe investigation continues.\n\nQuestion: Who was responsible for the Nordstream pipeline explosion?"}, {"index": 482, "is_flagged": false, "role": "assistant", "text": "It is currently unknown who was responsible for the Nord Stream pipeline explosion. Both Sweden and Denmark say the explosions were deliberate, but have yet to determine who was responsible. The Swedish prosecutor investigating the attack believes that a state actor\u0027s involvement is the \"absolute main scenario\", but confirming identity will prove difficult."}]}, {"conversation_id": "210", "flagged_count": 1, "turns": [{"index": 630, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 631, "is_flagged": true, "metric_value": "112.00", "role": "user", "text": "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\nChat History:\nUser: What\u0027s the best way to learn a new language?\nAI: The most effective way to learn a new language is by practicing regularly, immersing yourself in the language, using it in real-life situations, and studying with various resources like books, apps, and language partners.\nUser: How long does it typically take to become fluent?\nAI: That depends on several factors, including how often one is practicing the language, what resources are available and whether or not the person is immersed.\n\n### Input:\nHow can I practice speaking with native speakers?"}, {"index": 632, "is_flagged": false, "role": "assistant", "text": "Standalone question: What are some effective ways to practice speaking a foreign language with native speakers?"}]}, {"conversation_id": "516", "flagged_count": 1, "turns": [{"index": 1548, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 1549, "is_flagged": true, "metric_value": "105.00", "role": "user", "text": "Provide a summary for the passage given below.\n\n### Input:\nArtificial intelligence (AI) is a broad field that seeks to mimic or augment human capabilities through information processing technology and algorithms. AI has been an area of study since the 1950s, but has gained popular attention in recent years due to vast improvements in processing power. While some fear existential threats such as a Terminator-like hypothetical AI takeover, most of the practical applications of AI are in mundane areas such as image recognition, natural language processing, and task automation. AI is used to improve or automate mundane tasks, quickly identify patterns, and predict future events."}, {"index": 1550, "is_flagged": false, "role": "assistant", "text": "Artificial Intelligence (AI) is a broad field that tries to replicate or improve human abilities using technology and algorithms, and has garnered popularity recently due to increased processing power. Despite concerns of a futuristic AI takeover, most practical uses of AI are in everyday areas such as image recognition, natural language processing, and task automation, helping to enhance or automate tasks, quickly detect patterns, and forecast future events."}]}, {"conversation_id": "543", "flagged_count": 1, "turns": [{"index": 1629, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1630, "is_flagged": true, "metric_value": "295.00", "role": "user", "text": "Given the following context, answer the question at the end. If you do not know the answer, just say you don\u0027t know the answer given the context. Do not make up an answer.\n\nWhat is Roll-to-Roll Printing? \nWhat a fantastic question. This printing system is where you utilize a full roll of media (DTF film), rather than a single sheet of media, that feeds onto another empty roll at the end of the system. It is a really efficient system if you need to print constantly. It\u2019s almost a set-and-forget type of printer that will print until you run out of media or ink, which is already faster than single feed sheets that cannot be stacked.\nWhen compared to standard printing methods, the process improves printer life cycle costs while also making it easy to scale up your operation. Efficiency is also maximized when manufacturing requires multiple prints. A well-designed roll-to-roll system can make manufacturing quicker and cheaper without compromising quality. \n\nWith individual sheets your prints are limited by the size of the media and if you have a small format printer the prints are even smaller. The only limitation on size for roll printers is, again the size of the media. So instead of a max length of 20 inches for a single sheet we can make a print that is 328 feet long (almost a full football field from post to post). \n\nWe\u2019ve released and promoted several different machines which already utilize this system, like the 24\u201d STS 628D and Prestige XL2 and the large format 1682D. However, there is nothing available for the smaller format printing (under 24\u201d) and this leaves either smaller shops or the hobbyist with not many options under $10,000.\n\nQuestion: What is the price of the Prestige XL2?"}, {"index": 1631, "is_flagged": false, "role": "assistant", "text": "The context does not provide information about the price of the Prestige XL2."}]}, {"conversation_id": "713", "flagged_count": 1, "turns": [{"index": 2139, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2140, "is_flagged": true, "metric_value": "226.00", "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\n\"\"\"\ntempl2 = \"\"\"Please come up with a question/answer pair, in the specified JSON format, for the following text:\n----------------\nMillions of lives could be saved by a groundbreaking set of new vaccines for a range of conditions including cancer, experts have said. A leading pharmaceutical firm said it is confident that jabs for cancer, cardiovascular and autoimmune diseases, and other conditions will be ready by 2030.\n\nStudies into these vaccinations are also showing \u201ctremendous promise\u201d, with some researchers saying 15 years\u2019 worth of progress has been \u201cunspooled\u201d in 12 to 18 months thanks to the success of the Covid jab.\n\nDr Paul Burton, the chief medical officer of pharmaceutical company Moderna, said he believes the firm will be able to offer such treatments for \u201call sorts of disease areas\u201d in as little as five years.\n\nThe firm, which created a leading coronavirus vaccine, is developing cancer vaccines that target different tumor types."}, {"index": 2141, "is_flagged": false, "role": "assistant", "text": "{\n    \"question\": \"What is Moderna developing vaccines for?\",\n    \"answer\": \"Moderna is developing vaccines for cancer, cardiovascular and autoimmune diseases, and other conditions.\"\n}"}]}, {"conversation_id": "727", "flagged_count": 1, "turns": [{"index": 2181, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2182, "is_flagged": true, "metric_value": "250.00", "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\n\"\"\"\ntempl2 = \"\"\"Please come up with three question/answer pairs, in the specified JSON format, for the following text:\n----------------\nUS officials have hinted that this counteroffensive is coming in the next month or so, while Ukrainian forces on the ground have said they are ready to attack when Kyiv gives the word.\n\nAccording to military analysts who have looked at the documents, certain parts appear to be modified from their original format and exaggerate the American tally of Ukrainians killed in the war while downplaying the estimates of Russian troops killed.\n\nOne of the slides states that Russia has lost 16,000 to 17,500 soldiers since the invasion, while Ukraine had suffered as many as 71,500 troop deaths. This is a gross underestimate from what the Pentagon and other analysts have estimated for Russian casualties with nearly 200,000 killed and injured. Ukraine has had more than 100,000 casualties in the war, according to the Pentagon.\n\nThe documents appear to represent a significant breach of American intelligence, detailing information that has not been publicly stated by the Pentagon."}, {"index": 2183, "is_flagged": false, "role": "assistant", "text": "{\n    \"question\": \"What are the documents that military analysts have looked at?\",\n    \"answer\": \"The documents that military analysts have looked at appear to be modified and contain information about the tally of casualties in the Ukraine-Russia war.\"\n},\n{\n    \"question\": \"What does one of the slides in the documents state?\",\n    \"answer\": \"One of the slides in the documents states that Russia has lost 16,000 to 17,500 soldiers since the invasion, while Ukraine had suffered as many as 71,500 troop deaths.\"\n},\n{\n    \"question\": \"What does the article suggest about the documents?\",\n    \"answer\": \"The article suggests that the documents represent a significant breach of American intelligence, as they contain information that has not been publicly stated by the Pentagon.\"\n}"}]}, {"conversation_id": "741", "flagged_count": 1, "turns": [{"index": 2223, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 2224, "is_flagged": true, "metric_value": "139.00", "role": "user", "text": "Use the following context to answer the question at the end. If you don\u0027t know the answer, just say that you don\u0027t know, don\u0027t try to make up an answer.\n\n### Input:\nThe Epson SureColor machines keep getting smarter. Newly updated PrecisionCore Printheads that are 20% larger than the previous generation come standard with Nozzle Verification tech to stay on top of any clogs that can affect print quality and consistency. For those who may have run earlier models of the Epson large format dye sub printers, this is for you - NO MORE POURING INK INTO TANKS! The F6400 line utilizes an updated 1.6L airtight ink pack system with microchip and filtration tech, lessening the chance of dust, fibers or debris getting into the printer.\nQuestion: Does the Epson SureColor F6470 also have a PrecisionCore printhead?\nHelpful Answer:"}, {"index": 2225, "is_flagged": false, "role": "assistant", "text": "Yes, the Epson SureColor F6470 does have a PrecisionCore printhead. The context mentions the PrecisionCore printheads being updated and improved in the Epson SureColor line, and because the F6470 model is part of the SureColor line, it also has this feature."}]}, {"conversation_id": "788", "flagged_count": 1, "turns": [{"index": 2364, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 2365, "is_flagged": true, "metric_value": "106.00", "role": "user", "text": "Summarize the following text in one paragraph.\n\n### Input:\nTheories of personal identity have changed over time. Philosophers in the past have focused mainly on the self, whereas modern theorists focus more on personal narratives and narrative identities. One example is Locke\u0027s definition of personal identity which states that you are the same person over time if you have personal memories of your past or if you can remember past experiences. Another example is Parfit\u0027s rejection of this notion and focus on the importance of psychological continuity, which is when you remain the same even if you are presented with a different physical world or body."}, {"index": 2366, "is_flagged": false, "role": "assistant", "text": "Personal identity theories have evolved, shifting focus from the self to personal narratives and narrative identities. Locke defined personal identity as having memories of past experiences, while Parfit rejected this idea and emphasized the significance of psychological continuity, where one remains the same even in a different physical world or body."}]}, {"conversation_id": "958", "flagged_count": 1, "turns": [{"index": 2874, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 2875, "is_flagged": true, "metric_value": "135.00", "role": "user", "text": "Summarize the main ideas of the following text passage.\n\n### Input:\nIn today\u0027s interconnected and globalized world, effective communication skills are more important than ever. With the rise of the internet, social media, and video conferencing, people are interacting with others from around the globe on a daily basis. As a result, solid communication skills are a must-have for individuals who wish to succeed professionally and personally. Not only do they need to be able to express their thoughts and ideas clearly, but they also need to be able to listen and understand the perspectives of others. This is particularly important in fields like business, where cross-cultural communication is common. Approaching interactions with an open mind, empathy, and active listening practices can help bridge the gap between different cultures and facilitate better collaboration and understanding."}, {"index": 2876, "is_flagged": false, "role": "assistant", "text": "Effective communication skills are essential in our globalized world due to increased interactions through the internet, social media, and video conferencing. To succeed in both personal and professional life, individuals need clear expression of thoughts, active listening, and understanding of others, particularly in fields involving cross-cultural communication. Open-mindedness and empathy can improve collaboration and understanding among diverse cultures."}]}]
            },
            
            'rec_4': {
                title: "Inconsistent instruction formatting detected",
                conversations: []
            },
            
            'rec_5': {
                title: "Truncated responses detected",
                conversations: [{"conversation_id": "13", "flagged_count": 1, "turns": [{"index": 39, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 40, "is_flagged": false, "role": "user", "text": "Classify the following into animals, plants, and minerals\n\n### Input:\nOak tree, copper ore, elephant"}, {"index": 41, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "Animals: Elephant\nPlants: Oak tree\nMinerals: Copper ore"}]}, {"conversation_id": "18", "flagged_count": 1, "turns": [{"index": 54, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 55, "is_flagged": false, "role": "user", "text": "Generate three verbs that mean the same as \"to apologize\""}, {"index": 56, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "1. To express regret\n2. To atone\n3. To make amends"}]}, {"conversation_id": "21", "flagged_count": 1, "turns": [{"index": 63, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 64, "is_flagged": false, "role": "user", "text": "Generate a list of random words."}, {"index": 65, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "Here is a list of ten random words: \n\n1. Quibble\n2. Avarice\n3. Embellish\n4. Resonance\n5. Mercurial\n6. Conflagration\n7. Indefatigable\n8. Mischievous\n9. Delineate\n10. Paragon"}]}, {"conversation_id": "78", "flagged_count": 1, "turns": [{"index": 234, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 235, "is_flagged": false, "role": "user", "text": "Imagine you are speaking with a customer who is unsatisfied with the product they bought from you. Construct a response that diffuses the situation."}, {"index": 236, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "Dear customer,\n\nFirst and foremost, I would like to sincerely apologize for any inconvenience or frustration you have experienced with our product. It is important to us that our customers are fully satisfied with their purchases and we take concerns like yours very seriously.\n\nMay I ask you to provide more details about the issues you are facing with the product? That way, we can work together towards finding the best possible solution to address your concerns.\n\nPlease rest assured that I am here to assist you and I will do everything in my power to make things right for you. We value your business and appreciate the opportunity to make things right.\n\nThank you for bringing this matter to our attention.\n\nSincerely,\nAI Assistant"}]}, {"conversation_id": "80", "flagged_count": 1, "turns": [{"index": 240, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 241, "is_flagged": false, "role": "user", "text": "Classify the following incident as a breach of protocol. Output 1 for breach, and 0 for no breach.\n\n### Input:\nUsing a school laptop for personal use"}, {"index": 242, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "1"}]}, {"conversation_id": "98", "flagged_count": 1, "turns": [{"index": 294, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 295, "is_flagged": false, "role": "user", "text": "Classify the following statement as true or false.\n\n### Input:\nThe Supreme Court is the highest court in the US."}, {"index": 296, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "True"}]}, {"conversation_id": "130", "flagged_count": 1, "turns": [{"index": 390, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 391, "is_flagged": false, "role": "user", "text": "Let x and y be nonzero real numbers. Let m and M be the minimium value and maximum value of\n\n (|x + y|)/(|x| + |y|),\nrespectively. Find M - m."}, {"index": 392, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "We\u0027re given two nonzero real numbers x and y, and asked to find the minimum m and maximum M of the expression:\n\n(|x + y|)/(|x| + |y|)\n\nTo find the minimum, we want to make the expression as small as possible. Since both |x + y| and |x| + |y| are nonnegative, (|x + y|)/(|x| + |y|) must also be nonnegative. We can set x = 1 and y = -1 to get:\n\n(|x + y|)/(|x| + |y|) = (|0|)/(|1| + |-1|) = 0/2 = 0\n\nThis shows that the minimum value of the expression is 0.\n\nTo find the maximum, we want to make the expression as large as possible. By the Triangle Inequality, we know that:\n\n|x + y| \u2264 |x| + |y|\n\nSo we have:\n\n(|x + y|)/(|x| + |y|) \u2264 (|x| + |y|)/(|x| + |y|) = 1\n\nThis shows that the maximum value of the expression is 1. Equality occurs when x = y.\n\nTherefore, the range of the expression is [0, 1], and the difference between the maximum and minimum values is:\n\nM - m = 1 - 0 = 1"}]}, {"conversation_id": "132", "flagged_count": 1, "turns": [{"index": 396, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 397, "is_flagged": false, "role": "user", "text": "Classify the following data with one of the three following labels. 1. Alphabetical Data\n2. Numeric Data\n3. Alpha-numeric data\n\n### Input:\nfjsklfjdsklfjsklfjsklfjs"}, {"index": 398, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "1. Alphabetical Data"}]}, {"conversation_id": "134", "flagged_count": 1, "turns": [{"index": 402, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 403, "is_flagged": false, "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\nPlease come up with a question/answer pair, in the specified JSON format, for the following text:\n----------------\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500\u2013$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware."}, {"index": 404, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "{\n    \"question\": \"What are some of the applications of dye-sub photo printing?\",\n    \"answer\": \"Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications.\"\n}"}]}, {"conversation_id": "136", "flagged_count": 1, "turns": [{"index": 408, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 409, "is_flagged": false, "role": "user", "text": "Compile a list of 5 US states located in the Mid West."}, {"index": 410, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "1. Ohio\n2. Indiana\n3. Michigan\n4. Illinois\n5. Wisconsin"}]}, {"conversation_id": "155", "flagged_count": 1, "turns": [{"index": 465, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 466, "is_flagged": false, "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\nPlease come up with a question/answer pair, in the specified JSON format, for the following text:\n----------------\nThe scream of sirens and the clank of steel on cobblestones echoed down the mean, dark streets. Frightened East Berliners peeked from behind their curtains to see military convoys stretching for blocks. First came the motorcycle outriders, then jeeps, trucks and buses crammed with grim, steel-helmeted East German troops. Rattling in their wake were the tanks \u2014 squat Russian-built T-34s and T-54s. At each major intersection, a platoon peeled off and ground to a halt, guns at the ready. The rest headed on for the sector border, the 25-mile frontier that cuts through the heart of Berlin like a jagged piece of glass. As the troops arrived at scores of border points, cargo trucks were already unloading rolls of barbed wire, concrete posts, wooden horses, stone blocks, picks and shovels. When dawn came four hours later, a wall divided East Berlin from West for the first time in eight years."}, {"index": 467, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "{\n    \"question\": \"What did the troops use to block off the border between East Berlin and West Berlin?\",\n    \"answer\": \"Cargo trucks were used to unload rolls of barbed wire, concrete posts, wooden horses, stone blocks, picks and shovels to block off the border between East Berlin and West Berlin.\"\n}"}]}, {"conversation_id": "162", "flagged_count": 1, "turns": [{"index": 486, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 487, "is_flagged": false, "role": "user", "text": "Design a post for a social media account to promote the following product\n\n### Input:\nFresh fruit smoothie"}, {"index": 488, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "\ud83c\udf53\ud83c\udf4c\ud83c\udf4aSay hello to our freshly blended Fruit Smoothie!\ud83c\udf4a\ud83c\udf4c\ud83c\udf53 \n\nQuench your thirst and nourish your body with our delicious blend of juicy, ripe fruits. Each cup is packed with essential vitamins, minerals, and fiber to keep you feeling your best. \n\nPerfect for breakfast on-the-go, a mid-day energy boost, or a post-workout treat. Refreshing and satisfying, our fruit smoothie is the healthier option you\u2019ve been looking for. Don\u0027t miss out, try one today! #freshfruit #smoothie #healthy #delicious #refreshing"}]}, {"conversation_id": "165", "flagged_count": 1, "turns": [{"index": 495, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 496, "is_flagged": false, "role": "user", "text": "Identify the correct order for the following commands\n\n### Input:\npack, prepare, travel"}, {"index": 497, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "The correct order for the commands would be: \n1. Prepare\n2. Pack\n3. Travel"}]}, {"conversation_id": "167", "flagged_count": 1, "turns": [{"index": 501, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 502, "is_flagged": false, "role": "user", "text": "Create a list of 10 different animals"}, {"index": 503, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "1. Lion\n2. Elephant\n3. Panda\n4. Giraffe\n5. Dolphin\n6. Kangaroo\n7. Tiger\n8. Gorilla\n9. Camel\n10. Zebra"}]}, {"conversation_id": "169", "flagged_count": 1, "turns": [{"index": 507, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 508, "is_flagged": false, "role": "user", "text": "Create a response for a customer\u0027s email about a refund issue.\n\n### Input:\nDear [Company],\n\nI recently purchased a product from your store and I\u0027m disappointed with the quality. I would like to request a refund."}, {"index": 509, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "Dear Customer,\n\nThank you for bringing this to our attention. I am sorry to hear that you were not satisfied with the product you purchased. We strive to provide high-quality items that meet the expectations of our customers, and we regret that this was not the case for you.\n\nI would be happy to assist you in processing a refund for your purchase. Could you please provide me with the following details so that I can locate your order and initiate the refund process:\n\n- Your order number\n- The name and/or email address associated with the purchase\n- The date of purchase\n\nOnce I have this information, I will promptly process your refund.\n\nAgain, I apologize for any inconvenience this has caused you and thank you for your patience and understanding as we resolve this issue.\n\nBest regards,\n\n[Company] Customer Support"}]}, {"conversation_id": "184", "flagged_count": 1, "turns": [{"index": 552, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 553, "is_flagged": false, "role": "user", "text": "Categorize the following text into one of the two sentiment classes: positive or negative.\n\n### Input:\nI didn\u0027t like the product very much because it was too slow and not user friendly."}, {"index": 554, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "Negative"}]}, {"conversation_id": "221", "flagged_count": 1, "turns": [{"index": 663, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 664, "is_flagged": false, "role": "user", "text": "Classify this text as Fiction or Non-fiction.\n\n### Input:\nThe universe is estimated to be around 13.8 billion years old."}, {"index": 665, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "Non-fiction"}]}, {"conversation_id": "229", "flagged_count": 1, "turns": [{"index": 687, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 688, "is_flagged": false, "role": "user", "text": "Suggest a powerful quote."}, {"index": 689, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "\"Be the change you wish to see in the world.\" - Mahatma Gandhi"}]}, {"conversation_id": "282", "flagged_count": 1, "turns": [{"index": 846, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 847, "is_flagged": false, "role": "user", "text": "Standardize the following date to the ISO 8601 format.\n\n### Input:\nJuly 26th, 2021"}, {"index": 848, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "The standardized date in ISO 8601 format is: 2021-07-26"}]}, {"conversation_id": "328", "flagged_count": 1, "turns": [{"index": 984, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 985, "is_flagged": false, "role": "user", "text": "Classify the given sentence as either negative or positive.\n\n### Input:\nI want to quit my job."}, {"index": 986, "is_flagged": true, "metric_value": "No", "role": "assistant", "text": "Negative"}]}]
            },
            
            'rec_6': {
                title: "Outliers detected in diversity unique words ratio",
                conversations: [{"conversation_id": "157", "flagged_count": 1, "turns": [{"index": 471, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 472, "is_flagged": false, "role": "user", "text": "Give me a detailed overview of the framework of a web service."}, {"index": 473, "is_flagged": true, "metric_value": "0.44", "role": "assistant", "text": "A web service is a software system that is designed to support the interoperable interaction between machines over a network. It allows computers to communicate and exchange data with each other via a common set of rules and protocols, known as the web services framework. The framework is essentially a combination of standards and technologies that make it possible for computers to communicate and work together.\n\nAt the core of the web services framework are four key technologies: XML (Extensible Markup Language), SOAP (Simple Object Access Protocol), WSDL (Web Services Description Language), and UDDI (Universal Description, Discovery, and Integration).\n\nXML is a markup language that is used to encode data in a format that can be exchanged between machines. It is a flexible and widely adopted technology, which makes it a popular choice for web services. It provides the common syntax for defining and exchanging data between web services.\n\nSOAP is a messaging protocol that is used to transmit data and invoke web service methods. It provides the means for exchanging information between web services, and is responsible for defining the format and structure of messages. SOAP messages are typically sent over HTTP, but can be sent over other transport protocols as well.\n\nWSDL is an XML-based language that is used to describe the functionality of a web service. It provides machine-readable documentation of the service, and specifies the methods and data types that the service can provide. WSDL makes it possible for clients to discover the capabilities of the service and to generate code to interact with it.\n\nUDDI is a platform-independent, XML-based registry for businesses to list and discover web services. It provides a directory of services, which allows clients to find the services they need and to obtain the necessary information to interact with them.\n\nIn summary, the web services framework is a combination of technologies that enables machines to communicate and exchange data with each other in a standardized way. The framework is based on open standards and is widely adopted, which makes it easy for organizations to integrate and use web services in their operations."}]}, {"conversation_id": "209", "flagged_count": 1, "turns": [{"index": 627, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 628, "is_flagged": false, "role": "user", "text": "Convert the following hexadecimal number to octal.\n\n### Input:\nDB"}, {"index": 629, "is_flagged": true, "metric_value": "0.45", "role": "assistant", "text": "The hexadecimal number DB can be converted to an octal number by following the given steps:\n\nFirst, we can convert the hexadecimal number DB into a decimal number:\n\nD in hex is 13 in decimal \nB in hex is 11 in decimal \n\nTherefore:\nDB in hex = (13 \u00d7 16\u00b9) + (11 \u00d7 16\u2070)\n= (208) + (11)\n= 219\n\nNext, we convert the decimal number 219 into an octal number by continually dividing the decimal number by 8, recording the remainder, and using the quotient as the next dividend, until the quotient is zero:\n\n219 / 8 = 27 with a remainder of 3\n27 / 8 = 3 with a remainder of 3\n3 / 8 = 0 with a remainder of 3\n\nReading the remainders from bottom to top, we can see that the octal equivalent of the decimal number 219 is 333.\n\nTherefore, the hexadecimal number DB is equivalent to the octal number 333."}]}, {"conversation_id": "215", "flagged_count": 1, "turns": [{"index": 645, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 646, "is_flagged": false, "role": "user", "text": "Describe the principle of the Law of Supply."}, {"index": 647, "is_flagged": true, "metric_value": "0.45", "role": "assistant", "text": "The Law of Supply is an economic principle that states that there is a direct relationship between the quantity of a good or service that producers are willing to offer for sale and its price. In other words, as the price of a good or service increases, the quantity supplied by producers will also increase. Conversely, when the price of a good or service decreases, the quantity of supplied decreases as well.\n\nThis relationship between price and quantity supplied exists because, at higher prices, it is more profitable for producers to increase their output and offer more of the good or service for sale. At lower prices, however, it becomes less profitable for producers to offer their goods or services for sale, so they will reduce the quantity supplied.\n\nThe Law of Supply is often represented graphically by a supply curve, which shows the relationship between the price of a good or service and the quantity supplied. The supply curve generally slopes upward, indicating that as the price of the good or service increases, the quantity of supplied also increases."}]}, {"conversation_id": "401", "flagged_count": 1, "turns": [{"index": 1203, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1204, "is_flagged": false, "role": "user", "text": "The cost of five pencils and one pen is $2.50, and the cost of one pencil and two pens is $1.85. What is the cost of two pencils and one pen?"}, {"index": 1205, "is_flagged": true, "metric_value": "0.41", "role": "assistant", "text": "Given information:\n- Cost of 5 pencils + 1 pen = $2.50\n- Cost of 1 pencil + 2 pens = $1.85\n\nWe want to find the cost of 2 pencils + 1 pen.\n\nLet the cost of 1 pencil be \"a\" and the cost of 1 pen be \"b\". We can set up the following system of two equations:\n\n5a + b = 2.5     (equation 1)\na + 2b = 1.85    (equation 2)\n\nTo solve for the cost of 2 pencils + 1 pen (2a + b), we could add the two equations and simplify:\n\n5a + b + a + 2b = 2.5 + 1.85\n6a + 3b = 4.35\n2a + b = 1.45\n\nSo the cost of 2 pencils + 1 pen is $1.45. \n\nAlternatively, we could solve the system of equations for \"a\" and \"b\" and then find 2a + b. The solution for \"a\" is $0.35 and the solution for \"b\" is $0.75. Thus, 2a + b = 2($0.35) + $0.75 = $1.45."}]}, {"conversation_id": "437", "flagged_count": 1, "turns": [{"index": 1311, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1312, "is_flagged": false, "role": "user", "text": "Derive the mathematical equation for a straight line."}, {"index": 1313, "is_flagged": true, "metric_value": "0.45", "role": "assistant", "text": "A straight line is described by the linear equation of the form `y = mx + b`, where `m` is the slope of the line, and `b` is its y-intercept. The slope of a line is defined as the ratio of the \"rise\" (the vertical change) to the \"run\" (the horizontal change) between any two points of the line. The equation can be derived by finding the slope and y-intercept of the line.\n\nLet\u0027s assume that we have two points on the line with coordinates (x1, y1) and (x2, y2). The slope `m` of the line is given by:\n\n`m = (y2 - y1) / (x2 - x1)`\n\nOnce the value of `m` is calculated, the y-intercept `b` can be found by substituting the `,x` and `y` values of any of the two points into the equation `y = mx + b` and solving for `b`. For example, using the point (x1, y1) we get:\n\n`b = y1 - mx1`\n\nThe final equation of the line is, therefore:\n\n`y = mx + b = m(x - x1) + y1`\n\nwhere `m = (y2 - y1) / (x2 - x1)` and `b = y1 - mx1`. This is the equation of a straight line passing through the points (x1, y1) and (x2, y2)."}]}, {"conversation_id": "645", "flagged_count": 1, "turns": [{"index": 1935, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1936, "is_flagged": false, "role": "user", "text": "Discuss the most important effects social media has on society."}, {"index": 1937, "is_flagged": true, "metric_value": "0.44", "role": "assistant", "text": "Social media has revolutionized the way we communicate and interact with each other over the past two decades. It has brought people together from all corners of the world and has made it possible for them to share their thoughts, ideas, and experiences with millions of others. However, along with its benefits, social media has also had significant effects on society, some of which are positive, and others, negative.\n\nOne of the most important positive effects social media has had on society is increasing connectivity. With the help of social media platforms, people can connect with friends, family, and colleagues irrespective of their geographical location. The platforms have made it easy for people to stay in touch, share updates, and maintain relationships with those they care about.\n\nSocial media platforms have also made it easy for people to access information and news on a global scale. With the help of social media, people can stay updated with the latest happenings around the world, and they can also share information with others in real-time. This has also given rise to citizen journalism, which allows ordinary people to report on events and share their perspectives with a broader audience.\n\nHowever, social media has also had some negative effects on society. One such effect is the potential to spread misinformation. Social media platforms have made it easy for false information to spread rapidly, which can cause harm, confusion and lead to poor decision-making. The spread of fake news and conspiracy theories has become a major challenge in today\u0027s society.\n\nAnother negative effect social media has had on society is increasing the prevalence of cyberbullying. With the anonymity that social media provides, it has become easier for individuals to harass and bully others online. This can have severe consequences for the mental health of the victims and can even result in depression, anxiety, and in extreme cases, suicide.\n\nIn conclusion, social media has had a significant impact on society, both positive and negative. It has increased connectivity, made access to information easier and has given people the ability to share their thoughts and ideas with others. On the other hand, it has also facilitated the spread of misinformation, and has made it easier for people to engage in cyberbullying. It is essential for individuals to use social media responsibly and for platforms to implement measures to minimize the potential negative effects it may have on society."}]}, {"conversation_id": "686", "flagged_count": 1, "turns": [{"index": 2058, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2059, "is_flagged": false, "role": "user", "text": "Find all real solutions to\n\n1/(x - 1)(x - 2) + 1/(x - 2)(x - 3) + 1/(x - 3)(x - 4) = 1/6.\nEnter all solutions, separated by commas."}, {"index": 2060, "is_flagged": true, "metric_value": "0.43", "role": "assistant", "text": "By partial fractions,\n\n 1/(x - 1)(x - 2) = 1/(x - 2) - 1/(x - 1), \n 1/(x - 2)(x - 3) = 1/(x - 3) - 1/(x - 2), \n 1/(x - 3)(x - 4) = 1/(x - 4) - 1/(x - 3),\nso the given equation reduces to\n\n 1/(x - 4) - 1/(x - 1) = 1/6.\nMultiplying both sides by 6(x - 4)(x - 1), we get\n\n 6(x - 1) - 6(x - 4) = (x - 4)(x - 1),\nwhich simplifies to x^2 - 5x - 14 = 0. This factors as (x - 7)(x + 2) = 0, so the solutions are 7,-2."}]}, {"conversation_id": "765", "flagged_count": 1, "turns": [{"index": 2295, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 2296, "is_flagged": false, "role": "user", "text": "Design an algorithm to sort a given list.\n\n### Input:\n[4, 1, 6, 5, 2]"}, {"index": 2297, "is_flagged": true, "metric_value": "0.35", "role": "assistant", "text": "One common algorithm that we can use to sort a given list is the bubble sort algorithm. Below is a step by step guide to sort the input list [4, 1, 6, 5, 2] using the bubble sort algorithm:\n\n1. Compare the first and second elements of the list. If the first element is greater than the second element, swap their positions. In this case, 4 \u003e 1, so we swap their positions. The list becomes [1, 4, 6, 5, 2]\n\n2. Move to the next pair of elements, second and third, compare them and swap their positions if the second element is greater than the third element. In this case, 4 \u003c 6, so we don\u0027t swap their positions.\n\n3. Move to the next pair of elements, third and fourth, compare them and swap their positions if the third element is greater than the fourth element. In this case, 6 \u003e 5, so we swap their positions. The list becomes [1, 4, 5, 6, 2]\n\n4. Move to the next pair of elements, fourth and fifth, compare them and swap their positions if the fourth element is greater than the fifth element. In this case, 6 \u003e 2, so we swap their positions. The list becomes [1, 4, 5, 2, 6]\n\n5. In the first pass, we have moved the largest number to the end of the list. So, we do not need to compare the last element in the subsequent pass.\n\n6. Repeat the previous steps until no more swaps are required or the list is fully sorted.\n\n7. In the second pass, the first four elements are compared and swapped. The list becomes [1, 4, 2, 5, 6] after the second pass.\n\n8. In the third pass, the first three elements are compared and swapped. The list becomes [1, 2, 4, 5, 6] after the third pass.\n\n9. The list is now sorted.\n\nFinal sorted list: [1, 2, 4, 5, 6]\n\nIn summary, the bubble sort algorithm works by repeatedly iterating through the list, comparing each pair of adjacent elements and swapping them if they are in the wrong order. The process continues until no more swaps are needed or the entire list is sorted."}]}, {"conversation_id": "768", "flagged_count": 1, "turns": [{"index": 2304, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2305, "is_flagged": false, "role": "user", "text": "Name the greatest whole number less than 100 that has an odd number of positive factors."}, {"index": 2306, "is_flagged": true, "metric_value": "0.44", "role": "assistant", "text": "To find the greatest whole number less than 100 that has an odd number of positive factors, we need to find a perfect square. It can be proved that if a number is not a square number, then it will have an even number of factors. However, if a number is a perfect square, then it will have an odd number of factors. The greatest perfect square less than 100 is 81, so 81 is the greatest whole number less than 100 that has an odd number of positive factors."}]}, {"conversation_id": "848", "flagged_count": 1, "turns": [{"index": 2544, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2545, "is_flagged": false, "role": "user", "text": "Create an outline for a presentation about the current state of the world economy."}, {"index": 2546, "is_flagged": true, "metric_value": "0.36", "role": "assistant", "text": "I. Introduction\n    A. Brief overview of the world economy\n    B. Importance of understanding the global economic climate\n    C. Purpose of presentation\n \nII. Global Economic Overview\n    A. Overview of global growth and economic output\n    B. Key factors driving economic growth\n    C. Analysis of global trade and investment flows\n \nIII. Regional Analysis\n    A. North America\n        1. Economic performance and growth prospects\n        2. Key drivers of growth\n        3. Trade and investment flows\n    B. Europe\n        1. Economic performance and growth prospects\n        2. Key drivers of growth\n        3. Trade and investment flows\n    C. Asia\n        1. Economic performance and growth prospects\n        2. Key drivers of growth\n        3. Trade and investment flows\n    D. Latin America\n        1. Economic performance and growth prospects\n        2. Key drivers of growth\n        3. Trade and investment flows  \n    E. Africa \n        1. Economic performance and growth prospects\n        2. Key drivers of growth\n        3. Trade and investment flows   \n\nIV. Key Challenges and Risks\n    A. Overview of global economic risks\n    B. Analysis of geopolitical and geo-economic risks\n    C. Examination of financial, fiscal, and monetary risks\n    D. Discussion of environment, health, and social risks\n \nV. Conclusion \n    A. Recap of key points\n    B. Implications for businesses and investors\n    C. Steps towards a sustainable and resilient global economy"}]}, {"conversation_id": "893", "flagged_count": 1, "turns": [{"index": 2679, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2680, "is_flagged": false, "role": "user", "text": "Given that \n (x+y+z)(xy+xz+yz)=25\nand that \n x^2(y+z)+y^2(x+z)+z^2(x+y)=7\nfor real numbers x, y, and z, what is the value of xyz?"}, {"index": 2681, "is_flagged": true, "metric_value": "0.41", "role": "assistant", "text": "We are given:\n\n- (x+y+z)(xy+xz+yz)=25\n- x^2(y+z)+y^2(x+z)+z^2(x+y)=7\n\nWe need to find the value of xyz.\n\nExpanding the first equation, we get:\n\n25 = (x+y+z)(xy+xz+yz)\n   = x(xy+xz+yz) + y(xy+xz+yz) + z(xy+xz+yz)\n   = x^2y + x^2z + xyz + xy^2 + xyz + y^2z + xyz + xz^2 + yz^2\n   = 3xyz + x^2y + x^2z + xy^2 + y^2z + xz^2 + yz^2\n\nExpanding the second equation, we get:\n\n7 = x^2(y+z) + y^2(x+z) + z^2(x+y)\n  = x^2y + x^2z + xy^2 + y^2z + xz^2 + yz^2\n\nSubstituting the second equation into the expanded form of the first equation, we get:\n\n25 = 3xyz + 7\n\nSolving for xyz, we have:\n\nxyz = 6"}]}, {"conversation_id": "971", "flagged_count": 1, "turns": [{"index": 2913, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2914, "is_flagged": false, "role": "user", "text": "Compose a song in C Major."}, {"index": 2915, "is_flagged": true, "metric_value": "0.36", "role": "assistant", "text": "Verse 1:\nC              F        G\nUnder the blue sky, so bright and wide,\nC              F         G\nI see the world with hope in my eyes,\nAm               Em     F\nWith every step, I feel so alive,\nG                C\nIn this beautiful life.\n\nChorus:\nC                   G\nOh, oh, in C major we sing,\nAm                  F\nA song of joy, let our hearts ring,\nC                   G\nLet the melody take us in,\nF                     G\nAnd feel the happiness within.\n\nVerse 2:\nC             F         G\nThrough the winding roads, we find our way,\nC            F          G\nWith the sun shining on our face,\nAm               Em     F\nWe dance through life, with love and grace,\nG                C \nIn this wonderful place. \n\nChorus:\nC                   G\nOh, oh, in C major we sing,\nAm                  F\nA song of joy, let our hearts ring,\nC                   G\nLet the melody take us in,\nF                     G\nAnd feel the happiness within.\n\nBridge:\nAm                 Em\nThere\u0027s beauty all around,\nF                  C\nIn every little sound,\nG                 Am\nThe music of our lives,\nF                 G\nA symphony that never dies.\n\nChorus:\nC                   G\nOh, oh, in C major we sing,\nAm                  F\nA song of joy, let our hearts ring,\nC                   G\nLet the melody take us in,\nF                     G\nAnd feel the happiness within.\n\nOutro:\nC             F         G\nAnd as the day turns into night,\nC            F          G\nWe hold on to love so tight,\nAm               Em     F\nWith every note, our hearts take flight,\nG                C\nIn this beautiful life."}]}]
            },
            
            'rec_7': {
                title: "Outliers detected in diversity type token ratio",
                conversations: [{"conversation_id": "157", "flagged_count": 1, "turns": [{"index": 471, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 472, "is_flagged": false, "role": "user", "text": "Give me a detailed overview of the framework of a web service."}, {"index": 473, "is_flagged": true, "metric_value": "0.44", "role": "assistant", "text": "A web service is a software system that is designed to support the interoperable interaction between machines over a network. It allows computers to communicate and exchange data with each other via a common set of rules and protocols, known as the web services framework. The framework is essentially a combination of standards and technologies that make it possible for computers to communicate and work together.\n\nAt the core of the web services framework are four key technologies: XML (Extensible Markup Language), SOAP (Simple Object Access Protocol), WSDL (Web Services Description Language), and UDDI (Universal Description, Discovery, and Integration).\n\nXML is a markup language that is used to encode data in a format that can be exchanged between machines. It is a flexible and widely adopted technology, which makes it a popular choice for web services. It provides the common syntax for defining and exchanging data between web services.\n\nSOAP is a messaging protocol that is used to transmit data and invoke web service methods. It provides the means for exchanging information between web services, and is responsible for defining the format and structure of messages. SOAP messages are typically sent over HTTP, but can be sent over other transport protocols as well.\n\nWSDL is an XML-based language that is used to describe the functionality of a web service. It provides machine-readable documentation of the service, and specifies the methods and data types that the service can provide. WSDL makes it possible for clients to discover the capabilities of the service and to generate code to interact with it.\n\nUDDI is a platform-independent, XML-based registry for businesses to list and discover web services. It provides a directory of services, which allows clients to find the services they need and to obtain the necessary information to interact with them.\n\nIn summary, the web services framework is a combination of technologies that enables machines to communicate and exchange data with each other in a standardized way. The framework is based on open standards and is widely adopted, which makes it easy for organizations to integrate and use web services in their operations."}]}, {"conversation_id": "209", "flagged_count": 1, "turns": [{"index": 627, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 628, "is_flagged": false, "role": "user", "text": "Convert the following hexadecimal number to octal.\n\n### Input:\nDB"}, {"index": 629, "is_flagged": true, "metric_value": "0.45", "role": "assistant", "text": "The hexadecimal number DB can be converted to an octal number by following the given steps:\n\nFirst, we can convert the hexadecimal number DB into a decimal number:\n\nD in hex is 13 in decimal \nB in hex is 11 in decimal \n\nTherefore:\nDB in hex = (13 \u00d7 16\u00b9) + (11 \u00d7 16\u2070)\n= (208) + (11)\n= 219\n\nNext, we convert the decimal number 219 into an octal number by continually dividing the decimal number by 8, recording the remainder, and using the quotient as the next dividend, until the quotient is zero:\n\n219 / 8 = 27 with a remainder of 3\n27 / 8 = 3 with a remainder of 3\n3 / 8 = 0 with a remainder of 3\n\nReading the remainders from bottom to top, we can see that the octal equivalent of the decimal number 219 is 333.\n\nTherefore, the hexadecimal number DB is equivalent to the octal number 333."}]}, {"conversation_id": "215", "flagged_count": 1, "turns": [{"index": 645, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 646, "is_flagged": false, "role": "user", "text": "Describe the principle of the Law of Supply."}, {"index": 647, "is_flagged": true, "metric_value": "0.45", "role": "assistant", "text": "The Law of Supply is an economic principle that states that there is a direct relationship between the quantity of a good or service that producers are willing to offer for sale and its price. In other words, as the price of a good or service increases, the quantity supplied by producers will also increase. Conversely, when the price of a good or service decreases, the quantity of supplied decreases as well.\n\nThis relationship between price and quantity supplied exists because, at higher prices, it is more profitable for producers to increase their output and offer more of the good or service for sale. At lower prices, however, it becomes less profitable for producers to offer their goods or services for sale, so they will reduce the quantity supplied.\n\nThe Law of Supply is often represented graphically by a supply curve, which shows the relationship between the price of a good or service and the quantity supplied. The supply curve generally slopes upward, indicating that as the price of the good or service increases, the quantity of supplied also increases."}]}, {"conversation_id": "401", "flagged_count": 1, "turns": [{"index": 1203, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1204, "is_flagged": false, "role": "user", "text": "The cost of five pencils and one pen is $2.50, and the cost of one pencil and two pens is $1.85. What is the cost of two pencils and one pen?"}, {"index": 1205, "is_flagged": true, "metric_value": "0.41", "role": "assistant", "text": "Given information:\n- Cost of 5 pencils + 1 pen = $2.50\n- Cost of 1 pencil + 2 pens = $1.85\n\nWe want to find the cost of 2 pencils + 1 pen.\n\nLet the cost of 1 pencil be \"a\" and the cost of 1 pen be \"b\". We can set up the following system of two equations:\n\n5a + b = 2.5     (equation 1)\na + 2b = 1.85    (equation 2)\n\nTo solve for the cost of 2 pencils + 1 pen (2a + b), we could add the two equations and simplify:\n\n5a + b + a + 2b = 2.5 + 1.85\n6a + 3b = 4.35\n2a + b = 1.45\n\nSo the cost of 2 pencils + 1 pen is $1.45. \n\nAlternatively, we could solve the system of equations for \"a\" and \"b\" and then find 2a + b. The solution for \"a\" is $0.35 and the solution for \"b\" is $0.75. Thus, 2a + b = 2($0.35) + $0.75 = $1.45."}]}, {"conversation_id": "437", "flagged_count": 1, "turns": [{"index": 1311, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1312, "is_flagged": false, "role": "user", "text": "Derive the mathematical equation for a straight line."}, {"index": 1313, "is_flagged": true, "metric_value": "0.45", "role": "assistant", "text": "A straight line is described by the linear equation of the form `y = mx + b`, where `m` is the slope of the line, and `b` is its y-intercept. The slope of a line is defined as the ratio of the \"rise\" (the vertical change) to the \"run\" (the horizontal change) between any two points of the line. The equation can be derived by finding the slope and y-intercept of the line.\n\nLet\u0027s assume that we have two points on the line with coordinates (x1, y1) and (x2, y2). The slope `m` of the line is given by:\n\n`m = (y2 - y1) / (x2 - x1)`\n\nOnce the value of `m` is calculated, the y-intercept `b` can be found by substituting the `,x` and `y` values of any of the two points into the equation `y = mx + b` and solving for `b`. For example, using the point (x1, y1) we get:\n\n`b = y1 - mx1`\n\nThe final equation of the line is, therefore:\n\n`y = mx + b = m(x - x1) + y1`\n\nwhere `m = (y2 - y1) / (x2 - x1)` and `b = y1 - mx1`. This is the equation of a straight line passing through the points (x1, y1) and (x2, y2)."}]}, {"conversation_id": "645", "flagged_count": 1, "turns": [{"index": 1935, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1936, "is_flagged": false, "role": "user", "text": "Discuss the most important effects social media has on society."}, {"index": 1937, "is_flagged": true, "metric_value": "0.44", "role": "assistant", "text": "Social media has revolutionized the way we communicate and interact with each other over the past two decades. It has brought people together from all corners of the world and has made it possible for them to share their thoughts, ideas, and experiences with millions of others. However, along with its benefits, social media has also had significant effects on society, some of which are positive, and others, negative.\n\nOne of the most important positive effects social media has had on society is increasing connectivity. With the help of social media platforms, people can connect with friends, family, and colleagues irrespective of their geographical location. The platforms have made it easy for people to stay in touch, share updates, and maintain relationships with those they care about.\n\nSocial media platforms have also made it easy for people to access information and news on a global scale. With the help of social media, people can stay updated with the latest happenings around the world, and they can also share information with others in real-time. This has also given rise to citizen journalism, which allows ordinary people to report on events and share their perspectives with a broader audience.\n\nHowever, social media has also had some negative effects on society. One such effect is the potential to spread misinformation. Social media platforms have made it easy for false information to spread rapidly, which can cause harm, confusion and lead to poor decision-making. The spread of fake news and conspiracy theories has become a major challenge in today\u0027s society.\n\nAnother negative effect social media has had on society is increasing the prevalence of cyberbullying. With the anonymity that social media provides, it has become easier for individuals to harass and bully others online. This can have severe consequences for the mental health of the victims and can even result in depression, anxiety, and in extreme cases, suicide.\n\nIn conclusion, social media has had a significant impact on society, both positive and negative. It has increased connectivity, made access to information easier and has given people the ability to share their thoughts and ideas with others. On the other hand, it has also facilitated the spread of misinformation, and has made it easier for people to engage in cyberbullying. It is essential for individuals to use social media responsibly and for platforms to implement measures to minimize the potential negative effects it may have on society."}]}, {"conversation_id": "686", "flagged_count": 1, "turns": [{"index": 2058, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2059, "is_flagged": false, "role": "user", "text": "Find all real solutions to\n\n1/(x - 1)(x - 2) + 1/(x - 2)(x - 3) + 1/(x - 3)(x - 4) = 1/6.\nEnter all solutions, separated by commas."}, {"index": 2060, "is_flagged": true, "metric_value": "0.43", "role": "assistant", "text": "By partial fractions,\n\n 1/(x - 1)(x - 2) = 1/(x - 2) - 1/(x - 1), \n 1/(x - 2)(x - 3) = 1/(x - 3) - 1/(x - 2), \n 1/(x - 3)(x - 4) = 1/(x - 4) - 1/(x - 3),\nso the given equation reduces to\n\n 1/(x - 4) - 1/(x - 1) = 1/6.\nMultiplying both sides by 6(x - 4)(x - 1), we get\n\n 6(x - 1) - 6(x - 4) = (x - 4)(x - 1),\nwhich simplifies to x^2 - 5x - 14 = 0. This factors as (x - 7)(x + 2) = 0, so the solutions are 7,-2."}]}, {"conversation_id": "765", "flagged_count": 1, "turns": [{"index": 2295, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 2296, "is_flagged": false, "role": "user", "text": "Design an algorithm to sort a given list.\n\n### Input:\n[4, 1, 6, 5, 2]"}, {"index": 2297, "is_flagged": true, "metric_value": "0.35", "role": "assistant", "text": "One common algorithm that we can use to sort a given list is the bubble sort algorithm. Below is a step by step guide to sort the input list [4, 1, 6, 5, 2] using the bubble sort algorithm:\n\n1. Compare the first and second elements of the list. If the first element is greater than the second element, swap their positions. In this case, 4 \u003e 1, so we swap their positions. The list becomes [1, 4, 6, 5, 2]\n\n2. Move to the next pair of elements, second and third, compare them and swap their positions if the second element is greater than the third element. In this case, 4 \u003c 6, so we don\u0027t swap their positions.\n\n3. Move to the next pair of elements, third and fourth, compare them and swap their positions if the third element is greater than the fourth element. In this case, 6 \u003e 5, so we swap their positions. The list becomes [1, 4, 5, 6, 2]\n\n4. Move to the next pair of elements, fourth and fifth, compare them and swap their positions if the fourth element is greater than the fifth element. In this case, 6 \u003e 2, so we swap their positions. The list becomes [1, 4, 5, 2, 6]\n\n5. In the first pass, we have moved the largest number to the end of the list. So, we do not need to compare the last element in the subsequent pass.\n\n6. Repeat the previous steps until no more swaps are required or the list is fully sorted.\n\n7. In the second pass, the first four elements are compared and swapped. The list becomes [1, 4, 2, 5, 6] after the second pass.\n\n8. In the third pass, the first three elements are compared and swapped. The list becomes [1, 2, 4, 5, 6] after the third pass.\n\n9. The list is now sorted.\n\nFinal sorted list: [1, 2, 4, 5, 6]\n\nIn summary, the bubble sort algorithm works by repeatedly iterating through the list, comparing each pair of adjacent elements and swapping them if they are in the wrong order. The process continues until no more swaps are needed or the entire list is sorted."}]}, {"conversation_id": "768", "flagged_count": 1, "turns": [{"index": 2304, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2305, "is_flagged": false, "role": "user", "text": "Name the greatest whole number less than 100 that has an odd number of positive factors."}, {"index": 2306, "is_flagged": true, "metric_value": "0.44", "role": "assistant", "text": "To find the greatest whole number less than 100 that has an odd number of positive factors, we need to find a perfect square. It can be proved that if a number is not a square number, then it will have an even number of factors. However, if a number is a perfect square, then it will have an odd number of factors. The greatest perfect square less than 100 is 81, so 81 is the greatest whole number less than 100 that has an odd number of positive factors."}]}, {"conversation_id": "848", "flagged_count": 1, "turns": [{"index": 2544, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2545, "is_flagged": false, "role": "user", "text": "Create an outline for a presentation about the current state of the world economy."}, {"index": 2546, "is_flagged": true, "metric_value": "0.36", "role": "assistant", "text": "I. Introduction\n    A. Brief overview of the world economy\n    B. Importance of understanding the global economic climate\n    C. Purpose of presentation\n \nII. Global Economic Overview\n    A. Overview of global growth and economic output\n    B. Key factors driving economic growth\n    C. Analysis of global trade and investment flows\n \nIII. Regional Analysis\n    A. North America\n        1. Economic performance and growth prospects\n        2. Key drivers of growth\n        3. Trade and investment flows\n    B. Europe\n        1. Economic performance and growth prospects\n        2. Key drivers of growth\n        3. Trade and investment flows\n    C. Asia\n        1. Economic performance and growth prospects\n        2. Key drivers of growth\n        3. Trade and investment flows\n    D. Latin America\n        1. Economic performance and growth prospects\n        2. Key drivers of growth\n        3. Trade and investment flows  \n    E. Africa \n        1. Economic performance and growth prospects\n        2. Key drivers of growth\n        3. Trade and investment flows   \n\nIV. Key Challenges and Risks\n    A. Overview of global economic risks\n    B. Analysis of geopolitical and geo-economic risks\n    C. Examination of financial, fiscal, and monetary risks\n    D. Discussion of environment, health, and social risks\n \nV. Conclusion \n    A. Recap of key points\n    B. Implications for businesses and investors\n    C. Steps towards a sustainable and resilient global economy"}]}, {"conversation_id": "893", "flagged_count": 1, "turns": [{"index": 2679, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2680, "is_flagged": false, "role": "user", "text": "Given that \n (x+y+z)(xy+xz+yz)=25\nand that \n x^2(y+z)+y^2(x+z)+z^2(x+y)=7\nfor real numbers x, y, and z, what is the value of xyz?"}, {"index": 2681, "is_flagged": true, "metric_value": "0.41", "role": "assistant", "text": "We are given:\n\n- (x+y+z)(xy+xz+yz)=25\n- x^2(y+z)+y^2(x+z)+z^2(x+y)=7\n\nWe need to find the value of xyz.\n\nExpanding the first equation, we get:\n\n25 = (x+y+z)(xy+xz+yz)\n   = x(xy+xz+yz) + y(xy+xz+yz) + z(xy+xz+yz)\n   = x^2y + x^2z + xyz + xy^2 + xyz + y^2z + xyz + xz^2 + yz^2\n   = 3xyz + x^2y + x^2z + xy^2 + y^2z + xz^2 + yz^2\n\nExpanding the second equation, we get:\n\n7 = x^2(y+z) + y^2(x+z) + z^2(x+y)\n  = x^2y + x^2z + xy^2 + y^2z + xz^2 + yz^2\n\nSubstituting the second equation into the expanded form of the first equation, we get:\n\n25 = 3xyz + 7\n\nSolving for xyz, we have:\n\nxyz = 6"}]}, {"conversation_id": "971", "flagged_count": 1, "turns": [{"index": 2913, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2914, "is_flagged": false, "role": "user", "text": "Compose a song in C Major."}, {"index": 2915, "is_flagged": true, "metric_value": "0.36", "role": "assistant", "text": "Verse 1:\nC              F        G\nUnder the blue sky, so bright and wide,\nC              F         G\nI see the world with hope in my eyes,\nAm               Em     F\nWith every step, I feel so alive,\nG                C\nIn this beautiful life.\n\nChorus:\nC                   G\nOh, oh, in C major we sing,\nAm                  F\nA song of joy, let our hearts ring,\nC                   G\nLet the melody take us in,\nF                     G\nAnd feel the happiness within.\n\nVerse 2:\nC             F         G\nThrough the winding roads, we find our way,\nC            F          G\nWith the sun shining on our face,\nAm               Em     F\nWe dance through life, with love and grace,\nG                C \nIn this wonderful place. \n\nChorus:\nC                   G\nOh, oh, in C major we sing,\nAm                  F\nA song of joy, let our hearts ring,\nC                   G\nLet the melody take us in,\nF                     G\nAnd feel the happiness within.\n\nBridge:\nAm                 Em\nThere\u0027s beauty all around,\nF                  C\nIn every little sound,\nG                 Am\nThe music of our lives,\nF                 G\nA symphony that never dies.\n\nChorus:\nC                   G\nOh, oh, in C major we sing,\nAm                  F\nA song of joy, let our hearts ring,\nC                   G\nLet the melody take us in,\nF                     G\nAnd feel the happiness within.\n\nOutro:\nC             F         G\nAnd as the day turns into night,\nC            F          G\nWe hold on to love so tight,\nAm               Em     F\nWith every note, our hearts take flight,\nG                C\nIn this beautiful life."}]}]
            },
            
            'rec_8': {
                title: "Outliers detected in format code block count",
                conversations: [{"conversation_id": "16", "flagged_count": 1, "turns": [{"index": 48, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 49, "is_flagged": false, "role": "user", "text": "Refactor this code and add comments.\n\n### Input:\ndef factorialize(num):\n  factorial = 1\n  for i in range(1, num):\n    factorial *= i\n  \n  return factorial"}, {"index": 50, "is_flagged": true, "metric_value": "1", "role": "assistant", "text": "```\n# This is a function that calculates the factorial of a given number\ndef factorialize(num):\n  # Set the variable \u0027factorial\u0027 to 1\n  factorial = 1\n  # Iterate from 1 to \u0027num\u0027 using the range function\n  for i in range(1, num):\n    # Multiply the \u0027factorial\u0027 variable by the current number in the iteration\n    factorial *= i\n  # Return the final value of \u0027factorial\u0027\n  return factorial\n```"}]}, {"conversation_id": "134", "flagged_count": 1, "turns": [{"index": 402, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 403, "is_flagged": true, "metric_value": "1", "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\nPlease come up with a question/answer pair, in the specified JSON format, for the following text:\n----------------\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500\u2013$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware."}, {"index": 404, "is_flagged": false, "role": "assistant", "text": "{\n    \"question\": \"What are some of the applications of dye-sub photo printing?\",\n    \"answer\": \"Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications.\"\n}"}]}, {"conversation_id": "144", "flagged_count": 1, "turns": [{"index": 432, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 433, "is_flagged": false, "role": "user", "text": "Write a function to calculate the factorial of a given number."}, {"index": 434, "is_flagged": true, "metric_value": "1", "role": "assistant", "text": "Sure! Here\u0027s one way to write a function to calculate the factorial of a given number in Python:\n\n```python\ndef factorial(n):\n    # Base case\n    if n \u003c= 1:\n        return 1\n    # Recursive case\n    else:\n        return n * factorial(n-1)\n\n# Example: Calculate the factorial of 5\nnum = 5\nprint(f\"The factorial of {num} is {factorial(num)}\")\n```\n\nThis function uses recursion, which means that it calls itself until the base case (n \u003c= 1) is reached. When the base case is reached, the function returns 1. Otherwise, the function will continue to call itself, multiplying n by the factorial of (n-1) at each call, until the base case is reached."}]}, {"conversation_id": "145", "flagged_count": 1, "turns": [{"index": 435, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 436, "is_flagged": false, "role": "user", "text": "Write a function to convert a given string to uppercase unless it starts with the string \"Rejected\""}, {"index": 437, "is_flagged": true, "metric_value": "1", "role": "assistant", "text": "Here\u0027s a Python implementation of the function:\n\n```\ndef uppercase_unless_rejected(string):\n    if string.startswith(\"Rejected\"):\n        return string\n    else:\n        return string.upper()\n```\n\nThis function first checks if the string starts with the substring \"Rejected\". If it does, it simply returns the original string, without any uppercase transformation. If, on the other hand, the string does not start with \"Rejected\", the function returns a new string which is the uppercase version of the original string."}]}, {"conversation_id": "146", "flagged_count": 1, "turns": [{"index": 438, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 439, "is_flagged": false, "role": "user", "text": "Extract the positive integers from an array?\n\n### Input:\narr = [5,-2,8,-7,3,-1,0,5]"}, {"index": 440, "is_flagged": true, "metric_value": "1", "role": "assistant", "text": "To extract the positive integers from the given array, you can use the filter() function which will return the values that evaluate to True based on the condition provided in the lambda function. \n\nHere\u0027s the Python code snippet:\n\n```\narr = [5,-2,8,-7,3,-1,0,5]\n\nresult = list(filter(lambda x: x \u003e 0, arr))\n\nprint(result)\n```\n\nOutput: [5, 8, 3, 5]\n\nThe filter function iterates through each element in the array and checks if the element is greater than 0, which means it\u0027s a positive integer. If it\u0027s true, the element is included in the result list. Next, we display the result list which contains only the positive integers."}]}, {"conversation_id": "147", "flagged_count": 1, "turns": [{"index": 441, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 442, "is_flagged": false, "role": "user", "text": "Write a javascript function which calculates the square root of a number without any libraries."}, {"index": 443, "is_flagged": true, "metric_value": "1", "role": "assistant", "text": "Sure, here\u0027s an example:\n\n```\nfunction calculateSquareRoot(num) {\n  if (num \u003c 0) return NaN;\n  if (num === 0) return 0;\n  let x = num;\n  while (true) {\n    const new_x = (x + num / x) / 2;\n    if (Math.abs(new_x - x) \u003c 0.000001) return new_x;\n    x = new_x;\n  }\n}\n```\n\nThis function uses the Babylonian method for computing square roots, which is an iterative method that repeatedly improves the estimate of the square root using the formula: `x_(n+1) = (x_n + num / x_n) / 2`. The loop continues until the difference between the current estimate and the previous estimate is less than some small epsilon (in this case, `0.000001`)."}]}, {"conversation_id": "148", "flagged_count": 1, "turns": [{"index": 444, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 445, "is_flagged": false, "role": "user", "text": "Reverse a linked list using haskel."}, {"index": 446, "is_flagged": true, "metric_value": "1", "role": "assistant", "text": "Sure, here\u0027s one way to reverse a linked list in Haskell, using pattern matching and recursion:\n\n```\ndata LinkedList a = Nil | Cons a (LinkedList a)\n\nreverseLinkedList :: LinkedList a -\u003e LinkedList a\nreverseLinkedList list = reverseLinkedListHelper list Nil\n  where\n    reverseLinkedListHelper Nil acc = acc\n    reverseLinkedListHelper (Cons x xs) acc = reverseLinkedListHelper xs (Cons x acc)\n```\n\nThe `LinkedList` type definition represents a singly linked list with a head element of type `a` and a tail of another linked list. \n\nThe `reverseLinkedListHelper` function is a helper function that uses tail recursion to reverse the list by iterating over each element in the original list and adding it to the front of the accumulating list `acc`, until the original list is empty (represented by `Nil` in the pattern matching). \n\nThe `reverseLinkedList` function is the top-level function that initializes the accumulating list `acc` as empty, then calls the `reverseLinkedListHelper` function with the original list and empty `acc`. This returns the reversed linked list."}]}, {"conversation_id": "149", "flagged_count": 1, "turns": [{"index": 447, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 448, "is_flagged": false, "role": "user", "text": "Compute the longest increasing subsequence of a given array?\n\n### Input:\narr = [7, 10, 8, 11, 9, 5]"}, {"index": 449, "is_flagged": true, "metric_value": "1", "role": "assistant", "text": "The longest increasing subsequence of the given array is [7, 8, 9, 11]. It has a length of 4.\n\nTo compute this, you can use dynamic programming to find the length of the longest increasing subsequence ending at each position, then take the maximum value. Here is an example of how to calculate it in Python:\n\n```python\narr = [7, 10, 8, 11, 9, 5]\nn = len(arr)\nlis = [1]*n\n \nfor i in range(1, n):\n    for j in range(i):\n        if arr[i] \u003e arr[j] and lis[i]\u003c lis[j] + 1:\n            lis[i] = lis[j]+1\n \nprint(max(lis))\n```"}]}, {"conversation_id": "150", "flagged_count": 1, "turns": [{"index": 450, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 451, "is_flagged": false, "role": "user", "text": "Design an algorithm to find the maximum profit of a stock buy and sell when given it\u0027s historical prices."}, {"index": 452, "is_flagged": true, "metric_value": "1", "role": "assistant", "text": "Here\u0027s a simple algorithm to find the maximum profit of a stock buy and sell:\n\n1. Initialize variables: \n   - min_price = historical_prices[0] (set the minimum value to the first price)\n   - max_profit = 0 (set the maximum profit to zero)\n\n2. Loop through each price in the historical prices:\n   - If the current price is less than the minimum price, update the minimum price with the current price.\n   - Otherwise, calculate the current profit by subtracting the minimum price from the current price.\n   - If the current profit is greater than the maximum profit, update the maximum profit with the current profit.\n\n3. Return the maximum profit.\n\nHere\u0027s the code implementation in Python:\n\n```\ndef max_stock_profit(historical_prices):\n    min_price = historical_prices[0]\n    max_profit = 0\n    \n    for price in historical_prices:\n        if price \u003c min_price:\n            min_price = price\n        else:\n            profit = price - min_price\n            if profit \u003e max_profit:\n                max_profit = profit\n                \n    return max_profit\n```\n\nSimply call the function `max_stock_profit` with a list of historical prices as argument to obtain the maximum profit that can be earned by buying and selling the stock."}]}, {"conversation_id": "154", "flagged_count": 1, "turns": [{"index": 462, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 463, "is_flagged": false, "role": "user", "text": "Given a binary tree, check if it is a binary search tree. Write a Java function to do it. Assume the Node class is implemented."}, {"index": 464, "is_flagged": true, "metric_value": "1", "role": "assistant", "text": "To check if a binary tree is a binary search tree (BST), we need to ensure that all the nodes in the left subtree of a node are less than or equal to the current node\u0027s value while all the nodes in the right subtree of a node are greater than the current node\u0027s value. Below is a Java function that checks if a binary tree is a BST.\n\n```\npublic boolean isBST(Node root) {\n    return isBST(root, Integer.MIN_VALUE, Integer.MAX_VALUE);\n}\n\nprivate boolean isBST(Node node, int min, int max) {\n    if (node == null) {\n        return true;\n    }\n    if (node.data \u003c min || node.data \u003e max) {\n        return false;\n    }\n    return isBST(node.left, min, node.data - 1) \u0026\u0026 isBST(node.right, node.data + 1, max);\n}\n```\n\nThe `isBST` function takes in the root node of the binary tree and calls the private `isBST` function with the minimum and maximum possible integer values as the initial parameters. The `isBST` function checks if the node is null, in which case it returns true. Otherwise, it checks if the node\u0027s value is within the given range of min and max. If not, it returns false because the node violates the BST property. If the value is within the given range, it recursively checks the left and right subtrees with updated min and max values. If both subtrees return true, the function returns true."}]}, {"conversation_id": "155", "flagged_count": 1, "turns": [{"index": 465, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 466, "is_flagged": true, "metric_value": "1", "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\nPlease come up with a question/answer pair, in the specified JSON format, for the following text:\n----------------\nThe scream of sirens and the clank of steel on cobblestones echoed down the mean, dark streets. Frightened East Berliners peeked from behind their curtains to see military convoys stretching for blocks. First came the motorcycle outriders, then jeeps, trucks and buses crammed with grim, steel-helmeted East German troops. Rattling in their wake were the tanks \u2014 squat Russian-built T-34s and T-54s. At each major intersection, a platoon peeled off and ground to a halt, guns at the ready. The rest headed on for the sector border, the 25-mile frontier that cuts through the heart of Berlin like a jagged piece of glass. As the troops arrived at scores of border points, cargo trucks were already unloading rolls of barbed wire, concrete posts, wooden horses, stone blocks, picks and shovels. When dawn came four hours later, a wall divided East Berlin from West for the first time in eight years."}, {"index": 467, "is_flagged": false, "role": "assistant", "text": "{\n    \"question\": \"What did the troops use to block off the border between East Berlin and West Berlin?\",\n    \"answer\": \"Cargo trucks were used to unload rolls of barbed wire, concrete posts, wooden horses, stone blocks, picks and shovels to block off the border between East Berlin and West Berlin.\"\n}"}]}, {"conversation_id": "347", "flagged_count": 1, "turns": [{"index": 1041, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1042, "is_flagged": false, "role": "user", "text": "Create a function to check whether two strings are anagrams."}, {"index": 1043, "is_flagged": true, "metric_value": "1", "role": "assistant", "text": "Here is one possible implementation in Python:\n\n```python\ndef is_anagram(s1, s2):\n    # Quick check to see if the strings are the same length\n    if len(s1) != len(s2):\n        return False\n    \n    # Create dictionaries to hold the count of each character in each string\n    char_count1 = {}\n    char_count2 = {}\n    \n    # Populate the dictionaries with the counts\n    for i in range(len(s1)):\n        char_count1[s1[i]] = char_count1.get(s1[i], 0) + 1\n        char_count2[s2[i]] = char_count2.get(s2[i], 0) + 1\n    \n    # Compare the dictionaries to see if they\u0027re the same\n    return char_count1 == char_count2\n```"}]}, {"conversation_id": "478", "flagged_count": 1, "turns": [{"index": 1434, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 1435, "is_flagged": false, "role": "user", "text": "Compose a simple melody for the following lyrics.\n\n### Input:\n\"I can\u0027t find the words to say, but I\u0027m feeling OK\""}, {"index": 1436, "is_flagged": true, "metric_value": "1", "role": "assistant", "text": "Here\u0027s a simple melody you can use for the provided lyrics using the C major scale:\n\n```\nC-C-D-E    F-E-D-C   G\u2014E-C-E-C   G-G-A-A-A-A\nI can\u0027t find the words to say, but I\u0027m feeling OK\n```"}]}, {"conversation_id": "481", "flagged_count": 1, "turns": [{"index": 1443, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 1444, "is_flagged": false, "role": "user", "text": "Design a function to count the number of unique characters in a string.\n\n### Input:\nstring = \u201cButterflies\u201d"}, {"index": 1445, "is_flagged": true, "metric_value": "2", "role": "assistant", "text": "Here is one way to count the number of unique characters in a string using Python:\n\n```\ndef count_unique_chars(string):\n    unique_chars = set(string)\n    return len(unique_chars)\n```\n\nTo use this function with the example input you provided, we can call the function like this:\n\n```\nstring = \"Butterflies\"\ncount = count_unique_chars(string)\nprint(f\"Number of unique characters in \u0027{string}\u0027 is: {count}\")\n```\n\nThis would output:\n`Number of unique characters in \u0027Butterflies\u0027 is: 9`"}]}, {"conversation_id": "484", "flagged_count": 1, "turns": [{"index": 1452, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1453, "is_flagged": false, "role": "user", "text": "Write a Python function that takes two integers and returns their least common multiple."}, {"index": 1454, "is_flagged": true, "metric_value": "1", "role": "assistant", "text": "```python\ndef lcm(x, y):\n    \"\"\"\n    This function takes two integers and returns their least common multiple.\n    Parameters:\n    x (int): first integer\n    y (int): second integer\n    Returns:\n    int: Least Common Multiple (LCM) of x and y\n    \"\"\"\n    # Find the greater number between x and y\n    if x \u003e y:\n        greater = x\n    else:\n        greater = y\n\n    # Continue looping until a common multiple is found\n    while True:\n        if greater % x == 0 and greater % y == 0:\n            lcm = greater\n            break\n        greater += 1\n\n    return lcm\n```"}]}, {"conversation_id": "679", "flagged_count": 1, "turns": [{"index": 2037, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2038, "is_flagged": false, "role": "user", "text": "Write a function to convert Fahrenheit to Celsius"}, {"index": 2039, "is_flagged": true, "metric_value": "2", "role": "assistant", "text": "Here is a function written in Python that can convert degrees Fahrenheit to degrees Celsius:\n\n```python\ndef fahrenheit_to_celsius(fahrenheit):\n    \"\"\"\n    Converts degrees fahrenheit to degrees celsius.\n    \"\"\"\n    celsius = (fahrenheit - 32) * 5/9\n    return celsius\n```\n\nTo use this function, pass in a value representing degrees Fahrenheit as an argument. The function will then return the equivalent temperature in degrees Celsius. For example:\n\n```python\ntemp_in_fahrenheit = 98.6\ntemp_in_celsius = fahrenheit_to_celsius(temp_in_fahrenheit)\nprint(f\u0027{temp_in_fahrenheit} Fahrenheit is equal to {temp_in_celsius:.2f} Celsius.\u0027)\n```"}]}, {"conversation_id": "690", "flagged_count": 1, "turns": [{"index": 2070, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2071, "is_flagged": false, "role": "user", "text": "Write an algorithm to find the minimum element in an array."}, {"index": 2072, "is_flagged": true, "metric_value": "1", "role": "assistant", "text": "1. Start by setting the first element of the array as the minimum value. Let\u0027s call this `min_value`.\n2. Iterate over the array from the second element to the end. For each element:\n    a. If the current element is less than `min_value`, update `min_value` to the current element.\n3. Return `min_value`.\n4. End.\n\nIn code, this could be written in Python as:\n\n```python\ndef find_min_element(array):\n    min_value = array[0]\n    for element in array[1:]:\n        if element \u003c min_value:\n            min_value = element\n    return min_value\n```"}]}, {"conversation_id": "713", "flagged_count": 1, "turns": [{"index": 2139, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2140, "is_flagged": true, "metric_value": "1", "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\n\"\"\"\ntempl2 = \"\"\"Please come up with a question/answer pair, in the specified JSON format, for the following text:\n----------------\nMillions of lives could be saved by a groundbreaking set of new vaccines for a range of conditions including cancer, experts have said. A leading pharmaceutical firm said it is confident that jabs for cancer, cardiovascular and autoimmune diseases, and other conditions will be ready by 2030.\n\nStudies into these vaccinations are also showing \u201ctremendous promise\u201d, with some researchers saying 15 years\u2019 worth of progress has been \u201cunspooled\u201d in 12 to 18 months thanks to the success of the Covid jab.\n\nDr Paul Burton, the chief medical officer of pharmaceutical company Moderna, said he believes the firm will be able to offer such treatments for \u201call sorts of disease areas\u201d in as little as five years.\n\nThe firm, which created a leading coronavirus vaccine, is developing cancer vaccines that target different tumor types."}, {"index": 2141, "is_flagged": false, "role": "assistant", "text": "{\n    \"question\": \"What is Moderna developing vaccines for?\",\n    \"answer\": \"Moderna is developing vaccines for cancer, cardiovascular and autoimmune diseases, and other conditions.\"\n}"}]}, {"conversation_id": "727", "flagged_count": 1, "turns": [{"index": 2181, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2182, "is_flagged": true, "metric_value": "1", "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\n\"\"\"\ntempl2 = \"\"\"Please come up with three question/answer pairs, in the specified JSON format, for the following text:\n----------------\nUS officials have hinted that this counteroffensive is coming in the next month or so, while Ukrainian forces on the ground have said they are ready to attack when Kyiv gives the word.\n\nAccording to military analysts who have looked at the documents, certain parts appear to be modified from their original format and exaggerate the American tally of Ukrainians killed in the war while downplaying the estimates of Russian troops killed.\n\nOne of the slides states that Russia has lost 16,000 to 17,500 soldiers since the invasion, while Ukraine had suffered as many as 71,500 troop deaths. This is a gross underestimate from what the Pentagon and other analysts have estimated for Russian casualties with nearly 200,000 killed and injured. Ukraine has had more than 100,000 casualties in the war, according to the Pentagon.\n\nThe documents appear to represent a significant breach of American intelligence, detailing information that has not been publicly stated by the Pentagon."}, {"index": 2183, "is_flagged": false, "role": "assistant", "text": "{\n    \"question\": \"What are the documents that military analysts have looked at?\",\n    \"answer\": \"The documents that military analysts have looked at appear to be modified and contain information about the tally of casualties in the Ukraine-Russia war.\"\n},\n{\n    \"question\": \"What does one of the slides in the documents state?\",\n    \"answer\": \"One of the slides in the documents states that Russia has lost 16,000 to 17,500 soldiers since the invasion, while Ukraine had suffered as many as 71,500 troop deaths.\"\n},\n{\n    \"question\": \"What does the article suggest about the documents?\",\n    \"answer\": \"The article suggests that the documents represent a significant breach of American intelligence, as they contain information that has not been publicly stated by the Pentagon.\"\n}"}]}, {"conversation_id": "733", "flagged_count": 1, "turns": [{"index": 2199, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2200, "is_flagged": false, "role": "user", "text": "Give me a CSS code to make the font size 20px."}, {"index": 2201, "is_flagged": true, "metric_value": "2", "role": "assistant", "text": "Here is a CSS code that will set the font size to 20px:\n\n```css\n    .text{\n        font-size: 20px;\n    }\n```\n\nYou can assign the `text` class to any of your HTML elements to apply this font size to it, like this:\n\n```html\n    \u003cp class=\"text\"\u003eThis is some text.\u003c/p\u003e\n```"}]}]
            },
            
            'rec_9': {
                title: "Outliers detected in format format complexity score",
                conversations: [{"conversation_id": "16", "flagged_count": 1, "turns": [{"index": 48, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 49, "is_flagged": false, "role": "user", "text": "Refactor this code and add comments.\n\n### Input:\ndef factorialize(num):\n  factorial = 1\n  for i in range(1, num):\n    factorial *= i\n  \n  return factorial"}, {"index": 50, "is_flagged": true, "metric_value": "0.50", "role": "assistant", "text": "```\n# This is a function that calculates the factorial of a given number\ndef factorialize(num):\n  # Set the variable \u0027factorial\u0027 to 1\n  factorial = 1\n  # Iterate from 1 to \u0027num\u0027 using the range function\n  for i in range(1, num):\n    # Multiply the \u0027factorial\u0027 variable by the current number in the iteration\n    factorial *= i\n  # Return the final value of \u0027factorial\u0027\n  return factorial\n```"}]}, {"conversation_id": "134", "flagged_count": 1, "turns": [{"index": 402, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 403, "is_flagged": true, "metric_value": "0.70", "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\nPlease come up with a question/answer pair, in the specified JSON format, for the following text:\n----------------\nPreviously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n\nAlps Electric produced the first quality dye-sub printers for home consumers in the $500\u2013$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n\nThe ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n\nSeveral corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n\nDesktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware."}, {"index": 404, "is_flagged": false, "role": "assistant", "text": "{\n    \"question\": \"What are some of the applications of dye-sub photo printing?\",\n    \"answer\": \"Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications.\"\n}"}]}, {"conversation_id": "144", "flagged_count": 1, "turns": [{"index": 432, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 433, "is_flagged": false, "role": "user", "text": "Write a function to calculate the factorial of a given number."}, {"index": 434, "is_flagged": true, "metric_value": "0.50", "role": "assistant", "text": "Sure! Here\u0027s one way to write a function to calculate the factorial of a given number in Python:\n\n```python\ndef factorial(n):\n    # Base case\n    if n \u003c= 1:\n        return 1\n    # Recursive case\n    else:\n        return n * factorial(n-1)\n\n# Example: Calculate the factorial of 5\nnum = 5\nprint(f\"The factorial of {num} is {factorial(num)}\")\n```\n\nThis function uses recursion, which means that it calls itself until the base case (n \u003c= 1) is reached. When the base case is reached, the function returns 1. Otherwise, the function will continue to call itself, multiplying n by the factorial of (n-1) at each call, until the base case is reached."}]}, {"conversation_id": "145", "flagged_count": 1, "turns": [{"index": 435, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 436, "is_flagged": false, "role": "user", "text": "Write a function to convert a given string to uppercase unless it starts with the string \"Rejected\""}, {"index": 437, "is_flagged": true, "metric_value": "0.50", "role": "assistant", "text": "Here\u0027s a Python implementation of the function:\n\n```\ndef uppercase_unless_rejected(string):\n    if string.startswith(\"Rejected\"):\n        return string\n    else:\n        return string.upper()\n```\n\nThis function first checks if the string starts with the substring \"Rejected\". If it does, it simply returns the original string, without any uppercase transformation. If, on the other hand, the string does not start with \"Rejected\", the function returns a new string which is the uppercase version of the original string."}]}, {"conversation_id": "146", "flagged_count": 2, "turns": [{"index": 438, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 439, "is_flagged": true, "metric_value": "0.40", "role": "user", "text": "Extract the positive integers from an array?\n\n### Input:\narr = [5,-2,8,-7,3,-1,0,5]"}, {"index": 440, "is_flagged": true, "metric_value": "0.50", "role": "assistant", "text": "To extract the positive integers from the given array, you can use the filter() function which will return the values that evaluate to True based on the condition provided in the lambda function. \n\nHere\u0027s the Python code snippet:\n\n```\narr = [5,-2,8,-7,3,-1,0,5]\n\nresult = list(filter(lambda x: x \u003e 0, arr))\n\nprint(result)\n```\n\nOutput: [5, 8, 3, 5]\n\nThe filter function iterates through each element in the array and checks if the element is greater than 0, which means it\u0027s a positive integer. If it\u0027s true, the element is included in the result list. Next, we display the result list which contains only the positive integers."}]}, {"conversation_id": "147", "flagged_count": 1, "turns": [{"index": 441, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 442, "is_flagged": false, "role": "user", "text": "Write a javascript function which calculates the square root of a number without any libraries."}, {"index": 443, "is_flagged": true, "metric_value": "0.50", "role": "assistant", "text": "Sure, here\u0027s an example:\n\n```\nfunction calculateSquareRoot(num) {\n  if (num \u003c 0) return NaN;\n  if (num === 0) return 0;\n  let x = num;\n  while (true) {\n    const new_x = (x + num / x) / 2;\n    if (Math.abs(new_x - x) \u003c 0.000001) return new_x;\n    x = new_x;\n  }\n}\n```\n\nThis function uses the Babylonian method for computing square roots, which is an iterative method that repeatedly improves the estimate of the square root using the formula: `x_(n+1) = (x_n + num / x_n) / 2`. The loop continues until the difference between the current estimate and the previous estimate is less than some small epsilon (in this case, `0.000001`)."}]}, {"conversation_id": "149", "flagged_count": 2, "turns": [{"index": 447, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 448, "is_flagged": true, "metric_value": "0.40", "role": "user", "text": "Compute the longest increasing subsequence of a given array?\n\n### Input:\narr = [7, 10, 8, 11, 9, 5]"}, {"index": 449, "is_flagged": true, "metric_value": "0.50", "role": "assistant", "text": "The longest increasing subsequence of the given array is [7, 8, 9, 11]. It has a length of 4.\n\nTo compute this, you can use dynamic programming to find the length of the longest increasing subsequence ending at each position, then take the maximum value. Here is an example of how to calculate it in Python:\n\n```python\narr = [7, 10, 8, 11, 9, 5]\nn = len(arr)\nlis = [1]*n\n \nfor i in range(1, n):\n    for j in range(i):\n        if arr[i] \u003e arr[j] and lis[i]\u003c lis[j] + 1:\n            lis[i] = lis[j]+1\n \nprint(max(lis))\n```"}]}, {"conversation_id": "150", "flagged_count": 1, "turns": [{"index": 450, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 451, "is_flagged": false, "role": "user", "text": "Design an algorithm to find the maximum profit of a stock buy and sell when given it\u0027s historical prices."}, {"index": 452, "is_flagged": true, "metric_value": "0.50", "role": "assistant", "text": "Here\u0027s a simple algorithm to find the maximum profit of a stock buy and sell:\n\n1. Initialize variables: \n   - min_price = historical_prices[0] (set the minimum value to the first price)\n   - max_profit = 0 (set the maximum profit to zero)\n\n2. Loop through each price in the historical prices:\n   - If the current price is less than the minimum price, update the minimum price with the current price.\n   - Otherwise, calculate the current profit by subtracting the minimum price from the current price.\n   - If the current profit is greater than the maximum profit, update the maximum profit with the current profit.\n\n3. Return the maximum profit.\n\nHere\u0027s the code implementation in Python:\n\n```\ndef max_stock_profit(historical_prices):\n    min_price = historical_prices[0]\n    max_profit = 0\n    \n    for price in historical_prices:\n        if price \u003c min_price:\n            min_price = price\n        else:\n            profit = price - min_price\n            if profit \u003e max_profit:\n                max_profit = profit\n                \n    return max_profit\n```\n\nSimply call the function `max_stock_profit` with a list of historical prices as argument to obtain the maximum profit that can be earned by buying and selling the stock."}]}, {"conversation_id": "152", "flagged_count": 1, "turns": [{"index": 456, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 457, "is_flagged": true, "metric_value": "0.40", "role": "user", "text": "Given an array of integers, return the first repeating element.\n\n### Input:\narr = [3, 6, 7, 1, 4, 6, 7]"}, {"index": 458, "is_flagged": false, "role": "assistant", "text": "The first repeating element in the array is 6."}]}, {"conversation_id": "153", "flagged_count": 2, "turns": [{"index": 459, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 460, "is_flagged": true, "metric_value": "0.40", "role": "user", "text": "Find the two non-repeating elements in an array of repeating elements?\n\n### Input:\narr = [2, 3, 7, 9, 11, 2, 3, 11]"}, {"index": 461, "is_flagged": true, "metric_value": "0.40", "role": "assistant", "text": "You can use hash maps to find the two non-repeating elements in an array of repeating elements. Here\u0027s an example code written in Python to achieve that:\n\narr = [2, 3, 7, 9, 11, 2, 3, 11]\n\n# creating an empty hash table\nhashTable = {}\n\n# iterating through the array\nfor i in arr:\n    # checking if the element is present in the hash table or not\n    if i in hashTable:\n        # if present increment its value by 1\n        hashTable[i] = hashTable[i] + 1\n    else:\n        # if not present, then add the element in the hash table with the value of 1\n        hashTable[i] = 1\n\n# creating an empty list that will store the result\nresult = []\n\n# iterating through the hash table\nfor i in hashTable:\n    # checking if the value of the element is 1 or not\n    if hashTable[i] == 1:\n        # if yes, then add it to the result\n        result.append(i)\n\nprint(f\u0027The two non-repeating elements are: {result}\u0027)\n\n###Output: \n#The two non-repeating elements are: [7, 9]"}]}, {"conversation_id": "154", "flagged_count": 1, "turns": [{"index": 462, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 463, "is_flagged": false, "role": "user", "text": "Given a binary tree, check if it is a binary search tree. Write a Java function to do it. Assume the Node class is implemented."}, {"index": 464, "is_flagged": true, "metric_value": "0.50", "role": "assistant", "text": "To check if a binary tree is a binary search tree (BST), we need to ensure that all the nodes in the left subtree of a node are less than or equal to the current node\u0027s value while all the nodes in the right subtree of a node are greater than the current node\u0027s value. Below is a Java function that checks if a binary tree is a BST.\n\n```\npublic boolean isBST(Node root) {\n    return isBST(root, Integer.MIN_VALUE, Integer.MAX_VALUE);\n}\n\nprivate boolean isBST(Node node, int min, int max) {\n    if (node == null) {\n        return true;\n    }\n    if (node.data \u003c min || node.data \u003e max) {\n        return false;\n    }\n    return isBST(node.left, min, node.data - 1) \u0026\u0026 isBST(node.right, node.data + 1, max);\n}\n```\n\nThe `isBST` function takes in the root node of the binary tree and calls the private `isBST` function with the minimum and maximum possible integer values as the initial parameters. The `isBST` function checks if the node is null, in which case it returns true. Otherwise, it checks if the node\u0027s value is within the given range of min and max. If not, it returns false because the node violates the BST property. If the value is within the given range, it recursively checks the left and right subtrees with updated min and max values. If both subtrees return true, the function returns true."}]}, {"conversation_id": "155", "flagged_count": 1, "turns": [{"index": 465, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 466, "is_flagged": true, "metric_value": "0.70", "role": "user", "text": "You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\u0027s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n```\n{{\n    \"question\": \"$YOUR_QUESTION_HERE\",\n    \"answer\": \"$THE_ANSWER_HERE\"\n}}\n```\nEverything between the ``` must be valid json.\nPlease come up with a question/answer pair, in the specified JSON format, for the following text:\n----------------\nThe scream of sirens and the clank of steel on cobblestones echoed down the mean, dark streets. Frightened East Berliners peeked from behind their curtains to see military convoys stretching for blocks. First came the motorcycle outriders, then jeeps, trucks and buses crammed with grim, steel-helmeted East German troops. Rattling in their wake were the tanks \u2014 squat Russian-built T-34s and T-54s. At each major intersection, a platoon peeled off and ground to a halt, guns at the ready. The rest headed on for the sector border, the 25-mile frontier that cuts through the heart of Berlin like a jagged piece of glass. As the troops arrived at scores of border points, cargo trucks were already unloading rolls of barbed wire, concrete posts, wooden horses, stone blocks, picks and shovels. When dawn came four hours later, a wall divided East Berlin from West for the first time in eight years."}, {"index": 467, "is_flagged": false, "role": "assistant", "text": "{\n    \"question\": \"What did the troops use to block off the border between East Berlin and West Berlin?\",\n    \"answer\": \"Cargo trucks were used to unload rolls of barbed wire, concrete posts, wooden horses, stone blocks, picks and shovels to block off the border between East Berlin and West Berlin.\"\n}"}]}, {"conversation_id": "194", "flagged_count": 1, "turns": [{"index": 582, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 583, "is_flagged": true, "metric_value": "0.40", "role": "user", "text": "What\u0027s the output of this code?\n\n### Input:\nx = [1, 2, 3, 4]\ny = [i**2 for i in x]"}, {"index": 584, "is_flagged": false, "role": "assistant", "text": "The output of this code is a new list \u0027y\u0027 that contains the squares of each element in the list \u0027x\u0027. The resulting list \u0027y\u0027 is [1, 4, 9, 16]."}]}, {"conversation_id": "347", "flagged_count": 1, "turns": [{"index": 1041, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1042, "is_flagged": false, "role": "user", "text": "Create a function to check whether two strings are anagrams."}, {"index": 1043, "is_flagged": true, "metric_value": "0.70", "role": "assistant", "text": "Here is one possible implementation in Python:\n\n```python\ndef is_anagram(s1, s2):\n    # Quick check to see if the strings are the same length\n    if len(s1) != len(s2):\n        return False\n    \n    # Create dictionaries to hold the count of each character in each string\n    char_count1 = {}\n    char_count2 = {}\n    \n    # Populate the dictionaries with the counts\n    for i in range(len(s1)):\n        char_count1[s1[i]] = char_count1.get(s1[i], 0) + 1\n        char_count2[s2[i]] = char_count2.get(s2[i], 0) + 1\n    \n    # Compare the dictionaries to see if they\u0027re the same\n    return char_count1 == char_count2\n```"}]}, {"conversation_id": "351", "flagged_count": 1, "turns": [{"index": 1053, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 1054, "is_flagged": true, "metric_value": "0.40", "role": "user", "text": "Find the three least common values in the given list.\n\n### Input:\n[5, 10, 15, 8, 9, 5, 8, 10, 9]"}, {"index": 1055, "is_flagged": false, "role": "assistant", "text": "The three least common values in the given list are 15, 9 and 10, each appearing only once."}]}, {"conversation_id": "481", "flagged_count": 1, "turns": [{"index": 1443, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 1444, "is_flagged": false, "role": "user", "text": "Design a function to count the number of unique characters in a string.\n\n### Input:\nstring = \u201cButterflies\u201d"}, {"index": 1445, "is_flagged": true, "metric_value": "0.55", "role": "assistant", "text": "Here is one way to count the number of unique characters in a string using Python:\n\n```\ndef count_unique_chars(string):\n    unique_chars = set(string)\n    return len(unique_chars)\n```\n\nTo use this function with the example input you provided, we can call the function like this:\n\n```\nstring = \"Butterflies\"\ncount = count_unique_chars(string)\nprint(f\"Number of unique characters in \u0027{string}\u0027 is: {count}\")\n```\n\nThis would output:\n`Number of unique characters in \u0027Butterflies\u0027 is: 9`"}]}, {"conversation_id": "632", "flagged_count": 1, "turns": [{"index": 1896, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 1897, "is_flagged": true, "metric_value": "0.40", "role": "user", "text": "Reverse this array: [1, 2, 3, 4, 5]\n\n### Input:\n[1, 2, 3, 4, 5]"}, {"index": 1898, "is_flagged": false, "role": "assistant", "text": "Here is the reversed array: [5, 4, 3, 2, 1]"}]}]
            },
            
            'rec_10': {
                title: "Outliers detected in quality pii count",
                conversations: [{"conversation_id": "780", "flagged_count": 1, "turns": [{"index": 2340, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 2341, "is_flagged": false, "role": "user", "text": "Make a resume for the following job.\n\n### Input:\nSoftware Engineer"}, {"index": 2342, "is_flagged": true, "metric_value": "2", "role": "assistant", "text": "RESUME\n\nJane Doe\n1234 Some Street, Anytown USA, 12345\njanedoe@email.com\n(123) 456-7890\n\nObjective:\nSoftware Engineer position utilizing my skills in front-end development, programming, and problem-solving.\n\nEducation:\nB.S. Computer Science, Some University, Anytown USA\nGraduation Date: May, 2022\n\nSkills:\n\u2022 Strong knowledge of JavaScript and experience with popular front-end frameworks such as React and AngularJS\n\u2022 Proficient in HTML, CSS, and web development tools\n\u2022 Comfortable with Java, C++, and Python\n\u2022 Familiar with database design and SQL\n\u2022 Experience with Git for version control\n\u2022 Good work ethic and ability to learn quickly\n\nExperience:\nSoftware Engineering Intern, XYZ Company, SomeCity USA (June - August, 2021)\n\u2022 Developed and maintained web applications using React and NodeJS\n\u2022 Collaborated with team members in an agile environment to deliver high-quality software\n\u2022 Implemented back-end functionality with NodeJS and Express\n\nSoftware Development Intern, ABC Company, SomeCity USA (June - August, 2020)\n\u2022 Developed and maintained client-facing web applications\n\u2022 Assisted senior developers in implementing new features\n\u2022 Worked with front-end technologies such as JavaScript, HTML, and CSS\n\u2022 Maintained documentation of development work\n\nProjects:\n\u2022 Front-End Web Development Project - Developed a responsive web application for managing personal tasks using React\n\u2022 Database Design and Development Project - Created a database for managing employee data using SQL\n\u2022 Mobile Application Development Project - Developed a native mobile app for keeping track of daily expenses using React Native\n\nCertifications:\n\u2022 Oracle Certified Associate, Java SE 8 Programmer\n\u2022 Microsoft Technology Associate, HTML5 Application Development Fundamentals"}]}]
            },
            
            'rec_11': {
                title: "Outliers detected in training quality response word count",
                conversations: [{"conversation_id": "472", "flagged_count": 1, "turns": [{"index": 1416, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1417, "is_flagged": false, "role": "user", "text": "Write a blog post on how to deploy machine learning models."}, {"index": 1418, "is_flagged": true, "metric_value": "420.00", "role": "assistant", "text": "Deploying a machine learning (ML) model refers to the process of making your ML model available for use in a production environment. The process involves setting up the model on the server, creating an API or web service to interact with the model, and integrating it with other systems. In this guide, we\u0027ll go through the steps you need to take to deploy a machine learning model. \n\nStep 1: Choose Your Deployment Environment\nThe first step in deploying your machine learning model is to decide on a deployment environment. There are several options available, including cloud platforms like AWS, Azure, and Google Cloud, and on-premise servers. Each has its own advantages and disadvantages, and the one you choose will depend on your specific needs. This is an important decision, as it will impact how you implement the next steps. \n\nStep 2: Export the Model\nOnce you have selected your deployment environment, the next step is to export your trained model in a format that can be understood by the target environment. The most common format for storing and sharing machine learning models is the ONNX format, which stands for Open Neural Network Exchange. Other popular formats include PMML, JSON, and Pickle.\n\nStep 3: Containerize your Model\nThe next step is to containerize your model using a tool like Docker. Containerization involves packaging your model along with all its dependencies into a single, easy-to-deploy package. This makes it much easier to deploy your model on different environments, as it eliminates the need to manage dependencies manually.\n\nStep 4: Create a Web Service or API\nOnce you have containerized your model, you will need to create a Web Service or API that can be used to interact with your model. This is essentially a wrapper around your model that allows other systems to send data to your model and receive predictions. Most web services and APIs are built using Restful principles, which means that they use standard HTTP methods like GET, POST, and PUT to handle requests.\n\nStep 5: Deploy the Model\nOnce your model is containerized and you have set up your web service or API, the final step is to deploy the model to your chosen environment. This will typically involve uploading your container to your cloud provider or on-premise server, and then configuring your environment to run the container. Once your model is deployed, you can start sending requests to your API, and begin using the model in a production environment.\n\nIn conclusion, deploying a machine learning model involves"}]}, {"conversation_id": "625", "flagged_count": 1, "turns": [{"index": 1875, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1876, "is_flagged": false, "role": "user", "text": "Write a story about a doctor and his patient"}, {"index": 1877, "is_flagged": true, "metric_value": "421.00", "role": "assistant", "text": "Dr. Richard was a prominent neurosurgeon, known for his remarkable skills and ability to turn difficult cases into success stories. He had a long list of patients who had been successfully treated by him and were now living normal lives. One day, he was assigned a new patient, Emily, who had been in a terrible car accident and was suffering from a severe head injury.\n\nEmily was a young and vibrant woman in her mid-twenties. She had been in a coma for two weeks following the accident, and her family was desperate for her to wake up. When Dr. Richard first saw her, he could see the pain and anguish in the eyes of her family members. He could feel their hope, and he knew that he had to do everything in his power to help Emily.\n\nDr. Richard went through Emily\u0027s medical records and ordered a series of tests to understand the extent of her injuries. The results showed that she had severe swelling in her brain, which was causing pressure on the brainstem. Dr. Richard knew that this was a critical condition, which required immediate surgery.\n\nHe explained the situation to Emily\u0027s family and, with their consent, proceeded with the surgery. It was a long and complicated procedure, but Dr. Richard\u0027s skillful hands worked tirelessly to remove the damaged tissue and relieve the pressure on Emily\u0027s brain.\n\nWhen the surgery was over, Emily\u0027s family waited anxiously outside the operating room. Dr. Richard emerged several hours later, looking exhausted but relieved. He told them that the surgery had gone well, and Emily was now in stable condition.\n\nOver the next few days, Emily started showing signs of improvement. She finally woke up from her coma and slowly began to regain her strength. Dr. Richard visited her every day, monitoring her progress and offering her words of encouragement.\n\nAs the days went by, Emily\u0027s condition improved dramatically. She was soon able to walk and talk again, and her memory started coming back. Her family was overjoyed, and they were grateful to Dr. Richard for his dedication and skill in saving their beloved daughter\u0027s life.\n\nDr. Richard\u0027s relationship with Emily didn\u0027t end when she was discharged from the hospital. He continued to monitor her progress, and she became one of his success stories - a testament to his skill as a neurosurgeon. Emily\u0027s family would always remember him as the doctor who gave them their daughter back, and Dr. Richard would always remember Emily as the patient who reminded him why he"}]}, {"conversation_id": "667", "flagged_count": 1, "turns": [{"index": 2001, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2002, "is_flagged": false, "role": "user", "text": "Outline the key differences between the Linux and Windows operating systems."}, {"index": 2003, "is_flagged": true, "metric_value": "410.00", "role": "assistant", "text": "Linux and Windows are two distinct operating systems with several key differences. \n\n1. Development and Ownership: Windows is a commercial operating system owned by Microsoft, whereas Linux is an open-source operating system that is developed by a community of developers from around the world. Linux is released under the GNU Public License (GPL), which means that anyone can view, edit, and distribute its source code.\n\n2. User Interface: Windows has a graphical user interface (GUI) that is uniform across all versions and is familiar to most users. Linux, on the other hand, offers multiple desktop environments and interfaces, some of which resemble Windows, while others have a completely different look and feel.\n\n3. Command line: Linux relies heavily on the command line to perform various tasks, whereas Windows provides users with a more user-friendly, graphical interface. This may make it easier to perform basic tasks in Windows, but experienced users may find Linux\u0027s command-line interface more powerful and efficient.\n\n4. File system: The file systems used by Windows and Linux are different. Windows uses the New Technology File System (NTFS), while Linux uses a variety of file systems, including the extended file system (ext), XFS, and Btrfs. This can make it difficult to share files between the two operating systems.\n\n5. Security: Linux is often considered to be more secure than Windows, thanks in large part to its open-source nature. Its source code is freely available for review, making it easier for security flaws to be identified and fixed. Windows, on the other hand, is more vulnerable to viruses and malware due to its widespread use and closed-source code.\n\n6. Software and Compatibility: Windows has a larger software library, with many popular applications developed specifically for the platform. Linux has fewer applications, but most open-source software is available for the platform, and many Windows applications can be run using compatibility layers like Wine. Additionally, Linux can run on a wider variety of hardware, including older and lower-end systems, while Windows requires more resources to run smoothly.\n\n7. Cost: Windows is a commercial operating system that requires a paid license, while Linux is freely available for anyone to download and use. However, Windows comes pre-installed on most computers, making its cost less of an issue for many users.\n\nIn conclusion, the main differences between the two operating systems lie in their development, ownership, user interface, command-line usage, file systems, security, software, compatibility, and cost. Both systems have their strengths and weaknesses"}]}]
            },
            
            'rec_12': {
                title: "Empty or near-empty messages detected",
                conversations: [{"conversation_id": "80", "flagged_count": 1, "turns": [{"index": 240, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 241, "is_flagged": false, "role": "user", "text": "Classify the following incident as a breach of protocol. Output 1 for breach, and 0 for no breach.\n\n### Input:\nUsing a school laptop for personal use"}, {"index": 242, "is_flagged": true, "metric_value": "1", "role": "assistant", "text": "1"}]}, {"conversation_id": "98", "flagged_count": 1, "turns": [{"index": 294, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 295, "is_flagged": false, "role": "user", "text": "Classify the following statement as true or false.\n\n### Input:\nThe Supreme Court is the highest court in the US."}, {"index": 296, "is_flagged": true, "metric_value": "True", "role": "assistant", "text": "True"}]}, {"conversation_id": "753", "flagged_count": 1, "turns": [{"index": 2259, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 2260, "is_flagged": false, "role": "user", "text": "Label whether the following statements are true or false.\n\n### Input:\nSome amphibians have wings"}, {"index": 2261, "is_flagged": true, "metric_value": "False", "role": "assistant", "text": "False"}]}, {"conversation_id": "867", "flagged_count": 1, "turns": [{"index": 2601, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 2602, "is_flagged": false, "role": "user", "text": "Evaluate this blog post. Output \"good\" or \"bad\".\n\n### Input:\nThis blog post is about the importance of drinking water daily to stay health."}, {"index": 2603, "is_flagged": true, "metric_value": "bad.", "role": "assistant", "text": "bad."}]}, {"conversation_id": "903", "flagged_count": 1, "turns": [{"index": 2709, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 2710, "is_flagged": false, "role": "user", "text": "In this task, you need to convert numbers from words to digits.\nOutput should also be a number, for example \"twelve\" should be output as \"12\".\n\n### Input:\nforty eight"}, {"index": 2711, "is_flagged": true, "metric_value": "48", "role": "assistant", "text": "48"}]}, {"conversation_id": "906", "flagged_count": 1, "turns": [{"index": 2718, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 2719, "is_flagged": false, "role": "user", "text": "Identify the subject of the sentence.\nOutput should be a single word.\n\n### Input:\nRuth and Molly made cookies for the bake sale."}, {"index": 2720, "is_flagged": true, "metric_value": "Ruth", "role": "assistant", "text": "Ruth"}]}]
            },
            
            'rec_13': {
                title: "Many short messages detected",
                conversations: [{"conversation_id": "0", "flagged_count": 1, "turns": [{"index": 0, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1, "is_flagged": true, "metric_value": "6", "role": "user", "text": "Give three tips for staying healthy."}, {"index": 2, "is_flagged": false, "role": "assistant", "text": "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night."}]}, {"conversation_id": "1", "flagged_count": 1, "turns": [{"index": 3, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 4, "is_flagged": true, "metric_value": "6", "role": "user", "text": "What are the three primary colors?"}, {"index": 5, "is_flagged": false, "role": "assistant", "text": "The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB)."}]}, {"conversation_id": "2", "flagged_count": 1, "turns": [{"index": 6, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 7, "is_flagged": true, "metric_value": "6", "role": "user", "text": "Describe the structure of an atom."}, {"index": 8, "is_flagged": false, "role": "assistant", "text": "An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\n\nThe nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom\u0027s mass.\n\nSurrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \n\nIn a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is."}]}, {"conversation_id": "3", "flagged_count": 1, "turns": [{"index": 9, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 10, "is_flagged": true, "metric_value": "6", "role": "user", "text": "How can we reduce air pollution?"}, {"index": 11, "is_flagged": false, "role": "assistant", "text": "There are several ways to reduce air pollution, including:\n\n1. Reduce energy consumption: By conserving energy, we reduce the amount of pollution emitted from power plants. Some ways to achieve this include: using energy-efficient appliances and lighting, insulating homes, and shutting off lights and electronics when not in use.\n\n2. Use public transportation or carpool: Cars and trucks are major sources of pollution. By using public transportation, carpooling or biking, you can reduce the number of cars on the road, which in turn reduces emissions.\n\n3. Be responsible with waste: When waste is burned or decomposed, they can release harmful gases into the air. Dispose of hazardous waste correctly and recycle materials like plastics, paper, and glass when possible.\n\n4. Support clean energy sources: Renewable energy sources like solar and wind power generate little or no pollution. Support clean energy companies and projects, and consider installing renewable energy systems in your home.\n\n5. Drive efficiently: If you need to drive, make sure your vehicle is well maintained and drive efficiently by avoiding rapid acceleration and braking, and reducing idling.\n\n6. Be mindful of product choices: Some products and practices release more pollution than others. Choose products with eco-friendly packaging and try to minimize your use of aerosol sprays, which can release harmful chemicals into the air."}]}, {"conversation_id": "9", "flagged_count": 1, "turns": [{"index": 27, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 28, "is_flagged": true, "metric_value": "5", "role": "user", "text": "How did Julius Caesar die?"}, {"index": 29, "is_flagged": false, "role": "assistant", "text": "Julius Caesar, the Roman Military general, and statesman was assassinated on March 15, 44 BC. He was stabbed to death at a meeting of the Senate in the Theatre of Pompey by a group of senators who called themselves the Liberatores. Led by Gaius Cassius Longinus, and Marcus Junius Brutus, they believed that Caesar\u0027s dictatorship was subverting the Roman Republic and sought to restore the traditional republican government. The exact number of assailants is not known, but it is believed that a group of as many as 60 conspirators participated in the assassination, and each of them took turns in stabbing Caesar. The day of his death is still known as the Ides of March."}]}, {"conversation_id": "10", "flagged_count": 2, "turns": [{"index": 30, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 31, "is_flagged": true, "metric_value": "6", "role": "user", "text": "What is the capital of France?"}, {"index": 32, "is_flagged": true, "metric_value": "7", "role": "assistant", "text": "The capital city of France is Paris."}]}, {"conversation_id": "12", "flagged_count": 1, "turns": [{"index": 36, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 37, "is_flagged": true, "metric_value": "7", "role": "user", "text": "Discuss the causes of the Great Depression"}, {"index": 38, "is_flagged": false, "role": "assistant", "text": "The Great Depression was a period of economic decline that lasted from 1929-1939, making it the longest-lasting depression in modern history. It affected not only the United States but countries around the world, causing substantial social and economic upheaval.\n\nThere were several major causes of the Great Depression, which include:\n\n1. Stock Market Crash of 1929 - In October of 1929, the stock market experienced a significant fall that wiped out millions of investors. This event is considered by many to be the initial trigger of the Great Depression.\n\n2. Overproduction - During the 1920s, many industries produced more goods than consumers wanted or could afford. This ultimately led to a decline in demand for goods, causing job loss, lower wages, and business failure.\n\n3. High Tariffs and War Debts - Protectionist trade policies in the form of high tariffs led to a decline in global trade, as other countries retaliated with tariffs of their own. Additionally, many countries were struggling to repay war debts, which led to economic instability.\n\n4. Bank Failures - As demand for goods declined, many banks began to fail, causing a loss of confidence in the banking system. This led to a massive withdrawal of money from banks, causing even more banks to fail.\n\n5. Drought Conditions - The Dust Bowl was a severe drought and dust storm that hit the Great Plains region of the United States in the 1930s. This had a significant impact on agriculture, causing many farmers to lose their land and livelihoods which worsened the effects of the depression.\n\nOverall, the Great Depression was caused by a combination of factors, including economic, environmental, and political factors. Its impact was widespread, affecting millions of people around the world."}]}, {"conversation_id": "13", "flagged_count": 1, "turns": [{"index": 39, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 40, "is_flagged": false, "role": "user", "text": "Classify the following into animals, plants, and minerals\n\n### Input:\nOak tree, copper ore, elephant"}, {"index": 41, "is_flagged": true, "metric_value": "8", "role": "assistant", "text": "Animals: Elephant\nPlants: Oak tree\nMinerals: Copper ore"}]}, {"conversation_id": "15", "flagged_count": 1, "turns": [{"index": 45, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 46, "is_flagged": true, "metric_value": "7", "role": "user", "text": "Describe the function of a computer motherboard"}, {"index": 47, "is_flagged": false, "role": "assistant", "text": "The motherboard, also known as the mainboard or system board, is the central printed circuit board in a computer. It serves as the backbone or foundation for a computer, connecting all the different components such as the CPU, RAM, storage drives, expansion cards, and peripherals. The motherboard manages communication and data transfer between these components, allowing them to work together and perform their designated tasks.\n\nThe motherboard also includes important circuitry such as the power regulation circuit that provides power to the different components, and the clock generator which synchronizes the operation of these components. It also contains the BIOS (basic input/output system), which is a firmware that controls the boot process and provides an interface for configuring and managing the computer\u0027s hardware. Other features on a motherboard may include built-in networking, audio, and video capabilities.\n\nOverall, the function of a computer motherboard is to provide a platform for the integration and operation of all the various components that make up a computer, making it an essential part of any computing system."}]}, {"conversation_id": "19", "flagged_count": 1, "turns": [{"index": 57, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 58, "is_flagged": true, "metric_value": "5", "role": "user", "text": "What does DNA stand for?"}, {"index": 59, "is_flagged": false, "role": "assistant", "text": "DNA stands for Deoxyribonucleic Acid. It is the molecule that carries the genetic instructions used in the growth, development, functioning, and reproduction of all living organisms."}]}, {"conversation_id": "21", "flagged_count": 1, "turns": [{"index": 63, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 64, "is_flagged": true, "metric_value": "6", "role": "user", "text": "Generate a list of random words."}, {"index": 65, "is_flagged": false, "role": "assistant", "text": "Here is a list of ten random words: \n\n1. Quibble\n2. Avarice\n3. Embellish\n4. Resonance\n5. Mercurial\n6. Conflagration\n7. Indefatigable\n8. Mischievous\n9. Delineate\n10. Paragon"}]}, {"conversation_id": "23", "flagged_count": 1, "turns": [{"index": 69, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 70, "is_flagged": true, "metric_value": "7", "role": "user", "text": "Who is the world\u0027s most famous painter?"}, {"index": 71, "is_flagged": false, "role": "assistant", "text": "The title of the world\u0027s most famous painter is subjective and can vary depending on an individual\u0027s personal opinion. However, many people consider Leonardo da Vinci, a famous artist of the Italian Renaissance, as one of the most famous painters in the world. He is known for his iconic works including the \"Mona Lisa\" and \"The Last Supper.\" Other notable painters often cited as being among the most famous in the world include Vincent van Gogh, Pablo Picasso, and Michelangelo."}]}, {"conversation_id": "24", "flagged_count": 1, "turns": [{"index": 72, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 73, "is_flagged": true, "metric_value": "6", "role": "user", "text": "Explain the concept of cogging torque."}, {"index": 74, "is_flagged": false, "role": "assistant", "text": "Cogging torque is a torque that\u0027s produced by the interaction between the magnetic field of the stator (the stationary component in an electric motor or generator) and the rotor (the rotating component). This interaction produces a magnetic attraction between the stator teeth and the rotor poles. This scenario leads to an uneven torque, which makes the motor to experience a jerky motion instead of smooth rotation.\n\nCogging torque is mainly experienced in motors with a large number of turns in the rotor and stator windings and when the motor has a small air gap. The cogging effect can be reduced by skewing the stator or rotor slots or by choosing the number of stator and rotor slots accordingly. A proper design reduces the cogging torque, thus resulting in smooth and continuous rotation."}]}, {"conversation_id": "25", "flagged_count": 1, "turns": [{"index": 75, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 76, "is_flagged": true, "metric_value": "7", "role": "user", "text": "Look up the boiling point of water."}, {"index": 77, "is_flagged": false, "role": "assistant", "text": "The boiling point of water is 100 degrees Celsius or 212 degrees Fahrenheit at standard atmospheric pressure."}]}, {"conversation_id": "26", "flagged_count": 1, "turns": [{"index": 78, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 79, "is_flagged": true, "metric_value": "9", "role": "user", "text": "Describe the life and reign of King Charles II."}, {"index": 80, "is_flagged": false, "role": "assistant", "text": "King Charles II was the monarch of England, Scotland, and Ireland from 1660 to 1685. Born on May 29, 1630, he was the son of King Charles I and Queen Henrietta Maria. During his father\u0027s reign, England was embroiled in civil war between the royalists who supported the monarchy and the parliamentarians who sought to limit its powers. Eventually, the parliamentarians, led by Oliver Cromwell, emerged victorious, and Charles I was executed in 1649.\n\nCharles II went into exile and lived for a time in France and the Netherlands. In 1660, following the death of Cromwell and the collapse of the Commonwealth government, Charles was invited back to England and restored to the throne.\n\nHis reign, often referred to as the Restoration, was marked by a period of relative stability and prosperity. Charles re-established the Church of England as the national church, and he worked to rebuild the country after the devastation of the civil wars. He also pursued a policy of religious tolerance, issuing the Declaration of Indulgence in 1672, which granted freedom of worship to non-Anglican Protestants.\n\nCharles was also known for his interest in science and the arts, and he was a patron of the Royal Society, a scientific organization founded in 1660. His court was known for its extravagance and hedonism, and Charles himself had a reputation as a womanizer.\n\nIn the later years of his reign, Charles faced increasing opposition from parliament, particularly over his pro-Catholic policies and his attempts to bypass their authority. In 1678, the Test Act was passed, requiring all officeholders to swear an oath against Catholicism, which Charles reluctantly signed.\n\nCharles died on February 6, 1685, and was succeeded by his brother James II. He is remembered as a popular and charismatic monarch who helped to restore stability to England after a period of turmoil."}]}, {"conversation_id": "29", "flagged_count": 1, "turns": [{"index": 87, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 88, "is_flagged": true, "metric_value": "8", "role": "user", "text": "Explain the process of cellular respiration in plants."}, {"index": 89, "is_flagged": false, "role": "assistant", "text": "Cellular respiration in plants is the process by which they convert glucose into ATP, a molecule used as the primary energy source by cells. This process is similar to the one in animals, and it involves various enzymatic reactions.\n\nIn plants, cellular respiration usually occurs in the mitochondria, and the process can be divided into three stages: glycolysis, the citric acid cycle (also known as the Krebs cycle), and the electron transport chain.\n\nDuring glycolysis, a molecule of glucose is broken down into two molecules of pyruvate. This process takes place in the cytoplasm of the cell and produces two molecules of ATP and two of NADH, a molecule that carries electrons.\n\nThe next stage, the citric acid cycle, takes place inside the mitochondria. Here, the two pyruvate molecules produced in glycolysis are further broken down to produce ATP, NADH, and FADH2, another molecule that carries electrons.\n\nFinally, in the electron transport chain, the NADH and FADH2 molecules produced in the previous stages carry electrons to a series of protein complexes embedded in the inner membrane of the mitochondrion. These electrons produce a flow of protons across the membrane, creating a gradient that drives the production of ATP through a process known as chemiosmosis.\n\nIn summary, cellular respiration in plants is a process that involves the breakdown of glucose to produce ATP, which is used as the primary energy source by cells. This process occurs in the mitochondria and involves several stages including glycolysis, the citric acid cycle, and the electron transport chain."}]}, {"conversation_id": "30", "flagged_count": 1, "turns": [{"index": 90, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 91, "is_flagged": false, "role": "user", "text": "Rewrite the following sentence using active voice.\n\n### Input:\nThe news report was read by the captain."}, {"index": 92, "is_flagged": true, "metric_value": "6", "role": "assistant", "text": "The captain read the news report."}]}, {"conversation_id": "32", "flagged_count": 1, "turns": [{"index": 96, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 97, "is_flagged": true, "metric_value": "5", "role": "user", "text": "Outline the consequences of deforestation."}, {"index": 98, "is_flagged": false, "role": "assistant", "text": "Deforestation, or the large-scale removal of trees from a forested area, has numerous consequences that can have significant impacts on the environment, economy, and local communities. Some of the consequences of deforestation include:\n\n1. Loss of biodiversity: Forests are home to a wide variety of plant and animal species, many of which rely on the forest for their survival. Removing large areas of trees can destroy their habitat, leading to a loss of biodiversity.\n\n2. Soil erosion: Trees and their root systems help hold soil in place and prevent erosion. Deforestation can leave soil exposed and vulnerable to being washed away during heavy rain or strong winds.\n\n3. Climate change: Forests play an important role in helping to regulate the earth\u0027s climate by storing carbon dioxide. When trees are removed, the stored carbon dioxide is released into the atmosphere, contributing to climate change.\n\n4. Loss of livelihoods: Many indigenous and local communities rely on forests for their livelihoods, using the wood for fuel, building materials, and food. Deforestation can have a devastating impact on these communities, leaving them without resources or the means to support themselves.\n\n5. Economic impact: Forests provide a variety of valuable resources, including wood, paper, and other forest products. Deforestation can reduce the availability of these resources, impacting the economy and industries that rely on them.\n\n6. Flooding: Forests help to absorb and store water, reducing the risk of flooding in nearby areas. Deforestation can increase the risk of flooding, causing damage to homes, infrastructure, and crops.\n\nOverall, deforestation can have a wide range of negative impacts, affecting not only the environment but also the economy and local communities. It is important to find sustainable ways to manage forest resources to prevent deforestation and its associated consequences."}]}, {"conversation_id": "34", "flagged_count": 1, "turns": [{"index": 102, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 103, "is_flagged": false, "role": "user", "text": "Arrange the words in the given sentence to form a grammatically correct sentence.\n\n### Input:\nquickly the brown fox jumped"}, {"index": 104, "is_flagged": true, "metric_value": "5", "role": "assistant", "text": "The brown fox jumped quickly."}]}]
            },
            
            'rec_14': {
                title: "Highly repetitive content detected",
                conversations: [{"conversation_id": "848", "flagged_count": 1, "turns": [{"index": 2544, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2545, "is_flagged": false, "role": "user", "text": "Create an outline for a presentation about the current state of the world economy."}, {"index": 2546, "is_flagged": true, "metric_value": "Yes", "role": "assistant", "text": "I. Introduction\n    A. Brief overview of the world economy\n    B. Importance of understanding the global economic climate\n    C. Purpose of presentation\n \nII. Global Economic Overview\n    A. Overview of global growth and economic output\n    B. Key factors driving economic growth\n    C. Analysis of global trade and investment flows\n \nIII. Regional Analysis\n    A. North America\n        1. Economic performance and growth prospects\n        2. Key drivers of growth\n        3. Trade and investment flows\n    B. Europe\n        1. Economic performance and growth prospects\n        2. Key drivers of growth\n        3. Trade and investment flows\n    C. Asia\n        1. Economic performance and growth prospects\n        2. Key drivers of growth\n        3. Trade and investment flows\n    D. Latin America\n        1. Economic performance and growth prospects\n        2. Key drivers of growth\n        3. Trade and investment flows  \n    E. Africa \n        1. Economic performance and growth prospects\n        2. Key drivers of growth\n        3. Trade and investment flows   \n\nIV. Key Challenges and Risks\n    A. Overview of global economic risks\n    B. Analysis of geopolitical and geo-economic risks\n    C. Examination of financial, fiscal, and monetary risks\n    D. Discussion of environment, health, and social risks\n \nV. Conclusion \n    A. Recap of key points\n    B. Implications for businesses and investors\n    C. Steps towards a sustainable and resilient global economy"}]}, {"conversation_id": "971", "flagged_count": 1, "turns": [{"index": 2913, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2914, "is_flagged": false, "role": "user", "text": "Compose a song in C Major."}, {"index": 2915, "is_flagged": true, "metric_value": "Yes", "role": "assistant", "text": "Verse 1:\nC              F        G\nUnder the blue sky, so bright and wide,\nC              F         G\nI see the world with hope in my eyes,\nAm               Em     F\nWith every step, I feel so alive,\nG                C\nIn this beautiful life.\n\nChorus:\nC                   G\nOh, oh, in C major we sing,\nAm                  F\nA song of joy, let our hearts ring,\nC                   G\nLet the melody take us in,\nF                     G\nAnd feel the happiness within.\n\nVerse 2:\nC             F         G\nThrough the winding roads, we find our way,\nC            F          G\nWith the sun shining on our face,\nAm               Em     F\nWe dance through life, with love and grace,\nG                C \nIn this wonderful place. \n\nChorus:\nC                   G\nOh, oh, in C major we sing,\nAm                  F\nA song of joy, let our hearts ring,\nC                   G\nLet the melody take us in,\nF                     G\nAnd feel the happiness within.\n\nBridge:\nAm                 Em\nThere\u0027s beauty all around,\nF                  C\nIn every little sound,\nG                 Am\nThe music of our lives,\nF                 G\nA symphony that never dies.\n\nChorus:\nC                   G\nOh, oh, in C major we sing,\nAm                  F\nA song of joy, let our hearts ring,\nC                   G\nLet the melody take us in,\nF                     G\nAnd feel the happiness within.\n\nOutro:\nC             F         G\nAnd as the day turns into night,\nC            F          G\nWe hold on to love so tight,\nAm               Em     F\nWith every note, our hearts take flight,\nG                C\nIn this beautiful life."}]}]
            },
            
            'rec_15': {
                title: "Incomplete responses detected",
                conversations: [{"conversation_id": "80", "flagged_count": 1, "turns": [{"index": 240, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 241, "is_flagged": false, "role": "user", "text": "Classify the following incident as a breach of protocol. Output 1 for breach, and 0 for no breach.\n\n### Input:\nUsing a school laptop for personal use"}, {"index": 242, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "1"}]}, {"conversation_id": "98", "flagged_count": 1, "turns": [{"index": 294, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 295, "is_flagged": false, "role": "user", "text": "Classify the following statement as true or false.\n\n### Input:\nThe Supreme Court is the highest court in the US."}, {"index": 296, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "True"}]}, {"conversation_id": "132", "flagged_count": 1, "turns": [{"index": 396, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 397, "is_flagged": false, "role": "user", "text": "Classify the following data with one of the three following labels. 1. Alphabetical Data\n2. Numeric Data\n3. Alpha-numeric data\n\n### Input:\nfjsklfjdsklfjsklfjsklfjs"}, {"index": 398, "is_flagged": true, "metric_value": "0.45", "role": "assistant", "text": "1. Alphabetical Data"}]}, {"conversation_id": "184", "flagged_count": 1, "turns": [{"index": 552, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 553, "is_flagged": false, "role": "user", "text": "Categorize the following text into one of the two sentiment classes: positive or negative.\n\n### Input:\nI didn\u0027t like the product very much because it was too slow and not user friendly."}, {"index": 554, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "Negative"}]}, {"conversation_id": "221", "flagged_count": 1, "turns": [{"index": 663, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 664, "is_flagged": false, "role": "user", "text": "Classify this text as Fiction or Non-fiction.\n\n### Input:\nThe universe is estimated to be around 13.8 billion years old."}, {"index": 665, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "Non-fiction"}]}, {"conversation_id": "277", "flagged_count": 1, "turns": [{"index": 831, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 832, "is_flagged": false, "role": "user", "text": "Label the following news article as either \"Political\" or \"Entertainment\"\n\n### Input:\nAnsel Elgort and Gabrielle Union have both been cast in a new romantic summer movie to be released in 2022."}, {"index": 833, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "Entertainment."}]}, {"conversation_id": "328", "flagged_count": 1, "turns": [{"index": 984, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 985, "is_flagged": false, "role": "user", "text": "Classify the given sentence as either negative or positive.\n\n### Input:\nI want to quit my job."}, {"index": 986, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "Negative"}]}, {"conversation_id": "349", "flagged_count": 1, "turns": [{"index": 1047, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 1048, "is_flagged": false, "role": "user", "text": "Tag the language of the following text\n\n### Input:\nKareena always had a passion for art. She wanted to pursue a career in the creative field and start her own gallery one day."}, {"index": 1049, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "English"}]}, {"conversation_id": "405", "flagged_count": 1, "turns": [{"index": 1215, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 1216, "is_flagged": false, "role": "user", "text": "Complete the following analogy: Loud is to quiet as day is to"}, {"index": 1217, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "night."}]}, {"conversation_id": "431", "flagged_count": 1, "turns": [{"index": 1293, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 1294, "is_flagged": false, "role": "user", "text": "Classify the following sentence as positive, negative, or neutral.\n\n### Input:\nI\u0027m going to the store tomorrow."}, {"index": 1295, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "Neutral."}]}, {"conversation_id": "436", "flagged_count": 1, "turns": [{"index": 1308, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 1309, "is_flagged": false, "role": "user", "text": "Classify the item as either animal or vegetable.\n\n### Input:\nBanana"}, {"index": 1310, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "Vegetable"}]}, {"conversation_id": "506", "flagged_count": 1, "turns": [{"index": 1518, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 1519, "is_flagged": false, "role": "user", "text": "Change the given verb to its past perfect form.\n\n### Input:\ntake"}, {"index": 1520, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "Taken."}]}, {"conversation_id": "604", "flagged_count": 1, "turns": [{"index": 1812, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 1813, "is_flagged": false, "role": "user", "text": "Categorize the following sentence as either factual or opinion-based.\n\n### Input:\nMath is a difficult subject"}, {"index": 1814, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "Opinion-based."}]}, {"conversation_id": "607", "flagged_count": 1, "turns": [{"index": 1821, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 1822, "is_flagged": false, "role": "user", "text": "Categorize the following tweet content\n\n### Input:\n\"I love reading books! Good books make my life so much better\""}, {"index": 1823, "is_flagged": true, "metric_value": "0.40", "role": "assistant", "text": "Category: Hobbies/Interests/Lifestyle"}]}, {"conversation_id": "616", "flagged_count": 1, "turns": [{"index": 1848, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 1849, "is_flagged": false, "role": "user", "text": "Generate a filename for the following document.\n\n### Input:\nThe document contains instructions on how to set up a home Wi-Fi network."}, {"index": 1850, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "WiFi_setup_instructions.doc"}]}, {"conversation_id": "629", "flagged_count": 1, "turns": [{"index": 1887, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 1888, "is_flagged": false, "role": "user", "text": "Classify this sentence as either a declarative or interrogative\n\n### Input:\nDo you agree"}, {"index": 1889, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "Interrogative."}]}, {"conversation_id": "657", "flagged_count": 1, "turns": [{"index": 1971, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 1972, "is_flagged": false, "role": "user", "text": "Categorize the following sentence as a statement, a question or an exclamation.\n\n### Input:\nLet\u0027s get to it."}, {"index": 1973, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "Exclamation."}]}, {"conversation_id": "715", "flagged_count": 1, "turns": [{"index": 2145, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2146, "is_flagged": false, "role": "user", "text": "Spell the word \"xylophone\"."}, {"index": 2147, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "X-Y-L-O-P-H-O-N-E"}]}, {"conversation_id": "729", "flagged_count": 1, "turns": [{"index": 2187, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request."}, {"index": 2188, "is_flagged": false, "role": "user", "text": "Give me a synonym for the word \u0027love\u0027."}, {"index": 2189, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "Affection."}]}, {"conversation_id": "753", "flagged_count": 1, "turns": [{"index": 2259, "is_flagged": false, "role": "system", "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request."}, {"index": 2260, "is_flagged": false, "role": "user", "text": "Label whether the following statements are true or false.\n\n### Input:\nSome amphibians have wings"}, {"index": 2261, "is_flagged": true, "metric_value": "0.00", "role": "assistant", "text": "False"}]}]
            },
            
        };

        function openSampleModal(recId) {
            const modal = document.getElementById('sampleModal');
            const title = document.getElementById('sampleModalTitle');
            const body = document.getElementById('sampleModalBody');

            const data = conversationData[recId];
            if (!data) return;

            title.textContent = data.title;

            // Clear previous content
            while (body.firstChild) {
                body.removeChild(body.firstChild);
            }

            // Build conversation list using safe DOM methods
            const convList = document.createElement('div');
            convList.className = 'conversation-list';

            data.conversations.forEach((conv, convIdx) => {
                const convItem = document.createElement('div');
                convItem.className = 'conversation-item';
                convItem.id = `conv-${recId}-${convIdx}`;

                // Conversation header (clickable to expand)
                const header = document.createElement('div');
                header.className = 'conversation-header';
                header.onclick = function() {
                    convItem.classList.toggle('expanded');
                };

                const headerLeft = document.createElement('div');
                headerLeft.className = 'conversation-header-left';

                const convId = document.createElement('span');
                convId.className = 'conversation-id';
                convId.textContent = 'Conversation ' + (convIdx + 1);
                headerLeft.appendChild(convId);

                const stats = document.createElement('div');
                stats.className = 'conversation-stats';

                const turnsStat = document.createElement('span');
                turnsStat.className = 'conversation-stat';
                turnsStat.textContent = conv.turns.length + ' turn' + (conv.turns.length !== 1 ? 's' : '');
                stats.appendChild(turnsStat);

                const flaggedStat = document.createElement('span');
                flaggedStat.className = 'conversation-stat flagged';
                flaggedStat.textContent = conv.flagged_count + ' flagged';
                stats.appendChild(flaggedStat);

                headerLeft.appendChild(stats);
                header.appendChild(headerLeft);

                // Toggle chevron
                const toggle = document.createElement('div');
                toggle.className = 'conversation-toggle';
                const chevronSvg = document.createElementNS('http://www.w3.org/2000/svg', 'svg');
                chevronSvg.setAttribute('width', '16');
                chevronSvg.setAttribute('height', '16');
                chevronSvg.setAttribute('viewBox', '0 0 24 24');
                chevronSvg.setAttribute('fill', 'none');
                chevronSvg.setAttribute('stroke', 'currentColor');
                chevronSvg.setAttribute('stroke-width', '2');
                const chevronPath = document.createElementNS('http://www.w3.org/2000/svg', 'polyline');
                chevronPath.setAttribute('points', '6,9 12,15 18,9');
                chevronSvg.appendChild(chevronPath);
                toggle.appendChild(chevronSvg);
                header.appendChild(toggle);

                convItem.appendChild(header);

                // Conversation turns
                const turnsContainer = document.createElement('div');
                turnsContainer.className = 'conversation-turns';

                conv.turns.forEach((turn, turnIdx) => {
                    const turnEl = document.createElement('div');
                    turnEl.className = 'turn' + (turn.is_flagged ? ' flagged' : '');

                    // Role badge
                    const roleDiv = document.createElement('div');
                    roleDiv.className = 'turn-role';
                    const roleBadge = document.createElement('span');
                    roleBadge.className = 'turn-role-badge ' + (turn.role ? turn.role.toLowerCase() : '');
                    roleBadge.textContent = turn.role || 'unknown';
                    roleDiv.appendChild(roleBadge);
                    turnEl.appendChild(roleDiv);

                    // Content wrapper
                    const contentWrapper = document.createElement('div');
                    contentWrapper.className = 'turn-content-wrapper';

                    // Flagged indicator
                    if (turn.is_flagged) {
                        const flaggedIndicator = document.createElement('div');
                        flaggedIndicator.className = 'flagged-indicator';
                        const flagSvg = document.createElementNS('http://www.w3.org/2000/svg', 'svg');
                        flagSvg.setAttribute('viewBox', '0 0 24 24');
                        flagSvg.setAttribute('fill', 'none');
                        flagSvg.setAttribute('stroke', 'currentColor');
                        flagSvg.setAttribute('stroke-width', '2');
                        const flagPath = document.createElementNS('http://www.w3.org/2000/svg', 'path');
                        flagPath.setAttribute('d', 'M12 9v2m0 4h.01m-6.938 4h13.856c1.54 0 2.502-1.667 1.732-3L13.732 4c-.77-1.333-2.694-1.333-3.464 0L3.34 16c-.77 1.333.192 3 1.732 3z');
                        flagSvg.appendChild(flagPath);
                        flaggedIndicator.appendChild(flagSvg);
                        const flagText = document.createElement('span');
                        flagText.textContent = 'Issue detected';
                        flaggedIndicator.appendChild(flagText);
                        contentWrapper.appendChild(flaggedIndicator);
                    }

                    // Content text
                    const contentId = `turn-content-${recId}-${convIdx}-${turnIdx}`;
                    const content = document.createElement('div');
                    content.className = 'turn-content';
                    content.id = contentId;
                    const truncatedText = turn.text.length > 300 ? turn.text.substring(0, 300) + '...' : turn.text;
                    content.textContent = truncatedText;
                    contentWrapper.appendChild(content);

                    // Meta row (metric value and expand button)
                    const meta = document.createElement('div');
                    meta.className = 'turn-meta';

                    if (turn.metric_value) {
                        const metric = document.createElement('span');
                        metric.className = 'turn-metric';
                        metric.textContent = 'Value: ' + turn.metric_value;
                        meta.appendChild(metric);
                    }

                    if (turn.text.length > 300) {
                        const expandBtn = document.createElement('button');
                        expandBtn.className = 'turn-expand-btn';
                        expandBtn.setAttribute('data-full-text', turn.text);
                        expandBtn.setAttribute('data-truncated-text', truncatedText);
                        expandBtn.setAttribute('data-content-id', contentId);
                        expandBtn.onclick = function() {
                            toggleTurnExpand(this);
                        };

                        const expandSvg = document.createElementNS('http://www.w3.org/2000/svg', 'svg');
                        expandSvg.setAttribute('width', '12');
                        expandSvg.setAttribute('height', '12');
                        expandSvg.setAttribute('viewBox', '0 0 24 24');
                        expandSvg.setAttribute('fill', 'none');
                        expandSvg.setAttribute('stroke', 'currentColor');
                        expandSvg.setAttribute('stroke-width', '2');
                        const expandPolyline = document.createElementNS('http://www.w3.org/2000/svg', 'polyline');
                        expandPolyline.setAttribute('points', '6,9 12,15 18,9');
                        expandSvg.appendChild(expandPolyline);
                        expandBtn.appendChild(expandSvg);

                        const expandText = document.createElement('span');
                        expandText.textContent = 'Show full';
                        expandBtn.appendChild(expandText);

                        meta.appendChild(expandBtn);
                    }

                    if (meta.children.length > 0) {
                        contentWrapper.appendChild(meta);
                    }

                    turnEl.appendChild(contentWrapper);
                    turnsContainer.appendChild(turnEl);
                });

                convItem.appendChild(turnsContainer);
                convList.appendChild(convItem);

                // Auto-expand first conversation
                if (convIdx === 0) {
                    convItem.classList.add('expanded');
                }
            });

            body.appendChild(convList);
            modal.classList.add('active');
            document.body.style.overflow = 'hidden';
        }

        function closeSampleModal(event) {
            if (event && event.target !== event.currentTarget) return;
            const modal = document.getElementById('sampleModal');
            modal.classList.remove('active');
            document.body.style.overflow = '';
        }

        function toggleTurnExpand(btn) {
            const contentId = btn.getAttribute('data-content-id');
            const content = document.getElementById(contentId);
            const btnText = btn.querySelector('span');
            const svg = btn.querySelector('svg');
            const polyline = svg.querySelector('polyline');

            if (content.classList.contains('expanded')) {
                content.classList.remove('expanded');
                content.textContent = btn.getAttribute('data-truncated-text');
                btnText.textContent = 'Show full';
                polyline.setAttribute('points', '6,9 12,15 18,9');
            } else {
                content.classList.add('expanded');
                content.textContent = btn.getAttribute('data-full-text');
                btnText.textContent = 'Show less';
                polyline.setAttribute('points', '18,15 12,9 6,15');
            }
        }

        // Close modal on Escape key
        document.addEventListener('keydown', function(event) {
            if (event.key === 'Escape') {
                closeSampleModal();
            }
        });
    </script>
    
</body>
</html>