{
  "embedding_clusters": null,
  "question_diversity_clusters": {
    "total_clusters": 3,
    "noise_count": 51995,
    "total_questions": 52002,
    "concentrated_count": 0,
    "distribution": [
      {
        "cluster_id": -1,
        "label": "Noise",
        "count": 51995,
        "percentage": 100.0,
        "samples": [
          {
            "index": 1,
            "text": "Give three tips for staying healthy.",
            "role": "user",
            "conversation_id": "0"
          },
          {
            "index": 4,
            "text": "What are the three primary colors?",
            "role": "user",
            "conversation_id": "1"
          },
          {
            "index": 7,
            "text": "Describe the structure of an atom.",
            "role": "user",
            "conversation_id": "2"
          },
          {
            "index": 10,
            "text": "How can we reduce air pollution?",
            "role": "user",
            "conversation_id": "3"
          },
          {
            "index": 13,
            "text": "Describe a time when you had to make a difficult decision.",
            "role": "user",
            "conversation_id": "4"
          }
        ]
      },
      {
        "cluster_id": 0,
        "label": "Cluster 0",
        "count": 2,
        "percentage": 0.0,
        "samples": [
          {
            "index": 4279,
            "text": "Explain the difference between artificial intelligence and machine learning",
            "role": "user",
            "conversation_id": "1426"
          },
          {
            "index": 61291,
            "text": "Explain the difference between Machine Learning and Artificial Intelligence.",
            "role": "user",
            "conversation_id": "20430"
          }
        ]
      },
      {
        "cluster_id": 1,
        "label": "Cluster 1",
        "count": 2,
        "percentage": 0.0,
        "samples": [
          {
            "index": 59197,
            "text": "Describe the difference between aerobic and anaerobic exercise.",
            "role": "user",
            "conversation_id": "19732"
          },
          {
            "index": 83923,
            "text": "Describe the differences between anaerobic and aerobic exercise.",
            "role": "user",
            "conversation_id": "27974"
          }
        ]
      },
      {
        "cluster_id": 2,
        "label": "Cluster 2",
        "count": 3,
        "percentage": 0.0,
        "samples": [
          {
            "index": 138193,
            "text": "Cite which sources were used in the paper\n\n### Input:\nAbstract: Fine-tuning continuous prompts for target tasks has recently emerged as a compact alternative to full model fine-tuning. Motivated by these promising results, we investigate the feasibility of extracting a discrete (textual) interpretation of continuous prompts that is faithful to the problem they solve. In practice, we observe a \"wayward\" behavior between the task solved by continuous prompts and their nearest neighbor discrete projections: We can find continuous prompts that solve a task while being projected to an arbitrary text (e.g., definition of a different or even a contradictory task), while being within a very small (2%) margin of the best continuous prompt of the same size for the task. We provide intuitions behind this odd and surprising behavior, as well as extensive empirical analyses quantifying the effect of various parameters. For instance, for larger model sizes we observe higher waywardness, i.e, we can find prompts that more closely map to any arbitrary text with a smaller drop in accuracy. These findings have important implications relating to the difficulty of faithfully interpreting continuous prompts and their generalization across models and tasks, providing guidance for future progress in prompting language models.",
            "role": "user",
            "conversation_id": "46064"
          },
          {
            "index": 138196,
            "text": "Classify the abstract under a label.\n\n### Input:\nAbstract: Fine-tuning continuous prompts for target tasks has recently emerged as a compact alternative to full model fine-tuning. Motivated by these promising results, we investigate the feasibility of extracting a discrete (textual) interpretation of continuous prompts that is faithful to the problem they solve. In practice, we observe a \"wayward\" behavior between the task solved by continuous prompts and their nearest neighbor discrete projections: We can find continuous prompts that solve a task while being projected to an arbitrary text (e.g., definition of a different or even a contradictory task), while being within a very small (2%) margin of the best continuous prompt of the same size for the task. We provide intuitions behind this odd and surprising behavior, as well as extensive empirical analyses quantifying the effect of various parameters. For instance, for larger model sizes we observe higher waywardness, i.e, we can find prompts that more closely map to any arbitrary text with a smaller drop in accuracy. These findings have important implications relating to the difficulty of faithfully interpreting continuous prompts and their generalization across models and tasks, providing guidance for future progress in prompting language models.",
            "role": "user",
            "conversation_id": "46065"
          },
          {
            "index": 138211,
            "text": "State the main arguments that the abstract makes\n\n### Input:\nAbstract: Fine-tuning continuous prompts for target tasks has recently emerged as a compact alternative to full model fine-tuning. Motivated by these promising results, we investigate the feasibility of extracting a discrete (textual) interpretation of continuous prompts that is faithful to the problem they solve. In practice, we observe a \"wayward\" behavior between the task solved by continuous prompts and their nearest neighbor discrete projections: We can find continuous prompts that solve a task while being projected to an arbitrary text (e.g., definition of a different or even a contradictory task), while being within a very small (2%) margin of the best continuous prompt of the same size for the task. We provide intuitions behind this odd and surprising behavior, as well as extensive empirical analyses quantifying the effect of various parameters. For instance, for larger model sizes we observe higher waywardness, i.e, we can find prompts that more closely map to any arbitrary text with a smaller drop in accuracy. These findings have important implications relating to the difficulty of faithfully interpreting continuous prompts and their generalization across models and tasks, providing guidance for future progress in prompting language models.",
            "role": "user",
            "conversation_id": "46070"
          }
        ]
      }
    ]
  }
}