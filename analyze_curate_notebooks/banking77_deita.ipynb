{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e1fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the config file\n",
    "config_path = \"/Users/ryanarman/code/oumi/configs/examples/analyze/analyze_deita.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334e3fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS has been disabled - forcing CPU-only mode\n",
      "PyTorch device: cpu\n",
      "PyTorch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# IMPORTANT: Set these BEFORE importing torch or any ML libraries\n",
    "# Disable all GPU/MPS backends to prevent crashes with IFD analyzer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Disable CUDA\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Disable MPS memory allocation\n",
    "os.environ[\"DISABLE_MPS_COMPAT\"] = \"1\"  # Additional MPS disable flag\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"  # Disable HuggingFace telemetry\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"  # Allow model downloads\n",
    "\n",
    "# Force CPU usage in PyTorch to avoid MPS crashes\n",
    "import torch\n",
    "\n",
    "# Forcefully disable MPS before anything else\n",
    "torch.set_default_device(\"cpu\")\n",
    "if hasattr(torch.backends, \"mps\"):\n",
    "    # Monkey-patch to prevent MPS usage\n",
    "    original_is_available = torch.backends.mps.is_available\n",
    "    torch.backends.mps.is_available = lambda: False\n",
    "    print(\"MPS has been disabled - forcing CPU-only mode\")\n",
    "else:\n",
    "    print(\"Using CPU for all computations\")\n",
    "\n",
    "print(f\"PyTorch device: {torch.get_default_device()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "430925a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 7 analyzers: ['evol_complexity', 'evol_quality', 'repr_diversity', 'ifd', 'length', 'diversity', 'quality']\n",
      "[2025-12-30 11:31:52,074][oumi][rank0][pid:41580][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'custom'\n",
      "[2025-12-30 11:31:52,074][oumi.utils.analysis_utils][rank0][pid:41580][MainThread][INFO]][analysis_utils.py:225] Loaded text dataset from: /Users/ryanarman/code/scratch/ryan_hillclimbing_experiments/banking77/notebooks/data/banking77_train.jsonl\n",
      "[2025-12-30 11:31:52,075][oumi][rank0][pid:41580][MainThread][INFO]][dataset_analyzer.py:154] Loaded dataset from config: None\n",
      "[2025-12-30 11:31:52,075][oumi][rank0][pid:41580][MainThread][INFO]][dataset_analyzer.py:304] Initialized sample analyzer: evol_complexity\n",
      "[2025-12-30 11:31:52,075][oumi][rank0][pid:41580][MainThread][INFO]][dataset_analyzer.py:304] Initialized sample analyzer: evol_quality\n",
      "[2025-12-30 11:31:52,089][oumi][rank0][pid:41580][MainThread][INFO]][dataset_analyzer.py:304] Initialized sample analyzer: repr_diversity\n",
      "[2025-12-30 11:31:52,089][oumi][rank0][pid:41580][MainThread][INFO]][dataset_analyzer.py:304] Initialized sample analyzer: ifd\n",
      "[2025-12-30 11:31:52,207][oumi][rank0][pid:41580][MainThread][INFO]][dataset_analyzer.py:304] Initialized sample analyzer: length\n",
      "[2025-12-30 11:31:52,207][oumi][rank0][pid:41580][MainThread][INFO]][dataset_analyzer.py:304] Initialized sample analyzer: diversity\n",
      "[2025-12-30 11:31:52,208][oumi][rank0][pid:41580][MainThread][INFO]][dataset_analyzer.py:304] Initialized sample analyzer: quality\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from oumi.core.configs import AnalyzeConfig\n",
    "from oumi.core.analyze.dataset_analyzer import DatasetAnalyzer\n",
    "\n",
    "# Path to your dataset file\n",
    "dataset_path = \"/Users/ryanarman/code/scratch/ryan_hillclimbing_experiments/banking77/notebooks/data/banking77_train.jsonl\"\n",
    "\n",
    "# Load the config from YAML\n",
    "config = AnalyzeConfig.from_yaml(\n",
    "    config_path=config_path,\n",
    ")\n",
    "\n",
    "config.sample_count = 2\n",
    "\n",
    "# Override the dataset settings to use your local file\n",
    "config.dataset_path = dataset_path\n",
    "config.dataset_name = None  # Clear dataset_name so it uses dataset_path instead\n",
    "\n",
    "# Optionally update output path\n",
    "config.output_path = \"./analysis_output/banking77\"\n",
    "\n",
    "# IMPORTANT: Disable analyzers that require large model downloads or have issues\n",
    "# IFD requires downloading Qwen model and may cause MPS crashes\n",
    "# fasttext requires additional dependencies\n",
    "# repr_diversity and question_diversity download embedding models\n",
    "print(f\"Running {len(config.analyzers)} analyzers: {[a.id for a in config.analyzers]}\")\n",
    "\n",
    "# Validate the configuration\n",
    "config.finalize_and_validate()\n",
    "\n",
    "# Create the analyzer\n",
    "analyzer = DatasetAnalyzer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d0ca499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:31:52,215][oumi][rank0][pid:41580][MainThread][INFO]][dataset_analyzer.py:347] Starting analysis of dataset: None\n",
      "[2025-12-30 11:31:52,216][oumi][rank0][pid:41580][MainThread][INFO]][dataset_analyzer.py:348] Using 7 sample analyzers: ['evol_complexity', 'evol_quality', 'repr_diversity', 'ifd', 'length', 'diversity', 'quality']\n",
      "[2025-12-30 11:31:52,216][oumi][rank0][pid:41580][MainThread][INFO]][dataset_analyzer.py:371] Analyzing 2 of 8002 conversations\n",
      "[2025-12-30 11:31:52,217][oumi][rank0][pid:41580][MainThread][INFO]][dataset_analyzer.py:421] Converting conversation dataset with 8002 items\n",
      "[2025-12-30 11:31:52,217][oumi][rank0][pid:41580][MainThread][INFO]][dataset_analyzer.py:428] Limiting analysis to first 2 items (dataset has 8002 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting Unknown Dataset to DataFrames: 100%|██████████| 2/2 [00:00<00:00, 885.25item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:31:52,241][oumi][rank0][pid:41580][MainThread][INFO]][evol_complexity_analyzer.py:317] Computing complexity scores for 2 samples in column 'conversation_text_content'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing complexity (conversation_text_content):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:31:52,261][oumi][rank0][pid:41580][MainThread][INFO]][evol_base.py:221] Initialized Evol analyzer with model: gpt-5-mini, engine: OPENAI\n",
      "[2025-12-30 11:32:09,178][oumi][rank0][pid:41580][MainThread][WARNING]][evol_base.py:375] Expected 3 evolutions, got 0. Padding with duplicates.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing complexity (conversation_text_content):  50%|█████     | 1/2 [00:26<00:26, 26.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:32:18,739][oumi][rank0][pid:41580][MainThread][WARNING]][evol_base.py:375] Expected 3 evolutions, got 0. Padding with duplicates.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing complexity (conversation_text_content): 100%|██████████| 2/2 [00:26<00:00, 13.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:32:18,744][oumi][rank0][pid:41580][MainThread][INFO]][evol_complexity_analyzer.py:414] Column 'conversation_text_content': Mean complexity score = 0.000\n",
      "[2025-12-30 11:32:18,747][oumi][rank0][pid:41580][MainThread][INFO]][evol_quality_analyzer.py:398] Computing quality scores for 2 samples in column 'conversation_text_content'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing quality (conversation_text_content):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:32:18,751][oumi][rank0][pid:41580][MainThread][INFO]][evol_base.py:221] Initialized Evol analyzer with model: claude-4-5-haiku, engine: ANTHROPIC\n",
      "[2025-12-30 11:32:18,911][oumi][rank0][pid:41580][MainThread][WARNING]][evol_base.py:298] LLM call failed (attempt 1): Non-retriable error: model: claude-4-5-haiku\n",
      "[2025-12-30 11:32:19,056][oumi][rank0][pid:41580][MainThread][WARNING]][evol_base.py:298] LLM call failed (attempt 2): Non-retriable error: model: claude-4-5-haiku\n",
      "[2025-12-30 11:32:19,392][oumi][rank0][pid:41580][MainThread][WARNING]][evol_quality_analyzer.py:450] Failed to analyze sample 0: LLM call failed after 3 attempts: Non-retriable error: model: claude-4-5-haiku\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing quality (conversation_text_content):  50%|█████     | 1/2 [00:00<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:32:19,583][oumi][rank0][pid:41580][MainThread][WARNING]][evol_base.py:298] LLM call failed (attempt 1): Non-retriable error: model: claude-4-5-haiku\n",
      "[2025-12-30 11:32:19,791][oumi][rank0][pid:41580][MainThread][WARNING]][evol_base.py:298] LLM call failed (attempt 2): Non-retriable error: model: claude-4-5-haiku\n",
      "[2025-12-30 11:32:20,323][oumi][rank0][pid:41580][MainThread][WARNING]][evol_quality_analyzer.py:450] Failed to analyze sample 1: LLM call failed after 3 attempts: Non-retriable error: model: claude-4-5-haiku\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing quality (conversation_text_content): 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:32:20,329][oumi][rank0][pid:41580][MainThread][INFO]][evol_quality_analyzer.py:503] Column 'conversation_text_content': Mean quality score = 0.500\n",
      "[2025-12-30 11:32:20,331][oumi][rank0][pid:41580][MainThread][INFO]][repr_diversity_analyzer.py:363] Computing diversity scores for 2 samples in column 'conversation_text_content'...\n",
      "[2025-12-30 11:32:20,331][oumi][rank0][pid:41580][MainThread][INFO]][repr_diversity_analyzer.py:165] Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|██████████| 2/2 [00:00<00:00, 49.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:32:21,459][oumi][rank0][pid:41580][MainThread][INFO]][repr_diversity_analyzer.py:230] Computing nearest neighbor distances for 2 samples (k=1)...\n",
      "[2025-12-30 11:32:21,461][oumi][rank0][pid:41580][MainThread][INFO]][repr_diversity_analyzer.py:556] Column 'conversation_text_content': 2/2 samples (100.0%) are redundant\n",
      "[2025-12-30 11:32:21,462][oumi][rank0][pid:41580][MainThread][INFO]][ifd_analyzer.py:154] Loading model for IFD analysis: Qwen/Qwen3-0.6B\n",
      "[2025-12-30 11:32:23,337][oumi][rank0][pid:41580][MainThread][INFO]][ifd_analyzer.py:193] Loaded Qwen/Qwen3-0.6B on cpu (dtype: torch.float32)\n",
      "[2025-12-30 11:32:23,338][oumi][rank0][pid:41580][MainThread][WARNING]][ifd_analyzer.py:655] Could not find instruction and response columns. For flat format, set instruction_column and response_column, or use columns like 'instruction'/'prompt' and 'response'/'output'. For conversation format, ensure 'text_content' and 'role' exist. Available columns: ['conversation_index', 'conversation_id', 'num_messages', 'conversation_text_content', 'conversation_text_content__evol_complexity__score', 'conversation_text_content__evol_complexity__rank', 'conversation_text_content__evol_complexity__headroom', 'conversation_text_content__evol_quality__score', 'conversation_text_content__evol_quality__rank', 'conversation_text_content__evol_quality__improvement_potential', 'conversation_text_content__repr_diversity__nn_distance', 'conversation_text_content__repr_diversity__score', 'conversation_text_content__repr_diversity__is_redundant', 'conversation_text_content__repr_diversity__percentile']\n",
      "[2025-12-30 11:32:23,349][oumi][rank0][pid:41580][MainThread][INFO]][evol_complexity_analyzer.py:317] Computing complexity scores for 2 samples in column 'text_content'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing complexity (text_content):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:32:38,609][oumi][rank0][pid:41580][MainThread][WARNING]][evol_base.py:375] Expected 3 evolutions, got 0. Padding with duplicates.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing complexity (text_content):  50%|█████     | 1/2 [00:19<00:19, 19.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:32:57,624][oumi][rank0][pid:41580][MainThread][WARNING]][evol_base.py:298] LLM call failed (attempt 1): Non-retriable error: Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\n",
      "[2025-12-30 11:33:11,817][oumi][rank0][pid:41580][MainThread][WARNING]][evol_base.py:375] Expected 3 evolutions, got 0. Padding with duplicates.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing complexity (text_content): 100%|██████████| 2/2 [00:54<00:00, 27.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:33:17,932][oumi][rank0][pid:41580][MainThread][INFO]][evol_complexity_analyzer.py:414] Column 'text_content': Mean complexity score = 0.000\n",
      "[2025-12-30 11:33:17,933][oumi][rank0][pid:41580][MainThread][INFO]][evol_quality_analyzer.py:398] Computing quality scores for 2 samples in column 'text_content'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing quality (text_content):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:33:18,264][oumi][rank0][pid:41580][MainThread][WARNING]][evol_base.py:298] LLM call failed (attempt 1): Non-retriable error: model: claude-4-5-haiku\n",
      "[2025-12-30 11:33:18,442][oumi][rank0][pid:41580][MainThread][WARNING]][evol_base.py:298] LLM call failed (attempt 2): Non-retriable error: model: claude-4-5-haiku\n",
      "[2025-12-30 11:33:18,595][oumi][rank0][pid:41580][MainThread][WARNING]][evol_quality_analyzer.py:450] Failed to analyze sample 2: LLM call failed after 3 attempts: Non-retriable error: model: claude-4-5-haiku\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing quality (text_content):  50%|█████     | 1/2 [00:00<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:33:18,752][oumi][rank0][pid:41580][MainThread][WARNING]][evol_base.py:298] LLM call failed (attempt 1): Non-retriable error: model: claude-4-5-haiku\n",
      "[2025-12-30 11:33:18,958][oumi][rank0][pid:41580][MainThread][WARNING]][evol_base.py:298] LLM call failed (attempt 2): Non-retriable error: model: claude-4-5-haiku\n",
      "[2025-12-30 11:33:19,096][oumi][rank0][pid:41580][MainThread][WARNING]][evol_quality_analyzer.py:450] Failed to analyze sample 5: LLM call failed after 3 attempts: Non-retriable error: model: claude-4-5-haiku\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing quality (text_content): 100%|██████████| 2/2 [00:01<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:33:19,101][oumi][rank0][pid:41580][MainThread][INFO]][evol_quality_analyzer.py:503] Column 'text_content': Mean quality score = 0.500\n",
      "[2025-12-30 11:33:19,102][oumi][rank0][pid:41580][MainThread][INFO]][repr_diversity_analyzer.py:363] Computing diversity scores for 6 samples in column 'text_content'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|██████████| 6/6 [00:00<00:00, 85.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 11:33:19,176][oumi][rank0][pid:41580][MainThread][INFO]][repr_diversity_analyzer.py:230] Computing nearest neighbor distances for 6 samples (k=5)...\n",
      "[2025-12-30 11:33:19,177][oumi][rank0][pid:41580][MainThread][INFO]][repr_diversity_analyzer.py:556] Column 'text_content': 0/6 samples (0.0%) are redundant\n",
      "[2025-12-30 11:33:19,178][oumi][rank0][pid:41580][MainThread][INFO]][ifd_analyzer.py:643] Detected conversation format. Computing IFD for assistant messages using preceding user messages as instructions.\n",
      "[2025-12-30 11:33:19,967][oumi][rank0][pid:41580][MainThread][INFO]][ifd_analyzer.py:521] IFD analysis complete. Processed 2 assistant messages out of 2 total.\n",
      "Total conversations analyzed: 2\n"
     ]
    }
   ],
   "source": [
    "# Run the analysis\n",
    "analyzer.analyze_dataset()\n",
    "\n",
    "# The results are stored in analyzer object\n",
    "if analyzer._analysis_results:\n",
    "    print(\n",
    "        f\"Total conversations analyzed: {analyzer._analysis_results.conversations_analyzed}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a07ae1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(schema): 48\n",
      "len(merged_columns): 48\n"
     ]
    }
   ],
   "source": [
    "schema = analyzer.get_schema()\n",
    "print(f\"len(schema): {len(schema)}\")\n",
    "merged_columns = analyzer.analysis_df.columns\n",
    "print(f\"len(merged_columns): {len(merged_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0c2ff22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['evol_complexity',\n",
       " 'evol_quality',\n",
       " 'repr_diversity',\n",
       " 'ifd',\n",
       " 'length',\n",
       " 'diversity',\n",
       " 'quality']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer_names = [a.id for a in config.analyzers]\n",
    "analyzer_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ee896",
   "metadata": {},
   "source": [
    "# Conv level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d13a64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['evol_complexity',\n",
       " 'evol_quality',\n",
       " 'repr_diversity',\n",
       " 'ifd',\n",
       " 'length',\n",
       " 'diversity',\n",
       " 'quality']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d56f05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No columns found for analyzer: ifd\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.analyze.column_utils import (\n",
    "    filter_analyzer_columns,\n",
    "    get_analyzer_columns_by_analyzer,\n",
    "    parse_analyzer_column_name,\n",
    ")\n",
    "\n",
    "conv_columns = analyzer.conversation_df.columns\n",
    "row = analyzer.conversation_df.iloc[0]\n",
    "\n",
    "# Choose the analzyer to analyze\n",
    "analyzer_name = analyzer_names[3]\n",
    "\n",
    "\n",
    "filtered_cols = filter_analyzer_columns(conv_columns, analyzer_id=analyzer_name)\n",
    "if filtered_cols:\n",
    "    print(f\"Analyzer: {analyzer_name}\")\n",
    "    info = parse_analyzer_column_name(filtered_cols[0])\n",
    "    # print(\"\\nInput:\")\n",
    "    # print(f\"source_column: {info.source_column}\")\n",
    "    # print(f\"{row[info.source_column]}\\n\")\n",
    "\n",
    "    for col in filtered_cols:\n",
    "        info = parse_analyzer_column_name(col)\n",
    "        print(f\"metric: {info.metric_name}\")\n",
    "        # print(f\"type: {schema[col]['type']}\")\n",
    "        # print(f\"content_type: {schema[col]['content_type']}\")\n",
    "        print(f\"description: {schema[col]['description']}\")\n",
    "        print(f\"value: {row[col]}\")\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(f\"No columns found for analyzer: {analyzer_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abfccb",
   "metadata": {},
   "source": [
    "# Message level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb677ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m row = analyzer.message_df.iloc[\u001b[32m2\u001b[39m]\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Choose the analzyer to analyze\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m analyzer_name = \u001b[43manalyzer_names\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     14\u001b[39m filtered_cols = filter_analyzer_columns(msg_columns, analyzer_id=analyzer_name)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filtered_cols:\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "from oumi.core.analyze.column_utils import (\n",
    "    filter_analyzer_columns,\n",
    "    get_analyzer_columns_by_analyzer,\n",
    "    parse_analyzer_column_name,\n",
    ")\n",
    "\n",
    "msg_columns = analyzer.message_df.columns\n",
    "row = analyzer.message_df.iloc[2]\n",
    "\n",
    "# Choose the analzyer to analyze\n",
    "analyzer_name = analyzer_names[0]\n",
    "\n",
    "\n",
    "filtered_cols = filter_analyzer_columns(msg_columns, analyzer_id=analyzer_name)\n",
    "if filtered_cols:\n",
    "    print(f\"Analyzer: {analyzer_name}\")\n",
    "    info = parse_analyzer_column_name(filtered_cols[0])\n",
    "\n",
    "    print(\"\\nInput:\")\n",
    "    print(f\"[{row['role']}]: {row[info.source_column]}\\n\")\n",
    "    # print(f\"source_column: {info.source_column}\")\n",
    "    print(f\"{row[info.source_column]}\\n\")\n",
    "\n",
    "    for col in filtered_cols:\n",
    "        info = parse_analyzer_column_name(col)\n",
    "        print(f\"metric: {info.metric_name}\")\n",
    "        # print(f\"type: {schema[col]['type']}\")\n",
    "        # print(f\"content_type: {schema[col]['content_type']}\")\n",
    "        print(f\"description: {schema[col]['description']}\")\n",
    "        print(f\"value: {row[col]}\")\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(f\"No columns found for analyzer: {analyzer_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782aee00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
