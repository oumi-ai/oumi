{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470e1fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available preset prompts: ['instruction_quality', 'response_quality', 'conversation_coherence', 'safety', 'helpfulness', 'factuality']\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.analyze.llm_judge_analyzer import LLMJudgeAnalyzer\n",
    "\n",
    "print(\"Available preset prompts:\", LLMJudgeAnalyzer.list_presets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "334e3fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS has been disabled - forcing CPU-only mode\n",
      "PyTorch device: cpu\n",
      "PyTorch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# IMPORTANT: Set these BEFORE importing torch or any ML libraries\n",
    "# Disable all GPU/MPS backends to prevent crashes with IFD analyzer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Disable CUDA\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Disable MPS memory allocation\n",
    "os.environ[\"DISABLE_MPS_COMPAT\"] = \"1\"  # Additional MPS disable flag\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"  # Disable HuggingFace telemetry\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"  # Allow model downloads\n",
    "\n",
    "# Force CPU usage in PyTorch to avoid MPS crashes\n",
    "import torch\n",
    "\n",
    "# Forcefully disable MPS before anything else\n",
    "torch.set_default_device(\"cpu\")\n",
    "if hasattr(torch.backends, \"mps\"):\n",
    "    # Monkey-patch to prevent MPS usage\n",
    "    original_is_available = torch.backends.mps.is_available\n",
    "    torch.backends.mps.is_available = lambda: False\n",
    "    print(\"MPS has been disabled - forcing CPU-only mode\")\n",
    "else:\n",
    "    print(\"Using CPU for all computations\")\n",
    "\n",
    "print(f\"PyTorch device: {torch.get_default_device()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2fd63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.core.configs import AnalyzeConfig, SampleAnalyzerParams\n",
    "\n",
    "# Option 1: Built-in helpfulness preset (NOT recommended for classification)\n",
    "# This evaluates the response text alone without seeing the user query\n",
    "llm_helpfulness_params_preset = SampleAnalyzerParams(\n",
    "    id=\"llm_judge\",\n",
    "    params={\n",
    "        \"prompt_preset\": \"helpfulness\",\n",
    "        \"inference_config\": {\n",
    "            \"model_name\": \"gpt-4o-mini\",\n",
    "            \"engine\": \"openai\",\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 256,\n",
    "        },\n",
    "        \"batch_size\": 10,\n",
    "        \"max_text_length\": 4000,\n",
    "        \"parse_json_response\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Option 2: Custom \"Classification Helpfulness\" - Conversation-level (RECOMMENDED)\n",
    "# Evaluates if the classification is appropriate for the user's query\n",
    "llm_helpfulness_params_conversation = SampleAnalyzerParams(\n",
    "    id=\"llm_judge\",\n",
    "    params={\n",
    "        \"prompt\": \"\"\"Evaluate how helpful this banking intent classification is (0-10).\n",
    "\n",
    "Context: User asks a banking question, assistant classifies it into one of 77 intent categories (by ID number).\n",
    "\n",
    "CONVERSATION FORMAT:\n",
    "The conversation below contains three parts:\n",
    "1. SYSTEM: Instructions and the full list of 77 intent categories with their IDs\n",
    "2. USER: The banking query to classify\n",
    "3. ASSISTANT: The classification result (a single numeric ID 0-76)\n",
    "\n",
    "⚠️ IMPORTANT: The assistant's classification is at the VERY END of the conversation (after \"ASSISTANT:\").\n",
    "Look for the last line starting with \"ASSISTANT:\" to find the classification ID.\n",
    "\n",
    "Conversation to evaluate:\n",
    "{text}\n",
    "\n",
    "Evaluation criteria for helpfulness:\n",
    "- Does the classification ID seem appropriate for the user's query?\n",
    "- Would this classification lead to helpful downstream actions?\n",
    "- Is it the most relevant intent among potential options?\n",
    "- Does the assistant provide ONLY the ID (good format)?\n",
    "\n",
    "Scoring guide:\n",
    "- 10: Perfect classification, highly relevant to query, correct format\n",
    "- 7-9: Good classification, reasonable fit for the query\n",
    "- 4-6: Mediocre, somewhat relevant but not ideal\n",
    "- 1-3: Poor classification, wrong intent for the query\n",
    "- 0: No classification or completely wrong\n",
    "\n",
    "Examples of good vs bad:\n",
    "Good (score 9-10):\n",
    "  USER: \"My card was declined at the store\"\n",
    "  ASSISTANT: 25  ← (ID 25 = declined_card_payment, highly relevant)\n",
    "\n",
    "Bad (score 1-3):\n",
    "  USER: \"My card was declined at the store\"\n",
    "  ASSISTANT: 0   ← (ID 0 = activate_my_card, wrong intent)\n",
    "\n",
    "Respond with JSON:\n",
    "- \"score\": 0-10 (10 = maximally helpful classification)\n",
    "- \"label\": \"very_helpful\", \"helpful\", \"somewhat_helpful\", \"not_helpful\"\n",
    "- \"reasoning\": brief explanation of why this classification is/isn't helpful for the user's query\n",
    "\n",
    "JSON response:\"\"\",\n",
    "        \"analyze_message_level\": False,\n",
    "        \"analyze_conversation_level\": True,  # See full context\n",
    "        \"inference_config\": {\n",
    "            \"model_name\": \"gpt-4o-mini\",\n",
    "            \"engine\": \"openai\",\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 256,\n",
    "        },\n",
    "        \"batch_size\": 10,\n",
    "        \"max_text_length\": 8000,\n",
    "        \"parse_json_response\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Option 3: Custom \"Format Check\" - Message-level for assistant only\n",
    "# Just validates format, not semantic appropriateness\n",
    "llm_helpfulness_params_format = SampleAnalyzerParams(\n",
    "    id=\"llm_judge\",\n",
    "    params={\n",
    "        \"prompt\": \"\"\"Evaluate this assistant's classification response format (0-10).\n",
    "\n",
    "Context: The assistant must classify a banking query by responding with a single intent ID (0-76).\n",
    "\n",
    "Assistant response to evaluate:\n",
    "{text}\n",
    "\n",
    "Evaluation criteria:\n",
    "- Is it a single numeric ID (0-76)?\n",
    "- Is the format correct (just the number, no extra text)?\n",
    "- Is it a valid ID in the expected range?\n",
    "\n",
    "Good examples: \"59\", \"0\", \"42\" (single numbers)\n",
    "Bad examples: \"activate_my_card\" (label instead of ID), \"I think it's 59\" (extra text), \"\" (empty)\n",
    "\n",
    "Respond with JSON:\n",
    "- \"score\": 0-10 (10 = perfect format)\n",
    "  * 10: Valid ID (0-76), correct format\n",
    "  * 5-9: Valid ID but minor formatting issues\n",
    "  * 1-4: Invalid format or out of range\n",
    "  * 0: Empty or completely wrong\n",
    "- \"label\": \"excellent\", \"good\", \"needs_improvement\", \"poor\"\n",
    "- \"reasoning\": brief explanation\n",
    "\n",
    "JSON response:\"\"\",\n",
    "        \"filter_role\": \"assistant\",\n",
    "        \"analyze_message_level\": True,\n",
    "        \"analyze_conversation_level\": False,\n",
    "        \"inference_config\": {\n",
    "            \"model_name\": \"gpt-4o-mini\",\n",
    "            \"engine\": \"openai\",\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 256,\n",
    "        },\n",
    "        \"batch_size\": 10,\n",
    "        \"max_text_length\": 4000,\n",
    "        \"parse_json_response\": True,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f50f608d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 17:10:12,266][oumi][rank0][pid:72116][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'custom'\n",
      "[2025-12-30 17:10:12,267][oumi.utils.analysis_utils][rank0][pid:72116][MainThread][INFO]][analysis_utils.py:225] Loaded text dataset from: /Users/ryanarman/code/scratch/ryan_hillclimbing_experiments/banking77/notebooks/data/banking77_train.jsonl\n",
      "[2025-12-30 17:10:12,267][oumi][rank0][pid:72116][MainThread][INFO]][dataset_analyzer.py:154] Loaded dataset from config: None\n",
      "[2025-12-30 17:10:12,268][oumi][rank0][pid:72116][MainThread][INFO]][dataset_analyzer.py:304] Initialized sample analyzer: llm_judge\n",
      "[2025-12-30 17:10:12,303][oumi][rank0][pid:72116][MainThread][INFO]][dataset_analyzer.py:347] Starting analysis of dataset: None\n",
      "[2025-12-30 17:10:12,304][oumi][rank0][pid:72116][MainThread][INFO]][dataset_analyzer.py:348] Using 1 sample analyzers: ['llm_judge']\n",
      "[2025-12-30 17:10:12,305][oumi][rank0][pid:72116][MainThread][INFO]][dataset_analyzer.py:371] Analyzing 5 of 8002 conversations\n",
      "[2025-12-30 17:10:12,305][oumi][rank0][pid:72116][MainThread][INFO]][dataset_analyzer.py:421] Converting conversation dataset with 8002 items\n",
      "[2025-12-30 17:10:12,306][oumi][rank0][pid:72116][MainThread][INFO]][dataset_analyzer.py:428] Limiting analysis to first 5 items (dataset has 8002 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting Unknown Dataset to DataFrames: 100%|██████████| 5/5 [00:00<00:00, 2360.86item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 17:10:12,313][oumi][rank0][pid:72116][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:10<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 17:10:22,430][oumi][rank0][pid:72116][MainThread][INFO]][llm_judge_analyzer.py:780] Skipping message-level analysis (analyze_message_level=False). Set analyze_message_level=True to enable.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from oumi.core.configs import AnalyzeConfig\n",
    "from oumi.core.analyze.dataset_analyzer import DatasetAnalyzer\n",
    "\n",
    "# Create the full AnalyzeConfig\n",
    "config = AnalyzeConfig(\n",
    "    # Dataset configuration\n",
    "    dataset_path=\"/Users/ryanarman/code/scratch/ryan_hillclimbing_experiments/banking77/notebooks/data/banking77_train.jsonl\",\n",
    "    # Analysis configuration\n",
    "    sample_count=5,  # Analyze first 100 samples (LLM calls can be expensive!)\n",
    "    analyzers=[\n",
    "        llm_helpfulness_params_conversation,\n",
    "    ],  # Just the LLM judge\n",
    "    # Output configuration\n",
    "    output_path=\"./analysis_output/banking77_llm_judge\",\n",
    "    generate_report=False,  # Optional: generate HTML report\n",
    ")\n",
    "\n",
    "# Finalize and run\n",
    "config.finalize_and_validate()\n",
    "\n",
    "from oumi.core.analyze.dataset_analyzer import DatasetAnalyzer\n",
    "\n",
    "analyzer = DatasetAnalyzer(config)\n",
    "analyzer.analyze_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a07ae1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(schema): 12\n",
      "len(merged_columns): 12\n"
     ]
    }
   ],
   "source": [
    "schema = analyzer.get_schema()\n",
    "print(f\"len(schema): {len(schema)}\")\n",
    "merged_columns = analyzer.analysis_df.columns\n",
    "print(f\"len(merged_columns): {len(merged_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0c2ff22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['llm_judge']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer_names = [a.id for a in config.analyzers]\n",
    "analyzer_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ee896",
   "metadata": {},
   "source": [
    "# Conv level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c0a267c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_index</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>num_messages</th>\n",
       "      <th>conversation_text_content</th>\n",
       "      <th>conversation_text_content__llm_judge__score</th>\n",
       "      <th>conversation_text_content__llm_judge__label</th>\n",
       "      <th>conversation_text_content__llm_judge__reasoning</th>\n",
       "      <th>conversation_text_content__llm_judge__raw_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>SYSTEM: You are a banking intent classifier. C...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>very_helpful</td>\n",
       "      <td>The classification ID 52 (request_refund) is p...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 10,\\n  \"label\": \"very_h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SYSTEM: You are a banking intent classifier. C...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>very_helpful</td>\n",
       "      <td>The classification ID 69 (verify_my_identity) ...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 10,\\n  \"label\": \"very_h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>SYSTEM: You are a banking intent classifier. C...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>helpful</td>\n",
       "      <td>The classification ID 59 (top_up_failed) is hi...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 9,\\n  \"label\": \"helpful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>SYSTEM: You are a banking intent classifier. C...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>somewhat_helpful</td>\n",
       "      <td>The classification ID 54 (Refund_not_showing_u...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 4,\\n  \"label\": \"somewha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>SYSTEM: You are a banking intent classifier. C...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>helpful</td>\n",
       "      <td>The classification ID 57 (top_up_by_card_charg...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 9,\\n  \"label\": \"helpful...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   conversation_index conversation_id  num_messages  \\\n",
       "0                   0               0             3   \n",
       "1                   1               1             3   \n",
       "2                   2               2             3   \n",
       "3                   3               3             3   \n",
       "4                   4               4             3   \n",
       "\n",
       "                           conversation_text_content  \\\n",
       "0  SYSTEM: You are a banking intent classifier. C...   \n",
       "1  SYSTEM: You are a banking intent classifier. C...   \n",
       "2  SYSTEM: You are a banking intent classifier. C...   \n",
       "3  SYSTEM: You are a banking intent classifier. C...   \n",
       "4  SYSTEM: You are a banking intent classifier. C...   \n",
       "\n",
       "   conversation_text_content__llm_judge__score  \\\n",
       "0                                         10.0   \n",
       "1                                         10.0   \n",
       "2                                          9.0   \n",
       "3                                          4.0   \n",
       "4                                          9.0   \n",
       "\n",
       "  conversation_text_content__llm_judge__label  \\\n",
       "0                                very_helpful   \n",
       "1                                very_helpful   \n",
       "2                                     helpful   \n",
       "3                            somewhat_helpful   \n",
       "4                                     helpful   \n",
       "\n",
       "     conversation_text_content__llm_judge__reasoning  \\\n",
       "0  The classification ID 52 (request_refund) is p...   \n",
       "1  The classification ID 69 (verify_my_identity) ...   \n",
       "2  The classification ID 59 (top_up_failed) is hi...   \n",
       "3  The classification ID 54 (Refund_not_showing_u...   \n",
       "4  The classification ID 57 (top_up_by_card_charg...   \n",
       "\n",
       "  conversation_text_content__llm_judge__raw_response  \n",
       "0  ```json\\n{\\n  \"score\": 10,\\n  \"label\": \"very_h...  \n",
       "1  ```json\\n{\\n  \"score\": 10,\\n  \"label\": \"very_h...  \n",
       "2  ```json\\n{\\n  \"score\": 9,\\n  \"label\": \"helpful...  \n",
       "3  ```json\\n{\\n  \"score\": 4,\\n  \"label\": \"somewha...  \n",
       "4  ```json\\n{\\n  \"score\": 9,\\n  \"label\": \"helpful...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.conversation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d56f05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer: llm_judge\n",
      "metric: score\n",
      "description: LLM judge score (0-10, higher = better quality)\n",
      "value: 4.0\n",
      "\n",
      "\n",
      "metric: label\n",
      "description: LLM judge label/category for the sample\n",
      "value: somewhat_helpful\n",
      "\n",
      "\n",
      "metric: reasoning\n",
      "description: LLM judge reasoning/explanation\n",
      "value: The classification ID 54 (Refund_not_showing_up) does not align with the user's query about adding funds and accepted payment methods. A more relevant classification would be related to top-up methods or payment options.\n",
      "\n",
      "\n",
      "metric: raw_response\n",
      "description: Raw LLM response before parsing\n",
      "value: ```json\n",
      "{\n",
      "  \"score\": 4,\n",
      "  \"label\": \"somewhat_helpful\",\n",
      "  \"reasoning\": \"The classification ID 54 (Refund_not_showing_up) does not align with the user's query about adding funds and accepted payment methods. A more relevant classification would be related to top-up methods or payment options.\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.analyze.column_utils import (\n",
    "    filter_analyzer_columns,\n",
    "    get_analyzer_columns_by_analyzer,\n",
    "    parse_analyzer_column_name,\n",
    ")\n",
    "\n",
    "conv_columns = analyzer.conversation_df.columns\n",
    "row = analyzer.conversation_df.iloc[3]\n",
    "\n",
    "# Choose the analzyer to analyze\n",
    "analyzer_name = analyzer_names[0]\n",
    "\n",
    "\n",
    "filtered_cols = filter_analyzer_columns(conv_columns, analyzer_id=analyzer_name)\n",
    "if filtered_cols:\n",
    "    print(f\"Analyzer: {analyzer_name}\")\n",
    "    info = parse_analyzer_column_name(filtered_cols[0])\n",
    "    # print(\"\\nInput:\")\n",
    "    # print(f\"source_column: {info.source_column}\")\n",
    "\n",
    "    # print(f\"{row[info.source_column]}\\n\")\n",
    "\n",
    "    for col in filtered_cols:\n",
    "        info = parse_analyzer_column_name(col)\n",
    "        print(f\"metric: {info.metric_name}\")\n",
    "        # print(f\"type: {schema[col]['type']}\")\n",
    "        # print(f\"content_type: {schema[col]['content_type']}\")\n",
    "        print(f\"description: {schema[col]['description']}\")\n",
    "        print(f\"value: {row[col]}\")\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(f\"No columns found for analyzer: {analyzer_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abfccb",
   "metadata": {},
   "source": [
    "# Message level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a7ee9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzer.message_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcb677ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No columns found for analyzer: llm_judge\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.analyze.column_utils import (\n",
    "    filter_analyzer_columns,\n",
    "    get_analyzer_columns_by_analyzer,\n",
    "    parse_analyzer_column_name,\n",
    ")\n",
    "\n",
    "msg_columns = analyzer.message_df.columns\n",
    "row = analyzer.message_df.iloc[2]\n",
    "\n",
    "# Choose the analzyer to analyze\n",
    "analyzer_name = analyzer_names[0]\n",
    "\n",
    "\n",
    "filtered_cols = filter_analyzer_columns(msg_columns, analyzer_id=analyzer_name)\n",
    "if filtered_cols:\n",
    "    print(f\"Analyzer: {analyzer_name}\")\n",
    "    info = parse_analyzer_column_name(filtered_cols[0])\n",
    "\n",
    "    print(\"\\nInput:\")\n",
    "    print(f\"[{row['role']}]\")\n",
    "    # print(f\"{row[info.source_column]}\\n\")\n",
    "    # print(f\"source_column: {info.source_column}\")\n",
    "    # print(f\"{row[info.source_column]}\\n\")\n",
    "\n",
    "    for col in filtered_cols:\n",
    "        info = parse_analyzer_column_name(col)\n",
    "        print(f\"metric: {info.metric_name}\")\n",
    "        # print(f\"type: {schema[col]['type']}\")\n",
    "        # print(f\"content_type: {schema[col]['content_type']}\")\n",
    "        print(f\"description: {schema[col]['description']}\")\n",
    "        print(f\"value: {row[col]}\")\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(f\"No columns found for analyzer: {analyzer_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782aee00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
