{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MPS has been disabled - forcing CPU-only mode\n",
            "PyTorch device: cpu\n",
            "PyTorch version: 2.6.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "# IMPORTANT: Set these BEFORE importing torch or any ML libraries\n",
        "# Disable all GPU/MPS backends to prevent crashes with IFD analyzer\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Disable CUDA\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Disable MPS memory allocation\n",
        "os.environ[\"DISABLE_MPS_COMPAT\"] = \"1\"  # Additional MPS disable flag\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"  # Disable HuggingFace telemetry\n",
        "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"  # Allow model downloads\n",
        "\n",
        "# Force CPU usage in PyTorch to avoid MPS crashes\n",
        "import torch\n",
        "\n",
        "# Forcefully disable MPS before anything else\n",
        "torch.set_default_device(\"cpu\")\n",
        "if hasattr(torch.backends, \"mps\"):\n",
        "    # Monkey-patch to prevent MPS usage\n",
        "    original_is_available = torch.backends.mps.is_available\n",
        "    torch.backends.mps.is_available = lambda: False\n",
        "    print(\"MPS has been disabled - forcing CPU-only mode\")\n",
        "else:\n",
        "    print(\"Using CPU for all computations\")\n",
        "\n",
        "print(f\"PyTorch device: {torch.get_default_device()}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Config loaded with 10 analyzers:\n",
            "  - length (type: length)\n",
            "  - token_stats (type: token_stats)\n",
            "  - cost (type: cost)\n",
            "  - fasttext (type: fasttext)\n",
            "  - embedding (type: embedding)\n",
            "  - question_diversity (type: question_diversity)\n",
            "  - repr_diversity (type: repr_diversity)\n",
            "  - validation_quality (type: llm_judge)\n",
            "  - instruction_quality (type: llm_judge)\n",
            "  - response_quality (type: llm_judge)\n",
            "üìÅ Output will be saved to: /Users/ryanarman/data/DMG/analysis_output\n",
            "‚úÖ Config validated successfully!\n",
            "[2026-01-12 16:52:25,913][oumi][rank0][pid:7752][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'custom'\n",
            "[2026-01-12 16:52:25,914][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:227] Loaded text dataset from: /Users/ryanarman/data/DMG/train.jsonl\n",
            "[2026-01-12 16:52:25,914][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:153] Loaded dataset from config: None\n",
            "[2026-01-12 16:52:26,040][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: length\n",
            "[2026-01-12 16:52:26,040][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: token_stats\n",
            "[2026-01-12 16:52:26,041][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: cost\n",
            "[2026-01-12 16:52:26,041][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: fasttext\n",
            "[2026-01-12 16:52:26,062][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: embedding\n",
            "[2026-01-12 16:52:26,063][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: question_diversity\n",
            "[2026-01-12 16:52:26,063][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: repr_diversity\n",
            "[2026-01-12 16:52:26,063][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: validation_quality (type: llm_judge)\n",
            "[2026-01-12 16:52:26,064][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: instruction_quality (type: llm_judge)\n",
            "[2026-01-12 16:52:26,064][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: response_quality (type: llm_judge)\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_PATH = \"/Users/ryanarman/data/DMG/analysis_output\"\n",
        "\n",
        "import os\n",
        "from oumi.core.configs import AnalyzeConfig\n",
        "from oumi.core.analyze.dataset_analyzer import DatasetAnalyzer\n",
        "\n",
        "# Load config from YAML file\n",
        "config = AnalyzeConfig.from_yaml(\n",
        "    \"/Users/ryanarman/code/oumi/configs/examples/analyze/analyze_dmg.yaml\"\n",
        ")\n",
        "\n",
        "# Override settings for this run\n",
        "dataset_path = \"/Users/ryanarman/data/DMG/train.jsonl\"\n",
        "config.dataset_path = dataset_path\n",
        "config.dataset_name = None  # Clear dataset_name so it uses dataset_path instead\n",
        "config.sample_count = 1000  # Adjust as needed\n",
        "config.chat_template = \"default\"\n",
        "\n",
        "# Set absolute output path (makes it easier to find the results!)\n",
        "config.output_path = OUTPUT_PATH\n",
        "\n",
        "print(f\"‚úÖ Config loaded with {len(config.analyzers)} analyzers:\")\n",
        "for analyzer in config.analyzers:\n",
        "    instance_id = analyzer.instance_id or analyzer.id\n",
        "    print(f\"  - {instance_id} (type: {analyzer.id})\")\n",
        "\n",
        "print(f\"üìÅ Output will be saved to: {config.output_path}\")\n",
        "\n",
        "# Validate the configuration\n",
        "config.finalize_and_validate()\n",
        "print(\"‚úÖ Config validated successfully!\")\n",
        "\n",
        "analyzer = DatasetAnalyzer(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:52:26,070][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:362] Starting analysis of dataset: None\n",
            "[2026-01-12 16:52:26,071][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:363] Using 10 sample analyzers: ['length', 'token_stats', 'cost', 'fasttext', 'embedding', 'question_diversity', 'repr_diversity', 'validation_quality', 'instruction_quality', 'response_quality']\n",
            "[2026-01-12 16:52:26,071][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:386] Analyzing 1000 of 5100 conversations\n",
            "[2026-01-12 16:52:26,072][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:438] Converting conversation dataset with 5100 items\n",
            "[2026-01-12 16:52:26,072][oumi][rank0][pid:7752][MainThread][INFO]][dataset_analyzer.py:445] Limiting analysis to first 1000 items (dataset has 5100 total)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting Unknown Dataset to DataFrames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01<00:00, 940.28item/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:52:27,196][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1337] Adding default schema entries for 1 columns not in base schema: ['trade']\n",
            "[2026-01-12 16:52:28,087][oumi][rank0][pid:7752][MainThread][INFO]][fasttext_analyzer.py:220] Initialized fast-langdetect for language detection\n",
            "[2026-01-12 16:52:28,088][oumi][rank0][pid:7752][MainThread][INFO]][fasttext_analyzer.py:458] Analyzing language for column: conversation_text_content\n",
            "[2026-01-12 16:52:28,707][oumi][rank0][pid:7752][MainThread][INFO]][embedding_analyzer.py:518] Computing embeddings for 1000 samples...\n",
            "[2026-01-12 16:52:28,708][oumi][rank0][pid:7752][MainThread][INFO]][embedding_analyzer.py:196] Loading embedding model: all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:08<00:00, 121.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:52:37,946][oumi][rank0][pid:7752][MainThread][INFO]][embedding_analyzer.py:537] Detecting semantic duplicates...\n",
            "[2026-01-12 16:52:38,086][oumi][rank0][pid:7752][MainThread][INFO]][embedding_analyzer.py:624] Detecting fuzzy duplicates using MinHash LSH...\n",
            "[2026-01-12 16:52:38,090][oumi][rank0][pid:7752][MainThread][INFO]][embedding_analyzer.py:350] Creating MinHash signatures for 1000 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating MinHash signatures: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:08<00:00, 112.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:52:47,028][oumi][rank0][pid:7752][MainThread][INFO]][embedding_analyzer.py:369] Finding fuzzy duplicates using LSH...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Finding duplicates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 1378.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:52:47,758][oumi][rank0][pid:7752][MainThread][INFO]][question_diversity_analyzer.py:464] Computing embeddings for 1000 user questions...\n",
            "[2026-01-12 16:52:47,758][oumi][rank0][pid:7752][MainThread][INFO]][question_diversity_analyzer.py:174] Loading embedding model: all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:07<00:00, 125.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:52:56,784][oumi][rank0][pid:7752][MainThread][INFO]][question_diversity_analyzer.py:469] Clustering 1000 questions using dbscan...\n",
            "[2026-01-12 16:52:57,092][oumi][rank0][pid:7752][MainThread][INFO]][question_diversity_analyzer.py:487] Found 1 clusters\n",
            "[2026-01-12 16:52:57,102][oumi][rank0][pid:7752][MainThread][INFO]][repr_diversity_analyzer.py:363] Computing diversity scores for 1000 samples in column 'conversation_text_content'...\n",
            "[2026-01-12 16:52:57,103][oumi][rank0][pid:7752][MainThread][INFO]][repr_diversity_analyzer.py:165] Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:08<00:00, 120.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:53:06,462][oumi][rank0][pid:7752][MainThread][INFO]][repr_diversity_analyzer.py:230] Computing nearest neighbor distances for 1000 samples (k=5)...\n",
            "[2026-01-12 16:53:06,478][oumi][rank0][pid:7752][MainThread][INFO]][repr_diversity_analyzer.py:556] Column 'conversation_text_content': 1000/1000 samples (100.0%) are redundant\n",
            "[2026-01-12 16:53:06,514][oumi][rank0][pid:7752][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|‚ñà‚ñà‚ñà‚ñé      | 334/1000 [00:08<00:06, 96.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:53:15,356][oumi][rank0][pid:7752][ThreadPoolExecutor-4_0][WARNING]][adaptive_concurrency_controller.py:237] Entering warmup state, but concurrency is already at maximum value. Consider raising the max concurrency.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:17<00:00, 55.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:53:24,651][oumi][rank0][pid:7752][MainThread][INFO]][llm_judge_analyzer.py:787] Skipping conversation-level analysis (analyze_conversation_level=False). Set analyze_conversation_level=True to enable.\n",
            "[2026-01-12 16:53:24,652][oumi][rank0][pid:7752][MainThread][INFO]][llm_judge_analyzer.py:787] Skipping conversation-level analysis (analyze_conversation_level=False). Set analyze_conversation_level=True to enable.\n",
            "[2026-01-12 16:53:25,778][oumi][rank0][pid:7752][MainThread][INFO]][fasttext_analyzer.py:458] Analyzing language for column: text_content\n",
            "[2026-01-12 16:53:26,522][oumi][rank0][pid:7752][MainThread][INFO]][embedding_analyzer.py:518] Computing embeddings for 3000 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:18<00:00, 162.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:53:45,008][oumi][rank0][pid:7752][MainThread][INFO]][embedding_analyzer.py:537] Detecting semantic duplicates...\n",
            "[2026-01-12 16:53:45,187][oumi][rank0][pid:7752][MainThread][INFO]][embedding_analyzer.py:624] Detecting fuzzy duplicates using MinHash LSH...\n",
            "[2026-01-12 16:53:45,189][oumi][rank0][pid:7752][MainThread][INFO]][embedding_analyzer.py:350] Creating MinHash signatures for 3000 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating MinHash signatures: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:12<00:00, 249.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:53:57,249][oumi][rank0][pid:7752][MainThread][INFO]][embedding_analyzer.py:369] Finding fuzzy duplicates using LSH...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Finding duplicates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:01<00:00, 2854.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:53:58,310][oumi][rank0][pid:7752][MainThread][INFO]][question_diversity_analyzer.py:464] Computing embeddings for 1000 user questions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:05<00:00, 184.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:54:03,748][oumi][rank0][pid:7752][MainThread][INFO]][question_diversity_analyzer.py:469] Clustering 1000 questions using dbscan...\n",
            "[2026-01-12 16:54:03,755][oumi][rank0][pid:7752][MainThread][INFO]][question_diversity_analyzer.py:482] Found 77 clusters, 792 unique/diverse questions (not similar to others)\n",
            "[2026-01-12 16:54:03,758][oumi][rank0][pid:7752][MainThread][INFO]][repr_diversity_analyzer.py:363] Computing diversity scores for 3000 samples in column 'text_content'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:18<00:00, 160.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:54:22,438][oumi][rank0][pid:7752][MainThread][INFO]][repr_diversity_analyzer.py:230] Computing nearest neighbor distances for 3000 samples (k=5)...\n",
            "[2026-01-12 16:54:22,641][oumi][rank0][pid:7752][MainThread][INFO]][repr_diversity_analyzer.py:556] Column 'text_content': 1939/3000 samples (64.6%) are redundant\n",
            "[2026-01-12 16:54:22,645][oumi][rank0][pid:7752][MainThread][INFO]][llm_judge_analyzer.py:780] Skipping message-level analysis (analyze_message_level=False). Set analyze_message_level=True to enable.\n",
            "[2026-01-12 16:54:22,648][oumi][rank0][pid:7752][MainThread][INFO]][llm_judge_analyzer.py:828] Evaluating 1000 'system' messages (filtered from 3000 total)\n",
            "[2026-01-12 16:54:22,656][oumi][rank0][pid:7752][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n",
            "[2026-01-12 16:54:22,661][oumi][rank0][pid:7752][MainThread][INFO]][llm_judge_analyzer.py:679] Batch of 1000: 1 unique to evaluate, 999 duplicates, 0 from cache\n",
            "[2026-01-12 16:54:25,295][oumi][rank0][pid:7752][MainThread][INFO]][llm_judge_analyzer.py:828] Evaluating 1000 'assistant' messages (filtered from 3000 total)\n",
            "[2026-01-12 16:54:25,301][oumi][rank0][pid:7752][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n",
            "[2026-01-12 16:54:25,307][oumi][rank0][pid:7752][MainThread][INFO]][llm_judge_analyzer.py:679] Batch of 1000: 999 unique to evaluate, 1 duplicates, 0 from cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 460/999 [00:07<00:05, 94.09it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:54:32,912][oumi][rank0][pid:7752][ThreadPoolExecutor-1007_0][WARNING]][adaptive_concurrency_controller.py:237] Entering warmup state, but concurrency is already at maximum value. Consider raising the max concurrency.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [00:15<00:00, 64.35it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total conversations analyzed: 1000\n"
          ]
        }
      ],
      "source": [
        "# Run the analysis\n",
        "analyzer.analyze_dataset()\n",
        "\n",
        "# The results are stored in analyzer object\n",
        "if analyzer._analysis_results:\n",
        "    print(\n",
        "        f\"Total conversations analyzed: {analyzer._analysis_results.conversations_analyzed}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:54:41,228][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1426] Saved message analysis to: /Users/ryanarman/data/DMG/analysis_output/messages_df.parquet\n",
            "[2026-01-12 16:54:41,247][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1432] Saved conversation analysis to: /Users/ryanarman/data/DMG/analysis_output/conversations_df.parquet\n",
            "[2026-01-12 16:54:41,296][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1438] Saved merged analysis to: /Users/ryanarman/data/DMG/analysis_output/merged_df.parquet\n",
            "[2026-01-12 16:54:41,298][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1450] Saved message schema to: /Users/ryanarman/data/DMG/analysis_output/message_schema.json\n",
            "[2026-01-12 16:54:41,299][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1457] Saved conversation schema to: /Users/ryanarman/data/DMG/analysis_output/conversation_schema.json\n",
            "[2026-01-12 16:54:41,300][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1463] Saved combined schemas to: /Users/ryanarman/data/DMG/analysis_output/schema.json\n",
            "[2026-01-12 16:54:41,301][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1470] Saved analysis summary to: /Users/ryanarman/data/DMG/analysis_output/analysis_summary.json\n",
            "[2026-01-12 16:54:41,302][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1472] All analyzer artifacts saved to: /Users/ryanarman/data/DMG/analysis_output\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import traceback\n",
        "from oumi.utils.analysis_utils import save_analyzer_artifacts\n",
        "\n",
        "# Save all analyzer artifacts (dataframes, schemas, summary)\n",
        "save_analyzer_artifacts(analyzer, Path(config.output_path), output_format=\"parquet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load artifacts and generate report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 16:54:41,317][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1551] Loaded message analysis from: /Users/ryanarman/data/DMG/analysis_output/messages_df\n",
            "[2026-01-12 16:54:41,327][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1558] Loaded conversation analysis from: /Users/ryanarman/data/DMG/analysis_output/conversations_df\n",
            "[2026-01-12 16:54:41,352][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1565] Loaded merged analysis from: /Users/ryanarman/data/DMG/analysis_output/merged_df\n",
            "[2026-01-12 16:54:41,353][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1574] Loaded combined schemas from: /Users/ryanarman/data/DMG/analysis_output/schema.json\n",
            "[2026-01-12 16:54:41,354][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1596] Loaded analysis summary from: /Users/ryanarman/data/DMG/analysis_output/analysis_summary.json\n",
            "[2026-01-12 16:54:41,355][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1598] Loaded analyzer artifacts from: /Users/ryanarman/data/DMG/analysis_output\n",
            "[2026-01-12 16:54:41,669][oumi.utils.analysis_utils][rank0][pid:7752][MainThread][INFO]][analysis_utils.py:1773] Regenerated 2 recommendations from artifacts with latest code\n",
            "[2026-01-12 16:54:42,545][oumi][rank0][pid:7752][MainThread][INFO]][report_generator.py:290] Generated HTML report: /Users/ryanarman/data/DMG/analysis_output/index.html\n",
            "[2026-01-12 16:54:42,546][oumi][rank0][pid:7752][MainThread][INFO]][report_generator.py:291] External data files written to: /Users/ryanarman/data/DMG/analysis_output/data\n",
            "‚úÖ Generated HTML report at: /Users/ryanarman/data/DMG/analysis_output/index.html\n",
            "\n",
            "üìÅ All results saved to: /Users/ryanarman/data/DMG/analysis_output\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_PATH = \"/Users/ryanarman/data/DMG/analysis_output\"\n",
        "from oumi.utils.analysis_utils import (\n",
        "    load_analyzer_artifacts,\n",
        "    regenerate_recommendations,\n",
        ")\n",
        "\n",
        "artifacts = load_analyzer_artifacts(OUTPUT_PATH)\n",
        "\n",
        "# Regenerate recommendations with latest code (e.g., updated duplicate detection)\n",
        "artifacts = regenerate_recommendations(artifacts, outlier_threshold=3.0)\n",
        "\n",
        "artifacts.keys()\n",
        "\n",
        "\n",
        "# Generate HTML report if configured\n",
        "\n",
        "\n",
        "try:\n",
        "    from oumi.core.analyze.report_generator import HTMLReportGenerator\n",
        "\n",
        "    report_gen = HTMLReportGenerator()\n",
        "    report_path = report_gen.generate_report(\n",
        "        artifacts=artifacts,\n",
        "        output_path=OUTPUT_PATH,\n",
        "        title=\"DMG Invoice Validation Analysis Report\",\n",
        "    )\n",
        "    print(f\"‚úÖ Generated HTML report at: {report_path / 'index.html'}\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  Plotly not installed. Skipping HTML report generation.\")\n",
        "    print(\"   Install with: pip install 'oumi[analyze_advanced]'\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Failed to generate HTML report: {e}\")\n",
        "    print(\"\\nüîç FULL TRACEBACK:\")\n",
        "    print(\"=\" * 70)\n",
        "    traceback.print_exc()\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüìÅ All results saved to: {OUTPUT_PATH}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "oumi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
