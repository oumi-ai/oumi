{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334e3fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS has been disabled - forcing CPU-only mode\n",
      "PyTorch device: cpu\n",
      "PyTorch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# IMPORTANT: Set these BEFORE importing torch or any ML libraries\n",
    "# Disable all GPU/MPS backends to prevent crashes with IFD analyzer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Disable CUDA\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Disable MPS memory allocation\n",
    "os.environ[\"DISABLE_MPS_COMPAT\"] = \"1\"  # Additional MPS disable flag\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"  # Disable HuggingFace telemetry\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"  # Allow model downloads\n",
    "\n",
    "# Force CPU usage in PyTorch to avoid MPS crashes\n",
    "import torch\n",
    "\n",
    "# Forcefully disable MPS before anything else\n",
    "torch.set_default_device(\"cpu\")\n",
    "if hasattr(torch.backends, \"mps\"):\n",
    "    # Monkey-patch to prevent MPS usage\n",
    "    original_is_available = torch.backends.mps.is_available\n",
    "    torch.backends.mps.is_available = lambda: False\n",
    "    print(\"MPS has been disabled - forcing CPU-only mode\")\n",
    "else:\n",
    "    print(\"Using CPU for all computations\")\n",
    "\n",
    "print(f\"PyTorch device: {torch.get_default_device()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f5f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"/Users/ryanarman/code/oumi/analysis_output/ultrachat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "430925a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Config loaded with 25 analyzers:\n",
      "  - length (type: length)\n",
      "  - diversity (type: diversity)\n",
      "  - format (type: format)\n",
      "  - quality (type: quality)\n",
      "  - content_pattern (type: content_pattern)\n",
      "  - embedding (type: embedding)\n",
      "  - question_diversity (type: question_diversity)\n",
      "  - repr_diversity (type: repr_diversity)\n",
      "  - conversation_structure (type: conversation_structure)\n",
      "  - response_completeness (type: response_completeness)\n",
      "  - training_quality (type: training_quality)\n",
      "  - task_category (type: task_category)\n",
      "  - safety (type: safety)\n",
      "  - difficulty (type: difficulty)\n",
      "  - input_quality (type: input_quality)\n",
      "  - instruct_reward (type: instruct_reward)\n",
      "  - cost (type: cost)\n",
      "  - helpfulness (type: llm_judge)\n",
      "  - instruction_quality (type: llm_judge)\n",
      "  - response_quality (type: llm_judge)\n",
      "  - excessive_politeness (type: llm_judge)\n",
      "  - roleplay_bleeding (type: llm_judge)\n",
      "  - reasoning_leakage (type: llm_judge)\n",
      "  - style_homogenization (type: llm_judge)\n",
      "  - repetitive_turns (type: llm_judge)\n",
      "üìÅ Output will be saved to: /Users/ryanarman/code/oumi/analysis_output/ultrachat\n",
      "‚úÖ Config validated successfully!\n",
      "[2026-01-07 14:56:38,988][oumi][rank0][pid:6650][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: UltrachatH4Dataset)... dataset_name: 'HuggingFaceH4/ultrachat_200k'\n",
      "[2026-01-07 14:56:40,959][oumi][rank0][pid:6650][MainThread][INFO]][base_map_dataset.py:486] Dataset Info:\n",
      "\tSplit: train_sft[:100]\n",
      "\tVersion: 0.0.0\n",
      "\tDataset size: 3047427114\n",
      "\tDownload size: 1624049723\n",
      "\tSize: 4671476837 bytes\n",
      "\tRows: 100\n",
      "\tColumns: ['prompt', 'prompt_id', 'messages']\n",
      "[2026-01-07 14:56:41,073][oumi][rank0][pid:6650][MainThread][INFO]][base_map_dataset.py:426] Loaded DataFrame with shape: (100, 3). Columns:\n",
      "prompt       object\n",
      "prompt_id    object\n",
      "messages     object\n",
      "dtype: object\n",
      "[2026-01-07 14:56:41,073][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:154] Loaded dataset from config: HuggingFaceH4/ultrachat_200k\n",
      "[2026-01-07 14:56:41,188][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: length\n",
      "[2026-01-07 14:56:41,189][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: diversity\n",
      "[2026-01-07 14:56:41,190][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: format\n",
      "[2026-01-07 14:56:41,190][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: quality\n",
      "[2026-01-07 14:56:41,190][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: content_pattern\n",
      "[2026-01-07 14:56:41,200][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: embedding\n",
      "[2026-01-07 14:56:41,201][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: question_diversity\n",
      "[2026-01-07 14:56:41,201][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: repr_diversity\n",
      "[2026-01-07 14:56:41,202][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: conversation_structure\n",
      "[2026-01-07 14:56:41,202][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: response_completeness\n",
      "[2026-01-07 14:56:41,203][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: training_quality\n",
      "[2026-01-07 14:56:41,203][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: task_category\n",
      "[2026-01-07 14:56:41,203][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: safety\n",
      "[2026-01-07 14:56:41,204][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: difficulty\n",
      "[2026-01-07 14:56:41,205][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: input_quality\n",
      "[2026-01-07 14:56:41,205][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: instruct_reward\n",
      "[2026-01-07 14:56:41,205][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: cost\n",
      "[2026-01-07 14:56:41,205][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: helpfulness (type: llm_judge)\n",
      "[2026-01-07 14:56:41,206][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: instruction_quality (type: llm_judge)\n",
      "[2026-01-07 14:56:41,207][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: response_quality (type: llm_judge)\n",
      "[2026-01-07 14:56:41,207][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: excessive_politeness (type: llm_judge)\n",
      "[2026-01-07 14:56:41,208][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: roleplay_bleeding (type: llm_judge)\n",
      "[2026-01-07 14:56:41,208][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: reasoning_leakage (type: llm_judge)\n",
      "[2026-01-07 14:56:41,208][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: style_homogenization (type: llm_judge)\n",
      "[2026-01-07 14:56:41,209][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: repetitive_turns (type: llm_judge)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from oumi.core.configs import AnalyzeConfig\n",
    "from oumi.core.analyze.dataset_analyzer import DatasetAnalyzer\n",
    "\n",
    "# Load config from YAML file\n",
    "config = AnalyzeConfig.from_yaml(\n",
    "    \"/Users/ryanarman/code/oumi/configs/examples/analyze/analyze_ultrachat.yaml\"\n",
    ")\n",
    "\n",
    "# Override settings for this run\n",
    "# dataset_path = \"/Users/ryanarman/code/scratch/ryan_hillclimbing_experiments/banking77/notebooks/data/banking77_train.jsonl\"\n",
    "# config.dataset_path = dataset_path\n",
    "# config.dataset_name = None  # Clear dataset_name so it uses dataset_path instead\n",
    "config.sample_count = 10\n",
    "config.chat_template = \"chat_ml\"\n",
    "\n",
    "# Set absolute output path (makes it easier to find the results!)\n",
    "config.output_path = OUTPUT_PATH\n",
    "\n",
    "print(f\"‚úÖ Config loaded with {len(config.analyzers)} analyzers:\")\n",
    "for analyzer in config.analyzers:\n",
    "    instance_id = analyzer.instance_id or analyzer.id\n",
    "    print(f\"  - {instance_id} (type: {analyzer.id})\")\n",
    "\n",
    "print(f\"üìÅ Output will be saved to: {config.output_path}\")\n",
    "\n",
    "# Validate the configuration\n",
    "config.finalize_and_validate()\n",
    "print(\"‚úÖ Config validated successfully!\")\n",
    "\n",
    "analyzer = DatasetAnalyzer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d0ca499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:56:41,216][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:363] Starting analysis of dataset: HuggingFaceH4/ultrachat_200k\n",
      "[2026-01-07 14:56:41,217][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:364] Using 25 sample analyzers: ['length', 'diversity', 'format', 'quality', 'content_pattern', 'embedding', 'question_diversity', 'repr_diversity', 'conversation_structure', 'response_completeness', 'training_quality', 'task_category', 'safety', 'difficulty', 'input_quality', 'instruct_reward', 'cost', 'helpfulness', 'instruction_quality', 'response_quality', 'excessive_politeness', 'roleplay_bleeding', 'reasoning_leakage', 'style_homogenization', 'repetitive_turns']\n",
      "[2026-01-07 14:56:41,218][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:387] Analyzing 10 of 100 conversations\n",
      "[2026-01-07 14:56:41,218][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:439] Converting conversation dataset with 100 items\n",
      "[2026-01-07 14:56:41,218][oumi][rank0][pid:6650][MainThread][INFO]][dataset_analyzer.py:446] Limiting analysis to first 10 items (dataset has 100 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting HuggingFaceH4/ultrachat_200k to DataFrames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 566.57item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:56:41,265][oumi][rank0][pid:6650][MainThread][INFO]][embedding_analyzer.py:518] Computing embeddings for 10 samples...\n",
      "[2026-01-07 14:56:41,266][oumi][rank0][pid:6650][MainThread][INFO]][embedding_analyzer.py:196] Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 93.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:56:42,847][oumi][rank0][pid:6650][MainThread][INFO]][embedding_analyzer.py:537] Detecting semantic duplicates...\n",
      "[2026-01-07 14:56:42,849][oumi][rank0][pid:6650][MainThread][INFO]][embedding_analyzer.py:624] Detecting fuzzy duplicates using MinHash LSH...\n",
      "[2026-01-07 14:56:42,849][oumi][rank0][pid:6650][MainThread][INFO]][embedding_analyzer.py:350] Creating MinHash signatures for 10 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating MinHash signatures: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 156.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:56:42,923][oumi][rank0][pid:6650][MainThread][INFO]][embedding_analyzer.py:369] Finding fuzzy duplicates using LSH...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding duplicates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 46294.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:56:42,928][oumi][rank0][pid:6650][MainThread][INFO]][question_diversity_analyzer.py:464] Computing embeddings for 10 user questions...\n",
      "[2026-01-07 14:56:42,928][oumi][rank0][pid:6650][MainThread][INFO]][question_diversity_analyzer.py:174] Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 101.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:56:44,576][oumi][rank0][pid:6650][MainThread][INFO]][question_diversity_analyzer.py:469] Clustering 10 questions using dbscan...\n",
      "[2026-01-07 14:56:44,772][oumi][rank0][pid:6650][MainThread][INFO]][question_diversity_analyzer.py:482] Found 0 clusters, 10 unique/diverse questions (not similar to others)\n",
      "[2026-01-07 14:56:44,776][oumi][rank0][pid:6650][MainThread][INFO]][repr_diversity_analyzer.py:363] Computing diversity scores for 10 samples in column 'conversation_text_content'...\n",
      "[2026-01-07 14:56:44,779][oumi][rank0][pid:6650][MainThread][INFO]][repr_diversity_analyzer.py:165] Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 93.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:56:46,579][oumi][rank0][pid:6650][MainThread][INFO]][repr_diversity_analyzer.py:230] Computing nearest neighbor distances for 10 samples (k=5)...\n",
      "[2026-01-07 14:56:46,580][oumi][rank0][pid:6650][MainThread][INFO]][repr_diversity_analyzer.py:556] Column 'conversation_text_content': 0/10 samples (0.0%) are redundant\n",
      "[2026-01-07 14:56:46,726][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:05<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:56:52,198][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:787] Skipping conversation-level analysis (analyze_conversation_level=False). Set analyze_conversation_level=True to enable.\n",
      "[2026-01-07 14:56:52,199][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:787] Skipping conversation-level analysis (analyze_conversation_level=False). Set analyze_conversation_level=True to enable.\n",
      "[2026-01-07 14:56:52,199][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:787] Skipping conversation-level analysis (analyze_conversation_level=False). Set analyze_conversation_level=True to enable.\n",
      "[2026-01-07 14:56:52,200][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:56:55,019][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:787] Skipping conversation-level analysis (analyze_conversation_level=False). Set analyze_conversation_level=True to enable.\n",
      "[2026-01-07 14:56:55,021][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:56:57,892][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:57:00,447][oumi][rank0][pid:6650][MainThread][INFO]][embedding_analyzer.py:518] Computing embeddings for 78 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00<00:00, 183.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:57:00,875][oumi][rank0][pid:6650][MainThread][INFO]][embedding_analyzer.py:537] Detecting semantic duplicates...\n",
      "[2026-01-07 14:57:00,877][oumi][rank0][pid:6650][MainThread][INFO]][embedding_analyzer.py:624] Detecting fuzzy duplicates using MinHash LSH...\n",
      "[2026-01-07 14:57:00,885][oumi][rank0][pid:6650][MainThread][INFO]][embedding_analyzer.py:350] Creating MinHash signatures for 78 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating MinHash signatures: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00<00:00, 525.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:57:01,073][oumi][rank0][pid:6650][MainThread][INFO]][embedding_analyzer.py:369] Finding fuzzy duplicates using LSH...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding duplicates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00<00:00, 23423.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:57:01,085][oumi][rank0][pid:6650][MainThread][INFO]][question_diversity_analyzer.py:464] Computing embeddings for 39 user questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:00<00:00, 169.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:57:01,319][oumi][rank0][pid:6650][MainThread][INFO]][question_diversity_analyzer.py:469] Clustering 39 questions using dbscan...\n",
      "[2026-01-07 14:57:01,323][oumi][rank0][pid:6650][MainThread][INFO]][question_diversity_analyzer.py:482] Found 0 clusters, 39 unique/diverse questions (not similar to others)\n",
      "[2026-01-07 14:57:01,324][oumi][rank0][pid:6650][MainThread][INFO]][repr_diversity_analyzer.py:363] Computing diversity scores for 78 samples in column 'text_content'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00<00:00, 178.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:57:01,765][oumi][rank0][pid:6650][MainThread][INFO]][repr_diversity_analyzer.py:230] Computing nearest neighbor distances for 78 samples (k=5)...\n",
      "[2026-01-07 14:57:01,770][oumi][rank0][pid:6650][MainThread][INFO]][repr_diversity_analyzer.py:556] Column 'text_content': 8/78 samples (10.3%) are redundant\n",
      "[2026-01-07 14:57:01,846][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:780] Skipping message-level analysis (analyze_message_level=False). Set analyze_message_level=True to enable.\n",
      "[2026-01-07 14:57:01,848][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:828] Evaluating 39 'user' messages (filtered from 78 total)\n",
      "[2026-01-07 14:57:01,853][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:07<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:57:09,664][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:828] Evaluating 39 'assistant' messages (filtered from 78 total)\n",
      "[2026-01-07 14:57:09,666][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:04<00:00,  8.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:57:14,480][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:828] Evaluating 39 'assistant' messages (filtered from 78 total)\n",
      "[2026-01-07 14:57:14,482][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:02<00:00, 14.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:57:17,233][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:780] Skipping message-level analysis (analyze_message_level=False). Set analyze_message_level=True to enable.\n",
      "[2026-01-07 14:57:17,235][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:828] Evaluating 39 'assistant' messages (filtered from 78 total)\n",
      "[2026-01-07 14:57:17,236][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:02<00:00, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:57:19,825][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:780] Skipping message-level analysis (analyze_message_level=False). Set analyze_message_level=True to enable.\n",
      "[2026-01-07 14:57:19,826][oumi][rank0][pid:6650][MainThread][INFO]][llm_judge_analyzer.py:780] Skipping message-level analysis (analyze_message_level=False). Set analyze_message_level=True to enable.\n",
      "Total conversations analyzed: 10\n"
     ]
    }
   ],
   "source": [
    "# Run the analysis\n",
    "analyzer.analyze_dataset()\n",
    "\n",
    "# The results are stored in analyzer object\n",
    "if analyzer._analysis_results:\n",
    "    print(\n",
    "        f\"Total conversations analyzed: {analyzer._analysis_results.conversations_analyzed}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "258614e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:57:25,303][oumi.utils.analysis_utils][rank0][pid:6650][MainThread][INFO]][analysis_utils.py:1411] Saved message analysis to: /Users/ryanarman/code/oumi/analysis_output/ultrachat/messages_df.parquet\n",
      "[2026-01-07 14:57:25,310][oumi.utils.analysis_utils][rank0][pid:6650][MainThread][INFO]][analysis_utils.py:1417] Saved conversation analysis to: /Users/ryanarman/code/oumi/analysis_output/ultrachat/conversations_df.parquet\n",
      "[2026-01-07 14:57:25,326][oumi.utils.analysis_utils][rank0][pid:6650][MainThread][INFO]][analysis_utils.py:1423] Saved merged analysis to: /Users/ryanarman/code/oumi/analysis_output/ultrachat/merged_df.parquet\n",
      "[2026-01-07 14:57:25,328][oumi.utils.analysis_utils][rank0][pid:6650][MainThread][INFO]][analysis_utils.py:1435] Saved message schema to: /Users/ryanarman/code/oumi/analysis_output/ultrachat/message_schema.json\n",
      "[2026-01-07 14:57:25,329][oumi.utils.analysis_utils][rank0][pid:6650][MainThread][INFO]][analysis_utils.py:1442] Saved conversation schema to: /Users/ryanarman/code/oumi/analysis_output/ultrachat/conversation_schema.json\n",
      "[2026-01-07 14:57:25,331][oumi.utils.analysis_utils][rank0][pid:6650][MainThread][INFO]][analysis_utils.py:1448] Saved combined schemas to: /Users/ryanarman/code/oumi/analysis_output/ultrachat/schema.json\n",
      "[2026-01-07 14:57:25,334][oumi.utils.analysis_utils][rank0][pid:6650][MainThread][INFO]][analysis_utils.py:1455] Saved analysis summary to: /Users/ryanarman/code/oumi/analysis_output/ultrachat/analysis_summary.json\n",
      "[2026-01-07 14:57:25,336][oumi.utils.analysis_utils][rank0][pid:6650][MainThread][INFO]][analysis_utils.py:1457] All analyzer artifacts saved to: /Users/ryanarman/code/oumi/analysis_output/ultrachat\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import traceback\n",
    "from oumi.utils.analysis_utils import save_analyzer_artifacts\n",
    "\n",
    "# Save all analyzer artifacts (dataframes, schemas, summary)\n",
    "save_analyzer_artifacts(analyzer, Path(config.output_path), output_format=\"parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dce9ae",
   "metadata": {},
   "source": [
    "# Load artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76218244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:57:48,637][oumi.utils.analysis_utils][rank0][pid:6650][MainThread][INFO]][analysis_utils.py:1536] Loaded message analysis from: /Users/ryanarman/code/oumi/analysis_output/ultrachat/messages_df\n",
      "[2026-01-07 14:57:48,644][oumi.utils.analysis_utils][rank0][pid:6650][MainThread][INFO]][analysis_utils.py:1543] Loaded conversation analysis from: /Users/ryanarman/code/oumi/analysis_output/ultrachat/conversations_df\n",
      "[2026-01-07 14:57:48,655][oumi.utils.analysis_utils][rank0][pid:6650][MainThread][INFO]][analysis_utils.py:1550] Loaded merged analysis from: /Users/ryanarman/code/oumi/analysis_output/ultrachat/merged_df\n",
      "[2026-01-07 14:57:48,656][oumi.utils.analysis_utils][rank0][pid:6650][MainThread][INFO]][analysis_utils.py:1559] Loaded combined schemas from: /Users/ryanarman/code/oumi/analysis_output/ultrachat/schema.json\n",
      "[2026-01-07 14:57:48,658][oumi.utils.analysis_utils][rank0][pid:6650][MainThread][INFO]][analysis_utils.py:1581] Loaded analysis summary from: /Users/ryanarman/code/oumi/analysis_output/ultrachat/analysis_summary.json\n",
      "[2026-01-07 14:57:48,659][oumi.utils.analysis_utils][rank0][pid:6650][MainThread][INFO]][analysis_utils.py:1583] Loaded analyzer artifacts from: /Users/ryanarman/code/oumi/analysis_output/ultrachat\n",
      "[2026-01-07 14:57:48,669][oumi.utils.analysis_utils][rank0][pid:6650][MainThread][INFO]][analysis_utils.py:1639] Regenerated 2 recommendations from artifacts with latest code\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['messages_df', 'conversations_df', 'merged_df', 'schemas', 'analysis_summary'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/Users/ryanarman/code/oumi/analysis_output/ultrachat\"\n",
    "from oumi.utils.analysis_utils import (\n",
    "    load_analyzer_artifacts,\n",
    "    regenerate_recommendations,\n",
    ")\n",
    "\n",
    "artifacts = load_analyzer_artifacts(OUTPUT_PATH)\n",
    "\n",
    "# Regenerate recommendations with latest code (e.g., updated duplicate detection)\n",
    "artifacts = regenerate_recommendations(artifacts, outlier_threshold=3.0)\n",
    "\n",
    "artifacts.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef5d8246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 14:57:50,179][oumi][rank0][pid:6650][MainThread][INFO]][report_generator.py:263] Generated HTML report: /Users/ryanarman/code/oumi/analysis_output/ultrachat/index.html\n",
      "[2026-01-07 14:57:50,180][oumi][rank0][pid:6650][MainThread][INFO]][report_generator.py:264] External data files written to: /Users/ryanarman/code/oumi/analysis_output/ultrachat/data\n",
      "‚úÖ Generated HTML report at: /Users/ryanarman/code/oumi/analysis_output/ultrachat/index.html\n",
      "\n",
      "üìÅ All results saved to: /Users/ryanarman/code/oumi/analysis_output/ultrachat\n"
     ]
    }
   ],
   "source": [
    "# Generate HTML report if configured\n",
    "\n",
    "\n",
    "try:\n",
    "    from oumi.core.analyze.report_generator import HTMLReportGenerator\n",
    "\n",
    "    report_gen = HTMLReportGenerator()\n",
    "    report_path = report_gen.generate_report(\n",
    "        artifacts=artifacts,\n",
    "        output_path=OUTPUT_PATH,\n",
    "        title=\"Ultrachat Analysis Report\",\n",
    "    )\n",
    "    print(f\"‚úÖ Generated HTML report at: {report_path / 'index.html'}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Plotly not installed. Skipping HTML report generation.\")\n",
    "    print(\"   Install with: pip install 'oumi[analyze_advanced]'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Failed to generate HTML report: {e}\")\n",
    "    print(\"\\nüîç FULL TRACEBACK:\")\n",
    "    print(\"=\" * 70)\n",
    "    traceback.print_exc()\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìÅ All results saved to: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a07ae1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(schema): 206\n",
      "len(merged_columns): 206\n"
     ]
    }
   ],
   "source": [
    "schema = analyzer.get_schema()\n",
    "print(f\"len(schema): {len(schema)}\")\n",
    "merged_columns = analyzer.analysis_df.columns\n",
    "print(f\"len(merged_columns): {len(merged_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0c2ff22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['length',\n",
       " 'diversity',\n",
       " 'format',\n",
       " 'quality',\n",
       " 'content_pattern',\n",
       " 'embedding',\n",
       " 'question_diversity',\n",
       " 'repr_diversity',\n",
       " 'conversation_structure',\n",
       " 'response_completeness',\n",
       " 'training_quality',\n",
       " 'task_category',\n",
       " 'safety',\n",
       " 'difficulty',\n",
       " 'input_quality',\n",
       " 'instruct_reward',\n",
       " 'cost',\n",
       " 'helpfulness',\n",
       " 'instruction_quality',\n",
       " 'response_quality',\n",
       " 'excessive_politeness',\n",
       " 'roleplay_bleeding',\n",
       " 'reasoning_leakage',\n",
       " 'style_homogenization',\n",
       " 'repetitive_turns']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer_names = [a.instance_id for a in config.analyzers]\n",
    "analyzer_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ee896",
   "metadata": {},
   "source": [
    "# Conv level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28c7969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_df = artifacts[\"conversations_df\"]\n",
    "msg_df = artifacts[\"messages_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e71af10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_index</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>num_messages</th>\n",
       "      <th>conversation_text_content</th>\n",
       "      <th>conversation_text_content__length__token_count</th>\n",
       "      <th>conversation_text_content__diversity__unique_words_ratio</th>\n",
       "      <th>conversation_text_content__format__has_markdown</th>\n",
       "      <th>conversation_text_content__format__has_json</th>\n",
       "      <th>conversation_text_content__format__has_code_blocks</th>\n",
       "      <th>conversation_text_content__format__code_block_count</th>\n",
       "      <th>...</th>\n",
       "      <th>conversation_text_content__roleplay_bleeding__reasoning</th>\n",
       "      <th>conversation_text_content__roleplay_bleeding__raw_response</th>\n",
       "      <th>conversation_text_content__style_homogenization__score</th>\n",
       "      <th>conversation_text_content__style_homogenization__label</th>\n",
       "      <th>conversation_text_content__style_homogenization__reasoning</th>\n",
       "      <th>conversation_text_content__style_homogenization__raw_response</th>\n",
       "      <th>conversation_text_content__repetitive_turns__score</th>\n",
       "      <th>conversation_text_content__repetitive_turns__label</th>\n",
       "      <th>conversation_text_content__repetitive_turns__reasoning</th>\n",
       "      <th>conversation_text_content__repetitive_turns__raw_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;|im_start|&gt;user\\nThese instructions apply to ...</td>\n",
       "      <td>729</td>\n",
       "      <td>0.423372</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>The assistant provided straightforward, factua...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 0,\\n  \"label\": \"none\",\\...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>The conversation contains some repetitive stru...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 7,\\n  \"label\": \"moderat...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>The conversation contains moderate repetition,...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 7,\\n  \"label\": \"moderat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>&lt;|im_start|&gt;user\\nWhich famous landmarks shoul...</td>\n",
       "      <td>1541</td>\n",
       "      <td>0.439195</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>The assistant provides informative and engagin...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 0,\\n  \"label\": \"none\",\\...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>The conversation contains some repetitive stru...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 6,\\n  \"label\": \"moderat...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>The conversation exhibits some repetition, par...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 6,\\n  \"label\": \"moderat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;|im_start|&gt;user\\nWrite a comprehensive blog p...</td>\n",
       "      <td>3007</td>\n",
       "      <td>0.293608</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>The assistant provided a natural and informati...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 0,\\n  \"label\": \"none\",\\...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>severe</td>\n",
       "      <td>The conversation exhibits a high degree of hom...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 8,\\n  \"label\": \"severe\"...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>The conversation exhibits some repetition, par...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 6,\\n  \"label\": \"moderat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;|im_start|&gt;user\\nDe Le√≥n, previewing the spee...</td>\n",
       "      <td>425</td>\n",
       "      <td>0.531056</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>The assistant provided a straightforward and i...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 0,\\n  \"label\": \"none\",\\...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>The conversation contains some generic phrasin...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 7,\\n  \"label\": \"moderat...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>minimal</td>\n",
       "      <td>While there is some repetition in discussing S...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 4,\\n  \"label\": \"minimal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;|im_start|&gt;user\\nWrite an essay that evaluate...</td>\n",
       "      <td>2195</td>\n",
       "      <td>0.336683</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>The assistant provides natural responses and p...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 0,\\n  \"label\": \"none\",\\...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>severe</td>\n",
       "      <td>The conversation exhibits a high degree of hom...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 8,\\n  \"label\": \"severe\"...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>The conversation exhibits some repetition, par...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 5,\\n  \"label\": \"moderat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   conversation_index conversation_id  num_messages  \\\n",
       "0                   0               0             8   \n",
       "1                   1               1            12   \n",
       "2                   2               2             8   \n",
       "3                   3               3             4   \n",
       "4                   4               4             8   \n",
       "\n",
       "                           conversation_text_content  \\\n",
       "0  <|im_start|>user\\nThese instructions apply to ...   \n",
       "1  <|im_start|>user\\nWhich famous landmarks shoul...   \n",
       "2  <|im_start|>user\\nWrite a comprehensive blog p...   \n",
       "3  <|im_start|>user\\nDe Le√≥n, previewing the spee...   \n",
       "4  <|im_start|>user\\nWrite an essay that evaluate...   \n",
       "\n",
       "   conversation_text_content__length__token_count  \\\n",
       "0                                             729   \n",
       "1                                            1541   \n",
       "2                                            3007   \n",
       "3                                             425   \n",
       "4                                            2195   \n",
       "\n",
       "   conversation_text_content__diversity__unique_words_ratio  \\\n",
       "0                                           0.423372          \n",
       "1                                           0.439195          \n",
       "2                                           0.293608          \n",
       "3                                           0.531056          \n",
       "4                                           0.336683          \n",
       "\n",
       "   conversation_text_content__format__has_markdown  \\\n",
       "0                                             True   \n",
       "1                                             True   \n",
       "2                                             True   \n",
       "3                                             True   \n",
       "4                                             True   \n",
       "\n",
       "   conversation_text_content__format__has_json  \\\n",
       "0                                        False   \n",
       "1                                        False   \n",
       "2                                        False   \n",
       "3                                        False   \n",
       "4                                        False   \n",
       "\n",
       "   conversation_text_content__format__has_code_blocks  \\\n",
       "0                                              False    \n",
       "1                                              False    \n",
       "2                                              False    \n",
       "3                                              False    \n",
       "4                                              False    \n",
       "\n",
       "   conversation_text_content__format__code_block_count  ...  \\\n",
       "0                                                  0    ...   \n",
       "1                                                  0    ...   \n",
       "2                                                  0    ...   \n",
       "3                                                  0    ...   \n",
       "4                                                  0    ...   \n",
       "\n",
       "  conversation_text_content__roleplay_bleeding__reasoning  \\\n",
       "0  The assistant provided straightforward, factua...        \n",
       "1  The assistant provides informative and engagin...        \n",
       "2  The assistant provided a natural and informati...        \n",
       "3  The assistant provided a straightforward and i...        \n",
       "4  The assistant provides natural responses and p...        \n",
       "\n",
       "   conversation_text_content__roleplay_bleeding__raw_response  \\\n",
       "0  ```json\\n{\\n  \"score\": 0,\\n  \"label\": \"none\",\\...            \n",
       "1  ```json\\n{\\n  \"score\": 0,\\n  \"label\": \"none\",\\...            \n",
       "2  ```json\\n{\\n  \"score\": 0,\\n  \"label\": \"none\",\\...            \n",
       "3  ```json\\n{\\n  \"score\": 0,\\n  \"label\": \"none\",\\...            \n",
       "4  ```json\\n{\\n  \"score\": 0,\\n  \"label\": \"none\",\\...            \n",
       "\n",
       "   conversation_text_content__style_homogenization__score  \\\n",
       "0                                                7.0        \n",
       "1                                                6.0        \n",
       "2                                                8.0        \n",
       "3                                                7.0        \n",
       "4                                                8.0        \n",
       "\n",
       "   conversation_text_content__style_homogenization__label  \\\n",
       "0                                           moderate        \n",
       "1                                           moderate        \n",
       "2                                             severe        \n",
       "3                                           moderate        \n",
       "4                                             severe        \n",
       "\n",
       "   conversation_text_content__style_homogenization__reasoning  \\\n",
       "0  The conversation contains some repetitive stru...            \n",
       "1  The conversation contains some repetitive stru...            \n",
       "2  The conversation exhibits a high degree of hom...            \n",
       "3  The conversation contains some generic phrasin...            \n",
       "4  The conversation exhibits a high degree of hom...            \n",
       "\n",
       "  conversation_text_content__style_homogenization__raw_response  \\\n",
       "0  ```json\\n{\\n  \"score\": 7,\\n  \"label\": \"moderat...              \n",
       "1  ```json\\n{\\n  \"score\": 6,\\n  \"label\": \"moderat...              \n",
       "2  ```json\\n{\\n  \"score\": 8,\\n  \"label\": \"severe\"...              \n",
       "3  ```json\\n{\\n  \"score\": 7,\\n  \"label\": \"moderat...              \n",
       "4  ```json\\n{\\n  \"score\": 8,\\n  \"label\": \"severe\"...              \n",
       "\n",
       "   conversation_text_content__repetitive_turns__score  \\\n",
       "0                                                7.0    \n",
       "1                                                6.0    \n",
       "2                                                6.0    \n",
       "3                                                4.0    \n",
       "4                                                5.0    \n",
       "\n",
       "   conversation_text_content__repetitive_turns__label  \\\n",
       "0                                           moderate    \n",
       "1                                           moderate    \n",
       "2                                           moderate    \n",
       "3                                            minimal    \n",
       "4                                           moderate    \n",
       "\n",
       "   conversation_text_content__repetitive_turns__reasoning  \\\n",
       "0  The conversation contains moderate repetition,...        \n",
       "1  The conversation exhibits some repetition, par...        \n",
       "2  The conversation exhibits some repetition, par...        \n",
       "3  While there is some repetition in discussing S...        \n",
       "4  The conversation exhibits some repetition, par...        \n",
       "\n",
       "   conversation_text_content__repetitive_turns__raw_response  \n",
       "0  ```json\\n{\\n  \"score\": 7,\\n  \"label\": \"moderat...          \n",
       "1  ```json\\n{\\n  \"score\": 6,\\n  \"label\": \"moderat...          \n",
       "2  ```json\\n{\\n  \"score\": 6,\\n  \"label\": \"moderat...          \n",
       "3  ```json\\n{\\n  \"score\": 4,\\n  \"label\": \"minimal...          \n",
       "4  ```json\\n{\\n  \"score\": 5,\\n  \"label\": \"moderat...          \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_df = analyzer.conversation_df\n",
    "conv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83afdc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8.0\n",
       "1    9.0\n",
       "2    8.0\n",
       "3    9.0\n",
       "4    8.0\n",
       "5    9.0\n",
       "6    9.0\n",
       "7    9.0\n",
       "8    9.0\n",
       "9    9.0\n",
       "Name: conversation_text_content__helpfulness__score, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_df.conversation_text_content__helpfulness__score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44709aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The conversation provides clear and relevant i...\n",
       "1    The conversation provides detailed and relevan...\n",
       "2    The conversation provides a comprehensive over...\n",
       "3    The conversation provides relevant information...\n",
       "4    The conversation provides a thorough evaluatio...\n",
       "5    The conversation provides a detailed and relev...\n",
       "6    The conversation provides a clear and structur...\n",
       "7    The conversation provides clear, detailed, and...\n",
       "8    The conversation provides detailed information...\n",
       "9    The conversation provides detailed instruction...\n",
       "Name: conversation_text_content__helpfulness__reasoning, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_df.conversation_text_content__helpfulness__reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d56f05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer: length\n",
      "metric: token_count\n",
      "description: Token count for conversation_text_content\n",
      "value: 729\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.analyze.column_utils import (\n",
    "    filter_analyzer_columns,\n",
    "    get_analyzer_columns_by_analyzer,\n",
    "    parse_analyzer_column_name,\n",
    ")\n",
    "\n",
    "conv_columns = analyzer.conversation_df.columns\n",
    "row = analyzer.conversation_df.iloc[0]\n",
    "\n",
    "# Choose the analzyer to analyze\n",
    "analyzer_name = analyzer_names[0]\n",
    "\n",
    "\n",
    "filtered_cols = filter_analyzer_columns(conv_columns, analyzer_id=analyzer_name)\n",
    "if filtered_cols:\n",
    "    print(f\"Analyzer: {analyzer_name}\")\n",
    "    info = parse_analyzer_column_name(filtered_cols[0])\n",
    "    # print(\"\\nInput:\")\n",
    "    # print(f\"source_column: {info.source_column}\")\n",
    "    # print(f\"{row[info.source_column]}\\n\")\n",
    "\n",
    "    for col in filtered_cols:\n",
    "        info = parse_analyzer_column_name(col)\n",
    "        print(f\"metric: {info.metric_name}\")\n",
    "        # print(f\"type: {schema[col]['type']}\")\n",
    "        # print(f\"content_type: {schema[col]['content_type']}\")\n",
    "        print(f\"description: {schema[col]['description']}\")\n",
    "        print(f\"value: {row[col]}\")\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(f\"No columns found for analyzer: {analyzer_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abfccb",
   "metadata": {},
   "source": [
    "# Message level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e25ccde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_index</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>message_index</th>\n",
       "      <th>message_id</th>\n",
       "      <th>role</th>\n",
       "      <th>text_content</th>\n",
       "      <th>text_content__length__token_count</th>\n",
       "      <th>text_content__diversity__unique_words_ratio</th>\n",
       "      <th>text_content__format__has_markdown</th>\n",
       "      <th>text_content__format__has_json</th>\n",
       "      <th>...</th>\n",
       "      <th>text_content__response_quality__reasoning</th>\n",
       "      <th>text_content__response_quality__raw_response</th>\n",
       "      <th>text_content__excessive_politeness__score</th>\n",
       "      <th>text_content__excessive_politeness__label</th>\n",
       "      <th>text_content__excessive_politeness__reasoning</th>\n",
       "      <th>text_content__excessive_politeness__raw_response</th>\n",
       "      <th>text_content__reasoning_leakage__score</th>\n",
       "      <th>text_content__reasoning_leakage__label</th>\n",
       "      <th>text_content__reasoning_leakage__reasoning</th>\n",
       "      <th>text_content__reasoning_leakage__raw_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>msg_0</td>\n",
       "      <td>user</td>\n",
       "      <td>These instructions apply to section-based them...</td>\n",
       "      <td>124</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>msg_1</td>\n",
       "      <td>assistant</td>\n",
       "      <td>This feature only applies to Collection pages ...</td>\n",
       "      <td>22</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>The response is somewhat relevant but lacks co...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 6,\\n  \"label\": \"needs_i...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>minimal</td>\n",
       "      <td>The response is direct and factual, with no si...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 2,\\n  \"label\": \"minimal...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>minimal</td>\n",
       "      <td>The response is mostly direct and provides a c...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 2,\\n  \"label\": \"minimal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>msg_2</td>\n",
       "      <td>user</td>\n",
       "      <td>Can you guide me through the process of enabli...</td>\n",
       "      <td>23</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>msg_3</td>\n",
       "      <td>assistant</td>\n",
       "      <td>Sure, here are the steps to enable the seconda...</td>\n",
       "      <td>184</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>The response is helpful and relevant, providin...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 9,\\n  \"label\": \"good\",\\...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>minimal</td>\n",
       "      <td>The response is direct and provides clear step...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 2,\\n  \"label\": \"minimal...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>The response provides a detailed step-by-step ...</td>\n",
       "      <td>```json\\n{\\n  \"score\": 5,\\n  \"label\": \"moderat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>msg_4</td>\n",
       "      <td>user</td>\n",
       "      <td>Can you provide me with a link to the document...</td>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   conversation_index conversation_id  message_index message_id       role  \\\n",
       "0                   0               0              0      msg_0       user   \n",
       "1                   0               0              1      msg_1  assistant   \n",
       "2                   0               0              2      msg_2       user   \n",
       "3                   0               0              3      msg_3  assistant   \n",
       "4                   0               0              4      msg_4       user   \n",
       "\n",
       "                                        text_content  \\\n",
       "0  These instructions apply to section-based them...   \n",
       "1  This feature only applies to Collection pages ...   \n",
       "2  Can you guide me through the process of enabli...   \n",
       "3  Sure, here are the steps to enable the seconda...   \n",
       "4  Can you provide me with a link to the document...   \n",
       "\n",
       "   text_content__length__token_count  \\\n",
       "0                                124   \n",
       "1                                 22   \n",
       "2                                 23   \n",
       "3                                184   \n",
       "4                                 14   \n",
       "\n",
       "   text_content__diversity__unique_words_ratio  \\\n",
       "0                                     0.739130   \n",
       "1                                     0.950000   \n",
       "2                                     0.954545   \n",
       "3                                     0.640000   \n",
       "4                                     1.000000   \n",
       "\n",
       "   text_content__format__has_markdown  text_content__format__has_json  ...  \\\n",
       "0                               False                           False  ...   \n",
       "1                               False                           False  ...   \n",
       "2                               False                           False  ...   \n",
       "3                                True                           False  ...   \n",
       "4                               False                           False  ...   \n",
       "\n",
       "           text_content__response_quality__reasoning  \\\n",
       "0                                               None   \n",
       "1  The response is somewhat relevant but lacks co...   \n",
       "2                                               None   \n",
       "3  The response is helpful and relevant, providin...   \n",
       "4                                               None   \n",
       "\n",
       "        text_content__response_quality__raw_response  \\\n",
       "0                                               None   \n",
       "1  ```json\\n{\\n  \"score\": 6,\\n  \"label\": \"needs_i...   \n",
       "2                                               None   \n",
       "3  ```json\\n{\\n  \"score\": 9,\\n  \"label\": \"good\",\\...   \n",
       "4                                               None   \n",
       "\n",
       "  text_content__excessive_politeness__score  \\\n",
       "0                                       NaN   \n",
       "1                                       2.0   \n",
       "2                                       NaN   \n",
       "3                                       2.0   \n",
       "4                                       NaN   \n",
       "\n",
       "   text_content__excessive_politeness__label  \\\n",
       "0                                       None   \n",
       "1                                    minimal   \n",
       "2                                       None   \n",
       "3                                    minimal   \n",
       "4                                       None   \n",
       "\n",
       "       text_content__excessive_politeness__reasoning  \\\n",
       "0                                               None   \n",
       "1  The response is direct and factual, with no si...   \n",
       "2                                               None   \n",
       "3  The response is direct and provides clear step...   \n",
       "4                                               None   \n",
       "\n",
       "    text_content__excessive_politeness__raw_response  \\\n",
       "0                                               None   \n",
       "1  ```json\\n{\\n  \"score\": 2,\\n  \"label\": \"minimal...   \n",
       "2                                               None   \n",
       "3  ```json\\n{\\n  \"score\": 2,\\n  \"label\": \"minimal...   \n",
       "4                                               None   \n",
       "\n",
       "   text_content__reasoning_leakage__score  \\\n",
       "0                                     NaN   \n",
       "1                                     2.0   \n",
       "2                                     NaN   \n",
       "3                                     5.0   \n",
       "4                                     NaN   \n",
       "\n",
       "  text_content__reasoning_leakage__label  \\\n",
       "0                                   None   \n",
       "1                                minimal   \n",
       "2                                   None   \n",
       "3                               moderate   \n",
       "4                                   None   \n",
       "\n",
       "          text_content__reasoning_leakage__reasoning  \\\n",
       "0                                               None   \n",
       "1  The response is mostly direct and provides a c...   \n",
       "2                                               None   \n",
       "3  The response provides a detailed step-by-step ...   \n",
       "4                                               None   \n",
       "\n",
       "       text_content__reasoning_leakage__raw_response  \n",
       "0                                               None  \n",
       "1  ```json\\n{\\n  \"score\": 2,\\n  \"label\": \"minimal...  \n",
       "2                                               None  \n",
       "3  ```json\\n{\\n  \"score\": 5,\\n  \"label\": \"moderat...  \n",
       "4                                               None  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_df = analyzer.message_df\n",
    "msg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9af5d145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     6.0\n",
       "3     9.0\n",
       "5     8.0\n",
       "7     8.0\n",
       "9     9.0\n",
       "11    9.0\n",
       "13    9.0\n",
       "15    9.0\n",
       "17    8.0\n",
       "19    8.0\n",
       "21    8.0\n",
       "23    8.0\n",
       "25    8.0\n",
       "27    9.0\n",
       "29    7.0\n",
       "31    9.0\n",
       "33    9.0\n",
       "35    9.0\n",
       "37    9.0\n",
       "39    9.0\n",
       "41    8.0\n",
       "43    9.0\n",
       "45    6.0\n",
       "47    6.0\n",
       "49    7.0\n",
       "51    9.0\n",
       "53    9.0\n",
       "55    9.0\n",
       "57    9.0\n",
       "59    9.0\n",
       "61    8.0\n",
       "63    9.0\n",
       "65    9.0\n",
       "67    9.0\n",
       "69    9.0\n",
       "71    9.0\n",
       "73    9.0\n",
       "75    8.0\n",
       "77    9.0\n",
       "Name: text_content__response_quality__score, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_df[msg_df.role == \"assistant\"].text_content__response_quality__score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "049d61c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmsg_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmsg_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_content__instruction_quality__score\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/oumi/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/oumi/lib/python3.11/site-packages/pandas/core/indexing.py:1752\u001b[39m, in \u001b[36m_iLocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index by location index with a non-integer key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1751\u001b[39m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1752\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._ixs(key, axis=axis)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/oumi/lib/python3.11/site-packages/pandas/core/indexing.py:1685\u001b[39m, in \u001b[36m_iLocIndexer._validate_integer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1683\u001b[39m len_axis = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj._get_axis(axis))\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key >= len_axis \u001b[38;5;129;01mor\u001b[39;00m key < -len_axis:\n\u001b[32m-> \u001b[39m\u001b[32m1685\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msingle positional indexer is out-of-bounds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "msg_df[msg_df.role == \"system\"].text_content__instruction_quality__score.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700cff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_df[msg_df.role == \"system\"].text_content__instruction_quality__label.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2255ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The instruction is clear and specific, with a defined goal of classifying user queries into one of 77 banking intents. It uses action verbs and provides context through examples and intent descriptions. However, it could be improved by clarifying the range of IDs (0-76) instead of 0-77, as the highest ID listed is 76.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_df[msg_df.role == \"system\"].text_content__instruction_quality__reasoning.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb677ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer: instruction_quality\n",
      "\n",
      "Input:\n",
      "[system]: You are a banking intent classifier. Classify the user's query into one of  77 banking intents (output is a single integer ID).\n",
      "\n",
      "IDs:\n",
      "\n",
      "0: activate_my_card\n",
      "1: age_limit\n",
      "2: apple_pay_or_google_pay\n",
      "3: atm_support\n",
      "4: automatic_top_up\n",
      "5: balance_not_updated_after_bank_transfer\n",
      "6: balance_not_updated_after_cheque_or_cash_deposit\n",
      "7: beneficiary_not_allowed\n",
      "8: cancel_transfer\n",
      "9: card_about_to_expire\n",
      "10: card_acceptance\n",
      "11: card_arrival\n",
      "12: card_delivery_estimate\n",
      "13: card_linking\n",
      "14: card_not_working\n",
      "15: card_payment_fee_charged\n",
      "16: card_payment_not_recognised\n",
      "17: card_payment_wrong_exchange_rate\n",
      "18: card_swallowed\n",
      "19: cash_withdrawal_charge\n",
      "20: cash_withdrawal_not_recognised\n",
      "21: change_pin\n",
      "22: compromised_card\n",
      "23: contactless_not_working\n",
      "24: country_support\n",
      "25: declined_card_payment\n",
      "26: declined_cash_withdrawal\n",
      "27: declined_transfer\n",
      "28: direct_debit_payment_not_recognised\n",
      "29: disposable_card_limits\n",
      "30: edit_personal_details\n",
      "31: exchange_charge\n",
      "32: exchange_rate\n",
      "33: exchange_via_app\n",
      "34: extra_charge_on_statement\n",
      "35: failed_transfer\n",
      "36: fiat_currency_support\n",
      "37: get_disposable_virtual_card\n",
      "38: get_physical_card\n",
      "39: getting_spare_card\n",
      "40: getting_virtual_card\n",
      "41: lost_or_stolen_card\n",
      "42: lost_or_stolen_phone\n",
      "43: order_physical_card\n",
      "44: passcode_forgotten\n",
      "45: pending_card_payment\n",
      "46: pending_cash_withdrawal\n",
      "47: pending_top_up\n",
      "48: pending_transfer\n",
      "49: pin_blocked\n",
      "50: receiving_money\n",
      "51: Refund_not_showing_up\n",
      "52: request_refund\n",
      "53: reverted_card_payment?\n",
      "54: supported_cards_and_currencies\n",
      "55: terminate_account\n",
      "56: top_up_by_bank_transfer_charge\n",
      "57: top_up_by_card_charge\n",
      "58: top_up_by_cash_or_cheque\n",
      "59: top_up_failed\n",
      "60: top_up_limits\n",
      "61: top_up_reverted\n",
      "62: topping_up_by_card\n",
      "63: transaction_charged_twice\n",
      "64: transfer_fee_charged\n",
      "65: transfer_into_account\n",
      "66: transfer_not_received_by_recipient\n",
      "67: transfer_timing\n",
      "68: unable_to_verify_identity\n",
      "69: verify_my_identity\n",
      "70: verify_source_of_funds\n",
      "71: verify_top_up\n",
      "72: virtual_card_not_working\n",
      "73: visa_or_mastercard\n",
      "74: why_verify_identity\n",
      "75: wrong_amount_of_cash_received\n",
      "76: wrong_exchange_rate_for_cash_withdrawal\n",
      "\n",
      "CRITICAL INSTRUCTIONS:\n",
      "1. Choose exactly one integer ID (0-76).\n",
      "2. Reply with ONLY that number. No words, no reasoning, no punctuation.\n",
      "Examples: 0, 1, 42\n",
      "\n",
      "EXAMPLES TO HELP DISTINGUISH SIMILAR INTENTS:\n",
      "EXAMPLES TO HELP DISTINGUISH SIMILAR INTENTS:\n",
      "\n",
      "1. card_arrival (ID 11) vs card_delivery_estimate (ID 12):\n",
      "   card_arrival = asking about YOUR specific card that hasn't arrived yet (tracking, status)\n",
      "   card_delivery_estimate = asking about general delivery timeframes/how long it takes\n",
      "   - Query: \"Could you send me and up date on the arrival of my card?\" ‚Üí 11\n",
      "   - Query: \"My card was supposed to arrive, but hasn't?\" ‚Üí 11\n",
      "   - Query: \"I have not received my card and it's been a week, what do I do?\" ‚Üí 11\n",
      "   - Query: \"can you express my card to me?\" ‚Üí 12\n",
      "   - Query: \"Can I choose the day it's delivered?\" ‚Üí 12\n",
      "   - Query: \"i need to add express delivery if that's an option\" ‚Üí 12\n",
      "\n",
      "2. card_linking (ID 13) vs activate_my_card (ID 0) vs lost_or_stolen_card (ID 41):\n",
      "   card_linking = reconnecting a card you found/retrieved\n",
      "   activate_my_card = activating a NEW card for first time\n",
      "   lost_or_stolen_card = reporting a card as lost/stolen\n",
      "   - Query: \"Okay, I found my card, can I put it back in the app?\" ‚Üí 13\n",
      "   - Query: \"I want to reactivate my card, I thought I had lost it but I found it.\" ‚Üí 13\n",
      "   - Query: \"What do I need to do for the card activation?\" ‚Üí 0\n",
      "   - Query: \"Tell me what I need to do to activate my card.\" ‚Üí 0\n",
      "   - Query: \"I left my card at a restaurant and now its missing.\" ‚Üí 41\n",
      "   - Query: \"Who can I speak with regarding a lost card?\" ‚Üí 41\n",
      "\n",
      "3. pin_blocked (ID 49) vs change_pin (ID 21):\n",
      "   pin_blocked = PIN is locked/blocked, need to unlock\n",
      "   change_pin = want to change PIN to a new one\n",
      "   - Query: \"How many times can I enter a wrong PIN before it is blocked?\" ‚Üí 49\n",
      "   - Query: \"I entered in the wrong PIN.  Please help me unlock my account.\" ‚Üí 49\n",
      "   - Query: \"Where do I need to go to change my PIN?\" ‚Üí 21\n",
      "   - Query: \"I really need to know how to change my pin.\" ‚Üí 21\n",
      "\n",
      "4. pending_cash_withdrawal (ID 46) vs declined_cash_withdrawal (ID 26) vs cash_withdrawal_not_recognised (ID 20):\n",
      "   pending_cash_withdrawal = withdrawal is processing/pending\n",
      "   declined_cash_withdrawal = withdrawal was rejected/declined\n",
      "   cash_withdrawal_not_recognised = withdrawal not showing in account\n",
      "   - Query: \"I tried to get cash at some ATM in the city centre earlier but the machine declined my card. I've seen it still shows up as pending in my account. Please cancel it immediately as I definitely have not received that money!\" ‚Üí 46\n",
      "   - Query: \"I attempted to get cash in the ATM but it was not authorized\" ‚Üí 26\n",
      "   - Query: \"My app says I withdraw money from my account from an ATM.\" ‚Üí 20\n",
      "\n",
      "5. verify_my_identity (ID 69) vs why_verify_identity (ID 74) vs unable_to_verify_identity (ID 68):\n",
      "   verify_my_identity = want to verify/complete verification\n",
      "   why_verify_identity = asking why verification is needed\n",
      "   unable_to_verify_identity = having trouble completing verification\n",
      "   - Query: \"If I'm getting my identity verified, what all do I need?\" ‚Üí 69\n",
      "   - Query: \"I do not feel comfortable verifying my identity.\" ‚Üí 74\n",
      "   - Query: \"My identity wasn't verified\" ‚Üí 68\n",
      "\n",
      "6. card_payment_wrong_exchange_rate (ID 17) vs wrong_exchange_rate_for_cash_withdrawal (ID 76) vs exchange_rate (ID 32):\n",
      "   card_payment_wrong_exchange_rate = wrong rate used for CARD payment\n",
      "   wrong_exchange_rate_for_cash_withdrawal = wrong rate used for CASH withdrawal\n",
      "   exchange_rate = asking about current/general exchange rates\n",
      "   - Query: \"I seem to have been charged to much for my holiday purchases, the exchange rate is wrong.\" ‚Üí 17\n",
      "   - Query: \"The exchange rate for foreign ATM currency is wrong.\" ‚Üí 76\n",
      "   - Query: \"How did you guys get your exchange rate?\" ‚Üí 32\n",
      "\n",
      "7. extra_charge_on_statement (ID 34) vs card_payment_fee_charged (ID 15):\n",
      "   extra_charge_on_statement = unexpected charge on statement\n",
      "   card_payment_fee_charged = fee charged for card payment\n",
      "   - Query: \"I was overcharged one extra pound!\" ‚Üí 34\n",
      "   - Query: \"There was an extra fee when I paid with my card, why was i charged this extra fee?\" ‚Üí 15\n",
      "\n",
      "8. getting_virtual_card (ID 40):\n",
      "   - Query: \"Where do I have access to a virtual card?\" ‚Üí 40\n",
      "   - Query: \"How can I receive a virtual card?\" ‚Üí 40\n",
      "\n",
      "\n",
      "Remember: Respond with ONLY the numeric ID, nothing else.\n",
      "\n",
      "You are a banking intent classifier. Classify the user's query into one of  77 banking intents (output is a single integer ID).\n",
      "\n",
      "IDs:\n",
      "\n",
      "0: activate_my_card\n",
      "1: age_limit\n",
      "2: apple_pay_or_google_pay\n",
      "3: atm_support\n",
      "4: automatic_top_up\n",
      "5: balance_not_updated_after_bank_transfer\n",
      "6: balance_not_updated_after_cheque_or_cash_deposit\n",
      "7: beneficiary_not_allowed\n",
      "8: cancel_transfer\n",
      "9: card_about_to_expire\n",
      "10: card_acceptance\n",
      "11: card_arrival\n",
      "12: card_delivery_estimate\n",
      "13: card_linking\n",
      "14: card_not_working\n",
      "15: card_payment_fee_charged\n",
      "16: card_payment_not_recognised\n",
      "17: card_payment_wrong_exchange_rate\n",
      "18: card_swallowed\n",
      "19: cash_withdrawal_charge\n",
      "20: cash_withdrawal_not_recognised\n",
      "21: change_pin\n",
      "22: compromised_card\n",
      "23: contactless_not_working\n",
      "24: country_support\n",
      "25: declined_card_payment\n",
      "26: declined_cash_withdrawal\n",
      "27: declined_transfer\n",
      "28: direct_debit_payment_not_recognised\n",
      "29: disposable_card_limits\n",
      "30: edit_personal_details\n",
      "31: exchange_charge\n",
      "32: exchange_rate\n",
      "33: exchange_via_app\n",
      "34: extra_charge_on_statement\n",
      "35: failed_transfer\n",
      "36: fiat_currency_support\n",
      "37: get_disposable_virtual_card\n",
      "38: get_physical_card\n",
      "39: getting_spare_card\n",
      "40: getting_virtual_card\n",
      "41: lost_or_stolen_card\n",
      "42: lost_or_stolen_phone\n",
      "43: order_physical_card\n",
      "44: passcode_forgotten\n",
      "45: pending_card_payment\n",
      "46: pending_cash_withdrawal\n",
      "47: pending_top_up\n",
      "48: pending_transfer\n",
      "49: pin_blocked\n",
      "50: receiving_money\n",
      "51: Refund_not_showing_up\n",
      "52: request_refund\n",
      "53: reverted_card_payment?\n",
      "54: supported_cards_and_currencies\n",
      "55: terminate_account\n",
      "56: top_up_by_bank_transfer_charge\n",
      "57: top_up_by_card_charge\n",
      "58: top_up_by_cash_or_cheque\n",
      "59: top_up_failed\n",
      "60: top_up_limits\n",
      "61: top_up_reverted\n",
      "62: topping_up_by_card\n",
      "63: transaction_charged_twice\n",
      "64: transfer_fee_charged\n",
      "65: transfer_into_account\n",
      "66: transfer_not_received_by_recipient\n",
      "67: transfer_timing\n",
      "68: unable_to_verify_identity\n",
      "69: verify_my_identity\n",
      "70: verify_source_of_funds\n",
      "71: verify_top_up\n",
      "72: virtual_card_not_working\n",
      "73: visa_or_mastercard\n",
      "74: why_verify_identity\n",
      "75: wrong_amount_of_cash_received\n",
      "76: wrong_exchange_rate_for_cash_withdrawal\n",
      "\n",
      "CRITICAL INSTRUCTIONS:\n",
      "1. Choose exactly one integer ID (0-76).\n",
      "2. Reply with ONLY that number. No words, no reasoning, no punctuation.\n",
      "Examples: 0, 1, 42\n",
      "\n",
      "EXAMPLES TO HELP DISTINGUISH SIMILAR INTENTS:\n",
      "EXAMPLES TO HELP DISTINGUISH SIMILAR INTENTS:\n",
      "\n",
      "1. card_arrival (ID 11) vs card_delivery_estimate (ID 12):\n",
      "   card_arrival = asking about YOUR specific card that hasn't arrived yet (tracking, status)\n",
      "   card_delivery_estimate = asking about general delivery timeframes/how long it takes\n",
      "   - Query: \"Could you send me and up date on the arrival of my card?\" ‚Üí 11\n",
      "   - Query: \"My card was supposed to arrive, but hasn't?\" ‚Üí 11\n",
      "   - Query: \"I have not received my card and it's been a week, what do I do?\" ‚Üí 11\n",
      "   - Query: \"can you express my card to me?\" ‚Üí 12\n",
      "   - Query: \"Can I choose the day it's delivered?\" ‚Üí 12\n",
      "   - Query: \"i need to add express delivery if that's an option\" ‚Üí 12\n",
      "\n",
      "2. card_linking (ID 13) vs activate_my_card (ID 0) vs lost_or_stolen_card (ID 41):\n",
      "   card_linking = reconnecting a card you found/retrieved\n",
      "   activate_my_card = activating a NEW card for first time\n",
      "   lost_or_stolen_card = reporting a card as lost/stolen\n",
      "   - Query: \"Okay, I found my card, can I put it back in the app?\" ‚Üí 13\n",
      "   - Query: \"I want to reactivate my card, I thought I had lost it but I found it.\" ‚Üí 13\n",
      "   - Query: \"What do I need to do for the card activation?\" ‚Üí 0\n",
      "   - Query: \"Tell me what I need to do to activate my card.\" ‚Üí 0\n",
      "   - Query: \"I left my card at a restaurant and now its missing.\" ‚Üí 41\n",
      "   - Query: \"Who can I speak with regarding a lost card?\" ‚Üí 41\n",
      "\n",
      "3. pin_blocked (ID 49) vs change_pin (ID 21):\n",
      "   pin_blocked = PIN is locked/blocked, need to unlock\n",
      "   change_pin = want to change PIN to a new one\n",
      "   - Query: \"How many times can I enter a wrong PIN before it is blocked?\" ‚Üí 49\n",
      "   - Query: \"I entered in the wrong PIN.  Please help me unlock my account.\" ‚Üí 49\n",
      "   - Query: \"Where do I need to go to change my PIN?\" ‚Üí 21\n",
      "   - Query: \"I really need to know how to change my pin.\" ‚Üí 21\n",
      "\n",
      "4. pending_cash_withdrawal (ID 46) vs declined_cash_withdrawal (ID 26) vs cash_withdrawal_not_recognised (ID 20):\n",
      "   pending_cash_withdrawal = withdrawal is processing/pending\n",
      "   declined_cash_withdrawal = withdrawal was rejected/declined\n",
      "   cash_withdrawal_not_recognised = withdrawal not showing in account\n",
      "   - Query: \"I tried to get cash at some ATM in the city centre earlier but the machine declined my card. I've seen it still shows up as pending in my account. Please cancel it immediately as I definitely have not received that money!\" ‚Üí 46\n",
      "   - Query: \"I attempted to get cash in the ATM but it was not authorized\" ‚Üí 26\n",
      "   - Query: \"My app says I withdraw money from my account from an ATM.\" ‚Üí 20\n",
      "\n",
      "5. verify_my_identity (ID 69) vs why_verify_identity (ID 74) vs unable_to_verify_identity (ID 68):\n",
      "   verify_my_identity = want to verify/complete verification\n",
      "   why_verify_identity = asking why verification is needed\n",
      "   unable_to_verify_identity = having trouble completing verification\n",
      "   - Query: \"If I'm getting my identity verified, what all do I need?\" ‚Üí 69\n",
      "   - Query: \"I do not feel comfortable verifying my identity.\" ‚Üí 74\n",
      "   - Query: \"My identity wasn't verified\" ‚Üí 68\n",
      "\n",
      "6. card_payment_wrong_exchange_rate (ID 17) vs wrong_exchange_rate_for_cash_withdrawal (ID 76) vs exchange_rate (ID 32):\n",
      "   card_payment_wrong_exchange_rate = wrong rate used for CARD payment\n",
      "   wrong_exchange_rate_for_cash_withdrawal = wrong rate used for CASH withdrawal\n",
      "   exchange_rate = asking about current/general exchange rates\n",
      "   - Query: \"I seem to have been charged to much for my holiday purchases, the exchange rate is wrong.\" ‚Üí 17\n",
      "   - Query: \"The exchange rate for foreign ATM currency is wrong.\" ‚Üí 76\n",
      "   - Query: \"How did you guys get your exchange rate?\" ‚Üí 32\n",
      "\n",
      "7. extra_charge_on_statement (ID 34) vs card_payment_fee_charged (ID 15):\n",
      "   extra_charge_on_statement = unexpected charge on statement\n",
      "   card_payment_fee_charged = fee charged for card payment\n",
      "   - Query: \"I was overcharged one extra pound!\" ‚Üí 34\n",
      "   - Query: \"There was an extra fee when I paid with my card, why was i charged this extra fee?\" ‚Üí 15\n",
      "\n",
      "8. getting_virtual_card (ID 40):\n",
      "   - Query: \"Where do I have access to a virtual card?\" ‚Üí 40\n",
      "   - Query: \"How can I receive a virtual card?\" ‚Üí 40\n",
      "\n",
      "\n",
      "Remember: Respond with ONLY the numeric ID, nothing else.\n",
      "\n",
      "metric: score\n",
      "description: LLM judge score (0-10, higher = better quality)\n",
      "value: 8.0\n",
      "\n",
      "\n",
      "metric: label\n",
      "description: LLM judge label/category for the sample\n",
      "value: good\n",
      "\n",
      "\n",
      "metric: reasoning\n",
      "description: LLM judge reasoning/explanation\n",
      "value: The instruction is clear and specific, with a defined goal of classifying user queries into one of 77 banking intents. It uses action verbs and provides context through examples and intent descriptions. However, it could be improved by clarifying the range of IDs (0-76) instead of 0-77, as the highest ID listed is 76.\n",
      "\n",
      "\n",
      "metric: raw_response\n",
      "description: Raw LLM response before parsing\n",
      "value: ```json\n",
      "{\n",
      "  \"score\": 8,\n",
      "  \"label\": \"good\",\n",
      "  \"reasoning\": \"The instruction is clear and specific, with a defined goal of classifying user queries into one of 77 banking intents. It uses action verbs and provides context through examples and intent descriptions. However, it could be improved by clarifying the range of IDs (0-76) instead of 0-77, as the highest ID listed is 76.\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.analyze.column_utils import (\n",
    "    filter_analyzer_columns,\n",
    "    get_analyzer_columns_by_analyzer,\n",
    "    parse_analyzer_column_name,\n",
    ")\n",
    "\n",
    "msg_columns = analyzer.message_df.columns\n",
    "row = analyzer.message_df.iloc[0]\n",
    "\n",
    "# Choose the analzyer to analyze\n",
    "analyzer_name = analyzer_names[3]\n",
    "\n",
    "\n",
    "filtered_cols = filter_analyzer_columns(msg_columns, analyzer_id=analyzer_name)\n",
    "if filtered_cols:\n",
    "    print(f\"Analyzer: {analyzer_name}\")\n",
    "    info = parse_analyzer_column_name(filtered_cols[0])\n",
    "\n",
    "    print(\"\\nInput:\")\n",
    "    print(f\"[{row['role']}]: {row[info.source_column]}\\n\")\n",
    "    # print(f\"source_column: {info.source_column}\")\n",
    "    print(f\"{row[info.source_column]}\\n\")\n",
    "\n",
    "    for col in filtered_cols:\n",
    "        info = parse_analyzer_column_name(col)\n",
    "        print(f\"metric: {info.metric_name}\")\n",
    "        # print(f\"type: {schema[col]['type']}\")\n",
    "        # print(f\"content_type: {schema[col]['content_type']}\")\n",
    "        print(f\"description: {schema[col]['description']}\")\n",
    "        print(f\"value: {row[col]}\")\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(f\"No columns found for analyzer: {analyzer_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782aee00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab0666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
