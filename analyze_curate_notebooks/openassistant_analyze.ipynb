{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS has been disabled - forcing CPU-only mode\n",
      "PyTorch device: cpu\n",
      "PyTorch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# IMPORTANT: Set these BEFORE importing torch or any ML libraries\n",
    "# Disable all GPU/MPS backends to prevent crashes with IFD analyzer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Disable CUDA\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Disable MPS memory allocation\n",
    "os.environ[\"DISABLE_MPS_COMPAT\"] = \"1\"  # Additional MPS disable flag\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"  # Disable HuggingFace telemetry\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"  # Allow model downloads\n",
    "\n",
    "# Force CPU usage in PyTorch to avoid MPS crashes\n",
    "import torch\n",
    "\n",
    "# Forcefully disable MPS before anything else\n",
    "torch.set_default_device(\"cpu\")\n",
    "if hasattr(torch.backends, \"mps\"):\n",
    "    # Monkey-patch to prevent MPS usage\n",
    "    original_is_available = torch.backends.mps.is_available\n",
    "    torch.backends.mps.is_available = lambda: False\n",
    "    print(\"MPS has been disabled - forcing CPU-only mode\")\n",
    "else:\n",
    "    print(\"Using CPU for all computations\")\n",
    "\n",
    "print(f\"PyTorch device: {torch.get_default_device()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading OpenAssistant OASST1 dataset...\n",
      "‚úÖ Downloaded dataset (size: 84437)\n",
      "üìù Converting to JSONL format...\n",
      "üìä Inspecting dataset structure...\n",
      "   Example keys: ['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted']\n",
      "üì¶ Grouping messages by conversation tree...\n",
      "   Found 9846 conversation trees\n",
      "üî® Building conversations from message trees...\n",
      "‚úÖ Saved 9846 conversations to /Users/ryanarman/data/openassistant/openassistant_oasst1.jsonl\n",
      "üìÅ Dataset path: /Users/ryanarman/data/openassistant/openassistant_oasst1.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Download and convert OpenAssistant dataset to JSONL format\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = Path(\"/Users/ryanarman/data/openassistant\")\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATASET_PATH = DATA_DIR / \"openassistant_oasst1.jsonl\"\n",
    "\n",
    "# Download OpenAssistant OASST1 if not already downloaded\n",
    "# Set FORCE_REGENERATE=True to regenerate the file even if it exists\n",
    "FORCE_REGENERATE = False\n",
    "\n",
    "if not DATASET_PATH.exists() or FORCE_REGENERATE:\n",
    "    print(\"üì• Downloading OpenAssistant OASST1 dataset...\")\n",
    "    dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"train\")\n",
    "\n",
    "    # Get dataset length safely\n",
    "    from datasets import Dataset\n",
    "\n",
    "    if isinstance(dataset, Dataset):\n",
    "        dataset_len = len(dataset)\n",
    "    else:\n",
    "        dataset_len = \"unknown\"\n",
    "\n",
    "    print(f\"‚úÖ Downloaded dataset (size: {dataset_len})\")\n",
    "    print(\"üìù Converting to JSONL format...\")\n",
    "\n",
    "    def build_conversation_from_tree(messages_in_tree):\n",
    "        \"\"\"Build a single conversation from a message tree.\n",
    "\n",
    "        OASST1 has messages with parent_id relationships. We need to:\n",
    "        1. Find root message (no parent_id)\n",
    "        2. Build conversation thread by following parent_id chains\n",
    "        \"\"\"\n",
    "        # Create a map of message_id -> message\n",
    "        msg_map = {}\n",
    "        for msg in messages_in_tree:\n",
    "            msg_id = msg.get(\"message_id\")\n",
    "            if msg_id:\n",
    "                msg_map[msg_id] = msg\n",
    "\n",
    "        # Find root message (no parent_id or parent_id not in this tree)\n",
    "        root = None\n",
    "        for msg in messages_in_tree:\n",
    "            parent_id = msg.get(\"parent_id\")\n",
    "            if not parent_id or parent_id not in msg_map:\n",
    "                root = msg\n",
    "                break\n",
    "\n",
    "        if not root:\n",
    "            return None\n",
    "\n",
    "        messages = []\n",
    "\n",
    "        def add_message(msg):\n",
    "            \"\"\"Recursively add message and its children to the conversation.\"\"\"\n",
    "            role = \"user\" if msg.get(\"role\") == \"prompter\" else \"assistant\"\n",
    "            content = msg.get(\"text\", \"\")\n",
    "            if content:  # Only add non-empty messages\n",
    "                messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "            # Find children (messages with this message as parent)\n",
    "            # Sort by created_date if available to maintain order\n",
    "            children = []\n",
    "            for child_msg in messages_in_tree:\n",
    "                if child_msg.get(\"parent_id\") == msg.get(\"message_id\"):\n",
    "                    children.append(child_msg)\n",
    "\n",
    "            # Sort children by created_date if available\n",
    "            if children and \"created_date\" in children[0]:\n",
    "                children.sort(key=lambda x: x.get(\"created_date\", \"\"))\n",
    "\n",
    "            for child in children:\n",
    "                add_message(child)\n",
    "\n",
    "        add_message(root)\n",
    "\n",
    "        return messages if messages else None\n",
    "\n",
    "    # Convert and save\n",
    "    # OASST1 structure: each example is a single message with message_tree_id\n",
    "    # We need to group by message_tree_id first, then build conversations\n",
    "    print(\"üìä Inspecting dataset structure...\")\n",
    "    first_example = next(iter(dataset))\n",
    "    print(f\"   Example keys: {list(first_example.keys())[:10]}\")  # Show first 10 keys\n",
    "\n",
    "    # Group messages by message_tree_id\n",
    "    print(\"üì¶ Grouping messages by conversation tree...\")\n",
    "    trees = {}\n",
    "    for example in dataset:\n",
    "        tree_id = example.get(\"message_tree_id\")\n",
    "        if tree_id:\n",
    "            if tree_id not in trees:\n",
    "                trees[tree_id] = []\n",
    "            trees[tree_id].append(example)\n",
    "\n",
    "    print(f\"   Found {len(trees)} conversation trees\")\n",
    "\n",
    "    # Build conversations from trees\n",
    "    print(\"üî® Building conversations from message trees...\")\n",
    "    count = 0\n",
    "    with open(DATASET_PATH, \"w\") as f:\n",
    "        for tree_id, messages_in_tree in trees.items():\n",
    "            conversation_messages = build_conversation_from_tree(messages_in_tree)\n",
    "            if conversation_messages and len(conversation_messages) > 0:\n",
    "                f.write(json.dumps({\"messages\": conversation_messages}) + \"\\n\")\n",
    "                count += 1\n",
    "\n",
    "    print(f\"‚úÖ Saved {count} conversations to {DATASET_PATH}\")\n",
    "else:\n",
    "    if FORCE_REGENERATE:\n",
    "        print(f\"‚úÖ Regenerated dataset at {DATASET_PATH}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Dataset already exists at {DATASET_PATH}\")\n",
    "        print(\n",
    "            \"   Set FORCE_REGENERATE=True to regenerate with proper conversation grouping\"\n",
    "        )\n",
    "\n",
    "print(f\"üìÅ Dataset path: {DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Config loaded with 20 analyzers:\n",
      "  - length (type: length)\n",
      "  - token_stats (type: token_stats)\n",
      "  - diversity (type: diversity)\n",
      "  - format (type: format)\n",
      "  - quality (type: quality)\n",
      "  - content_pattern (type: content_pattern)\n",
      "  - fasttext (type: fasttext)\n",
      "  - embedding (type: embedding)\n",
      "  - question_diversity (type: question_diversity)\n",
      "  - repr_diversity (type: repr_diversity)\n",
      "  - conversation_structure (type: conversation_structure)\n",
      "  - response_completeness (type: response_completeness)\n",
      "  - training_quality (type: training_quality)\n",
      "  - task_category (type: task_category)\n",
      "  - safety (type: safety)\n",
      "  - difficulty (type: difficulty)\n",
      "  - input_quality (type: input_quality)\n",
      "  - instruct_reward (type: instruct_reward)\n",
      "  - cost (type: cost)\n",
      "  - helpfulness (type: llm_judge)\n",
      "üìÅ Output will be saved to: /Users/ryanarman/code/oumi/analysis_output/openassistant\n",
      "üìÇ Dataset path: /Users/ryanarman/code/oumi/data/openassistant/openassistant_oasst1.jsonl\n",
      "‚úÖ Config validated successfully!\n",
      "[2026-01-12 16:04:40,605][oumi][rank0][pid:1937][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'custom'\n",
      "[2026-01-12 16:04:40,605][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:227] Loaded text dataset from: /Users/ryanarman/code/oumi/data/openassistant/openassistant_oasst1.jsonl\n",
      "[2026-01-12 16:04:40,606][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:153] Loaded dataset from config: None\n",
      "[2026-01-12 16:04:40,731][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: length\n",
      "[2026-01-12 16:04:40,732][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: token_stats\n",
      "[2026-01-12 16:04:40,732][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: diversity\n",
      "[2026-01-12 16:04:40,732][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: format\n",
      "[2026-01-12 16:04:40,734][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: quality\n",
      "[2026-01-12 16:04:40,734][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: content_pattern\n",
      "[2026-01-12 16:04:40,735][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: fasttext\n",
      "[2026-01-12 16:04:40,755][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: embedding\n",
      "[2026-01-12 16:04:40,756][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: question_diversity\n",
      "[2026-01-12 16:04:40,756][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: repr_diversity\n",
      "[2026-01-12 16:04:40,756][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: conversation_structure\n",
      "[2026-01-12 16:04:40,757][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: response_completeness\n",
      "[2026-01-12 16:04:40,757][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: training_quality\n",
      "[2026-01-12 16:04:40,757][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: task_category\n",
      "[2026-01-12 16:04:40,758][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: safety\n",
      "[2026-01-12 16:04:40,758][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: difficulty\n",
      "[2026-01-12 16:04:40,758][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: input_quality\n",
      "[2026-01-12 16:04:40,759][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: instruct_reward\n",
      "[2026-01-12 16:04:40,760][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: cost\n",
      "[2026-01-12 16:04:40,760][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: helpfulness (type: llm_judge)\n"
     ]
    }
   ],
   "source": [
    "# OUTPUT_PATH = \"/Users/ryanarman/code/oumi/analysis_output/openassistant\"\n",
    "OUTPUT_PATH = \"/Users/ryanarman/code/oumi/analysis_output/openassistant\"\n",
    "\n",
    "import os\n",
    "\n",
    "from oumi.core.analyze.dataset_analyzer import DatasetAnalyzer\n",
    "from oumi.core.configs import AnalyzeConfig\n",
    "\n",
    "# Load config from YAML file\n",
    "config = AnalyzeConfig.from_yaml(\n",
    "    \"/Users/ryanarman/code/oumi/configs/examples/analyze/analyze_openassistant.yaml\"\n",
    ")\n",
    "\n",
    "# Override settings for this run\n",
    "dataset_path = str(DATASET_PATH)\n",
    "config.dataset_path = dataset_path\n",
    "config.dataset_name = None  # Clear dataset_name so it uses dataset_path instead\n",
    "config.sample_count = 1000\n",
    "config.chat_template = \"default\"  # Simple template without special variables\n",
    "\n",
    "# Set absolute output path (makes it easier to find the results!)\n",
    "config.output_path = OUTPUT_PATH\n",
    "\n",
    "print(f\"‚úÖ Config loaded with {len(config.analyzers)} analyzers:\")\n",
    "for analyzer in config.analyzers:\n",
    "    instance_id = analyzer.instance_id or analyzer.id\n",
    "    print(f\"  - {instance_id} (type: {analyzer.id})\")\n",
    "\n",
    "print(f\"üìÅ Output will be saved to: {config.output_path}\")\n",
    "print(f\"üìÇ Dataset path: {config.dataset_path}\")\n",
    "\n",
    "# Validate the configuration\n",
    "config.finalize_and_validate()\n",
    "print(\"‚úÖ Config validated successfully!\")\n",
    "\n",
    "analyzer = DatasetAnalyzer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:04:40,778][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:362] Starting analysis of dataset: None\n",
      "[2026-01-12 16:04:40,779][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:363] Using 20 sample analyzers: ['length', 'token_stats', 'diversity', 'format', 'quality', 'content_pattern', 'fasttext', 'embedding', 'question_diversity', 'repr_diversity', 'conversation_structure', 'response_completeness', 'training_quality', 'task_category', 'safety', 'difficulty', 'input_quality', 'instruct_reward', 'cost', 'helpfulness']\n",
      "[2026-01-12 16:04:40,779][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:386] Analyzing 1000 of 9846 conversations\n",
      "[2026-01-12 16:04:40,780][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:438] Converting conversation dataset with 9846 items\n",
      "[2026-01-12 16:04:40,780][oumi][rank0][pid:1937][MainThread][INFO]][dataset_analyzer.py:445] Limiting analysis to first 1000 items (dataset has 9846 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting Unknown Dataset to DataFrames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01<00:00, 975.34item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:04:43,118][oumi][rank0][pid:1937][MainThread][INFO]][fasttext_analyzer.py:220] Initialized fast-langdetect for language detection\n",
      "[2026-01-12 16:04:43,119][oumi][rank0][pid:1937][MainThread][INFO]][fasttext_analyzer.py:458] Analyzing language for column: conversation_text_content\n",
      "[2026-01-12 16:04:43,438][oumi][rank0][pid:1937][MainThread][INFO]][embedding_analyzer.py:518] Computing embeddings for 1000 samples...\n",
      "[2026-01-12 16:04:43,438][oumi][rank0][pid:1937][MainThread][INFO]][embedding_analyzer.py:196] Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:06<00:00, 152.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:04:50,937][oumi][rank0][pid:1937][MainThread][INFO]][embedding_analyzer.py:537] Detecting semantic duplicates...\n",
      "[2026-01-12 16:04:50,944][oumi][rank0][pid:1937][MainThread][INFO]][embedding_analyzer.py:624] Detecting fuzzy duplicates using MinHash LSH...\n",
      "[2026-01-12 16:04:50,948][oumi][rank0][pid:1937][MainThread][INFO]][embedding_analyzer.py:350] Creating MinHash signatures for 1000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating MinHash signatures: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:04<00:00, 249.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:04:55,016][oumi][rank0][pid:1937][MainThread][INFO]][embedding_analyzer.py:369] Finding fuzzy duplicates using LSH...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding duplicates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 152116.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:04:55,027][oumi][rank0][pid:1937][MainThread][INFO]][question_diversity_analyzer.py:464] Computing embeddings for 1000 user questions...\n",
      "[2026-01-12 16:04:55,027][oumi][rank0][pid:1937][MainThread][INFO]][question_diversity_analyzer.py:174] Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:07<00:00, 125.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:05:03,940][oumi][rank0][pid:1937][MainThread][INFO]][question_diversity_analyzer.py:469] Clustering 1000 questions using dbscan...\n",
      "[2026-01-12 16:05:04,521][oumi][rank0][pid:1937][MainThread][INFO]][question_diversity_analyzer.py:482] Found 0 clusters, 1000 unique/diverse questions (not similar to others)\n",
      "[2026-01-12 16:05:04,529][oumi][rank0][pid:1937][MainThread][INFO]][repr_diversity_analyzer.py:363] Computing diversity scores for 1000 samples in column 'conversation_text_content'...\n",
      "[2026-01-12 16:05:04,530][oumi][rank0][pid:1937][MainThread][INFO]][repr_diversity_analyzer.py:165] Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:07<00:00, 139.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:05:12,604][oumi][rank0][pid:1937][MainThread][INFO]][repr_diversity_analyzer.py:230] Computing nearest neighbor distances for 1000 samples (k=5)...\n",
      "[2026-01-12 16:05:12,627][oumi][rank0][pid:1937][MainThread][INFO]][repr_diversity_analyzer.py:556] Column 'conversation_text_content': 182/1000 samples (18.2%) are redundant\n",
      "[2026-01-12 16:05:19,097][oumi][rank0][pid:1937][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 285/1000 [00:08<00:13, 53.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:05:27,743][oumi][rank0][pid:1937][ThreadPoolExecutor-7_0][WARNING]][adaptive_concurrency_controller.py:237] Entering warmup state, but concurrency is already at maximum value. Consider raising the max concurrency.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:19<00:00, 50.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:05:40,772][oumi][rank0][pid:1937][MainThread][INFO]][fasttext_analyzer.py:458] Analyzing language for column: text_content\n",
      "[2026-01-12 16:05:41,357][oumi][rank0][pid:1937][MainThread][INFO]][embedding_analyzer.py:518] Computing embeddings for 7211 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7211/7211 [00:39<00:00, 182.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:06:20,933][oumi][rank0][pid:1937][MainThread][INFO]][embedding_analyzer.py:537] Detecting semantic duplicates...\n",
      "[2026-01-12 16:06:21,127][oumi][rank0][pid:1937][MainThread][INFO]][embedding_analyzer.py:624] Detecting fuzzy duplicates using MinHash LSH...\n",
      "[2026-01-12 16:06:21,130][oumi][rank0][pid:1937][MainThread][INFO]][embedding_analyzer.py:350] Creating MinHash signatures for 7211 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating MinHash signatures: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7211/7211 [00:09<00:00, 788.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:06:30,333][oumi][rank0][pid:1937][MainThread][INFO]][embedding_analyzer.py:369] Finding fuzzy duplicates using LSH...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding duplicates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7211/7211 [00:00<00:00, 157596.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:06:30,396][oumi][rank0][pid:1937][MainThread][INFO]][question_diversity_analyzer.py:464] Computing embeddings for 2485 user questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2485/2485 [00:07<00:00, 314.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:06:38,294][oumi][rank0][pid:1937][MainThread][INFO]][question_diversity_analyzer.py:469] Clustering 2485 questions using dbscan...\n",
      "[2026-01-12 16:06:38,312][oumi][rank0][pid:1937][MainThread][INFO]][question_diversity_analyzer.py:482] Found 7 clusters, 2467 unique/diverse questions (not similar to others)\n",
      "[2026-01-12 16:06:38,317][oumi][rank0][pid:1937][MainThread][INFO]][repr_diversity_analyzer.py:363] Computing diversity scores for 7211 samples in column 'text_content'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7211/7211 [00:37<00:00, 194.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:07:15,325][oumi][rank0][pid:1937][MainThread][INFO]][repr_diversity_analyzer.py:230] Computing nearest neighbor distances for 7211 samples (k=5)...\n",
      "[2026-01-12 16:07:16,161][oumi][rank0][pid:1937][MainThread][INFO]][repr_diversity_analyzer.py:556] Column 'text_content': 2226/7211 samples (30.9%) are redundant\n",
      "[2026-01-12 16:07:19,024][oumi][rank0][pid:1937][MainThread][INFO]][llm_judge_analyzer.py:780] Skipping message-level analysis (analyze_message_level=False). Set analyze_message_level=True to enable.\n",
      "Total conversations analyzed: 1000\n"
     ]
    }
   ],
   "source": [
    "# Run the analysis\n",
    "analyzer.analyze_dataset()\n",
    "\n",
    "# The results are stored in analyzer object\n",
    "if analyzer._analysis_results:\n",
    "    print(\n",
    "        f\"Total conversations analyzed: {analyzer._analysis_results.conversations_analyzed}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:07:19,604][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:1426] Saved message analysis to: /Users/ryanarman/code/oumi/analysis_output/openassistant/messages_df.parquet\n",
      "[2026-01-12 16:07:19,621][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:1432] Saved conversation analysis to: /Users/ryanarman/code/oumi/analysis_output/openassistant/conversations_df.parquet\n",
      "[2026-01-12 16:07:19,679][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:1438] Saved merged analysis to: /Users/ryanarman/code/oumi/analysis_output/openassistant/merged_df.parquet\n",
      "[2026-01-12 16:07:19,681][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:1450] Saved message schema to: /Users/ryanarman/code/oumi/analysis_output/openassistant/message_schema.json\n",
      "[2026-01-12 16:07:19,682][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:1457] Saved conversation schema to: /Users/ryanarman/code/oumi/analysis_output/openassistant/conversation_schema.json\n",
      "[2026-01-12 16:07:19,684][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:1463] Saved combined schemas to: /Users/ryanarman/code/oumi/analysis_output/openassistant/schema.json\n",
      "[2026-01-12 16:07:19,686][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:1470] Saved analysis summary to: /Users/ryanarman/code/oumi/analysis_output/openassistant/analysis_summary.json\n",
      "[2026-01-12 16:07:19,687][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:1472] All analyzer artifacts saved to: /Users/ryanarman/code/oumi/analysis_output/openassistant\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "from oumi.utils.analysis_utils import save_analyzer_artifacts\n",
    "\n",
    "# Save all analyzer artifacts (dataframes, schemas, summary)\n",
    "save_analyzer_artifacts(analyzer, Path(config.output_path), output_format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:07:19,715][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:1551] Loaded message analysis from: /Users/ryanarman/code/oumi/analysis_output/openassistant/messages_df\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:07:19,725][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:1558] Loaded conversation analysis from: /Users/ryanarman/code/oumi/analysis_output/openassistant/conversations_df\n",
      "[2026-01-12 16:07:19,751][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:1565] Loaded merged analysis from: /Users/ryanarman/code/oumi/analysis_output/openassistant/merged_df\n",
      "[2026-01-12 16:07:19,753][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:1574] Loaded combined schemas from: /Users/ryanarman/code/oumi/analysis_output/openassistant/schema.json\n",
      "[2026-01-12 16:07:19,754][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:1596] Loaded analysis summary from: /Users/ryanarman/code/oumi/analysis_output/openassistant/analysis_summary.json\n",
      "[2026-01-12 16:07:19,755][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:1598] Loaded analyzer artifacts from: /Users/ryanarman/code/oumi/analysis_output/openassistant\n",
      "[2026-01-12 16:07:20,237][oumi.utils.analysis_utils][rank0][pid:1937][MainThread][INFO]][analysis_utils.py:1773] Regenerated 23 recommendations from artifacts with latest code\n",
      "[2026-01-12 16:07:23,525][oumi][rank0][pid:1937][MainThread][INFO]][report_generator.py:290] Generated HTML report: /Users/ryanarman/code/oumi/analysis_output/openassistant/index.html\n",
      "[2026-01-12 16:07:23,526][oumi][rank0][pid:1937][MainThread][INFO]][report_generator.py:291] External data files written to: /Users/ryanarman/code/oumi/analysis_output/openassistant/data\n",
      "‚úÖ Generated HTML report at: /Users/ryanarman/code/oumi/analysis_output/openassistant/index.html\n",
      "\n",
      "üìÅ All results saved to: /Users/ryanarman/code/oumi/analysis_output/openassistant\n"
     ]
    }
   ],
   "source": [
    "# OUTPUT_PATH = \"/Users/ryanarman/code/oumi/analysis_output/openassistant\"\n",
    "OUTPUT_PATH = \"/Users/ryanarman/code/oumi/analysis_output/openassistant\"\n",
    "\n",
    "from oumi.utils.analysis_utils import (\n",
    "    load_analyzer_artifacts,\n",
    "    regenerate_recommendations,\n",
    ")\n",
    "\n",
    "artifacts = load_analyzer_artifacts(OUTPUT_PATH)\n",
    "\n",
    "# Regenerate recommendations with latest code (e.g., updated duplicate detection)\n",
    "artifacts = regenerate_recommendations(artifacts, outlier_threshold=3.0)\n",
    "\n",
    "artifacts.keys()\n",
    "\n",
    "# Generate HTML report if configured\n",
    "\n",
    "\n",
    "try:\n",
    "    from oumi.core.analyze.report_generator import HTMLReportGenerator\n",
    "\n",
    "    report_gen = HTMLReportGenerator()\n",
    "    report_path = report_gen.generate_report(\n",
    "        artifacts=artifacts,\n",
    "        output_path=OUTPUT_PATH,\n",
    "        title=\"OpenAssistant Analysis Report\",\n",
    "    )\n",
    "    print(f\"‚úÖ Generated HTML report at: {report_path / 'index.html'}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Plotly not installed. Skipping HTML report generation.\")\n",
    "    print(\"   Install with: pip install 'oumi[analyze_advanced]'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Failed to generate HTML report: {e}\")\n",
    "    print(\"\\nüîç FULL TRACEBACK:\")\n",
    "    print(\"=\" * 70)\n",
    "    traceback.print_exc()\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìÅ All results saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['conversation_index', 'conversation_id', 'num_messages',\n",
       "       'conversation_text_content',\n",
       "       'conversation_text_content__length__token_count',\n",
       "       'conversation_text_content__diversity__unique_words_ratio',\n",
       "       'conversation_text_content__format__has_markdown',\n",
       "       'conversation_text_content__format__has_json',\n",
       "       'conversation_text_content__format__has_code_blocks',\n",
       "       'conversation_text_content__format__code_block_count',\n",
       "       'conversation_text_content__format__code_block_languages',\n",
       "       'conversation_text_content__format__has_urls',\n",
       "       'conversation_text_content__format__has_emails',\n",
       "       'conversation_text_content__format__format_complexity_score',\n",
       "       'conversation_text_content__quality__has_pii',\n",
       "       'conversation_text_content__quality__pii_types',\n",
       "       'conversation_text_content__quality__pii_count',\n",
       "       'conversation_text_content__quality__has_encoding_issues',\n",
       "       'conversation_text_content__quality__repetition_ratio',\n",
       "       'conversation_text_content__quality__has_high_repetition',\n",
       "       'conversation_text_content__content_pattern__has_placeholder',\n",
       "       'conversation_text_content__content_pattern__placeholder_count',\n",
       "       'conversation_text_content__content_pattern__placeholder_types',\n",
       "       'conversation_text_content__content_pattern__has_hallucinated_experience',\n",
       "       'conversation_text_content__content_pattern__has_nooutput',\n",
       "       'conversation_text_content__content_pattern__has_refusal',\n",
       "       'conversation_text_content__fasttext__detected_language',\n",
       "       'conversation_text_content__fasttext__language_confidence',\n",
       "       'conversation_text_content__fasttext__language_name',\n",
       "       'conversation_text_content__fasttext__low_confidence',\n",
       "       'conversation_text_content__fasttext__detected_script',\n",
       "       'conversation_text_content__fasttext__is_multilingual',\n",
       "       'conversation_text_content__embedding__duplicate_group',\n",
       "       'conversation_text_content__embedding__has_semantic_duplicate',\n",
       "       'conversation_text_content__embedding__fuzzy_duplicate_group',\n",
       "       'conversation_text_content__embedding__has_fuzzy_duplicate',\n",
       "       'conversation_text_content__embedding__fuzzy_jaccard_score',\n",
       "       'conversation_text_content__question_diversity__cluster_id',\n",
       "       'conversation_text_content__question_diversity__cluster_size',\n",
       "       'conversation_text_content__question_diversity__is_concentrated',\n",
       "       'conversation_text_content__repr_diversity__nn_distance',\n",
       "       'conversation_text_content__repr_diversity__score',\n",
       "       'conversation_text_content__repr_diversity__is_redundant',\n",
       "       'conversation_text_content__repr_diversity__percentile',\n",
       "       'conversation_text_content__response_completeness__is_complete',\n",
       "       'conversation_text_content__response_completeness__score',\n",
       "       'conversation_text_content__response_completeness__ends_naturally',\n",
       "       'conversation_text_content__response_completeness__has_conclusion',\n",
       "       'conversation_text_content__response_completeness__truncation_type',\n",
       "       'conversation_text_content__training_quality__response_completeness_score',\n",
       "       'conversation_text_content__training_quality__has_proper_ending',\n",
       "       'conversation_text_content__training_quality__has_structure',\n",
       "       'conversation_text_content__training_quality__response_word_count',\n",
       "       'conversation_text_content__task_category__category',\n",
       "       'conversation_text_content__task_category__confidence',\n",
       "       'conversation_text_content__task_category__is_stem',\n",
       "       'conversation_text_content__task_category__is_conversational',\n",
       "       'conversation_text_content__safety__score',\n",
       "       'conversation_text_content__safety__is_safe',\n",
       "       'conversation_text_content__safety__risk_level',\n",
       "       'conversation_text_content__safety__categories_triggered',\n",
       "       'conversation_text_content__difficulty__score',\n",
       "       'conversation_text_content__difficulty__tier',\n",
       "       'conversation_text_content__difficulty__requires_reasoning',\n",
       "       'conversation_text_content__difficulty__requires_domain_knowledge',\n",
       "       'conversation_text_content__difficulty__constraint_count',\n",
       "       'conversation_text_content__input_quality__tier',\n",
       "       'conversation_text_content__input_quality__score',\n",
       "       'conversation_text_content__input_quality__is_ambiguous',\n",
       "       'conversation_text_content__input_quality__is_answerable',\n",
       "       'conversation_text_content__input_quality__has_sufficient_context',\n",
       "       'conversation_text_content__instruct_reward__score',\n",
       "       'conversation_text_content__instruct_reward__tier',\n",
       "       'conversation_text_content__instruct_reward__helpfulness',\n",
       "       'conversation_text_content__instruct_reward__completeness',\n",
       "       'conversation_text_content__instruct_reward__clarity',\n",
       "       'conversation_text_content__cost__fits_context_4k',\n",
       "       'conversation_text_content__cost__context_utilization_4k',\n",
       "       'conversation_text_content__cost__tokens_wasted_4k',\n",
       "       'conversation_text_content__cost__fits_context_8k',\n",
       "       'conversation_text_content__cost__context_utilization_8k',\n",
       "       'conversation_text_content__cost__tokens_wasted_8k',\n",
       "       'conversation_text_content__helpfulness__score',\n",
       "       'conversation_text_content__helpfulness__label',\n",
       "       'conversation_text_content__helpfulness__reasoning',\n",
       "       'conversation_text_content__helpfulness__raw_response'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = artifacts[\"schemas\"][\"conversation_schema\"]\n",
    "conv_df = artifacts[\"conversations_df\"]\n",
    "conv_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer: fasttext\n",
      "\n",
      "Input:\n",
      "source_column: conversation_text_content\n",
      "USER: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\n",
      "ASSISTANT: Monopsony is a market structure in which there is a single buyer in a market. In the context of labor markets, a monopsony occurs when there is only one employer in a particular industry or geographic area, giving that employer significant power over the wages and working conditions of workers. This results in a situation where the employer has the ability to suppress wages and reduce the bargaining power of workers.\n",
      "\n",
      "Research has shown that the existence of monopsony power in labor markets can have negative effects on workers, such as lower wages, reduced benefits, and limited job opportunities. For example, a study by Autor, Dorn, and Hanson (2013) found evidence of monopsony power in certain U.S. labor markets, resulting in lower wages and reduced job opportunities for workers. Similarly, another study by Bharadwaj, Black, and Kolesnikova (2019) found that the presence of a single dominant employer in a local labor market was associated with lower wages for workers.\n",
      "USER: How can one fight back when a monospony had been created?\n",
      "ASSISTANT: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\n",
      "\n",
      "Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\n",
      "\n",
      "Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\n",
      "\n",
      "References:\n",
      "Bivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.\n",
      "USER: Now explain it to a dog\n",
      "ASSISTANT: Monopsony refers to a market structure where there is only one buyer of a good or service. In the context of labor markets, a monopsony occurs when there is only one employer in a particular geographic area or industry, giving them significant power to set wages and working conditions. Research has shown that the existence of monopsonies can lead to lower wages and reduced job opportunities for workers. For example, a study by the National Bureau of Economic Research found that in industries with high levels of concentration, workers earn lower wages and are less likely to receive benefits such as health insurance.\n",
      "USER: I would imagine this is similar or even the same as a \"monopoly\" but I am not sure.\n",
      "\n",
      "Assuming that this some kind of \"final state\" of a particular industry or even an entire economy, give me some larger historical examples of this kind of structure consolidating over time, and describe what circumstances are common prerequisite.\n",
      "ASSISTANT: Bouguereau died in 1905, so it is unlikely that any of his direct descendants are still alive. However, his works continue to be popular and are sold at auction and through art dealers. It is possible that some indirect descendants may benefit from the sale of his works.\n",
      "USER: What can be done at a regulatory level to ensure, in a monopsony, that the power over the employees isn't abused? List a few options and focus on the organs that should act.\n",
      "ASSISTANT: Yes, that's correct. Keeping the code for the TESR and the registration code in the same file can make it easier to manage and maintain the code. The ExampleMod class acts as a container for the TESR code, providing a convenient and logical grouping. The functionality of the code remains unchanged, and the TESR will still be properly registered and used to render the custom tile entities in the game.\n",
      "ASSISTANT: Here are some potential regulatory options to address monopsony power over employees:\n",
      "\n",
      "- Antitrust enforcement agencies like the FTC or DOJ could investigate and take action against dominant employers that are abusing their market power to suppress wages or worsen working conditions. They could force divestitures or other structural remedies.\n",
      "- Labor standards agencies could set industry-specific minimum wage floors or other regulations around compensation and working conditions. This would reduce the ability of dominant employers to push wages/conditions below certain thresholds.\n",
      "- Sector-specific regulators could impose requirements around how companies hire, compensate, and treat workers. For example, banking regulators could prohibit abusive practices by dominant banks with regard to their employees.\n",
      "- Legislatures could pass laws\n",
      "ASSISTANT: Regulatory intervention can be used to address the negative consequences of monopsony power in the labor market. A few options include:\n",
      "\n",
      "1. Antitrust enforcement: Antitrust laws can be used to prevent firms from engaging in practices that limit competition and suppress wages. The antitrust division of the Department of Justice and the Federal Trade Commission (FTC) are responsible for enforcing these laws.\n",
      "\n",
      "2. Labor standards and protections: Governments can set minimum wage standards and other labor protections to ensure that workers are paid a fair wage, even in industries dominated by a single buyer. The U.S. Department of Labor is responsible for enforcing these standards.\n",
      "\n",
      "3. Occupational licensing reform: Occupational licensing requirements can restrict worker mobility and increase the bargaining power of employers. Reforms to licensing requirements can reduce these barriers and increase competition in the labor market. State governments typically regulate occupational licensing.\n",
      "\n",
      "4. Collective bargaining rights: Collective bargaining by workers can increase their bargaining power and lead to higher wages. Governments can protect and promote collective bargaining rights to counteract the effects of monopsony power. The National Labor Relations Board is responsible for enforcing workers' rights to engage in collective bargaining in the U.S.\n",
      "\n",
      "These are just a few examples of regulatory options that can be used to address monopsony power in the labor market. The specific actions taken will depend on the particular circumstances and context of each case.\n",
      "\n",
      "metric: detected_language\n",
      "description: ISO 639-1 language code detected\n",
      "value: en\n",
      "\n",
      "\n",
      "metric: language_confidence\n",
      "description: Confidence score for language detection (0.0-1.0)\n",
      "value: 0.8847\n",
      "\n",
      "\n",
      "metric: language_name\n",
      "description: Full language name\n",
      "value: English\n",
      "\n",
      "\n",
      "metric: low_confidence\n",
      "description: Whether language detection confidence is low\n",
      "value: False\n",
      "\n",
      "\n",
      "metric: detected_script\n",
      "description: Detected script/writing system\n",
      "value: latin\n",
      "\n",
      "\n",
      "metric: is_multilingual\n",
      "description: Whether text contains multiple languages\n",
      "value: False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.analyze.column_utils import (\n",
    "    filter_analyzer_columns,\n",
    "    parse_analyzer_column_name,\n",
    ")\n",
    "\n",
    "row = conv_df.iloc[0]\n",
    "conv_columns = conv_df.columns\n",
    "\n",
    "# Choose the analzyer to analyze\n",
    "analyzer_name = \"fasttext\"\n",
    "\n",
    "\n",
    "filtered_cols = filter_analyzer_columns(conv_columns, analyzer_id=analyzer_name)\n",
    "filtered_cols\n",
    "\n",
    "if filtered_cols:\n",
    "    print(f\"Analyzer: {analyzer_name}\")\n",
    "    info = parse_analyzer_column_name(filtered_cols[0])\n",
    "    print(\"\\nInput:\")\n",
    "    print(f\"source_column: {info.source_column}\")\n",
    "    print(f\"{row[info.source_column]}\\n\")\n",
    "\n",
    "    for col in filtered_cols:\n",
    "        info = parse_analyzer_column_name(col)\n",
    "        print(f\"metric: {info.metric_name}\")\n",
    "        # print(f\"type: {schema[col]['type']}\")\n",
    "        # print(f\"content_type: {schema[col]['content_type']}\")\n",
    "        print(f\"description: {schema[col]['description']}\")\n",
    "        print(f\"value: {row[col]}\")\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(f\"No columns found for analyzer: {analyzer_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer: fasttext\n",
      "\n",
      "Input:\n",
      "source_column: text_content\n",
      "Seg√∫n Jean Piaget, estas son las 4 etapas del desarrollo cognitivo:\n",
      "1. Etapa sensiomotriz (0 a 2 a√±os). Durante esta etapa, los ni√±os interact√∫an f√≠sicamente con su entorno a trav√©s de juegos y experimentaci√≥n.\n",
      "2. Etapa preoperacional (2 a 7 a√±os). Durante esta etapa, los ni√±os pueden ponerse en el lugar de los dem√°s y jugar a hacer juegos de rol. Sin embargo, a√∫n tienen dificultades para acceder a pensamientos m√°s abstractos y a√∫n presentan egocentrismo.\n",
      "3. Etapa de operaciones concretas (7 a 12 a√±os). Durante esta etapa, los ni√±os pueden usar la l√≥gica para llegar a conclusiones v√°lidas, pero solo en situaciones concretas. Tambi√©n pueden categorizar aspectos de la realidad de una manera m√°s compleja y el pensamiento se vuelve menos egoc√©ntrico.\n",
      "4. Etapa de operaciones formales (desde los 12 a√±os hasta la vida adulta). Durante esta etapa, los ni√±os pueden utilizar la l√≥gica para llegar a conclusiones abstractas y pueden analizar y manipular esquemas de pensamiento.\n",
      "\n",
      "metric: detected_language\n",
      "description: ISO 639-1 language code detected\n",
      "value: es\n",
      "\n",
      "\n",
      "metric: language_confidence\n",
      "description: Confidence score for language detection (0.0-1.0)\n",
      "value: 0.9849\n",
      "\n",
      "\n",
      "metric: language_name\n",
      "description: Full language name\n",
      "value: Spanish\n",
      "\n",
      "\n",
      "metric: low_confidence\n",
      "description: Whether language detection confidence is low\n",
      "value: False\n",
      "\n",
      "\n",
      "metric: detected_script\n",
      "description: Detected script/writing system\n",
      "value: latin\n",
      "\n",
      "\n",
      "metric: is_multilingual\n",
      "description: Whether text contains multiple languages\n",
      "value: False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema_msg = artifacts[\"schemas\"][\"message_schema\"]\n",
    "msg_df = artifacts[\"messages_df\"]\n",
    "msg_df.columns\n",
    "\n",
    "from oumi.core.analyze.column_utils import (\n",
    "    filter_analyzer_columns,\n",
    "    parse_analyzer_column_name,\n",
    ")\n",
    "\n",
    "row = msg_df.iloc[20]\n",
    "msg_columns = msg_df.columns\n",
    "\n",
    "# Choose the analzyer to analyze\n",
    "analyzer_name = \"fasttext\"\n",
    "\n",
    "\n",
    "filtered_cols = filter_analyzer_columns(msg_columns, analyzer_id=analyzer_name)\n",
    "\n",
    "\n",
    "if filtered_cols:\n",
    "    print(f\"Analyzer: {analyzer_name}\")\n",
    "    info = parse_analyzer_column_name(filtered_cols[0])\n",
    "    print(\"\\nInput:\")\n",
    "    print(f\"source_column: {info.source_column}\")\n",
    "    print(f\"{row[info.source_column]}\\n\")\n",
    "\n",
    "    for col in filtered_cols:\n",
    "        info = parse_analyzer_column_name(col)\n",
    "        print(f\"metric: {info.metric_name}\")\n",
    "        # print(f\"type: {schema[col]['type']}\")\n",
    "        # print(f\"content_type: {schema[col]['content_type']}\")\n",
    "        print(f\"description: {schema_msg[col]['description']}\")\n",
    "        print(f\"value: {row[col]}\")\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(f\"No columns found for analyzer: {analyzer_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
