{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MPS has been disabled - forcing CPU-only mode\n",
            "PyTorch device: cpu\n",
            "PyTorch version: 2.6.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "# IMPORTANT: Set these BEFORE importing torch or any ML libraries\n",
        "# Disable all GPU/MPS backends to prevent crashes with IFD analyzer\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Disable CUDA\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Disable MPS memory allocation\n",
        "os.environ[\"DISABLE_MPS_COMPAT\"] = \"1\"  # Additional MPS disable flag\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"  # Disable HuggingFace telemetry\n",
        "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"  # Allow model downloads\n",
        "\n",
        "# Force CPU usage in PyTorch to avoid MPS crashes\n",
        "import torch\n",
        "\n",
        "# Forcefully disable MPS before anything else\n",
        "torch.set_default_device(\"cpu\")\n",
        "if hasattr(torch.backends, \"mps\"):\n",
        "    # Monkey-patch to prevent MPS usage\n",
        "    original_is_available = torch.backends.mps.is_available\n",
        "    torch.backends.mps.is_available = lambda: False\n",
        "    print(\"MPS has been disabled - forcing CPU-only mode\")\n",
        "else:\n",
        "    print(\"Using CPU for all computations\")\n",
        "\n",
        "print(f\"PyTorch device: {torch.get_default_device()}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dataset already exists at /Users/ryanarman/code/oumi/data/openassistant/openassistant_oasst1.jsonl\n",
            "üìÅ Dataset path: /Users/ryanarman/code/oumi/data/openassistant/openassistant_oasst1.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Download and convert OpenAssistant dataset to JSONL format\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Set paths\n",
        "DATA_DIR = Path(\"/Users/ryanarman/code/oumi/data/openassistant\")\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "DATASET_PATH = DATA_DIR / \"openassistant_oasst1.jsonl\"\n",
        "\n",
        "# Download OpenAssistant OASST1 if not already downloaded\n",
        "if not DATASET_PATH.exists():\n",
        "    print(\"üì• Downloading OpenAssistant OASST1 dataset...\")\n",
        "    dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"train\")\n",
        "\n",
        "    # Get dataset length safely\n",
        "    from datasets import Dataset\n",
        "\n",
        "    if isinstance(dataset, Dataset):\n",
        "        dataset_len = len(dataset)\n",
        "    else:\n",
        "        dataset_len = \"unknown\"\n",
        "\n",
        "    print(f\"‚úÖ Downloaded dataset (size: {dataset_len})\")\n",
        "    print(f\"üìù Converting to JSONL format...\")\n",
        "\n",
        "    def build_conversations_from_messages(all_messages):\n",
        "        \"\"\"Build conversations from OASST1 message tree structure.\n",
        "\n",
        "        OASST1 has messages with parent_id relationships. We need to:\n",
        "        1. Find root messages (no parent_id)\n",
        "        2. Build conversation threads by following parent_id chains\n",
        "        \"\"\"\n",
        "        # Create a map of message_id -> message\n",
        "        msg_map = {}\n",
        "        for msg in all_messages:\n",
        "            msg_id = msg.get(\"message_id\")\n",
        "            if msg_id:\n",
        "                msg_map[msg_id] = msg\n",
        "\n",
        "        # Find root messages (no parent_id or parent_id is None)\n",
        "        roots = []\n",
        "        for msg in all_messages:\n",
        "            parent_id = msg.get(\"parent_id\")\n",
        "            if not parent_id or parent_id not in msg_map:\n",
        "                roots.append(msg)\n",
        "\n",
        "        conversations = []\n",
        "        for root in roots:\n",
        "            messages = []\n",
        "\n",
        "            def add_message(msg):\n",
        "                role = \"user\" if msg.get(\"role\") == \"prompter\" else \"assistant\"\n",
        "                content = msg.get(\"text\", \"\")\n",
        "                if content:  # Only add non-empty messages\n",
        "                    messages.append({\"role\": role, \"content\": content})\n",
        "\n",
        "                # Find children (messages with this message as parent)\n",
        "                for child_id, child_msg in msg_map.items():\n",
        "                    if child_msg.get(\"parent_id\") == msg.get(\"message_id\"):\n",
        "                        add_message(child_msg)\n",
        "\n",
        "            add_message(root)\n",
        "\n",
        "            if messages:  # Only add conversations with at least one message\n",
        "                conversations.append({\"messages\": messages})\n",
        "\n",
        "        return conversations\n",
        "\n",
        "    def convert_example(example):\n",
        "        \"\"\"Convert a single OASST1 example to conversation format.\"\"\"\n",
        "        # OASST1 can have different structures:\n",
        "        # 1. Single message with parent_id (tree structure)\n",
        "        # 2. Pre-built conversation in \"messages\" field\n",
        "        # 3. Tree structure in \"message_tree\" field\n",
        "\n",
        "        if \"messages\" in example and isinstance(example[\"messages\"], list):\n",
        "            # Already in conversation format\n",
        "            return {\"messages\": example[\"messages\"]}\n",
        "        elif \"message_tree\" in example:\n",
        "            # Tree structure - build conversations\n",
        "            conversations = build_conversations_from_messages(example[\"message_tree\"])\n",
        "            return conversations[0] if conversations else None\n",
        "        elif \"text\" in example:\n",
        "            # Single message - create a simple conversation\n",
        "            role = \"user\" if example.get(\"role\") == \"prompter\" else \"assistant\"\n",
        "            return {\"messages\": [{\"role\": role, \"content\": example[\"text\"]}]}\n",
        "        else:\n",
        "            # Try to extract from available fields\n",
        "            return None\n",
        "\n",
        "    # Convert and save\n",
        "    # First, collect all messages to build conversation trees\n",
        "    # OASST1 structure: each example might be a single message or a conversation\n",
        "    print(\"üìä Inspecting dataset structure...\")\n",
        "    first_example = next(iter(dataset))\n",
        "    print(f\"   Example keys: {list(first_example.keys())[:10]}\")  # Show first 10 keys\n",
        "\n",
        "    count = 0\n",
        "    with open(DATASET_PATH, \"w\") as f:\n",
        "        for example in dataset:\n",
        "            conversation = convert_example(example)\n",
        "            if conversation and conversation.get(\"messages\"):\n",
        "                f.write(json.dumps(conversation) + \"\\n\")\n",
        "                count += 1\n",
        "\n",
        "    print(f\"‚úÖ Saved {count} conversations to {DATASET_PATH}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset already exists at {DATASET_PATH}\")\n",
        "\n",
        "print(f\"üìÅ Dataset path: {DATASET_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Config loaded with 19 analyzers:\n",
            "  - length (type: length)\n",
            "  - token_stats (type: token_stats)\n",
            "  - diversity (type: diversity)\n",
            "  - format (type: format)\n",
            "  - quality (type: quality)\n",
            "  - content_pattern (type: content_pattern)\n",
            "  - embedding (type: embedding)\n",
            "  - question_diversity (type: question_diversity)\n",
            "  - repr_diversity (type: repr_diversity)\n",
            "  - conversation_structure (type: conversation_structure)\n",
            "  - response_completeness (type: response_completeness)\n",
            "  - training_quality (type: training_quality)\n",
            "  - task_category (type: task_category)\n",
            "  - safety (type: safety)\n",
            "  - difficulty (type: difficulty)\n",
            "  - input_quality (type: input_quality)\n",
            "  - instruct_reward (type: instruct_reward)\n",
            "  - cost (type: cost)\n",
            "  - helpfulness (type: llm_judge)\n",
            "üìÅ Output will be saved to: /Users/ryanarman/code/oumi/analysis_output/openassistant\n",
            "üìÇ Dataset path: /Users/ryanarman/code/oumi/data/openassistant/openassistant_oasst1.jsonl\n",
            "‚úÖ Config validated successfully!\n",
            "[2026-01-08 16:52:18,502][oumi][rank0][pid:62985][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'custom'\n",
            "[2026-01-08 16:52:18,503][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:227] Loaded text dataset from: /Users/ryanarman/code/oumi/data/openassistant/openassistant_oasst1.jsonl\n",
            "[2026-01-08 16:52:18,504][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:154] Loaded dataset from config: None\n",
            "[2026-01-08 16:52:18,627][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: length\n",
            "[2026-01-08 16:52:18,628][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: token_stats\n",
            "[2026-01-08 16:52:18,628][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: diversity\n",
            "[2026-01-08 16:52:18,628][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: format\n",
            "[2026-01-08 16:52:18,628][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: quality\n",
            "[2026-01-08 16:52:18,629][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: content_pattern\n",
            "[2026-01-08 16:52:18,640][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: embedding\n",
            "[2026-01-08 16:52:18,640][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: question_diversity\n",
            "[2026-01-08 16:52:18,641][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: repr_diversity\n",
            "[2026-01-08 16:52:18,641][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: conversation_structure\n",
            "[2026-01-08 16:52:18,642][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: response_completeness\n",
            "[2026-01-08 16:52:18,642][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: training_quality\n",
            "[2026-01-08 16:52:18,642][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: task_category\n",
            "[2026-01-08 16:52:18,643][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: safety\n",
            "[2026-01-08 16:52:18,644][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: difficulty\n",
            "[2026-01-08 16:52:18,644][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: input_quality\n",
            "[2026-01-08 16:52:18,645][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: instruct_reward\n",
            "[2026-01-08 16:52:18,646][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: cost\n",
            "[2026-01-08 16:52:18,646][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:320] Initialized sample analyzer: helpfulness (type: llm_judge)\n"
          ]
        }
      ],
      "source": [
        "# OUTPUT_PATH = \"/Users/ryanarman/code/oumi/analysis_output/openassistant\"\n",
        "OUTPUT_PATH = \"/Users/ryanarman/code/oumi/analysis_output/openassistant\"\n",
        "\n",
        "import os\n",
        "from oumi.core.configs import AnalyzeConfig\n",
        "from oumi.core.analyze.dataset_analyzer import DatasetAnalyzer\n",
        "\n",
        "# Load config from YAML file\n",
        "config = AnalyzeConfig.from_yaml(\n",
        "    \"/Users/ryanarman/code/oumi/configs/examples/analyze/analyze_openassistant.yaml\"\n",
        ")\n",
        "\n",
        "# Override settings for this run\n",
        "dataset_path = str(DATASET_PATH)\n",
        "config.dataset_path = dataset_path\n",
        "config.dataset_name = None  # Clear dataset_name so it uses dataset_path instead\n",
        "config.sample_count = 50\n",
        "config.chat_template = \"default\"  # Simple template without special variables\n",
        "\n",
        "# Set absolute output path (makes it easier to find the results!)\n",
        "config.output_path = OUTPUT_PATH\n",
        "\n",
        "print(f\"‚úÖ Config loaded with {len(config.analyzers)} analyzers:\")\n",
        "for analyzer in config.analyzers:\n",
        "    instance_id = analyzer.instance_id or analyzer.id\n",
        "    print(f\"  - {instance_id} (type: {analyzer.id})\")\n",
        "\n",
        "print(f\"üìÅ Output will be saved to: {config.output_path}\")\n",
        "print(f\"üìÇ Dataset path: {config.dataset_path}\")\n",
        "\n",
        "# Validate the configuration\n",
        "config.finalize_and_validate()\n",
        "print(\"‚úÖ Config validated successfully!\")\n",
        "\n",
        "analyzer = DatasetAnalyzer(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:21,057][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:363] Starting analysis of dataset: None\n",
            "[2026-01-08 16:52:21,059][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:364] Using 19 sample analyzers: ['length', 'token_stats', 'diversity', 'format', 'quality', 'content_pattern', 'embedding', 'question_diversity', 'repr_diversity', 'conversation_structure', 'response_completeness', 'training_quality', 'task_category', 'safety', 'difficulty', 'input_quality', 'instruct_reward', 'cost', 'helpfulness']\n",
            "[2026-01-08 16:52:21,060][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:387] Analyzing 50 of 84437 conversations\n",
            "[2026-01-08 16:52:21,060][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:439] Converting conversation dataset with 84437 items\n",
            "[2026-01-08 16:52:21,061][oumi][rank0][pid:62985][MainThread][INFO]][dataset_analyzer.py:446] Limiting analysis to first 50 items (dataset has 84437 total)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting Unknown Dataset to DataFrames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 678.11item/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:21,172][oumi][rank0][pid:62985][MainThread][INFO]][embedding_analyzer.py:518] Computing embeddings for 50 samples...\n",
            "[2026-01-08 16:52:21,173][oumi][rank0][pid:62985][MainThread][INFO]][embedding_analyzer.py:196] Loading embedding model: all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 167.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:22,434][oumi][rank0][pid:62985][MainThread][INFO]][embedding_analyzer.py:537] Detecting semantic duplicates...\n",
            "[2026-01-08 16:52:22,438][oumi][rank0][pid:62985][MainThread][INFO]][embedding_analyzer.py:624] Detecting fuzzy duplicates using MinHash LSH...\n",
            "[2026-01-08 16:52:22,447][oumi][rank0][pid:62985][MainThread][INFO]][embedding_analyzer.py:350] Creating MinHash signatures for 50 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating MinHash signatures: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 552.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:22,565][oumi][rank0][pid:62985][MainThread][INFO]][embedding_analyzer.py:369] Finding fuzzy duplicates using LSH...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Finding duplicates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 20392.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:22,573][oumi][rank0][pid:62985][MainThread][INFO]][question_diversity_analyzer.py:464] Computing embeddings for 50 user questions...\n",
            "[2026-01-08 16:52:22,574][oumi][rank0][pid:62985][MainThread][INFO]][question_diversity_analyzer.py:174] Loading embedding model: all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 125.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:23,892][oumi][rank0][pid:62985][MainThread][INFO]][question_diversity_analyzer.py:469] Clustering 50 questions using dbscan...\n",
            "[2026-01-08 16:52:24,167][oumi][rank0][pid:62985][MainThread][INFO]][question_diversity_analyzer.py:482] Found 0 clusters, 50 unique/diverse questions (not similar to others)\n",
            "[2026-01-08 16:52:24,175][oumi][rank0][pid:62985][MainThread][INFO]][repr_diversity_analyzer.py:363] Computing diversity scores for 50 samples in column 'conversation_text_content'...\n",
            "[2026-01-08 16:52:24,183][oumi][rank0][pid:62985][MainThread][INFO]][repr_diversity_analyzer.py:165] Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 166.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:25,531][oumi][rank0][pid:62985][MainThread][INFO]][repr_diversity_analyzer.py:230] Computing nearest neighbor distances for 50 samples (k=5)...\n",
            "[2026-01-08 16:52:25,534][oumi][rank0][pid:62985][MainThread][INFO]][repr_diversity_analyzer.py:556] Column 'conversation_text_content': 11/50 samples (22.0%) are redundant\n",
            "[2026-01-08 16:52:25,615][oumi][rank0][pid:62985][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:04<00:00, 11.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:30,158][oumi][rank0][pid:62985][MainThread][INFO]][embedding_analyzer.py:518] Computing embeddings for 50 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 188.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:30,427][oumi][rank0][pid:62985][MainThread][INFO]][embedding_analyzer.py:537] Detecting semantic duplicates...\n",
            "[2026-01-08 16:52:30,429][oumi][rank0][pid:62985][MainThread][INFO]][embedding_analyzer.py:624] Detecting fuzzy duplicates using MinHash LSH...\n",
            "[2026-01-08 16:52:30,433][oumi][rank0][pid:62985][MainThread][INFO]][embedding_analyzer.py:350] Creating MinHash signatures for 50 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating MinHash signatures: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 651.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:30,543][oumi][rank0][pid:62985][MainThread][INFO]][embedding_analyzer.py:369] Finding fuzzy duplicates using LSH...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Finding duplicates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 27084.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:30,551][oumi][rank0][pid:62985][MainThread][INFO]][question_diversity_analyzer.py:464] Computing embeddings for 20 user questions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 120.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:30,723][oumi][rank0][pid:62985][MainThread][INFO]][question_diversity_analyzer.py:469] Clustering 20 questions using dbscan...\n",
            "[2026-01-08 16:52:30,727][oumi][rank0][pid:62985][MainThread][INFO]][question_diversity_analyzer.py:482] Found 0 clusters, 20 unique/diverse questions (not similar to others)\n",
            "[2026-01-08 16:52:30,729][oumi][rank0][pid:62985][MainThread][INFO]][repr_diversity_analyzer.py:363] Computing diversity scores for 50 samples in column 'text_content'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 168.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:31,030][oumi][rank0][pid:62985][MainThread][INFO]][repr_diversity_analyzer.py:230] Computing nearest neighbor distances for 50 samples (k=5)...\n",
            "[2026-01-08 16:52:31,034][oumi][rank0][pid:62985][MainThread][INFO]][repr_diversity_analyzer.py:556] Column 'text_content': 11/50 samples (22.0%) are redundant\n",
            "[2026-01-08 16:52:31,075][oumi][rank0][pid:62985][MainThread][INFO]][llm_judge_analyzer.py:780] Skipping message-level analysis (analyze_message_level=False). Set analyze_message_level=True to enable.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ryanarman/miniconda3/envs/oumi/lib/python3.11/site-packages/sklearn/base.py:1330: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/Users/ryanarman/miniconda3/envs/oumi/lib/python3.11/site-packages/sklearn/base.py:1330: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/Users/ryanarman/miniconda3/envs/oumi/lib/python3.11/site-packages/sklearn/base.py:1330: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total conversations analyzed: 50\n"
          ]
        }
      ],
      "source": [
        "# Run the analysis\n",
        "analyzer.analyze_dataset()\n",
        "\n",
        "# The results are stored in analyzer object\n",
        "if analyzer._analysis_results:\n",
        "    print(\n",
        "        f\"Total conversations analyzed: {analyzer._analysis_results.conversations_analyzed}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:33,494][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:1426] Saved message analysis to: /Users/ryanarman/code/oumi/analysis_output/openassistant/messages_df.parquet\n",
            "[2026-01-08 16:52:33,501][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:1432] Saved conversation analysis to: /Users/ryanarman/code/oumi/analysis_output/openassistant/conversations_df.parquet\n",
            "[2026-01-08 16:52:33,512][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:1438] Saved merged analysis to: /Users/ryanarman/code/oumi/analysis_output/openassistant/merged_df.parquet\n",
            "[2026-01-08 16:52:33,514][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:1450] Saved message schema to: /Users/ryanarman/code/oumi/analysis_output/openassistant/message_schema.json\n",
            "[2026-01-08 16:52:33,515][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:1457] Saved conversation schema to: /Users/ryanarman/code/oumi/analysis_output/openassistant/conversation_schema.json\n",
            "[2026-01-08 16:52:33,516][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:1463] Saved combined schemas to: /Users/ryanarman/code/oumi/analysis_output/openassistant/schema.json\n",
            "[2026-01-08 16:52:33,518][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:1470] Saved analysis summary to: /Users/ryanarman/code/oumi/analysis_output/openassistant/analysis_summary.json\n",
            "[2026-01-08 16:52:33,518][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:1472] All analyzer artifacts saved to: /Users/ryanarman/code/oumi/analysis_output/openassistant\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import traceback\n",
        "from oumi.utils.analysis_utils import save_analyzer_artifacts\n",
        "\n",
        "# Save all analyzer artifacts (dataframes, schemas, summary)\n",
        "save_analyzer_artifacts(analyzer, Path(config.output_path), output_format=\"parquet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load artifacts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:38,542][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:1551] Loaded message analysis from: /Users/ryanarman/code/oumi/analysis_output/openassistant/messages_df\n",
            "[2026-01-08 16:52:38,548][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:1558] Loaded conversation analysis from: /Users/ryanarman/code/oumi/analysis_output/openassistant/conversations_df\n",
            "[2026-01-08 16:52:38,559][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:1565] Loaded merged analysis from: /Users/ryanarman/code/oumi/analysis_output/openassistant/merged_df\n",
            "[2026-01-08 16:52:38,560][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:1574] Loaded combined schemas from: /Users/ryanarman/code/oumi/analysis_output/openassistant/schema.json\n",
            "[2026-01-08 16:52:38,562][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:1596] Loaded analysis summary from: /Users/ryanarman/code/oumi/analysis_output/openassistant/analysis_summary.json\n",
            "[2026-01-08 16:52:38,562][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:1598] Loaded analyzer artifacts from: /Users/ryanarman/code/oumi/analysis_output/openassistant\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ryanarman/miniconda3/envs/oumi/lib/python3.11/site-packages/sklearn/base.py:1330: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/Users/ryanarman/miniconda3/envs/oumi/lib/python3.11/site-packages/sklearn/base.py:1330: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/Users/ryanarman/miniconda3/envs/oumi/lib/python3.11/site-packages/sklearn/base.py:1330: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:38,648][oumi.utils.analysis_utils][rank0][pid:62985][MainThread][INFO]][analysis_utils.py:1654] Regenerated 10 recommendations from artifacts with latest code\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ryanarman/miniconda3/envs/oumi/lib/python3.11/site-packages/sklearn/base.py:1330: ConvergenceWarning:\n",
            "\n",
            "Number of distinct clusters (3) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
            "\n",
            "/Users/ryanarman/miniconda3/envs/oumi/lib/python3.11/site-packages/sklearn/base.py:1330: ConvergenceWarning:\n",
            "\n",
            "Number of distinct clusters (3) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
            "\n",
            "/Users/ryanarman/miniconda3/envs/oumi/lib/python3.11/site-packages/sklearn/base.py:1330: ConvergenceWarning:\n",
            "\n",
            "Number of distinct clusters (3) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-08 16:52:39,415][oumi][rank0][pid:62985][MainThread][INFO]][report_generator.py:290] Generated HTML report: /Users/ryanarman/code/oumi/analysis_output/openassistant/index.html\n",
            "[2026-01-08 16:52:39,416][oumi][rank0][pid:62985][MainThread][INFO]][report_generator.py:291] External data files written to: /Users/ryanarman/code/oumi/analysis_output/openassistant/data\n",
            "‚úÖ Generated HTML report at: /Users/ryanarman/code/oumi/analysis_output/openassistant/index.html\n",
            "\n",
            "üìÅ All results saved to: /Users/ryanarman/code/oumi/analysis_output/openassistant\n"
          ]
        }
      ],
      "source": [
        "# OUTPUT_PATH = \"/Users/ryanarman/code/oumi/analysis_output/openassistant\"\n",
        "OUTPUT_PATH = \"/Users/ryanarman/code/oumi/analysis_output/openassistant\"\n",
        "\n",
        "from oumi.utils.analysis_utils import (\n",
        "    load_analyzer_artifacts,\n",
        "    regenerate_recommendations,\n",
        ")\n",
        "\n",
        "artifacts = load_analyzer_artifacts(OUTPUT_PATH)\n",
        "\n",
        "# Regenerate recommendations with latest code (e.g., updated duplicate detection)\n",
        "artifacts = regenerate_recommendations(artifacts, outlier_threshold=3.0)\n",
        "\n",
        "artifacts.keys()\n",
        "\n",
        "# Generate HTML report if configured\n",
        "\n",
        "\n",
        "try:\n",
        "    from oumi.core.analyze.report_generator import HTMLReportGenerator\n",
        "\n",
        "    report_gen = HTMLReportGenerator()\n",
        "    report_path = report_gen.generate_report(\n",
        "        artifacts=artifacts,\n",
        "        output_path=OUTPUT_PATH,\n",
        "        title=\"OpenAssistant Analysis Report\",\n",
        "    )\n",
        "    print(f\"‚úÖ Generated HTML report at: {report_path / 'index.html'}\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  Plotly not installed. Skipping HTML report generation.\")\n",
        "    print(\"   Install with: pip install 'oumi[analyze_advanced]'\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Failed to generate HTML report: {e}\")\n",
        "    print(\"\\nüîç FULL TRACEBACK:\")\n",
        "    print(\"=\" * 70)\n",
        "    traceback.print_exc()\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüìÅ All results saved to: {OUTPUT_PATH}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "oumi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
