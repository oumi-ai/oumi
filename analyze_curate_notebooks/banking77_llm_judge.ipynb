{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470e1fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available preset prompts: ['instruction_quality', 'response_quality', 'conversation_coherence', 'safety', 'helpfulness', 'factuality']\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.analyze.llm_judge_analyzer import LLMJudgeAnalyzer\n",
    "\n",
    "print(\"Available preset prompts:\", LLMJudgeAnalyzer.list_presets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334e3fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS has been disabled - forcing CPU-only mode\n",
      "PyTorch device: cpu\n",
      "PyTorch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# IMPORTANT: Set these BEFORE importing torch or any ML libraries\n",
    "# Disable all GPU/MPS backends to prevent crashes with IFD analyzer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Disable CUDA\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Disable MPS memory allocation\n",
    "os.environ[\"DISABLE_MPS_COMPAT\"] = \"1\"  # Additional MPS disable flag\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"  # Disable HuggingFace telemetry\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"  # Allow model downloads\n",
    "\n",
    "# Force CPU usage in PyTorch to avoid MPS crashes\n",
    "import torch\n",
    "\n",
    "# Forcefully disable MPS before anything else\n",
    "torch.set_default_device(\"cpu\")\n",
    "if hasattr(torch.backends, \"mps\"):\n",
    "    # Monkey-patch to prevent MPS usage\n",
    "    original_is_available = torch.backends.mps.is_available\n",
    "    torch.backends.mps.is_available = lambda: False\n",
    "    print(\"MPS has been disabled - forcing CPU-only mode\")\n",
    "else:\n",
    "    print(\"Using CPU for all computations\")\n",
    "\n",
    "print(f\"PyTorch device: {torch.get_default_device()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2fd63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oumi.core.configs import AnalyzeConfig, SampleAnalyzerParams\n",
    "\n",
    "llm_instruction_quality_params = SampleAnalyzerParams(\n",
    "    id=\"llm_judge\",\n",
    "    params={\n",
    "        # Use a preset prompt (available: instruction_quality, response_quality,\n",
    "        # conversation_coherence, safety, helpfulness, factuality)\n",
    "        \"prompt_preset\": \"instruction_quality\",\n",
    "        # Inference configuration\n",
    "        \"inference_config\": {\n",
    "            \"model_name\": \"gpt-4o-mini\",  # or \"gpt-4o\", \"claude-3-5-sonnet-20241022\", etc.\n",
    "            \"engine\": \"openai\",  # or \"vllm\", \"native\" for local models\n",
    "            \"temperature\": 0.1,  # Low temperature for consistent judgments\n",
    "            \"max_tokens\": 256,\n",
    "            # For OpenAI, API key is read from OPENAI_API_KEY env var by default\n",
    "            # For Anthropic, use ANTHROPIC_API_KEY env var\n",
    "            # Or specify explicitly:\n",
    "            # \"api_key_env\": \"OPENAI_API_KEY\",  # or \"ANTHROPIC_API_KEY\"\n",
    "        },\n",
    "        \"batch_size\": 10,  # Process 10 samples at a time\n",
    "        \"max_text_length\": 4000,  # Truncate long texts\n",
    "        \"parse_json_response\": True,  # Parse JSON from LLM response\n",
    "    },\n",
    ")\n",
    "\n",
    "# Option 2: Using a custom prompt\n",
    "llm_judge_custom_params = SampleAnalyzerParams(\n",
    "    id=\"llm_judge\",\n",
    "    params={\n",
    "        # Custom prompt with {text} placeholder\n",
    "        \"prompt\": \"\"\"Evaluate this banking customer service response for quality (0-10).\n",
    "\n",
    "Consider:\n",
    "- Does it directly answer the customer's question?\n",
    "- Is it clear and professional?\n",
    "- Is it helpful and actionable?\n",
    "\n",
    "Response to evaluate:\n",
    "{text}\n",
    "\n",
    "Respond with JSON:\n",
    "- \"score\": 0-10\n",
    "- \"label\": \"excellent\", \"good\", \"needs_improvement\", or \"poor\"\n",
    "- \"reasoning\": brief explanation\n",
    "\n",
    "JSON response:\"\"\",\n",
    "        \"inference_config\": {\n",
    "            \"model_name\": \"gpt-4o-mini\",\n",
    "            \"engine\": \"openai\",\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 256,\n",
    "        },\n",
    "        \"batch_size\": 10,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f50f608d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 14:43:24,872][oumi][rank0][pid:55777][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'custom'\n",
      "[2025-12-30 14:43:24,873][oumi.utils.analysis_utils][rank0][pid:55777][MainThread][INFO]][analysis_utils.py:225] Loaded text dataset from: /Users/ryanarman/code/scratch/ryan_hillclimbing_experiments/banking77/notebooks/data/banking77_train.jsonl\n",
      "[2025-12-30 14:43:24,874][oumi][rank0][pid:55777][MainThread][INFO]][dataset_analyzer.py:154] Loaded dataset from config: None\n",
      "[2025-12-30 14:43:24,874][oumi][rank0][pid:55777][MainThread][INFO]][dataset_analyzer.py:304] Initialized sample analyzer: llm_judge\n",
      "[2025-12-30 14:43:24,875][oumi][rank0][pid:55777][MainThread][INFO]][dataset_analyzer.py:347] Starting analysis of dataset: None\n",
      "[2025-12-30 14:43:24,876][oumi][rank0][pid:55777][MainThread][INFO]][dataset_analyzer.py:348] Using 1 sample analyzers: ['llm_judge']\n",
      "[2025-12-30 14:43:24,876][oumi][rank0][pid:55777][MainThread][INFO]][dataset_analyzer.py:371] Analyzing 100 of 8002 conversations\n",
      "[2025-12-30 14:43:24,877][oumi][rank0][pid:55777][MainThread][INFO]][dataset_analyzer.py:421] Converting conversation dataset with 8002 items\n",
      "[2025-12-30 14:43:24,877][oumi][rank0][pid:55777][MainThread][INFO]][dataset_analyzer.py:428] Limiting analysis to first 100 items (dataset has 8002 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting Unknown Dataset to DataFrames: 100%|██████████| 100/100 [00:00<00:00, 4184.85item/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 14:43:24,941][oumi][rank0][pid:55777][MainThread][INFO]][llm_judge_analyzer.py:381] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.21s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-30 14:43:47,063][oumi][rank0][pid:55777][ThreadPoolExecutor-12_0][WARNING]][adaptive_concurrency_controller.py:237] Entering warmup state, but concurrency is already at maximum value. Consider raising the max concurrency.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.13s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.85s/it]\n",
      "100%|██████████| 10/10 [00:23<00:00,  2.37s/it]\n",
      "100%|██████████| 10/10 [00:20<00:00,  2.03s/it]\n",
      "100%|██████████| 10/10 [00:22<00:00,  2.21s/it]\n",
      "100%|██████████| 10/10 [00:19<00:00,  1.97s/it]\n",
      "100%|██████████| 10/10 [00:20<00:00,  2.09s/it]\n",
      "100%|██████████| 10/10 [00:19<00:00,  1.92s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.76s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.81s/it]\n",
      "100%|██████████| 7/7 [00:11<00:00,  1.59s/it]\n",
      "100%|██████████| 6/6 [00:08<00:00,  1.50s/it]\n",
      "100%|██████████| 6/6 [00:09<00:00,  1.63s/it]\n",
      "100%|██████████| 7/7 [00:12<00:00,  1.80s/it]\n",
      "100%|██████████| 7/7 [00:10<00:00,  1.55s/it]\n",
      "100%|██████████| 6/6 [00:09<00:00,  1.55s/it]\n",
      "100%|██████████| 6/6 [00:09<00:00,  1.64s/it]\n",
      "100%|██████████| 4/4 [00:07<00:00,  1.94s/it]\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.70s/it]\n",
      "100%|██████████| 7/7 [00:11<00:00,  1.69s/it]\n",
      "100%|██████████| 6/6 [00:09<00:00,  1.50s/it]\n",
      "100%|██████████| 5/5 [00:11<00:00,  2.31s/it]\n",
      "100%|██████████| 6/6 [00:10<00:00,  1.72s/it]\n",
      "100%|██████████| 4/4 [00:06<00:00,  1.67s/it]\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.61s/it]\n",
      "100%|██████████| 6/6 [00:09<00:00,  1.52s/it]\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.86s/it]\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.75s/it]\n",
      "100%|██████████| 5/5 [00:07<00:00,  1.56s/it]\n",
      "100%|██████████| 4/4 [00:06<00:00,  1.68s/it]\n",
      "100%|██████████| 5/5 [00:09<00:00,  1.87s/it]\n",
      "100%|██████████| 5/5 [00:09<00:00,  1.94s/it]\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.76s/it]\n",
      "100%|██████████| 6/6 [00:09<00:00,  1.57s/it]\n",
      "100%|██████████| 4/4 [00:06<00:00,  1.64s/it]\n",
      "100%|██████████| 4/4 [00:06<00:00,  1.64s/it]\n",
      "100%|██████████| 5/5 [00:07<00:00,  1.49s/it]\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.71s/it]\n",
      "100%|██████████| 4/4 [00:06<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete! Results saved to: ./analysis_output/banking77_llm_judge\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from oumi.core.configs import AnalyzeConfig\n",
    "from oumi.core.analyze.dataset_analyzer import DatasetAnalyzer\n",
    "\n",
    "# Create the full AnalyzeConfig\n",
    "config = AnalyzeConfig(\n",
    "    # Dataset configuration\n",
    "    dataset_path=\"/Users/ryanarman/code/scratch/ryan_hillclimbing_experiments/banking77/notebooks/data/banking77_train.jsonl\",\n",
    "    # Analysis configuration\n",
    "    sample_count=5,  # Analyze first 100 samples (LLM calls can be expensive!)\n",
    "    analyzers=[llm_instruction_quality_params],  # Just the LLM judge\n",
    "    # Output configuration\n",
    "    output_path=\"./analysis_output/banking77_llm_judge\",\n",
    "    generate_report=False,  # Optional: generate HTML report\n",
    ")\n",
    "\n",
    "# Finalize and run\n",
    "config.finalize_and_validate()\n",
    "\n",
    "from oumi.core.analyze.dataset_analyzer import DatasetAnalyzer\n",
    "\n",
    "analyzer = DatasetAnalyzer(config)\n",
    "analyzer.analyze_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a07ae1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(schema): 16\n",
      "len(merged_columns): 16\n"
     ]
    }
   ],
   "source": [
    "schema = analyzer.get_schema()\n",
    "print(f\"len(schema): {len(schema)}\")\n",
    "merged_columns = analyzer.analysis_df.columns\n",
    "print(f\"len(merged_columns): {len(merged_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0c2ff22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['llm_judge']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer_names = [a.id for a in config.analyzers]\n",
    "analyzer_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ee896",
   "metadata": {},
   "source": [
    "# Conv level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d56f05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer: llm_judge\n",
      "metric: score\n",
      "description: LLM judge score for the sample\n",
      "value: 8.0\n",
      "\n",
      "\n",
      "metric: label\n",
      "description: LLM judge label/category for the sample\n",
      "value: good\n",
      "\n",
      "\n",
      "metric: reasoning\n",
      "description: LLM judge reasoning/explanation\n",
      "value: The instruction is clear and specific, with a defined goal of classifying user queries into one of 77 banking intents. It uses action verbs and provides a comprehensive list of IDs with examples to help distinguish similar intents. However, it could benefit from slightly more context about the types of user queries expected, which would enhance clarity.\n",
      "\n",
      "\n",
      "metric: raw_response\n",
      "description: Raw LLM response before parsing\n",
      "value: ```json\n",
      "{\n",
      "  \"score\": 8,\n",
      "  \"label\": \"good\",\n",
      "  \"reasoning\": \"The instruction is clear and specific, with a defined goal of classifying user queries into one of 77 banking intents. It uses action verbs and provides a comprehensive list of IDs with examples to help distinguish similar intents. However, it could benefit from slightly more context about the types of user queries expected, which would enhance clarity.\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.analyze.column_utils import (\n",
    "    filter_analyzer_columns,\n",
    "    get_analyzer_columns_by_analyzer,\n",
    "    parse_analyzer_column_name,\n",
    ")\n",
    "\n",
    "conv_columns = analyzer.conversation_df.columns\n",
    "row = analyzer.conversation_df.iloc[0]\n",
    "\n",
    "# Choose the analzyer to analyze\n",
    "analyzer_name = analyzer_names[0]\n",
    "\n",
    "\n",
    "filtered_cols = filter_analyzer_columns(conv_columns, analyzer_id=analyzer_name)\n",
    "if filtered_cols:\n",
    "    print(f\"Analyzer: {analyzer_name}\")\n",
    "    info = parse_analyzer_column_name(filtered_cols[0])\n",
    "    # print(\"\\nInput:\")\n",
    "    # print(f\"source_column: {info.source_column}\")\n",
    "    # print(f\"{row[info.source_column]}\\n\")\n",
    "\n",
    "    for col in filtered_cols:\n",
    "        info = parse_analyzer_column_name(col)\n",
    "        print(f\"metric: {info.metric_name}\")\n",
    "        # print(f\"type: {schema[col]['type']}\")\n",
    "        # print(f\"content_type: {schema[col]['content_type']}\")\n",
    "        print(f\"description: {schema[col]['description']}\")\n",
    "        print(f\"value: {row[col]}\")\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(f\"No columns found for analyzer: {analyzer_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abfccb",
   "metadata": {},
   "source": [
    "# Message level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcb677ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer: llm_judge\n",
      "\n",
      "Input:\n",
      "[assistant]: 52\n",
      "\n",
      "52\n",
      "\n",
      "metric: score\n",
      "description: LLM judge score for the sample\n",
      "value: 1.0\n",
      "\n",
      "\n",
      "metric: label\n",
      "description: LLM judge label/category for the sample\n",
      "value: poor\n",
      "\n",
      "\n",
      "metric: reasoning\n",
      "description: LLM judge reasoning/explanation\n",
      "value: The instruction '52' lacks clarity, context, and specificity. It does not provide a clear goal or action verb, making it impossible to understand what is being requested.\n",
      "\n",
      "\n",
      "metric: raw_response\n",
      "description: Raw LLM response before parsing\n",
      "value: ```json\n",
      "{\n",
      "  \"score\": 1,\n",
      "  \"label\": \"poor\",\n",
      "  \"reasoning\": \"The instruction '52' lacks clarity, context, and specificity. It does not provide a clear goal or action verb, making it impossible to understand what is being requested.\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.analyze.column_utils import (\n",
    "    filter_analyzer_columns,\n",
    "    get_analyzer_columns_by_analyzer,\n",
    "    parse_analyzer_column_name,\n",
    ")\n",
    "\n",
    "msg_columns = analyzer.message_df.columns\n",
    "row = analyzer.message_df.iloc[2]\n",
    "\n",
    "# Choose the analzyer to analyze\n",
    "analyzer_name = analyzer_names[0]\n",
    "\n",
    "\n",
    "filtered_cols = filter_analyzer_columns(msg_columns, analyzer_id=analyzer_name)\n",
    "if filtered_cols:\n",
    "    print(f\"Analyzer: {analyzer_name}\")\n",
    "    info = parse_analyzer_column_name(filtered_cols[0])\n",
    "\n",
    "    print(\"\\nInput:\")\n",
    "    print(f\"[{row['role']}]: {row[info.source_column]}\\n\")\n",
    "    # print(f\"source_column: {info.source_column}\")\n",
    "    print(f\"{row[info.source_column]}\\n\")\n",
    "\n",
    "    for col in filtered_cols:\n",
    "        info = parse_analyzer_column_name(col)\n",
    "        print(f\"metric: {info.metric_name}\")\n",
    "        # print(f\"type: {schema[col]['type']}\")\n",
    "        # print(f\"content_type: {schema[col]['content_type']}\")\n",
    "        print(f\"description: {schema[col]['description']}\")\n",
    "        print(f\"value: {row[col]}\")\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(f\"No columns found for analyzer: {analyzer_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782aee00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
