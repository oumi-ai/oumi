{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Unknown analyzer: LengthAnalyzer\n"
          ]
        }
      ],
      "source": [
        "from oumi.analyze import generate_tests\n",
        "\n",
        "# Generate YAML test templates for an analyzer\n",
        "yaml_config = generate_tests(\"LengthAnalyzer\")\n",
        "print(yaml_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MPS has been disabled - forcing CPU-only mode\n",
            "PyTorch device: cpu\n",
            "PyTorch version: 2.6.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "# IMPORTANT: Set these BEFORE importing torch or any ML libraries\n",
        "# Disable all GPU/MPS backends to prevent crashes with IFD analyzer\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Disable CUDA\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Disable MPS memory allocation\n",
        "os.environ[\"DISABLE_MPS_COMPAT\"] = \"1\"  # Additional MPS disable flag\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"  # Disable HuggingFace telemetry\n",
        "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"  # Allow model downloads\n",
        "\n",
        "# Force CPU usage in PyTorch to avoid MPS crashes\n",
        "import torch\n",
        "\n",
        "# Forcefully disable MPS before anything else\n",
        "torch.set_default_device(\"cpu\")\n",
        "if hasattr(torch.backends, \"mps\"):\n",
        "    # Monkey-patch to prevent MPS usage\n",
        "    original_is_available = torch.backends.mps.is_available\n",
        "    torch.backends.mps.is_available = lambda: False\n",
        "    print(\"MPS has been disabled - forcing CPU-only mode\")\n",
        "else:\n",
        "    print(\"Using CPU for all computations\")\n",
        "\n",
        "print(f\"PyTorch device: {torch.get_default_device()}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Config loaded with 10 analyzers:\n",
            "  - length (type: length)\n",
            "  - token_stats (type: token_stats)\n",
            "  - cost (type: cost)\n",
            "  - fasttext (type: fasttext)\n",
            "  - embedding (type: embedding)\n",
            "  - question_diversity (type: question_diversity)\n",
            "  - repr_diversity (type: repr_diversity)\n",
            "  - adherence_quality (type: llm_judge)\n",
            "  - instruction_quality (type: llm_judge)\n",
            "  - response_quality (type: llm_judge)\n",
            "üìÅ Output will be saved to: /Users/ryanarman/data/ada\n",
            "‚úÖ Config validated successfully!\n",
            "[2026-01-12 17:06:57,673][oumi][rank0][pid:10118][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'custom'\n",
            "[2026-01-12 17:06:57,673][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:227] Loaded text dataset from: /Users/ryanarman/data/Ada/train.jsonl\n",
            "[2026-01-12 17:06:57,674][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:153] Loaded dataset from config: None\n",
            "[2026-01-12 17:06:57,808][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: length\n",
            "[2026-01-12 17:06:57,809][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: token_stats\n",
            "[2026-01-12 17:06:57,809][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: cost\n",
            "[2026-01-12 17:06:57,809][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: fasttext\n",
            "[2026-01-12 17:06:57,830][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: embedding\n",
            "[2026-01-12 17:06:57,831][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: question_diversity\n",
            "[2026-01-12 17:06:57,831][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: repr_diversity\n",
            "[2026-01-12 17:06:57,831][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: adherence_quality (type: llm_judge)\n",
            "[2026-01-12 17:06:57,831][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: instruction_quality (type: llm_judge)\n",
            "[2026-01-12 17:06:57,832][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:319] Initialized sample analyzer: response_quality (type: llm_judge)\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_PATH = \"/Users/ryanarman/data/ada\"\n",
        "\n",
        "import os\n",
        "from oumi.core.configs import AnalyzeConfig\n",
        "from oumi.core.analyze.dataset_analyzer import DatasetAnalyzer\n",
        "\n",
        "# Load config from YAML file\n",
        "config = AnalyzeConfig.from_yaml(\n",
        "    \"/Users/ryanarman/code/oumi/configs/examples/analyze/analyze_ada.yaml\"\n",
        ")\n",
        "\n",
        "# Override settings for this run\n",
        "dataset_path = \"/Users/ryanarman/data/Ada/train.jsonl\"\n",
        "config.dataset_path = dataset_path\n",
        "config.dataset_name = None  # Clear dataset_name so it uses dataset_path instead\n",
        "config.sample_count = 1000  # Adjust as needed\n",
        "config.chat_template = \"chat_ml\"\n",
        "\n",
        "# Set absolute output path (makes it easier to find the results!)\n",
        "config.output_path = OUTPUT_PATH\n",
        "\n",
        "print(f\"‚úÖ Config loaded with {len(config.analyzers)} analyzers:\")\n",
        "for analyzer in config.analyzers:\n",
        "    instance_id = analyzer.instance_id or analyzer.id\n",
        "    print(f\"  - {instance_id} (type: {analyzer.id})\")\n",
        "\n",
        "print(f\"üìÅ Output will be saved to: {config.output_path}\")\n",
        "\n",
        "# Validate the configuration\n",
        "config.finalize_and_validate()\n",
        "print(\"‚úÖ Config validated successfully!\")\n",
        "\n",
        "analyzer = DatasetAnalyzer(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 17:06:57,839][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:362] Starting analysis of dataset: None\n",
            "[2026-01-12 17:06:57,839][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:363] Using 10 sample analyzers: ['length', 'token_stats', 'cost', 'fasttext', 'embedding', 'question_diversity', 'repr_diversity', 'adherence_quality', 'instruction_quality', 'response_quality']\n",
            "[2026-01-12 17:06:57,840][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:386] Analyzing 1000 of 4300 conversations\n",
            "[2026-01-12 17:06:57,840][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:438] Converting conversation dataset with 4300 items\n",
            "[2026-01-12 17:06:57,840][oumi][rank0][pid:10118][MainThread][INFO]][dataset_analyzer.py:445] Limiting analysis to first 1000 items (dataset has 4300 total)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting Unknown Dataset to DataFrames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01<00:00, 656.60item/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 17:06:59,429][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1337] Adding default schema entries for 10 columns not in base schema: ['adherence_label', 'classification_json_clean', 'conversation_and_action_clean', 'conversation_length', 'failure_type', 'output_format', 'playbook', 'tool_complexity', 'user_coherence', 'user_input']\n",
            "[2026-01-12 17:07:00,475][oumi][rank0][pid:10118][MainThread][INFO]][fasttext_analyzer.py:220] Initialized fast-langdetect for language detection\n",
            "[2026-01-12 17:07:00,476][oumi][rank0][pid:10118][MainThread][INFO]][fasttext_analyzer.py:458] Analyzing language for column: conversation_text_content\n",
            "[2026-01-12 17:07:01,177][oumi][rank0][pid:10118][MainThread][INFO]][embedding_analyzer.py:518] Computing embeddings for 1000 samples...\n",
            "[2026-01-12 17:07:01,178][oumi][rank0][pid:10118][MainThread][INFO]][embedding_analyzer.py:196] Loading embedding model: all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:09<00:00, 100.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 17:07:12,203][oumi][rank0][pid:10118][MainThread][INFO]][embedding_analyzer.py:537] Detecting semantic duplicates...\n",
            "[2026-01-12 17:07:12,368][oumi][rank0][pid:10118][MainThread][INFO]][embedding_analyzer.py:624] Detecting fuzzy duplicates using MinHash LSH...\n",
            "[2026-01-12 17:07:12,373][oumi][rank0][pid:10118][MainThread][INFO]][embedding_analyzer.py:350] Creating MinHash signatures for 1000 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating MinHash signatures: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:08<00:00, 118.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 17:07:20,857][oumi][rank0][pid:10118][MainThread][INFO]][embedding_analyzer.py:369] Finding fuzzy duplicates using LSH...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Finding duplicates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 84074.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 17:07:20,873][oumi][rank0][pid:10118][MainThread][INFO]][question_diversity_analyzer.py:464] Computing embeddings for 1000 user questions...\n",
            "[2026-01-12 17:07:20,874][oumi][rank0][pid:10118][MainThread][INFO]][question_diversity_analyzer.py:174] Loading embedding model: all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:08<00:00, 118.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 17:07:30,253][oumi][rank0][pid:10118][MainThread][INFO]][question_diversity_analyzer.py:469] Clustering 1000 questions using dbscan...\n",
            "[2026-01-12 17:07:30,546][oumi][rank0][pid:10118][MainThread][INFO]][question_diversity_analyzer.py:487] Found 1 clusters\n",
            "[2026-01-12 17:07:30,559][oumi][rank0][pid:10118][MainThread][INFO]][repr_diversity_analyzer.py:363] Computing diversity scores for 1000 samples in column 'conversation_text_content'...\n",
            "[2026-01-12 17:07:30,561][oumi][rank0][pid:10118][MainThread][INFO]][repr_diversity_analyzer.py:165] Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:08<00:00, 117.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 17:07:40,067][oumi][rank0][pid:10118][MainThread][INFO]][repr_diversity_analyzer.py:230] Computing nearest neighbor distances for 1000 samples (k=5)...\n",
            "[2026-01-12 17:07:40,084][oumi][rank0][pid:10118][MainThread][INFO]][repr_diversity_analyzer.py:556] Column 'conversation_text_content': 1000/1000 samples (100.0%) are redundant\n",
            "[2026-01-12 17:07:40,126][oumi][rank0][pid:10118][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n",
            "[2026-01-12 17:07:40,138][oumi][rank0][pid:10118][MainThread][WARNING]][dataframe_analyzer.py:183] Analyzer adherence_quality failed: '\"reasoning\"'\n",
            "[2026-01-12 17:07:40,140][oumi][rank0][pid:10118][MainThread][INFO]][llm_judge_analyzer.py:787] Skipping conversation-level analysis (analyze_conversation_level=False). Set analyze_conversation_level=True to enable.\n",
            "[2026-01-12 17:07:40,148][oumi][rank0][pid:10118][MainThread][INFO]][llm_judge_analyzer.py:787] Skipping conversation-level analysis (analyze_conversation_level=False). Set analyze_conversation_level=True to enable.\n",
            "[2026-01-12 17:07:41,419][oumi][rank0][pid:10118][MainThread][INFO]][fasttext_analyzer.py:458] Analyzing language for column: text_content\n",
            "[2026-01-12 17:07:42,260][oumi][rank0][pid:10118][MainThread][INFO]][embedding_analyzer.py:518] Computing embeddings for 3000 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:20<00:00, 149.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 17:08:02,394][oumi][rank0][pid:10118][MainThread][INFO]][embedding_analyzer.py:537] Detecting semantic duplicates...\n",
            "[2026-01-12 17:08:02,723][oumi][rank0][pid:10118][MainThread][INFO]][embedding_analyzer.py:624] Detecting fuzzy duplicates using MinHash LSH...\n",
            "[2026-01-12 17:08:02,724][oumi][rank0][pid:10118][MainThread][INFO]][embedding_analyzer.py:350] Creating MinHash signatures for 3000 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating MinHash signatures: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:11<00:00, 268.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 17:08:13,940][oumi][rank0][pid:10118][MainThread][INFO]][embedding_analyzer.py:369] Finding fuzzy duplicates using LSH...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Finding duplicates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:01<00:00, 2791.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 17:08:15,026][oumi][rank0][pid:10118][MainThread][INFO]][question_diversity_analyzer.py:464] Computing embeddings for 1000 user questions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:08<00:00, 120.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 17:08:23,363][oumi][rank0][pid:10118][MainThread][INFO]][question_diversity_analyzer.py:469] Clustering 1000 questions using dbscan...\n",
            "[2026-01-12 17:08:23,375][oumi][rank0][pid:10118][MainThread][INFO]][question_diversity_analyzer.py:487] Found 2 clusters\n",
            "[2026-01-12 17:08:23,379][oumi][rank0][pid:10118][MainThread][INFO]][repr_diversity_analyzer.py:363] Computing diversity scores for 3000 samples in column 'text_content'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:19<00:00, 152.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 17:08:43,099][oumi][rank0][pid:10118][MainThread][INFO]][repr_diversity_analyzer.py:230] Computing nearest neighbor distances for 3000 samples (k=5)...\n",
            "[2026-01-12 17:08:43,300][oumi][rank0][pid:10118][MainThread][INFO]][repr_diversity_analyzer.py:556] Column 'text_content': 1963/3000 samples (65.4%) are redundant\n",
            "[2026-01-12 17:08:43,318][oumi][rank0][pid:10118][MainThread][INFO]][llm_judge_analyzer.py:780] Skipping message-level analysis (analyze_message_level=False). Set analyze_message_level=True to enable.\n",
            "[2026-01-12 17:08:43,327][oumi][rank0][pid:10118][MainThread][INFO]][llm_judge_analyzer.py:828] Evaluating 1000 'system' messages (filtered from 3000 total)\n",
            "[2026-01-12 17:08:43,345][oumi][rank0][pid:10118][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n",
            "[2026-01-12 17:08:43,361][oumi][rank0][pid:10118][MainThread][INFO]][llm_judge_analyzer.py:679] Batch of 1000: 1 unique to evaluate, 999 duplicates, 0 from cache\n",
            "[2026-01-12 17:08:45,300][oumi][rank0][pid:10118][MainThread][INFO]][llm_judge_analyzer.py:828] Evaluating 1000 'assistant' messages (filtered from 3000 total)\n",
            "[2026-01-12 17:08:45,305][oumi][rank0][pid:10118][MainThread][INFO]][llm_judge_analyzer.py:444] Initialized LLM Judge with model: gpt-4o-mini, engine: openai\n",
            "[2026-01-12 17:08:45,306][oumi][rank0][pid:10118][MainThread][WARNING]][dataframe_analyzer.py:183] Analyzer response_quality failed: '\"reasoning\"'\n",
            "Total conversations analyzed: 1000\n"
          ]
        }
      ],
      "source": [
        "# Run the analysis\n",
        "analyzer.analyze_dataset()\n",
        "\n",
        "# The results are stored in analyzer object\n",
        "if analyzer._analysis_results:\n",
        "    print(\n",
        "        f\"Total conversations analyzed: {analyzer._analysis_results.conversations_analyzed}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 17:08:45,757][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1426] Saved message analysis to: /Users/ryanarman/data/ada/messages_df.parquet\n",
            "[2026-01-12 17:08:45,803][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1432] Saved conversation analysis to: /Users/ryanarman/data/ada/conversations_df.parquet\n",
            "[2026-01-12 17:08:45,898][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1438] Saved merged analysis to: /Users/ryanarman/data/ada/merged_df.parquet\n",
            "[2026-01-12 17:08:45,899][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1450] Saved message schema to: /Users/ryanarman/data/ada/message_schema.json\n",
            "[2026-01-12 17:08:45,900][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1457] Saved conversation schema to: /Users/ryanarman/data/ada/conversation_schema.json\n",
            "[2026-01-12 17:08:45,902][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1463] Saved combined schemas to: /Users/ryanarman/data/ada/schema.json\n",
            "[2026-01-12 17:08:45,904][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1470] Saved analysis summary to: /Users/ryanarman/data/ada/analysis_summary.json\n",
            "[2026-01-12 17:08:45,904][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1472] All analyzer artifacts saved to: /Users/ryanarman/data/ada\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import traceback\n",
        "from oumi.utils.analysis_utils import save_analyzer_artifacts\n",
        "\n",
        "# Save all analyzer artifacts (dataframes, schemas, summary)\n",
        "save_analyzer_artifacts(analyzer, Path(config.output_path), output_format=\"parquet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load artifacts and generate report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-01-12 17:08:45,925][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1551] Loaded message analysis from: /Users/ryanarman/data/ada/messages_df\n",
            "[2026-01-12 17:08:45,973][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1558] Loaded conversation analysis from: /Users/ryanarman/data/ada/conversations_df\n",
            "[2026-01-12 17:08:46,015][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1565] Loaded merged analysis from: /Users/ryanarman/data/ada/merged_df\n",
            "[2026-01-12 17:08:46,016][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1574] Loaded combined schemas from: /Users/ryanarman/data/ada/schema.json\n",
            "[2026-01-12 17:08:46,017][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1596] Loaded analysis summary from: /Users/ryanarman/data/ada/analysis_summary.json\n",
            "[2026-01-12 17:08:46,018][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1598] Loaded analyzer artifacts from: /Users/ryanarman/data/ada\n",
            "[2026-01-12 17:08:46,373][oumi.utils.analysis_utils][rank0][pid:10118][MainThread][INFO]][analysis_utils.py:1773] Regenerated 2 recommendations from artifacts with latest code\n",
            "[2026-01-12 17:08:47,185][oumi][rank0][pid:10118][MainThread][INFO]][report_generator.py:290] Generated HTML report: /Users/ryanarman/data/ada/index.html\n",
            "[2026-01-12 17:08:47,186][oumi][rank0][pid:10118][MainThread][INFO]][report_generator.py:291] External data files written to: /Users/ryanarman/data/ada/data\n",
            "‚úÖ Generated HTML report at: /Users/ryanarman/data/ada/index.html\n",
            "\n",
            "üìÅ All results saved to: /Users/ryanarman/data/ada\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_PATH = \"/Users/ryanarman/data/ada\"\n",
        "from oumi.utils.analysis_utils import (\n",
        "    load_analyzer_artifacts,\n",
        "    regenerate_recommendations,\n",
        ")\n",
        "\n",
        "artifacts = load_analyzer_artifacts(OUTPUT_PATH)\n",
        "\n",
        "# Regenerate recommendations with latest code (e.g., updated duplicate detection)\n",
        "artifacts = regenerate_recommendations(artifacts, outlier_threshold=3.0)\n",
        "\n",
        "artifacts.keys()\n",
        "\n",
        "\n",
        "# Generate HTML report if configured\n",
        "\n",
        "\n",
        "try:\n",
        "    from oumi.core.analyze.report_generator import HTMLReportGenerator\n",
        "\n",
        "    report_gen = HTMLReportGenerator()\n",
        "    report_path = report_gen.generate_report(\n",
        "        artifacts=artifacts,\n",
        "        output_path=OUTPUT_PATH,\n",
        "        title=\"Ada Customer Service Adherence Analysis Report\",\n",
        "    )\n",
        "    print(f\"‚úÖ Generated HTML report at: {report_path / 'index.html'}\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  Plotly not installed. Skipping HTML report generation.\")\n",
        "    print(\"   Install with: pip install 'oumi[analyze_advanced]'\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Failed to generate HTML report: {e}\")\n",
        "    print(\"\\nüîç FULL TRACEBACK:\")\n",
        "    print(\"=\" * 70)\n",
        "    traceback.print_exc()\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüìÅ All results saved to: {OUTPUT_PATH}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "oumi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
